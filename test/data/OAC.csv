"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1940696","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Wei Ren","KY","University of Kentucky Research Foundation","Standard Grant","Amy Walton","09/30/2022","$240,032.00","","wei.ren@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","099Y, 7231","062Z, 7231, 9150","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2028495","IRNC: Core Improvement: Network for European, American, African, and Arctic Research (NEA3R)","OAC","International Res Ret Connect","10/01/2020","10/14/2020","Jennifer Schopf","IN","Indiana University","Continuing Grant","Kevin Thompson","09/30/2025","$2,902,625.00","Edward Moynihan, Thomas Fryer, Rene Buch, Matthews Mtumbuka","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7369","","$0.00","The Networks for European, American, African, and Arctic Research (NEA3R) collaboration is a cross-organizational project that will expand and enhance network services and capacity to support US research and education. NEA3R offers a collaborative suite of high-speed network connectivity offerings with additional targeted science support and measurement capabilities. The International Networks at Indiana University (IN@IU) team will jointly lead the NEA3R collaboration with an extensive set of partners and collaborators, including GÉANT, the European regional research and education network (REN), NORDUnet, the Nordic regional REN, and the east African REN, the UbuntuNet Alliance (UA). We will coordinate domestically with Internet2 and the Energy Sciences Network (ESnet). <br/><br/>The NEA3R project will provide two high-speed US-Europe circuits to provide reliable production services with 24x7 NOC support. These circuits will be part of the Advanced North Atlantic (ANA) collaboration and will enable dependable, redundant capacity required by US researchers. These links will support advanced networking technologies, as well measurement and monitoring using NetSage. The NEA3R project will support a system of logical circuits, referred to as Sister Circuits, that are provided by partners including GÉANT?s ANA link between New York and Paris, two NORDUnet links with endpoints in Amsterdam, one to Svalbard, Norway, and the other to Kajaani, Finland, and the UbuntuNet alliance link between London and Cape Town, South Africa. The project will also assist large scale physics, astronomy, and weather satellite applications, and establish reliable paths to advance the use of network test beds, such as our partner FABRIC.<br/><br/>NEA3R, through its resources and activities, has the potential to significantly enhance NSF-funded international science collaborations. Research partners include astronomy, high energy physics, and Earth observations, all of which can increase the economic competitiveness of the US. This project can enable faster time to scientific results and access to a broader set of resources and data. Through our extensive partnerships, NEA3R will provide enhanced infrastructure for research and education and provide US researchers and educators with access to a broad array of geographical, ethnic, and cultural resources. This increased access to educational materials and public engagement will, in turn, assist in the development of a diverse, globally competitive STEM workforce.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029235","Collaborative Research: IRNC Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","09/01/2020","08/24/2021","James Griffioen","KY","University of Kentucky Research Foundation","Continuing Grant","Kevin Thompson","08/31/2023","$224,994.00","Kenneth Calvert, Zongming Fei","griff@netlab.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7369","9150","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931298","Collaborative Research: Framework: Machine Learning Materials Innovation Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/09/2019","Dane Morgan","WI","University of Wisconsin-Madison","Standard Grant","Seung-Jong Park","09/30/2023","$1,580,643.00","Michael Ferris, Paul Voyles, Ryan Jacobs","ddmorgan@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7925, 8004, 9216, 9263","$0.00","Machine learning is rapidly changing our society, with computers recently gaining skills in many new tasks. These tasks range from understanding language to driving cars. Materials science and engineering is also being transformed. Many tasks are becoming increasingly accessible to machine learning algorithms. These range from predicting new data to analyzing images. Many basic machine learning algorithms are readily available. However the overall workflow involved in the application of machine learning for materials problems is still largely executed by hand. Getting results out is still done by traditional methods like publishing articles. There is an enormous opportunity to accelerate the growth and impact of machine learning in materials research. This requires improved cyberinfrastructure. This project will develop an approach to accelerate the entire machine learning workflow. Its output will include tools to easily develop datasets, manage model development, and output models. These will be reusable and reproducible for future use. This project will enable materials scientists and engineers to rapidly develop and deploy machine learning models. More importantly, the entire materials community will be able to quickly access these models. It will transform how we discover and develop advanced materials.<br/><br/>The project will have three major technical components: (i) A MAterials Simulation Toolkit for Machine Learning (MAST-ML) with workflow tools that will enable local or cloud-based multistep, automated execution of complex machine learning data analysis and model training, codified best practices, increased access to machine learning methods for non-experts, and accelerated model development; (ii) The Foundry Materials Informatics Environment that will provide flexible, integrated, cloud-based management of machine learning materials science and engineering projects, from organizing data to developing models to disseminating results that are machine and human accessible and reproducible in ways that support a networked materials innovation ecosystem, (iii) Representative science applications of machine learning materials science and engineering projects that will support infrastructure development and promotion, as well as demonstrate best practices on state-of-the-art materials science and engineering problems. In addition to its impact on materials science and engineering, this project will develop students and young researchers with the interdisciplinary skills of machine learning and materials science and engineering, and promote these new ideas to the broader materials community. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925687","CC* Team:  KyRC - A Kentucky Research Computing Team","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/15/2019","10/15/2020","Brian Nichols","KY","University of Kentucky Research Foundation","Continuing Grant","Kevin Thompson","06/30/2022","$1,399,638.00","Doyle Friskney, James Griffioen","bnichols@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7231, 8080","9150","$0.00","High performance computing centers now serve a broad set of user interests with researchers from all disciplines now leveraging computation and/or big data in interesting and novel ways. This change is not confined to top tier universities, but rather impacts researchers at all institutions of higher learning.  In short, the need to support researchers who span all academic areas and require a diverse set of cyberinfrastructure (CI) has become a key challenge for research centers and IT organizations nationally. Today, scientific discovery is enabled through compute-intensive and data-intensive research that would not be possible without advanced CI.  This project forms a collaborative, state-wide support team, the Kentucky Research Computing team (KyRC), that provides researchers with direct access to expert CI engineers to assist in applying, consulting, and supporting a wide-range of computational systems and research disciplines.  <br/><br/>The goal of KyRC is to serve a broad range of institutions across the state of Kentucky in higher education, smart cities, and community education and training efforts. KyRC provides support to a variety of emerging research areas in need of CI support that heretofore were not part of the computational community. In addition to state-wide support, KyRC collaborates with leading regional and national CI entities to facilitate a community of expertise. By providing access, education, and exposure to advanced CI, KyRC enables research programs to recruit more students (e.g., groups underrepresented in STEM) to computational research while enhancing the training of undergraduates, graduate students, and postdocs in Kentucky higher education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029283","IRNC: Core Improvement: Americas-Africa Lightpaths Express and Protect (AmLight-ExP)","OAC","International Res Ret Connect","12/15/2020","05/11/2021","Julio Ibarra","FL","Florida International University","Continuing Grant","Kevin Thompson","11/30/2025","$3,099,366.00","Heidi L. Morgan, Luis Lopez, Jeronimo Bezerra, Donald Cox","julio@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7369","","$0.00","Big data science in South America and Sub-Saharan Africa will dramatically evolve over the next five years, with increasing dependency on advanced cyberinfrastructure and programmable networking.  Significant projects include The Vera Rubin Observatory, the High Luminosity Large Hadron Collider experiments and the Square Kilometer Array in S. Africa. AmLight-ExP supports high-performance network connectivity required by international science and engineering research and education collaborations involving the NSF research community, with expansion to South America and West Africa.<br/><br/>AmLight-ExP is a reliable, leading-edge infrastructure for research and education. With significant investments from the Academic Network of São Paulo, Brazil?s Research and Education network, the Association of Universities for Research in Astronomy, the regional network of Latin America (RedCLARA), and national R&E network of S. Africa (TENET/SANReN), the total bandwidth provided by AmLight ExP between the U.S. South America, and Africa is expected to grow to over 4 Tbps in aggregate capacity from 2020-2025. This flexible inter-regional infrastructure enables science communities to expand research and learning activities, empowered through access to scalable optical spectrum on submarine cables and programmable networks. <br/><br/>AmLight-ExP increases the rate of discovery. Faster discovery means quicker focus on the greatest benefit for society. AmLight-ExP is a catalyst for new communities of researchers and learners with a bridge, linking U.S. Hispanic and African students, teachers and researchers. AmLight-ExP is committed to serving the needs of graduate and undergraduate education through models that bring together students and the networking community with scientists from all domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029260","Collaborative Research: IRNC: Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","09/01/2020","09/13/2021","Kuang-Ching Wang","SC","Clemson University","Continuing Grant","Kevin Thompson","08/31/2023","$317,209.00","Benjamin Kirtman, Richard Brooks","kwang@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","7369","9150","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940223","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Mila Sherman","MA","University of Massachusetts Amherst","Standard Grant","Amy Walton","09/30/2022","$260,925.00","","msherman@isenberg.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931561","Collaborative Research: Frameworks: Machine learning and FPGA computing for real-time applications in big-data physics experiments","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","10/01/2019","01/07/2022","Eliu Huerta Escudero","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","09/30/2022","$651,314.00","Daniel Katz, Eliu Huerta Escudero, Volodymyr Kindratenko","elihu@uchicago.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1253, 7244, 8004","026Z, 077Z, 7569, 7925, 8004","$0.00","The cyberinfrastructure needs for gravitational wave astrophysics, high energy physics, and large-scale electromagnetic surveys have rapidly evolved in recent years. The construction and upgrade of the facilities used to enable scientific discovery in these disparate fields of research have led to a common pair of computational grand challenges: (i) datasets with ever-increasing complexity and volume; and (ii) data mining analyses that must be performed in real-time with oversubscribed computational resources. Furthermore, the convergence of gravitational wave astrophysics with electromagnetic and astroparticle surveys, the very birth of Multi-Messenger Astrophysics, has already provided a glimpse of the transformational discoveries that it will enable in years to come. Given the unique potential for scientific discovery with the Large Hadron Collider (LHC) and the combination of the Laser Interferometer Gravitational-wave Observatory (LIGO) and the Large Synoptic Survey Telescope (LSST) for Multi-Messenger Astrophysics, the community needs to accelerate the development and exploitation of deep learning algorithms that will outperform existing approaches. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. It will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. Because these methods are also applicable to many other parts of our national and global economy and society, this work will positively impact many fields. The students and junior scientists to be mentored and trained in this research will interact closely with our industry partners, creating new career opportunities, and strengthening synergies between academia and industry. The team will share the algorithms with the community through open source software repositories, and through our tutorials and workshops the team will train the community regarding software credit and software citation.<br/><br/>In this project, the PIs will build upon our recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets, as open source software. This work combines scalable deep learning algorithms, trained with TB-size datasets within minutes using thousands of GPUs/CPUs, with state-of-the-art approaches to endow the predictions of deterministic deep learning models with complete posterior distributions. The team will also investigate the use of Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms to minimize the demands of future computing, which is a central goal for Multi-Messenger Astrophysics and particle physics. The open source tools to be developed as part of these activities will be readily shared with and adopted by LIGO, LHC, and LSST as core data analytics algorithms that will significantly increase the speed and depth of existing algorithms, enabling new physics while requiring minimal computational resources for real-time inferences analyses. The team will organize deep learning workshops and bootcamps to train students and researchers on how to use and contribute to our framework, creating a wide network of contributors and developers across key science missions. The team will leverage existing open source and interactive model repositories, such as the Data and Learning Hub for Science (DLHub) at Argonne, to reach out to a large cross-section of communities that analyze open datasets from LIGO, LHC, and LSST, and that will benefit from the use of these technologies that require minimal computational resources for inference tasks.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835817","Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure, Software Institutes","01/01/2019","05/19/2021","Yao Liang","IN","Indiana University","Standard Grant","Seung-Jong Park","12/31/2022","$530,249.00","Sudhakar Pamidighantam, Fengguang Song","yliang@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7726, 8004","062Z, 077Z, 7925, 8004, 9251","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835256","Element: Software: Data-Driven Auto-Adaptive Classification of Cryospheric Signatures as Informants for Ice-Dynamic Models","OAC","Polar Cyberinfrastructure, CESER-Cyberinfrastructure for, EarthCube","01/01/2019","09/06/2018","Ute Herzfeld","CO","University of Colorado at Boulder","Standard Grant","Seung-Jong Park","12/31/2022","$593,624.00","","uch5678@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","5407, 7684, 8074","026Z, 062Z, 072Z, 077Z, 1079, 7923, 8004","$0.00","The objective of this project is to develop and automatize a connection between Earth observation data and numerical models of Earth system processes. Both collection of Earth observation data from satellites and modeling of physical processes have seen unprecedented advances in recent years. However, data-derived information is not used to inform modeling in a systematic and automated fashion. This creates a bottleneck that is growing with the data revolution. The award supports the development of a software cyberinfrastructure aimed at reducing this bottleneck by automating classification and parameterization. The proposed cyberinfrastructure will be implemented in a general and transportable way, but its functionality will be demonstrated by addressing a concrete open problem in glaciology: the acceleration during a glacier surge, which is characterized by an increase to 100-200 times the flow normal velocity. Glacial accelerations are important, because they constitute the largest uncertainty in sea-level-rise assessment. The team, from University of Colorado will combine their expertise of field work and data collection with their background in software development to generate a high-quality application that will be made available under an open source license to the broader scientific community. The project will engage graduate and undergraduate students in the software development, thus contributing to the development of future generations of scientists and cyberinfrastructure professionals.  The results will also be used to inform activities in K-12 schools and other outreach efforts.<br/><br/>The proposed data-driven auto-adaptive classification system is expected to provide a tool to the Earth Sciences community that allows it to employ the unprecedented detail in satellite image and SAR data (WordView, Sentinel-1 and Sentinel-2) necessary to extract information on surface properties and processes that were previously indiscernible. In that the classification will automatically adapt to changing conditions in time and space, it will provide a consistent parameterization of spatial processes that can be used to drive numerical simulations of Earth system processes. Thus, a direct connection will be established between data analysis and modeling. In a pilot study, the data-modeling connection will be demonstrated through classification of crevasse patterns, which result from deformation, and optimization of the basal sliding parameter in a three-dimensional model of a glacier surge. Hence the pilot study will advance understanding of ice dynamics. A second application is a sea-ice classification system, aimed to aid in mapping and understanding the changing Arctic sea-ice cover. The planned automated connection between data-driven automated classification and optimization of model parameters is expected to lay the foundation for a transformation of the data-modeling world in Earth sciences, atmospheric and polar sciences. The project will also advance machine learning and spatial statistics through realization of a science-driven approach to computer science and cyberinfrastructure.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure (OAC) is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, The OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the OAC.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835821","Elements: Software: NSCI: A high performance suite of SVD related solvers for machine learning","OAC","Data Cyberinfrastructure","01/01/2019","09/06/2018","Andreas Stathopoulos","VA","College of William and Mary","Standard Grant","Robert Beverly","12/31/2022","$600,000.00","Zhenming Liu","andreas@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7726","026Z, 077Z, 7923, 8004","$0.00","The accrual of vast amounts of data is one of the defining characteristics of our century. With the help of computers, scientists use this data to make and test hypotheses, draw inferences, predict complex phenomena, and make educated policy decisions. Machine learning (ML) is an area in computer science that uses statistical methods to allow computers to ""learn"" from data, with and without human supervision. Central to the application of machine learning methods is the numerical computation of the Singular Value Decomposition (SVD) of matrices of very large dimension, often larger than a million or even a billion. Since ""off-the-shelf"" algorithms and SVD software, however, cannot handle matrices of very large dimension, iterative methods used in scientific computing are more appropriate. Yet their stringent approximation quality requirements are often excessive for downstream applications, and result in slow execution times. Recently, methods based on randomization have improved execution times, but their implementations relax the approximation quality, often to detrimental levels. This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. <br/><br/>This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. Specifically, the software package builds upon the state-of-the-art eigenvalue/singular value software package PRIMME that integrates cutting-edge iterative methods and high-performance implementations. The development of the package consists of two thrusts: (T1) Unifying state-of-the-art algorithmic techniques including randomized, streaming, and iterative methods, to deliver consistent experience for a diverse range of matrices with different quality requirements, hardware platforms and precisions, and programming environments. (T2) Developing software devices that enable downstream systems and SVD solvers to interoperate so that users can tune and customize solvers without being experts in numeric linear algebra.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827184","CC* Networking Infrastructure: Integrating Big Data Instrumentation into Campus Cyberinfrastructure","OAC","Campus Cyberinfrastructure","07/01/2018","06/29/2018","Xiaohui Carol Song","IN","Purdue University","Standard Grant","Kevin Thompson","06/30/2022","$323,327.00","Robin Tanamachi, Wen Jiang, Preston Smith","cxsong@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","8080","","$0.00","Rapid advancements in data-intensive scientific instrumentation have greatly outpaced the capability of campus networking infrastructures to effectively connect big data-producing facilities to powerful computing and storage systems. This delays the analysis, use and dissemination of a huge amount of valuable scientific data for discovery and innovation in multiple science and engineering domains. Purdue's project bridges this gap by improving the campus high-speed network infrastructure between five big data instrument facilities and the centrally supported cyberinfrastructure (CI) consisting of supercomputers, large storage systems and network connections to outside the campus. The facilities support accelerated research in new materials, understanding brain functions and viruses, monitoring lower atmospheric weather, employing geospatial data in teaching and public engagement, and secure computing. The project builds upon a strong partnership between campus information technology experts and domain scientists. It also helps prepare the next generation of CI professionals by pairing student workers with the campus CI experts.<br/><br/>The project adds high-speed Science DMZ connections to the five selected big data facilities enabling high-volume, high-velocity, or interactive science data flows to both the campus research cyberinfrastructure and off campus facilities. Existing Cisco Nexus 7710 network switches at the distribution layer are augmented with new 40-Gigabit networking modules, increasing bandwidth to 20-80 Gigabits per second. This should yield a significant increase in research productivity, and more timely publication and dissemination of research data and results, through faster data transfer, easier access to the central computing infrastructure, and more effectively utilizing Purdue's 100-Gigabit WAN connections and research network peerings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835379","Elements: Bringing Montage To Cutting Edge Science Environments","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","03/15/2019","03/11/2019","Graham Berriman","CA","California Institute of Technology","Standard Grant","Seung-Jong Park","02/28/2023","$598,369.00","","gbb@ipac.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","Astronomy is undergoing a transformation in the way data are acquired, and this is driving a corresponding transformation in the way astronomers process these data.  Telescopes and sky surveys that are operating now or will begin to operate in the coming years will deliver data that are too large and complex to analyze by the traditional method of downloading data to desktops and local clusters.  Thus a transformation is underway to use new technologies to process data. Astronomers have embraced the Python language for analysis because it provides the necessary flexible building blocks to handle complex data, and are embracing new Python-based technologies to manage and control processing that allows the software to run next to the data themselves. The Montage image mosaic engine, a toolkit already used widely by astronomers, will join this transformative community and deliver high-performance, next generation image processing capabilities for astronomers and computer scientists.  It will allow astronomers to create large-scale images of the sky, and study these images with the many powerful tools available in Python.<br/><br/>Python has become the language of choice for astronomy, and environments such as JupyterLabs and JupyterHub are almost certainly the science environments of the future. The LSST is committed to using such an environment for its science platform, which will be the primary way LSST users will discover, access and analyze data. Astronomy science archives are actively building similar platforms. NOAO has deployed their DataLab, which supports datasets acquired at Kitt Peak and CTIO. We will incorporate the functionality of the Montage image mosaic engine - a scalable toolkit written in ANSI-C and in wide use in astronomy and information technology - into environments such as these to unleash its full power when applied to large and complex new datasets. Moreover, the same functionality can be incorporated into a single desktop platform, or into a new scalable environment built to support a new project or mission, or into a distributed scalable environment such the Amazon Elastic Cloud (""bringing the code to the data"").  As a component-based toolkit, Montage will be well positioned to respond to the rapid changes expected as these new platforms develop and contribute substantially to understanding their performance and usefulness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835791","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","EarthCube","01/01/2019","02/25/2021","David Mencin","CO","UNAVCO, Inc.","Standard Grant","Amy Walton","12/31/2022","$830,728.00","David Mencin, Scott Baker, Kathleen Hodgkinson","mencin@unavco.org","6350 Nautilus Dr.","Boulder","CO","803015394","3033817500","CSE","8074","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838807","EAGER: Managing our expectations: quantifying and characterizing misleading trajectories in ecological processes","OAC","NSF Public Access Initiative","10/01/2018","08/09/2018","Christine Bahlai","OH","Kent State University","Standard Grant","Martin Halbert","09/30/2021","$175,624.00","Kaitlin Whitney","cbahlai@kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","CSE","7414","7916","$0.00","A fundamental problem in ecology is understanding how to scale discoveries: from patterns observed in the lab or the plot to the field or the region, or bridging between short term observations to long term trends and trajectories. The PIs propose a method to directly address the temporal aspects of scaling ecological observations, which involves reusing data from the two dozen Long Term Ecological Research (LTER) sites, an NSF program in place since the early 1980s.  The PIs intend to bridge the gap between short-term observations and the long-term trends using an automated approach of repeatedly sampling moving windows of data from existing long-term time series, and analyzing these sampled data as if they represented the entire dataset.  By compiling typical statistics used to describe the relationship in the sampled data and through repeated samplings, the results will provide insights to the questions, how often are the trends observed in short term data misleading, and can we use characteristics of these trends to predict our likelihood of being misled?  The experiences in reusing the LTER data will be captured and shared with the ecology and open science community.<br/> <br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835690","Elements: Software: Autonomous, Robust, and Optimal In-Silico Experimental Design Platform for Accelerating Innovations in Materials Discovery","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Byung-Jun Yoon","TX","Texas A&M Engineering Experiment Station","Standard Grant","Bogdan Mihaila","09/30/2022","$600,000.00","Raymundo Arroyave, Xiaoning Qian, Xiaofeng Qian","bjyoon@ece.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Accelerating the development of novel materials that have desirable properties is a critical challenge as it can facilitate advances in diverse fields across science, engineering, and medicine with significant contributions to economic growth. For example, the US Materials Genome Initiative calls for cutting the time for bringing new materials from discovery to deployment by half at a fraction of the cost, by integrating experiments, computer simulations, and data analytics. However, the current prevailing practice in materials discovery relies on trial-and-error experimental campaigns and/or high-throughput screening approaches, which cannot efficiently explore the huge design space to develop materials with the targeted properties. Furthermore, measurements of material composition, structure, and properties often contain considerable errors due to technical limitations in materials synthesis and characterization, making this exploration even more challenging. This project aims to develop a software platform for robust autonomous materials discovery that can shift the current trial-and-error practice to an informatics-driven one that can potentially expedite the discovery of novel materials at substantially reduced cost and time. Throughout the project, the PI and Co-PIs will mentor students and equip them with the skills necessary to tackle interdisciplinary problems that involve materials science, computing, optimization, and artificial intelligence. Research findings in the project will be incorporated into the courses taught by the PI and Co-PIs, thereby enriching the learning experience of students.<br/><br/>The objective of this project is to develop an effective in-silico experimental design platform to accelerate the discovery of novel materials. The platform will be built on optimal Bayesian learning and experimental design methodologies that can translate scientific principles in materials, physics, and chemistry into predictive models, in a way that takes model and data uncertainty into account. The optimal Bayesian experimental design framework will enable the collection of smart data that can help exploring the material design space efficiently, without relying on slow and costly trial-and-error and/or high-throughput screening approaches. The developed methodologies will be integrated into MSGalaxy, a modular scientific workflow management system, resulting in an accessible, reproducible, and transparent computational platform for accelerated materials discovery that allows easy and flexible customization as well as synergistic contributions from researchers across different disciplines.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2028501","IRNC: Core Improvement: TransPAC5: Multilateral Partnerships to Accelerate International Research and Experimentation","OAC","International Res Ret Connect","10/01/2020","10/14/2020","Jennifer Schopf","IN","Indiana University","Continuing Grant","Kevin Thompson","09/30/2025","$3,793,624.00","Bu Lee, Hans Addleman","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7369","","$0.00","TransPAC5: Multilateral Partnerships to Accelerate International Research and Experimentation builds on more than twenty years of collaboration in the Asia-Pacific region to provide network capacity and services in support of US-Asia science collaborations. TransPAC5 includes multilateral agreements and expanded networking partnerships with regional teams and NRENs associated with 33 countries. Relationships will continue with the Asia Pacific Advanced Network (APAN), the Trans-Eurasia Information Network (TEIN), and the Asia-Pacific Ring (APR) collaboration. TransPAC5 will augment both the available APR capacity and the APR membership with partners the Korea Research Environment Open Network (KREONET), the KOrea advanced REsearch Network (KOREN), the University of Hawaii, Australia's Academic and Research Network (AARNet), and European Nordic REN, NORDUnet.<br/><br/>TransPAC5 will advance knowledge and understanding across a broad set of international research collaborations through its support of both network capacity and services. The project doubles the current transoceanic high-speed offerings of the APR and leverages the unused capacity to formally offer pragmatic networking resources to the advanced network experiment community, enabling transformative approaches to be tested in production settings, not just test beds. The project supports large scale international R&E collaborations including  high energy physics, astronomy, and bioinformatics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017597","Collaborative Research: CyberTraining: Pilot: Interdisciplinary Training of Data-Centric Security and Resilience of Cyber-Physical Energy Infrastructures","OAC","CyberTraining - Training-based, Secure &Trustworthy Cyberspace","09/01/2020","06/24/2020","Yufei Tang","FL","Florida Atlantic University","Standard Grant","Joseph Whitmeyer","08/31/2022","$160,000.00","James VanZwieten, Jason Hallstrom, Xingquan Zhu","tangy@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","044Y, 8060","","$0.00","In this project, students and researchers are provided with mentored, hands-on training combining expertise across electrical engineering, communication, science and technology studies, and data science.  This establishes a novel model for energy cyberinfrastructure resilience education. The curriculum and instructional materials that are developed integrate advanced skills from multiple areas under the umbrella of cyber-physical energy systems. Participants develop and refine the multi-disciplinary skillsets needed for the data-centric energy industry using unique, remotely connected smart grid cyberinfrastructure. Participants extend their academic research portfolios, strengthening their career competitiveness as future cyberinfrastructure professionals and users. The two-week workshop immerses undergraduate/graduate students and research scientists in a unique training opportunity through laboratory demonstrations and mini projects. <br/><br/>Three main technical challenges are tackled in the project: (i) Establishment of a new remotely connected cyberinfrastructure platform among the collaborating universities. An existing hardware-in-the-loop power testbed using a real-time digital simulator is connected with a virtual network laboratory to characterize cyber-physical energy systems. The combined infrastructure is equipped with state-of-the-art hardware and software modules, where humans, machines, and power girds can interact and cooperate in a near-to-real learning environment. (ii) Implementation of a cybersecurity module in the virtual lab to simulate cyber threats, such as denial-of-service attacks and man-in-the-middle attacks. Participants create attack scenarios that are played out, where the attacking process and consequence can be visualized on the physical system. (iii) Development of data analytics techniques based on machine learning as a set of cyber defense mechanisms, such as data-driven adversarial event detection. Participants execute their cyber defense algorithms alongside the attack, and the effectiveness of these defenses is validated and visualized using the testbed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118155","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","10/01/2021","09/02/2021","Sapna Sarupria","SC","Clemson University","Standard Grant","Alan Sussman","11/30/2021","$105,000.00","","sarupria@umn.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","044Y","7231, 7301, 9102, 9150","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107230","Collaborative Research: OAC Core: Advancing Low-Power Computer Vision at the Edge","OAC","OAC-Advanced Cyberinfrast Core","07/01/2021","07/01/2021","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Seung-Jong Park","06/30/2024","$250,000.00","James Davis","yunglu@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","090Y","075Z, 7923","$0.00","This proposal enables low-power edge computers, such as mobile phones, drones, and Internet-of-Things devices, to benefit society. Computer vision is the technology to automatically analyze images and videos. Computer vision on these devices can keep humans safe, for example by spotting dangers in a factory or at a construction site. This project addresses two challenges that hamper practical adoption of computer vision on edge devices. The first challenge is that current computer vision approaches require powerful computers, but these computers are too far away and have long response time. This project brings the computers to the places where data is acquired. The project makes computer vision more efficient, so that visual data can be analyzed by small edge devices like phones and drones. The second challenge is that building complex software for computer vision is difficult. This project provides software engineering support for emerging computer vision technologies. As a result of addressing these two challenges, computer vision on the edge can become feasible.<br/><br/>Bringing computer vision (CV) to devices on the network edge is an essential component of realizing NSF's goal of distributed cyberinfrastructure. This project makes CV on the edge feasible and enables scientific and engineering innovation through improved response time, reduced need for network coverage, and decreased storage costs. This project solves two critical challenges that hinder the transition of edge-based CV into practice. (1) This project makes CV more efficient and edge-friendly. Current CV techniques (e.g., deep neural networks) assume server-class resources (such as graphics processing units, gigabytes of memory); these resources are not available at the edge. This project reduces the resource requirements needed for CV. The methods consider alternative neural network architectures and eliminate redundancies while processing visual data. This project also develops CV-specific distribution techniques to enable edge devices to collaborate on large vision tasks. (2) This project provides software engineering support for CV technologies. Solving real-world CV problems requires engineering new CV applications, often by re-implementing research model architectures as components in new designs. This project develops a library of exemplary CV model implementations for low-power platforms. These exemplars can be used as high-quality components in new CV applications. The project identifies factors that promote and inhibit the reproducibility of CV models. This project also identifies engineering best practices by surveying and interviewing experts in low-power CV and by studying their errors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039677","EAGER: Improving the Quality and Reducing the Burden of Producing and Reusing Publicly Accessible Research Data","OAC","NSF Public Access Initiative","09/01/2020","01/07/2022","Sarah Nusser","IA","Iowa State University","Standard Grant","Martin Halbert","08/31/2022","$300,000.00","Sarah Nusser, Alyssa Mikytuck, Alyssa Mikytuck, Gizem Korkmaz","nusser@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7414","7916, 9150","$0.00","Reusability of research data is central to successfully maximizing the benefits of public access to publicly funded research outputs.  The purpose of this EAGER proposal is to improve understanding of factors that lead to reusability of research data.  It proposes to do this by 1) developing a framework for understanding data reusability, 2) identify concomitant practices for improving data reusability, and 3) propose a path for accelerating the data reusability readiness of the research community.  The project would accomplish this through targeted research and feedback sought through a group meeting.  <br/><br/>Research teams at Iowa State University and the University of Virginia will pursue this goal by first gathering descriptive information on targeted research communities with different levels of maturity in their practices of research data sharing and reuse, as well as trusted repositories that have adopted practices for evaluating data sources prior to data ingest and reuse.  This descriptive information will inform a draft framework articulating common elements of data reuse, and processes for depositing data in ways that promote reusability.  The framework will also examine incentives and barriers to data reusability.  Feedback from a community meeting will inform subsequent refinement of the framework, which will be used in small scale tests using publicly accessible data sets and research groups at ISU and UVA.  The final version of the framework would include a discussion of strategies for accelerating the sharing of research data and the corresponding culture changes needed for success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826967","CC* NPEO: Toward the National Research Platform","OAC","Campus Cyberinfrastructure","10/01/2018","05/07/2021","Larry Smarr","CA","University of California-San Diego","Standard Grant","Kevin Thompson","09/30/2022","$2,532,000.00","Frank Wuerthwein, Tajana Rosing, Ilkay Altintas, Philip Papadopoulos","lsmarr@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8080","9251","$0.00","Academic researchers need a simple data sharing architecture with end-to-end 10-to-100Gbps performance to enable virtual co-location of large amounts of data with computing. End-to-end is a difficult problem to solve in general because the networks between ends (campuses, data repositories, etc.) typically traverse multiple network management domains: campus, regional, and national.  No one organization owns the responsibility for providing scientists with high-bandwidth disk-to-disk performance. Toward the National Research Platform (TNRP), addresses issues critical to scaling end-to-end data sharing. TNRP will instrument a large federation of heterogeneous ""national-regional-state"" networks (NRSNs) to greatly improve end-to-end network performance across the nation. <br/><br/>The goal of improving end-to-end network performance across the nation requires active participation of these distributed intermediate-level entities to reach out to their campuses. They are trusted conveners of their member institutions, contributing effectively to the ""people networking"" that is as necessary to the development of a full National Research Platform as is the stability, deployment, and performance of technology. TNRP's collaborating NRSNs structure leads to engagement of a large set of science applications, identified by the participating NRSNs and the Open Science Grid. <br/><br/>TNRP is highly instrumented to directly measure performance. Visualizations of disk-to-disk performance with passive and active network monitoring show intra- and inter-NSRN end-to-end performance. Internet2, critical for interconnecting regional networks, will provide an instrumented dedicated virtual network instance for the interconnection of TNRP's NRSNs. Cybersecurity is a continuing concern; evaluations of advanced containerized orchestration, hardware crypto engines, and novel IPv6 strategies are part of the TNRP plan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2036534","Building the research innovation workforce: a workshop to identify new insights and directions to advance the research computing community.","OAC","EDUCATION AND WORKFORCE","07/15/2020","09/25/2020","Thomas Hacker","IN","Purdue University","Standard Grant","Alan Sussman","06/30/2022","$163,960.00","Lisa Arafune, Preston Smith, Dana Brunson","tjhacker@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7361","026Z, 7556","$0.00","The research enterprise requires a skilled research innovation workforce, consisting of highly trained and skilled practitioners in areas such as advanced cyberinfrastructure (CI), research software engineering, operations, and user support. A recent report from the National Science and Technology Council highlighted the need for this workforce. This project will conduct a series of virtual workshop sessions among a variety of affinity groups that includes academics and practioners to discuss and explore the problems and challenges affecting the growth and vitality of the research innovation workforce, and to discuss and identify potential strategies and solutions to help address these challenges. The workshop will be conducted entirely virtually with the aid of a skilled workshop facilitator. The outcomes of the proposed workshop will be a set of recommendations to the National Science Foundation related to CI professional workforce development; a better understanding of the challenges and problems affecting research innovation gleaned from discussions within and across affinity groups; and increased awareness of potentially cross-cutting strategies and solutions to encourage the growth and health of the national research innovation workforce.<br/><br/>The workshop seeks to facilitate a discussion among the affinity groups who collectively participate in the research enterprise, specifically targeting issues related to building and enhancing the CI professional workforce (e.g. research software engineers, research data science professionals, advanced cyberinfrastructure systems professionals, etc.) with the goal of identifying problem areas, discussing and identifying potential solutions, and developing a set of recommendations for consideration by the national research community and the NSF. The workshop co-chairs and steering committee represent a broad group of stakeholders. The primary goal of the proposed workshop is to explore the challenges and potential strategies and solutions to address the need for an advanced CI research innovation workforce that is essential to advancing the frontiers of computational and data-intensive scientific research. The workshop will result in a report with recommendations for fostering growth and career paths for a diverse, inclusive, and emerging set of professionals that collaborates across traditional boundaries and enables research and education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835631","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Robert Beverly","01/31/2022","$500,000.00","","gcfexchange@gmail.com","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835574","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, Data Cyberinfrastructure","01/01/2019","08/27/2018","Gregory Newman","CO","Colorado State University","Standard Grant","Amy Walton","12/31/2022","$192,940.00","","Gregory.Newman@ColoState.Edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835473","Elements: Data:  Integrating Human and Machine for Post-Disaster Visual Data Analytics:  A Modern Media-Oriented Approach","OAC","Data Cyberinfrastructure","01/01/2019","08/18/2018","Shirley Dyke","IN","Purdue University","Standard Grant","Amy Walton","12/31/2022","$597,955.00","Bedrich Benes, Thomas Hacker","sdyke@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7726","062Z, 077Z, 7923","$0.00","This project creates a science-oriented visual data service that facilitates the query of datasets based on visual content. The approach allows a user to search for data based on visual similarity, even in cases where a term for the failure or observation does not yet have a scientific name.  The visual analysis data and application services will be deployed on a cloud-based platform.  The results will produce a framework enabling access to and analysis of a large amount of imagery from diverse sources.  <br/><br/>The research team creates VISER (Visual Structural Expertise Replicator), which will serve as a comprehensive cloud-based data analytics service and will facilitate the use of and integrate data and applications most needed by the user.  The framework will implement two novel concepts: data-as-a-service and applications-as-a-service, which will bring data and applications to the user without the need to configure software systems or packages.  The approach also employs artificial intelligence to interpret the contents of the images.  VISER will use convolutional neural networks (CNNs) to train custom classifiers for new categories.  Three applications will be developed and deployed within VISER:  App1 will extract relevant visual context, App2 will facilitate similarity-based visual searching (through the use of a Siamese CNN), and App3 will help perform automatic extraction of pre-event/pre-disaster images based on Google Street View.  The application of these tools would advance both the science of automated pattern recognition and of more effective construction techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835822","Framework: Data: HDR: Extensible Geospatial Data Framework towards FAIR (Findable, Accessible, Interoperable, Reusable) Science","OAC","Data Cyberinfrastructure","10/01/2018","08/09/2018","Xiaohui Carol Song","IN","Purdue University","Standard Grant","Amy Walton","09/30/2023","$4,571,811.00","Jian Jin, Uris Lantz Baldos, Venkatesh Merwade, Jack Smith","cxsong@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7726","062Z, 077Z, 7925","$0.00","This project provides seamless connections among platforms, data and tools, making large scientific and social geospatial datasets directly usable in scientific models and tools.  Users with little or no programming experience will be able to create and build data pipelines that collect and process data at multiple scales and convert such data into usable results.  Four case studies demonstrate the capability of the data framework: flood hazard prediction, plant phenotyping, water quality monitoring and sustainable development.<br/><br/>The project creates an extensible geospatial data framework (GeoEDF) to address prevalent geospatial data challenges, support domain science needs, and contribute to a national geospatial data and software ecosystem. Specific objectives include: <br/> - development of a plug-and-play data framework (GeoEDF),<br/> - use of the framework to address domain science needs, <br/> - development of interoperability with other cyberinfrastructures, and <br/> - dissemination of GeoEDF to the broader community. <br/>Use of modular plug-and-play application programming interfaces (APIs) would enable integration of domain-specific geospatial data in a meaningful and accessible manner, leveraging an existing cyberinfrastructure capability (HUBzero) to facilitate adoption and dissemination without reinventing existing components. The project engages a substantial number of domain scientists from a variety of stakeholder communities; by incorporating the framework into HUBzero-powered sites, the team anticipates having access to more than 750,000 users.  Training will be provided to the next-generation of researchers and professionals; internship programs are planned for undergraduate and underrepresented groups. The project allows users / scientists to connect a range of data sources, potentially increasing interdisciplinary work.  The ultimate goal is to put easy-to-use tools and platforms into the hands of researchers and students to conduct scientific investigations using findable, accessible, interoperable, and reusable (FAIR) science principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103967","Collaborative Research: Elements: SciMem: Enabling High Performance Multi-Scale Simulation on Big Memory Platforms","OAC","Software Institutes","06/01/2021","05/05/2021","Zhen Li","SC","Clemson University","Standard Grant","Tevfik Kosar","05/31/2024","$140,000.00","","zli7@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","8004","077Z, 7923, 8004, 9150","$0.00","Increasing system scalability is crucial to improving nation?s computation capabilities for scientific applications. However, some applications often face the scalability challenge from the perspective of memory capacity. This is especially true in multi-scale simulations when handling massive simulation data from different scales. The emerging big memory infrastructures have shown great potential to increase the simulation scale and solve larger numerical problems. However, using big memory architectures for the multi-scale simulation is challenging, because of limited computing capability in the big memory machines and memory heterogeneity introduced by big memory. There is a lack of a software infrastructure that can release the full power of big memory to accelerate multi-scale simulation. This project aims to create a capability and a software package (named SciMem) that enables high performance multi-scale simulation on big memory platforms. The techniques presented offer a path for general use of this structure for a wide variety of applications having a broad impact on science and engineering. There will be impact on the students through their direct involvement with the project and through the integration with the educational activities.<br/><br/>The project will enable high performance multi-scale simulations on big memory platforms through more efficient utilization of large and heterogeneous memory machines. Specifically, it will replace computations with pre-computed and stored in memory data on a heterogeneous computing systems. The developed tool, SciMem, will be integrated and tested with the popular parallel molecular dynamics simulator, LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator). The developed improvements in the use of computational resources will allow more accurate models of complex physical phenomena to be carried out on the emerging hardware systems. SciMem aims to bring a 10x performance improvement for certain larger-scale multi-scale simulations widely applied in the fields of computational chemistry and material science, e.g., quantum mechanical/molecular mechanical-based molecular dynamics (MD) simulation of catalysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104709","CDSE: Collaborative: Cyber Infrastructure to Enable Computer Vision Applications at the Edge Using Automated Contextual Analysis","OAC","CDS&E","09/01/2021","08/31/2021","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Tevfik Kosar","08/31/2024","$269,999.00","","yunglu@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","8084","026Z, 1504, 8084","$0.00","Digital cameras are deployed as network edge devices, gathering visual data for such tasks as autonomous driving, traffic analysis, and wildlife observation. Analyzing the vast amount of visual data is a challenge. Existing computer vision methods require fast computers that are beyond the computational capabilities of many edge devices. This project aims to improve the efficiency of computer vision methods so that they can run on battery-powered edge devices. Based on the visual data and complementary metadata (e.g., geographical location, local time), the project first extracts contextual information (such as a city street is expected to be busy at rush hour). The contextual information can help assist determine whether analysis results are correct. For example, a wild animal is not expected on a city street. Moreover, contextual information can improve efficiency.  Only certain pixels need to be analyzed (pixels on the road are useful for detecting cars, while pixels in the sky are not) and this can significantly reduce the amount of computation, thus enabling analysis on edge devices. This project constructs a cyberinfrastructure for three services: (1) understand contextual information to reduce the search space of analysis methods, (2) reduce computation by considering only necessary pixels, and (3) automate evaluation of analysis results based on the contextual information without human effort.<br/><br/>Understanding contextual information is achieved by using background segmentation, GPS-location-dependent logic, and image depth maps.  Background analysis leverages semantic segmentation and analysis over time to identify the background pixels and then generate inference rules via a background-implies-foreground relationship. If a pixel is consistently marked by the same semantic label across a long period of time, this pixel is classified as a background pixel. The background information can infer certain types of foreground objects. For example, if the background is city streets, the foreground objects can be vehicles or pedestrians; if a bison is detected, this is likely a mistake. This project processes only the foreground pixels by adding masks to the neural network layers. Masking convolution can substantially reduce the amount of computation with no loss of accuracy and no additional training is needed. Meanwhile, hierarchical neural networks can skip sections of a model based on context. For example, pixels in the sky only need to be processed by the hierarchy nodes that classify airplanes. The project provides an online service that can accept input data and analysis programs for automatic evaluation of the programs, without human created labels. The evaluation is based on the correlations of background and foreground objects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018617","MRI: Acquisition of High-Performance Computing Cluster for Research and Workforce Development at University of Cincinnati","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE, Data Cyberinfrastructure, Campus Cyberinfrastructure","08/01/2020","07/22/2020","Prashant Khare","OH","University of Cincinnati Main Campus","Standard Grant","Alejandro Suarez","07/31/2023","$600,000.00","Thomas Beck, Adam Aurisano, Kelly Cohen, Gowtham Atluri","prashant.khare@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1189, 7231, 7726, 8080","1189, 7726","$0.00","This project will the support the acquisition of a high-performance computing (HPC) and high-speed storage system at the University of Cincinnati (UC).  This instrument will meet the computing needs of UC researchers and scientists spanning a broad range of fields including, physics, chemistry, aerospace & mechanical engineering, computer science, bioinformatics, medicine, and digital humanities.  The research projects enabled by the HPC system in these areas will advance fundamental science and contribute to national priorities in health and national security.  This system will also serve as a launching facility for the development of novel algorithms, and initial and scalability testing of codes, which will then be ported to national supercomputing facilities.  Over 117 faculty and students will immediately benefit from this equipment.  In addition to scientific advancements and development of the next generation of HPC-enabled workforce, this project will provide opportunities for the training of HPC administrators and facilitators.  Through participation in the Open Science Grid, the equipment will also enable the research activities of external users.  This project will involve the participation of underrepresented groups through summer internships, and workshops at local inner-city schools.  Further, this project will offer the possibility of exposing researchers in humanities to the benefits of HPC through collaboration between the Advanced Research Computing and the Digital Scholarship Centers at UC.<br/><br/>The computing cluster will consist of Central Processing Units (CPU), Graphics Processing Units (GPU) and high-speed scratch storage to substantially enhance the capabilities of the Advanced Research Computing Center at UC and thereby support computational and data science researchers by providing HPC resources for large-scale scientific calculations, HTC for Monte Carlo type simulations, and GPUs for advanced analytics, artificial intelligence, and visualization, all of which will enable sophisticated and increasingly realistic modeling, simulation and data analysis.  These research efforts based on molecular dynamics and quantum chemistry simulations, computational fluid dynamics, multi-universe stochastic models, and data-intensive, deep learning and graph convolutional neural networks, will advance our knowledge of physical & cyber-physical systems, chemical and mechanical behaviors of materials, the structure and origin of our universe and contribute to the development of innovative applications such as brain-inspired computing, effective personalized precision medicine for complex diseases, carbon-neutral combustion devices, hypersonic propulsion systems, multi-UAV autonomous systems, and smart cities. These objectives closely align with one of UC?s enterprise-level programs, the Digital Futures initiative.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2028419","Travel Support: Student Program for Practice and Experience in Advanced Research Computing Conference (PEARC20)","OAC","EDUCATION AND WORKFORCE","07/01/2020","06/30/2020","Elizabett Hillery","IN","Purdue University","Standard Grant","Alan Sussman","06/30/2022","$10,000.00","Gwen Jacobs, Preston Smith","eahillery@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7361","026Z, 075Z, 7556, 9179","$0.00","The goal of the Practice and Experience in Advanced Research Computing (PEARC) Conference series is to provide a forum for discussing challenges, opportunities, and solutions among High Performance Computing (HPC) center directors and managers, computational scientists, end users, students, facilitators, educators, HPC system administrators and user support staff, as well as industry and government agency representatives from across the United States and around the world. The conference follows the successful five-year conference series that was hosted by the eXtreme Science and Engineering Discovery Environment (XSEDE) program. Building on the success of XSEDE conference series, PEARC aims to broaden the community by including additional campus, national, and international cyberinfrastructure and research computing partners. The conference will be held online from July 26-30, 2020.  The project will fund students to participate in the conference, especially in the student program activities, and also fund a small number of junior faculty and researchers to participate in the conference.  Some students are presenting papers and posters at the conference, will be given priority for funding  The project serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Due to Covid-19 travel restrictions, the conference will be fully virtual. The student program committee for the conference is recruiting students with an emphasis on diversity and inclusion of underrepresented groups and from a diverse set of institutions.  For the student program, students will participate in (1) a student mentor program, which pairs a student with a senior mentor to help guide the student through conference activities, (2) a student volunteer program, to help run and organize the virtual conference, (3), a speed networking session, where students can meet 1-on-1 with conference exhibitors, and (4) a panel discussion, where students will attend a panel conversation about careers in HPC and career paths into HPC. The funding provided by NSF will have a significant impact on the careers of the future generation of researchers in high performance computing, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027524","Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations.","OAC","COVID-19 Research, CYBERINFRASTRUCTURE","05/15/2020","05/19/2021","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Seung-Jong Park","04/30/2022","$57,999.00","Wei Zakharov, David Barbarash, David Ebert","yunglu@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","158Y, 7231","077Z, 096Z, 7914, 8004, 9251","$0.00","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.<br/><br/>We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of  COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2023755","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","01/20/2020","03/16/2020","Lan Wang","FL","University of Miami","Standard Grant","Amy Walton","09/30/2022","$254,633.00","","lanwang@mbs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940190","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Chaopeng Shen","PA","Pennsylvania State Univ University Park","Standard Grant","Amy Walton","09/30/2022","$254,964.00","","cxs1024@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835785","Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","XC-Crosscutting Activities Pro, Data Cyberinfrastructure, Software Institutes, EarthCube","01/01/2019","05/19/2021","Xu Liang","PA","University of Pittsburgh","Standard Grant","Seung-Jong Park","12/31/2022","$453,232.00","","xuliang@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","7222, 7726, 8004, 8074","062Z, 077Z, 7925, 8004, 9251","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835739","Elements: Data: U-Cube: A Cyberinfrastructure for Unified and Ubiquitous Urban Canopy Parameterization","OAC","Special Initiatives, Data Cyberinfrastructure, LARS SPECIAL PROGRAMS, EarthCube","01/01/2019","08/18/2018","Daniel Aliaga","IN","Purdue University","Standard Grant","Amy Walton","12/31/2022","$599,999.00","Rajesh Kalyanam, Dev Niyogi","aliaga@cs.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1642, 7726, 7790, 8074","062Z, 077Z, 1525, 4444, 7923","$0.00","Urban canopy parameters (UCPs) can be used in model simulations to study the health and behavior of a city, determine the ability to sustain a growing population, and study potential impacts of extreme weather events.  The ability to identify and compute urban canopy parameters has been a missing element in city models; this project develops that capability for use in city design and analysis, integrating weather models and remote sensing data to infer a 3D model of cities of various sizes.  The project deploys innovative science-based analysis tools within an extensible, broadly-available cyberinfrastructure portal, allowing users to ingest satellite imagery and other geographic information system (GIS) data to calculate urban canopy parameters.  The cyberinfrastructure would improve urban modeling and planning, particularly for extreme weather events.  The tools and high-performance computing and storage resources would be usable by other researchers through a portal.  Potential beneficiaries include smaller and disadvantaged cities and countries without the resources for urban characterization and modeling necessary for such urban planning.  There are also plans to transfer the results of this research to communities beyond college students -- to local teachers and secondary students and museums, and to the GIS urban planning user communities at local, state, and international levels.<br/><br/>The project develops cyberinfrastructure which would use a novel inverse modeling approach incorporating satellite images, social science and urban zonal data, to infer a 3D model of a city from which urban canopy parameters could be derived for use in simulation models.   The focus is on weather modeling, urban parameterization and a desire to better understand sustainable urbanization.  The main cyberinfrastructure products will be 3D urban models and UCP values for urban locations.  These UCP parameters will be used for fine-scale urban weather modeling, and evaluation of various classification techniques and simulation models in an integrated portal.   The approach differs from prior work that relied on simple urban canopy models, either tuned for a large metropolis or assuming that all cities are the same.  The team uses a cyberinfrastructure platform at Purdue (HubZERO) and the Geospatial Data Analysis Building Blocks (GABBs), a suite of software modules developed during a previously funded NSF Data Infrastructure project.  The resulting platform can be deployed using Amazon Web Services, extending built-in geospatial data capabilities and providing a scalable CI solution.  This platform can be used by researchers to test predictive models or deploy applications that have been developed.  The team has cultivated relationships with the research communities and stakeholders relevant to the proposed research.  Through the World Urban Database and Access Portal Tools (WUDAPT) project -- a community-based project to gather a census of cities around the world -- the team is already connected to the urban planning community globally.  The project will improve urban weather modeling accuracy and increase availability of and access to the new techniques, capabilities and dedicated cyberinfrastructure.   The results have the potential to support city officials and urban planners, especially in regions with the fastest rate of urbanization and/or those in developing countries, where access to computational resources is likely to be limited.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure will be jointly supported by the Division of Chemical, Bioengineering, Environmental, and Transport Systems, within the NSF Directorate for Engineering; and the Division of Atmospheric and Geospace Sciences and the Integrative and Collaborative Education and Research (ICER) Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840043","CICI: RDP: Supporting Controlled Unclassified Information with a Campus Awareness and Risk Management Framework","OAC","Cybersecurity Innovation","09/01/2018","11/05/2019","Baijian Yang","IN","Purdue University","Standard Grant","Robert Beverly","08/31/2022","$598,373.00","Preston Smith, Carolyn Ellis","byang@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","8027","","$0.00","Protecting Controlled Unclassified Information (CUI) is mandated by the executive order 13356, and today is required for research in sectors such as defense and aerospace. Regulatory requirements for research will increase, with CUI regulations covering categories including Agriculture, Financial, Legal Records, and Business information. When combined with existing regulations already seen by universities, such as HIPAA, and the European Union's GDPR, a well-defined and consistent framework for working with regulated data is critical for institutions of higher education. This project describes a cost-effective ecosystem (REED+) to manage regulated data that meets the compliance requirements found in a campus environment.  <br/><br/>The REED+ framework integrates NIST SP 800-171 and other related NIST publications as the foundation of the framework. This framework serves as a standard for campus IT to align with security regulations and best practices, and create a single process for intake, contracting, and facilitate easy mapping of controlled research to CI resources for the sponsored programs office, human subjects office, and export control office. The framework allows researchers to experience faster intake of new funded projects and be more competitive for research dollars. Using student-developed training materials and instruction, researchers, administrators, and campus IT are now able to more clearly understand previously complicated data security regulations affecting research projects. The ecosystem developed from this project enables new partnerships with government agencies, and industry partners from the defense, aerospace, and life science sectors. Experiences and best practices in providing cyberinfrastructure and security awareness developed from this collaboration are documented and shared with the broader CI and campus community through conferences, journals and workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2138776","EAGER: Reproducibility and Cyberinfrastructure for Computational and Data-Enabled Science","OAC","CYBERINFRASTRUCTURE","09/01/2021","08/26/2021","Victoria Stodden","CA","University of Southern California","Standard Grant","Bogdan Mihaila","03/31/2022","$129,750.00","","stodden@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7231","7916","$0.00","This project seeks to improve understanding of how the scientific community can adapt to the increasing use of computing and large-scale data resources. One challenge is ensuring that computational results - such as those from simulations - are ""reproducible,"" that is, the same results are obtained when one re-uses the same input data, methods, software and analysis conditions. In 2019, the National Academies of Science, Engineering, and Medicine (NASEM) issued a report on ""Reproducibility and Replication in Science"" with a series of recommendations. The project will assess the implications of these recommendations on the scientific discovery process for computationally- and data-enabled research.<br/><br/>The following research questions will guide this study: (1) what reproducibility issues are surfaced by the NASEM recommendations and what constraints and requirements do they imply for computational infrastructure?; and (2) what are the implications of these issues and constraints on the computational infrastructure ecosystem as a whole? To explore and illustrate answers to these questions, we will employ diverse scientific use cases chosen to cover different ways researchers interact with computational infrastructure. Formalisms will also be applied to the use cases to articulate the role of computational infrastructure in enabling transparency and reproducibility, and to elucidate how computational infrastructure can conform to the NASEM report recommendations. The overall aim is to articulate avenues for future research at the intersection of transparency, reproducibility and computational infrastructure that supports scientific discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115087","CICI: UCSS: Building a Community of Practice for Supporting Regulated Research","OAC","Cybersecurity Innovation","08/01/2021","08/23/2021","Carolyn Ellis","IN","Purdue University","Standard Grant","Robert Beverly","11/30/2021","$499,781.00","Erik Deumens","carolynEllis@ucsd.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","8027","7923, 8027, 9102","$0.00","The daily news clearly shows the increasing threat to safety and privacy of data, personal as well as intellectual property. The Department of Defense modified the DFARS clause to mandate that NIST 800-171 be followed for data classified and marked as CUI in 2017. The next evolution of this program is called Cybersecurity Maturity Model Certification CMMC. Other agencies, for example, Department of Education, have indicated that they are considering following a similar path to safeguard data. While these requirements improve the consistency of data handling between agencies and contractors and grantees, it leaves academic institutions to figure out how to meet such requirements in a cost-effective way that fits the research and education mission of the institution. Most institutions, agencies, and companies act in isolation with one-off contract language to address data security and safeguarding concerns. The Regulated Research Community of Practice (RRCoP) builds a network of people able to help each other in implementing an affordable but effective cybersecurity and compliance program at academic institutions. Even though cybersecurity has a clear and uniform goal of protecting data, a onesize solution does not fit all academic institutions.<br/><br/>Through this project, RRCoP accomplishes 1) Developing cybersecurity training resources that share validated and diverse best-practices. 2) Establishing a leadership training and development program accelerating availability of distributed university resources 3) Developing representation through strategic partnerships with industry and government entities. By supporting this community with development of a community strategic roadmap, regular discussions and workshops, and a repository of generalized and specific resources for handling regulated research programs RRCoP lowers the barrier to entry for institutions handling new regulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931391","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","Special Initiatives, Software Institutes","01/01/2020","09/04/2019","Steven Barrett","MA","Massachusetts Institute of Technology","Standard Grant","Seung-Jong Park","12/31/2022","$507,708.00","Raymond Speth","sbarrett@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1642, 8004","026Z, 077Z, 7925, 8004","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018926","CC* Compute: Private Campus Cloud for Data Analytics and Machine Learning","OAC","Campus Cyberinfrastructure","07/01/2020","05/11/2021","Preston Smith","IN","Purdue University","Standard Grant","Kevin Thompson","06/30/2022","$408,205.00","Norbert Neumeister, Erik Gough, Jennifer Wisecaver, Thomas Hacker","psmith@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","8080","9251","$0.00","New usage patterns of computing for research have emerged that rely on the availability of flexible, elastic, and highly specialized services. Uniform batch computing pools traditionally provided by high performance or high throughput computing environments have difficulty adapting to meet these requirements. A new approach that updates and evolves the research computing ecosystem is needed to respond to these needs. This new model, a ?Community Cloud?, provides a cost effective, highly responsive, sustainable, and customizable cloud and container computing solutions for specific applications and domain science communities.<br/><br/>This project, through the acquisition of a new compute cluster, knits together central and lab-scale data, instrument, and compute resources into a cloud ecosystem for researchers who need capabilities beyond batch computing, and extends the research computing ecosystem to include cloud capabilities at the campus level. The Community Cloud is designed to: 1) Devise a new approach to establish a community cloud service using virtualization, containers, and infrastructure-as-code (IAC) techniques to create running infrastructure as an artifact; 2) Support diverse science domains via an effective infrastructure that enables new kinds of discovery that cannot be well met through the use of traditional batch computing systems; 3) Develop a reference business model for the evolution of campus ?condo? cluster programs to sustainably operate a production community cloud; and 4) Enable scalable and sustainable instructional use of the proposed community cloud for courses, real-world training, and workforce development for the campus and national research computing communities. The new compute cluster includes 8 application nodes (1024 cores), 2 GPU nodes (8 gpus), 6 storage nodes (288 TB), and one bastion node, all interconnected through a 100Gb network, and managed using Kubernetes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005632","Category I: Anvil - A National Composable Advanced Computational Resource for the Future of Science and Engineering","OAC","Innovative HPC","10/01/2020","08/19/2021","Xiaohui Carol Song","IN","Purdue University","Cooperative Agreement","Robert Chadduck","09/30/2022","$22,392,344.00","Xiao Zhu, Rajesh Kalyanam, Preston Smith","cxsong@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7619","9102","$0.00","As computing permeates nearly all fields of science and engineering, there is an exponential growth of computing needs from both the traditional computing-intensive domains and the emerging new and more diverse fields of research. The rise of machine learning and artificial intelligence applications has accelerated and broadened the use of computational resources from research in creating new and more environmentally friendly materials to improving medicine in our fight against deadly diseases. There are three main challenges to meeting this rapidly evolving landscape of national computational needs: a shortage of capacity, increasingly diverse applications, and computational literacy and training. This project aims to meet these challenges and transform the way computing is delivered by developing and deploying a composable advanced computing resource, Anvil, to the national research community to significantly increase both the computing capacity and accessibility. Anvil integrates a large-capacity high-performance computing (HPC) cluster with a comprehensive ecosystem of software, access interfaces, programming environments, and composable services to form a seamless environment able to support a broad range of current and future science and engineering applications. Through a carefully designed student training program and partnerships with regional and other universities, XSEDE, and Women in HPC programs, this project will develop computing competency in the next-generation workforce, and engage and train a broader audience including underrepresented students at minority-serving and EPSCoR (Established Program to Stimulate Competitive Research) institutions.<br/><br/>Built with a forward-looking architecture with a high core count, and improved memory bandwidth and I/O, Anvil can effectively support traditional HPC with fast turnaround for high throughput, mid-scale computation jobs. Anvil consists of 1000 128-core computing nodes based on the next-generation AMD Epyc ?Milan"" architecture that can deliver a total peak performance of 5.3 Petaflops. Each node has 256 GB of memory, and a 100 gigabits/second bandwidth from the Mellanox HDR InfiniBand interconnect, allowing multiple jobs of up to 1024 cores to be run at full speed over the interconnect fabric. These nodes are complemented by 32 large-memory nodes with 1 TB of RAM each, and 16 Nvidia GPU nodes with 4 ?Volta Next? GPUs per node. The GPU nodes are capable of 1.57 petaflops of single-precision performance to support machine learning and a wide range of current and future science and engineering applications. Anvil?s multiple tiers of storage systems include a long-term archive, persistent file and campaign storage, a 10 PB scratch file system, a 3 PB flash burst buffer, and object storage to support a variety of workflows and storage needs. <br/><br/>Anvil will lower the barrier to entry to advanced computing CI by providing interactive computing and desktop environments that ease the transition for users from diverse domains new to HPC. By providing feature-rich interactive environments such as Open OnDemand and ThinLinc, users can rapidly become productive on Anvil through Linux and Windows desktops, or familiar tools through their browser (e.g., Jupyter, RStudio). Complex scientific software environments and application stacks will be supported via containers orchestrated within a powerful composable subsystem. Anvil supports cloud-bursting of computational workloads as well as use of public cloud machine learning platforms including GPU and FPGA accelerators and software tools to automate hyperparameter tuning and algorithm selection for exploratory ML research. An existing production-quality science gateway at Purdue will support XSEDE researchers to share their data and tools online and facilitate easy access to Anvil and other XSEDE resources in classroom instruction and training activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018401","CC* Planning: NC Regional Science DMZ","OAC","Campus Cyberinfrastructure","07/01/2020","03/22/2021","Tracy Futhey","NC","Duke University","Standard Grant","Kevin Thompson","10/31/2021","$99,796.00","Tracy Doaks, Kevin Davis, Jean Davis, Eva Kraus, Charles Kneifel","futhey@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8080","","$0.00","Scientific approaches relying on big data and on large data transfers require unique cyberinfrastructure, called Science DMZs, to support fast and easy data transfers and access of data. Typically, the benefits of these complicated Science DMZs have accrued to large, well-resourced ?Research-Intensive? institutions, which compounds an already growing disparity between large and small institutions, especially often-lesser-resourced minority serving institutions. This also makes it much more difficult for smaller institutions with a primarily instructional mission to expand existing research and expose students to scientific approaches or collaborations, including those that may rely on big data.<br/><br/>Through a partnership between Duke University, North Carolina Central University, Davidson College, and MCNC (NC?s regional research and education network provider), this project plans the creation of a shared Science DMZ (sS-DMZ) as a service of the North Carolina Research and Education Network so that lesser-resourced institutions do not have to bear all setup and maintenance costs and can instead commit their resources to local infrastructure, their scientific researchers and pursuit of new research grants. This project works with regional higher-ed institutions to 1) establish community trust and shared governance responsibility necessary to expand participation in a sS DMZ program and 2) architect the approach to build the technical solution of operating a sS-DMZ. The resulting proposal will also inform the feasibility of a sSDMZ in other Research and Education Networks in the United States as an approach to reduce overall costs while increasing access to Big Data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015848","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","09/01/2019","01/24/2020","Timo Heister","SC","Clemson University","Standard Grant","Seung-Jong Park","09/30/2023","$677,503.00","","heister@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004044","Collaborative research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","Software Institutes","07/01/2020","04/01/2020","Yosef Zlochower","NY","Rochester Institute of Tech","Standard Grant","Amy Walton","06/30/2024","$439,675.00","Joshua Faber, Manuela Campanelli","yosef@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","107y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952554","CAREER: A Novel and Fast Open-Source Code for Global Simulation of Stratified Convection and Magnetohydrodynamics of the Sun","OAC","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, SOLAR-TERRESTRIAL, Information Technology Researc, EDUCATION AND WORKFORCE","09/01/2019","01/07/2022","Chunlei Liang","NY","Clarkson University","Standard Grant","Alan Sussman","04/30/2023","$293,564.00","","cliang@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","1045, 1271, 1443, 1523, 1640, 7361","1045, 4444, 9263","$0.00","Non-technical: <br/>The goal of this project is to create a unique capability for predicting density-stratified magnetohydrodynamics of the Sun. This research is expected to lay a foundation for developing methods for predicting extreme space weather, e.g. the event of a ""super solar flare"" followed by an extreme geomagnetic storm. Scientific results of this research can help resolve several contradictory predictions from previous studies of the solar convection zone. The Principal Investigator (PI) will develop and disseminate a powerful open-source software package to the space weather and solar physics communities. The success of predicting severe space weather events has significant societal and economic impacts. PI will design high-order accurate computational algorithms suitable for exascale simulations that can perform a billion billion calculations per second. This software will run on massively parallel distributed-memory computers to predict coupled global and local dynamics of the sun. PI will reach out to K-12 students and demonstrate that science of the sun and high-performance computing are exciting and important to society.  Furthermore, PI will leverage outreach efforts with the High Altitude Observatory of the National Center for Atmospheric Research and other research centers.  This project, thus, serves the national interest as stated by NSF's mission: to promote the progress of science and to advance the national welfare.<br/><br/>Technical: <br/>The goal of this research program is to develop a novel, fully compressible model and an open-source community code for global simulations of the solar convection zone that includes the top near surface shear layer of the Sun. Current leading global simulations use an elastic approximation whose computational domains extend from the base of the solar convection zone and must stop at about 0.96 solar radius, stopping short of the top near surface shear layer where Mach number could reach unity. This research program will create a powerful open-source community code CHORUS++ to simulate magnetohydrodynamics of the solar convection zone.  CHORUS stands for Compressible High-ORder Unstructured-grid Spectral difference code which has been co-developed by the PI for hydrodynamics of the solar convection zone.  CHORUS++ will be equipped with variable mesh resolution capability to focus on targeted regions of interests. A fast local time-stepping algorithm will be designed and equipped for CHORUS++ for long-period time integration on massively parallel computers. These technical accomplishments can accelerate the original CHORUS code by a factor over 100.  The PI will conduct a series of global simulations of magnetohydrodynamics of the solar convection zone with unprecedented resolutions for predicting the differential rotation, meridional circulation, giant cells, and super-granulation of the sun."
"1940176","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Deborah Sunter","MA","Tufts University","Standard Grant","Amy Walton","09/30/2022","$201,722.00","","deborah.sunter@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835602","Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure","01/01/2019","09/07/2018","Lan Lin","IN","Ball State University","Standard Grant","Seung-Jong Park","12/31/2022","$39,996.00","","llin4@bsu.edu","2000 West University Avenue","Muncie","IN","473060155","7652851600","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835338","Framework: Software: Collaborative Research: CyberWater: An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure","01/01/2019","09/07/2018","Ibrahim Demir","IA","University of Iowa","Standard Grant","Seung-Jong Park","12/31/2022","$125,378.00","Witold Krajewski, Ricardo Mantilla","ibrahim-demir@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1908691","OAC Core: Small: Higher Order Solvers for Training Machine Learning Models","OAC","OAC-Advanced Cyberinfrast Core","07/01/2019","06/26/2019","Ananth Grama","IN","Purdue University","Standard Grant","Alan Sussman","06/30/2022","$495,699.00","","ayg@cs.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","090Y","026Z, 9179","$0.00","Machine learning (ML) techniques have emerged as a key enabling technology for a broad class of applications, from business enterprises to engineering design. These techniques rely on complex models that must be suitably trained on large amounts of data. This training process takes the form of mathematical optimization, which minimizes the error between model output and known output, for training data. Owing to the large number of degrees of freedom in the model, the complexity of the objective function being minimized, and the volume of training data, the process of effectively and efficiently training ML models is a critical step in machine learning.  The goal of this project is to develop novel optimization techniques, their implementations on large scale parallel platforms with GPU accelerators, validation in the context of diverse ML applications, and development of highly optimized, robust, and usable software tools and libraries. These software tools will be specialized to various ML models, and incorporated into commonly used software frameworks such as TensorFlow -- thus making them seamlessly accessible to a very large and diverse user community.  The robustness, performance, and scalability of the software provide unique capabilities, with the potential to redefine the state of the art in ML applications, in terms of supporting significantly more complex ML models, enhancing generalizability from training to test data, and significantly reducing training time. Building on these intellectual and broader impact goals, the project integrates a number of activities aimed at broadening participation and creating educational opportunities and content.  These include summer schools for undergraduate students to channel them into research careers, providing research opportunities for undergraduates through the school year, development of new educational material that integrates learning with hands-on use of software, and motivating novel formulations and methods in machine learning.<br/><br/>The technical goals of the project are accomplished through a combination of novel numerical methods, statistical sampling techniques, highly scalable parallel implementations, and efficient use of GPUs. The project has the following specific aims: (i) development of second order Newton-type methods for non-convex problems. Specifically, the project focuses on Trust Region (TR) and Cubic Regularization (CR) based methods that rely on approximations to the Hessian and Fisher information matrices to deliver highly efficient solvers; (ii) development of a complete Higher Order Optimization Procedures (HOOP) toolkit, including unbiased and biased sampled Hessians, block diagonal approximations of the Fisher matrix, efficient and effective preconditioners for the Conjugate Gradient (CG) and CG-Steihaug solvers, and problem-specific optimizations; (iii) development of efficient parallel methods based on a combination of Alternating Direction Method of Multipliers (ADMM) and parallel matrix solvers, for scalable hardware platforms with GPU accelerators, as well as an integration of the software into TensorFlow. The software will also be made available as containerized executables that can be instantiated at clients with minimal effort, as libraries that can be used to build new ML applications, and as web accessible services for education and training; and (iv) demonstration of the effectiveness of the new methods on important application classes, including solution of large-scale semi-definite programs (SDP), problems in matrix factorization and distance metric learning, and training of deep neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835452","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","09/26/2018","Timo Heister","SC","Clemson University","Standard Grant","Stefan Robila","03/31/2019","$700,000.00","","heister@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829764","CyberTraining: CIU:Cross-disciplinary Training for Findable, Accessible, Interoperable, and Reusable (FAIR) science","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Venkatesh Merwade","IN","Purdue University","Standard Grant","Bogdan Mihaila","08/31/2022","$498,148.00","Wan Ju Huang, Xiaohui Carol Song, Matthew Huber","vmerwade@ecn.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","Addressing the grand challenges associated with growing population, food and water security, frequently occurring natural disasters, and changing climate requires not only geoscience domain expertise, but also computational (cyber) skills to deal with big data analytics and simulations. However, many challenges exist in providing cyber training to students, including steep learning curve for instructors, time commitment and lack of resources in terms of training material. This project creates cyber training programs for earth science students and working professional in the form of modules or labs for use in an existing course, an independent one-credit course, summer workshops and boot camps. Students receive training in running state-of-the-art numerical models, and in analyzing massive simulated and observational data sets with advanced data analysis tools. The overall goal is to create a new generation of scientists to manage data-rich and computationally intensive tasks to become globally competitive in the STEM, thus fulfilling NSF's mission to promote the progress of science.  The training component of the project will make the science openly available and transparently reproducible using the best practices in Findable, Accessible, Interoperable, and Reusable (FAIR) science. The estimated number of trainees from this project is approximately 200 students at Purdue and potentially more than 400 students, faculty and working professionals outside Purdue.<br/><br/>This project brings cyber-enabled state-of-the-art computational tools into practice by training students and working professionals through courses, workshops and boot camps at multiple institutions, including Purdue University, University of New Hampshire and University of Alabama. The project creates a cyber training curriculum that is driven by the need to acquire expertise in the following six areas: data access, geo-processing, time series analysis, computational simulation, visualization and publication. These areas form the foundation of a modular cyber training framework that supports development and implementation of training materials targeting geoscience learners. The innovation lies in enhancing geosciences curriculum to include cyber training by providing a range of flexible training options for students and working professionals. A FAIR Cyber Training (FACT) Fellowship will enhance computational thinking among graduate students and create an avenue to tap the most talented students to train the next generation of cyber savvy earth scientists. This interdisciplinary collaboration from hydrology, climate science, computer science and instruction design  expertise enables holistic training covering both domain science and computational technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841480","Scalable Cyberinfrastructure for Early Warning Gravitational Wave Detections","OAC","CESER-Cyberinfrastructure for","10/01/2018","12/21/2018","Chad Hanna","PA","Pennsylvania State Univ University Park","Standard Grant","Bogdan Mihaila","09/30/2021","$983,911.00","","crh184@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","7684","020Z, 062Z","$0.00","Recent advances in astronomical facilities have opened new windows on the universe that extend observing capabilities beyond conventional telescopes. Joint observations that combine telescopes, neutrino detectors, and new gravitational wave detectors including the NSF-supported LIGO Observatory are revealing aspects of the universe that are presently a mystery. Just recently, signals from the collision of two extremely dense neutron stars - a merger known as GW170817 - were detected by both conventional telescopes and gravitational wave detectors.  The event was detected first in gravitational waves, two seconds later in gamma rays, after 10 hours in optical, ultraviolet, infrared, and much later in x-ray and radio waves.  From this single event, the world learned that some short-hard gamma ray bursts indicate neutron star mergers, that these mergers might be the origin of many elements in the periodic table such as gold and platinum, that gravity and light travel at the same speed, and that gravitational waves really could measure how fast the universe is expanding.  Despite what was learned, GW170817 left the world with many questions. What object was formed afterward? Was it another neutron star? Was it a black hole? Why was the gamma ray burst associated with GW170817 unlike anything else that had been observed? Answering these questions, and conducting these kinds of joint observations on a regular basis, requires significant computing and software infrastructure (cyberinfrastructure).<br/><br/>The project proposes to develop the cyberinfrastructure necessary to give earlier gravitational event alerts to other astronomical facilities than is currently possible, allowing researchers to collect as much data as possible about these new types of celestial events. This project will fortify the streaming data delivery of LIGO by producing sub-second data delivery to a streaming early warning search for neutron star mergers. Substantial automated monitoring and feedback will ensure the entire system operates without manual intervention. The project will capitalize upon existing NSF investments in cyber-infrastructure for real-time gravitational wave analysis and will significantly augment the data delivery and automation layer for detections which is presently a bottleneck and failure mode. Using gravitational waves to provide an early warning for robotic telescopes will significantly enhance the scientific utility of LIGO data and significantly facilitate multi-messenger astrophysics.  This proposal will promote the progress of science in two of the National Science Foundation's 10 Big Ideas: ""Harnessing the Data Revolution"" ""Windows on the Universe: The Era of Multi-Messenger Astrophysics"". <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756005","CRII:OAC: Novel techniques for improving convergence and scalability of a Monte Carlo radiation solver for large-scale combustion simulations","OAC","CRII CISE Research Initiation","03/01/2018","06/21/2019","Somesh Roy","WI","Marquette University","Standard Grant","Alan Sussman","08/31/2021","$190,951.00","","somesh.roy@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","026Y","026Z, 8228, 9251","$0.00","Combustion has been an important source of energy for ages and will continue to be so for considerable future.  With the help of high-performance computing (HPC), predictive and accurate combustion simulations have a tremendous potential to emerge as a cost-effective and reliable design, assessment, and decision-making tool for practical systems (e.g., gas turbines, internal combustion engines, furnaces, etc.).  Detailed predictive modeling of combustion system requires, among other things, detailed and accurate modeling of thermal radiation.  However, models for thermal radiation used in combustion simulations are usually over-simplified.  The main bottlenecks in using detailed radiation model are its high computational cost and poor parallel efficiency in HPC.  This project explores several novel ideas to increase efficiency and robustness of a high-fidelity radiation solver in HPC combustion simulations, leading to the possibility of performing predictive and accurate simulations of practical combustion systems in a realistic time-frame.  The ability to perform such large-scale reliable predictive simulation is not only important in the design process of real combustion devices but also essential to further our understanding of fundamentals of combustion processes.  Considering the ever-increasing need for cleaner combustion devices, this predictive capability can potentially have a significant effect in academic research, as well as in energy and transportation industry.  The project will also have an impact in popularizing computer programming in undergraduate students.  Therefore, this research aligns with the NSF's mission to promote the progress of science and to advance the national health, prosperity, and welfare. <br/> <br/>The radiation solver of choice in this project is a Monte-Carlo ray tracing-based (MCRT) solver.  It is one of the most accurate radiation solver available, and typically outshines all other radiation solvers as the complexity of the problem increases.  To achieve improvements in efficiency and scalability of the MCRT solver in HPC simulations of large-scale combustion systems, this research brings together ideas from different disciplines of mathematics, statistical theory, and computer science and applies them to solve an engineering problem.  Considering the fact that the performance of an MCRT solver in HPC primarily depends on the underlying statistical algorithm and computational load-balancing, the current research is divided into three primary tasks.  First, the project is developing new algorithms for improved convergence using special statistical distributions with low discrepancy.  Second, novel strategies for MCRT load management, both in terms of computational time and memory utilization, are being explored to improve scalability of the solver in HPC simulations.  Third, the improved MCRT solver are planned to be created as a modular, platform-independent solver module with standardized interfaces so that it can be used with any combustion and/or CFD solver without significant sacrifice of its performance.  By enhancing efficiency and scalability of MCRT, this work aims to enable more accurate predictive HPC simulations of large-scale combustion systems in a realistic timescale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919789","MRI: Acquisition of a Supercomputer to Enable Advanced Computational Science and Engineering Research and Education in Missouri","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2019","08/29/2019","Thomas Vojta","MO","Missouri University of Science and Technology","Standard Grant","Alejandro Suarez","09/30/2021","$1,960,000.00","Julia Medvedeva, Stephen Gao, Serhat Hosder","vojtat@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","1189, 1640","1189","$0.00","This Major Research Instrumentation (MRI) project will acquire and deploy a heterogenous high performance computing (HPC) system. The instrument enables the dramatic expansion of a HPC capabilities in the state of Missouri. Located on the campus of the Missouri University of Science and Technology in Rolla, the center connects several partner institutions, and thus will have a transformative and broad regional impact on research and training activities across the state of Missouri. Partners include the full four-campus University of Missouri (UM) System, and other institutions including Missouri State University, Southeast Missouri State University, historically black Lincoln University, College of the Ozarks, and others. The facility will enable over a dozen research groups to advance state-of-the-art science in a variety of science areas. The project will also promote collaboration, training and educational activities.<br/><br/>This investment provides new computational infrastructure (with a peak performance of more than 400 TFlops) necessary to advance science in several core areas relating to the study and design of advanced materials, spanning many length scales and ranging from fundamental to applied. Projects in the fields of Physics and Chemistry depend heavily on computation to understand and predict the quantum dynamics of systems of particles leading to the interpretation and prediction of the existence and behavior of exotic states of matter. Computational modeling in Mathematics and Statistics includes studying and optimizing charged plasma propulsion in space; it also extends into Biological investigations of DNA methylation which increases our understanding of a variety of diseases in plants (food sources) and humans (including cancer). Another area that will see immediate benefit is that of computation and data-driven Geophysics which involves the development of tools that are essential for the well-being of human society, including predicting and mitigating natural hazards such as earthquakes and volcanoes, as well as exploration and development of petroleum and other natural resources. Other projects include computational hypersonic fluid dynamics contributing to the design and optimization of high-speed aircraft and space vehicles. In addition to enabling and greatly enhancing a number of specific research projects, each of which benefits broad communities and society in general, a significant part of the broader impact of this project comes from the areas of training and education and increasing participation of communities underrepresented in STEM.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845799","CAREER: Scalable Approaches for Large-Scale Data-driven Bayesian Inverse Problems in High Dimensional Parameter Spaces","OAC","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS, CYBERINFRASTRUCTURE","01/15/2019","10/15/2020","Tan Bui-Thanh","TX","University of Texas at Austin","Continuing Grant","Alan Sussman","12/31/2023","$420,571.00","","tanbui@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1045, 1271, 7231","1045, 9263","$0.00","Inverse problems are contemporary tools in cyberinfrastructure and mathematical research, especially in inferring knowledge from observational and experimental data together with simulations and models. They are pervasive in scientific discovery and decision-making for complex, natural, engineered, and societal systems, and thus are of paramount importance across many disciplines including engineering mathematical and physical sciences. For inverse problems that serve as a basis for design, control, discovery, and decision-making, their solutions must be equipped with certain degree of confidence. Though the past decades have seen advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty in their solution remains challenging and an open problem facing the computational science and engineering community. The drastic increase in the quantity of measurements and data holds promise for data-driven scientific discoveries. However, much data remains unused as inversion - a systematic tool to infer knowledge from data - is unable to scale up to the quantity of data being generated. This proposal develops computational and data scalable strategies to tackle the challenge of large-scale data-driven statistical inverse problems in order to continue the pace of scientific discoveries and to promote the progress of science, aligned with NSF's mission. The proposed integrated research and education program contributes uncertainty quantification (UQ) skills to modern education and training of future STEM workforce; provides scalable inverse/UQ mathematical algorithms/software that potentially advance the frontiers of computational science and engineering; provides inverse/UQ cutting-edge algorithms/software that can potentially improve oil/gas discovery in order to meet the ever-increasing demand in energy; constitutes the PI?s ongoing contribution to the pipeline of US scientists, engineers, and innovators to maintain the US global leadership in technology and sciences; and educates and supplies additional leaders/experts from underrepresented minorities to Big-Data/UQ research communities.<br/><br/>This project develops an integrated education and cross-disciplinary research program that tackles big-data-driven large-scale uncertainty quantification (UQ) problems in high dimensional parameter spaces. The project rigorously develops a randomized misfit approach that exploits extreme computing systems to efficiently reduce the amount of ever-growing observational data. It develops a comprehensive ensemble transform approach that has potential to solves large-scale statistical Bayesian inverse problems in a scalable manner using current and future NSF computing infrastructures. The novelty of the proposed interdisciplinary approach is to bring together advances from stochastic programming, probability theory, parallel computing, and computer vision to produce a new and rigorous data reduction method for inverse/UQ problems; justifiable efficient sampling approaches for large-scale Bayesian inverse problems; and open-source software implementing these approaches. These products can enable mathematicians, scientists, and engineers in sensing-based disciplines to address challenging inverse/UQ problems that can lead to new scientific discoveries. Inverse seismic wave propagation is chosen as the demanding testbed for the developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828187","MRI: Acquisition of an HPC System for Data-Driven Discovery in Computational Astrophysics, Biology, Chemistry, and Materials Science","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2018","08/24/2018","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Alejandro Suarez","09/30/2022","$3,699,317.00","Surya Kalidindi, Charles Sherrill, Richard Vuduc, Deirdre Shoemaker","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1189, 1640","026Z, 1189","$0.00","The project funds the purchase of a high-performance computing and storage system at the Georgia Institute of Technology. This computing instrument will support data-driven research in astrophysics, biosciences, computational chemistry, materials and manufacturing, and computational science. These projects contribute to national initiatives in big data, strategic computing, materials genome, and manufacturing partnership; and NSF supported observatories such as the gravitational wave observatory and the South Pole neutrino observatory. The system also serves as a springboard for developments of codes, software prototyping, and scalability studies prior to using national supercomputers. Advances made in computational methods and scientific software are disseminated in the form of open-source codes and data analysis portals. Over 33 faculty, 54 research scientists/postdocs, 195 graduate students, and 56 undergraduate students will immediately benefit from the instrument. In addition, the system provides training opportunity at all levels from undergraduate students to early career researchers, in important interdisciplinary areas of national need. A fifth of the system capacity is utilized to enable research activities of regional partners, researchers from minority serving institutions, and other users nationally through XSEDE participation. The project involves undergraduate student participation from historically black colleges from Atlanta metropolitan area. Public outreach efforts are planned through videos of public interest and local events such as the Atlanta Science Festival.<br/><br/>The cluster will combine regular compute nodes with others configured to emphasize one of the following: big memory, big local storage, solid state storage, Graphics Processing Units (GPU), and ARM processors. In doing so, the system can be employed by a diversity of projects. In astrophysics, the instrument bolsters data-driven research including detection of gravitational waves, astrophysical neutrinos, and gamma rays. It does it by leveraging data from leading astroparticle observatories and contributing to their mission. It also leads to improved insights into formation of supermassive black holes and large-scale structure of the universe. The computing system also aids the development of parallel software in computational genomics, systems biology, and health analytics. Important applications in assembly and network analysis of plant genomes, and environmental metagenomics are pursued. The instrument also enables next generation algorithms and software for computational chemistry and expands the boundaries of molecular simulation. The system enables advances in density function theory, enhances studies of crystal defects and nanostructures, and injects novel use of machine learning techniques in computational chemistry. It also fosters the development of data science methodologies to identify building blocks of materials at multiple scales, thus significantly reducing the development and deployments cycles for new materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827126","CC* Integration: SENSELET: Sensory Network Infrastructure for Scientific Laboratory Environments","OAC","CISE Research Resources","10/01/2018","06/29/2018","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Deepankar Medhi","09/30/2021","$500,000.00","John Dallesasse, Tracy Smith, Roy Campbell, Kenton McHenry","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2890","9102","$0.00","Scientific instruments (e.g., scanning electron microscopes) are extensively used to<br/>discover new materials, develop novel semiconductor device fabrication recipes, and perform new biological processes.<br/>One way to speed up scientific discoveries is to provide scientists with advanced cyber-infrastructures to capture, transmit,<br/>store, share, analyze, and correlate as much environmental metadata (e.g., humidity, temperature)<br/>from scientific lab environments as possible. Current network infrastructure does not capture<br/>any external wireless sensory data around the instruments.<br/>The recent advent of low-cost, cloud-based sensors and the introduction of diverse wireless network<br/>technologies, low-cost mobile and personal devices, and Internet of Things (IoT) solutions provide a<br/>novel and viable path for automating sensory data collection in diverse science laboratory environments.<br/><br/>SENSELET, a SEnsory Network infrastructure for SciEntific Lab EnvironmenTs, has the goals of (a)<br/>deploying a diverse wireless and scalable sensory infrastructure close to scientific instruments, and (b)<br/>correlating and synchronizing sensory data with cloud-based instrument data and metadata in real-time<br/>and on-demand. The SENSELET infrastructure will provide additional measurements that will increase<br/>accuracy of scientific results, and enable better environmental monitoring and control of labs for lab<br/>managers. The SENSELET infrastructure will include (a) wireless sensors such as humidity, temperature,<br/>and vibration sensors; (b) an edge computing device with multiple wireless communication interfaces<br/>residing in the lab; and (c) private cloud computing service to store<br/>and correlate sensory data with instrument data in real-time or on-demand. SENSELET will<br/>provide trusted and real-time instrument data uploading, curation, search, and coordination services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811228","Collaborative Research: Photons from Binary Black Hole Inspirals","OAC","Leadership-Class Computing","08/01/2018","03/27/2018","Manuela Campanelli","NY","Rochester Institute of Tech","Standard Grant","Edward Walker","07/31/2019","$8,155.00","Yosef Zlochower","manuela@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7781","","$0.00","Every large galaxy contains a supermassive black hole in its center.   Galaxies merge from time to time, therefore it is expected, but not yet demonstrated, that recently merged galaxies hold two large black holes, which eventually form a bound pair and later merge. It is generally expected that supermassive black hole binaries can often accumulate sizable quantities of gas, and therefore be bright for a significant time before and during the merge, as well as the relaxation period there after.  However, because it is estimated that there are only very few such mergers per year in the entire Universe, understanding the specific features to search for is required to discover them. A team of researchers and graduate students from the Rochester Institute of Technology (RIT), Johns Hopkins University (JHU), and the University of Tulsa (TU) propose to use the Blue Waters supercomputer to perform the first realistic simulations of gas surrounding supermassive binary black holes on route to merger.  These simulations will provide predictions of light and timing signatures emitted by supermassive black hole mergers in order to allow for their observational discovery.<br/><br/>The focus of this project is to define the features that identify the process of supermassive black hole mergers, using large-scale numerical simulations that assume astrophysically relevant initial conditions and include all the principal relevant physics: magnetohydrodynamics, general relativity, and radiation transfer.  These simulations are extremely challenging computationally because of the complexity of the physics involved. However, the project introduces two significant computation innovations: a time-dependent high-order post-Newtonian spacetime to substitute for explicit solution of the Einstein Field Equations and a multipatch scheme to coordinate separate treatment of regions with contrasting grid requirements.  In addition to the simulation activities, the project is expected to create opportunities to integrate many existing research programs into a larger community of scientists, students, and the general public. It will foster cross-disciplinary collaboration among undergraduate students, graduate students, postdoctoral researchers and faculty at RIT, JHU and TU. Through this network, previous highly successful outreach programs will be able to reach an even larger and more diverse audience, and will provide many more opportunities for students to interact and collaborate with researchers outside their home institutions. It will also result in educating a number of graduate students and post-docs in the techniques of very large-scale computation, which will be important skills for their future careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761963","Spokes: SMALL: NORTHEAST: Collaborative: Building the Community to Address Data Integration of the Ecological Long Tail","OAC","BD Spokes -Big Data Regional I","09/15/2018","09/06/2018","Kenneth Chiu","NY","SUNY at Binghamton","Standard Grant","Martin Halbert","08/31/2022","$175,383.00","","kchiu@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","024Y","028Z, 8083","$0.00","Frequently research on data integration carried out by computer scientists and resulting tools must be modified to fit the needs of domain practitioners (ecologists in this case).  This challenge is a socio-technical, collective action problem that can be addressed through a combination of tools and incentives.  The project proposes to holding a series of workshops along with proofs-of-concept implementations. These workshops will result in approaches to decentralize the sharing of data in the long tail, through socio-technical approaches that appropriately incentivize and facilitate data integration by smaller labs.  Such an interdisciplinary community will provide crucial real-world input to computer science researchers, which will give their research into tools the potential for larger impact in ecological practice and will yield better tools for ecologists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031953","Frontera Travel Grant: Computational Studies of Transition Metal-Catalyzed Reactions","OAC","Leadership-Class Computing","09/01/2020","05/12/2020","Peng Liu","PA","University of Pittsburgh","Standard Grant","Edward Walker","08/31/2021","$10,000.00","","pengliu@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031704","NSF Frontera Allocation Travel Grant","OAC","Leadership-Class Computing","09/01/2020","05/12/2020","Lijun Liu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2021","$10,000.00","","ljliu@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835838","Element:Software:Enabling Millisecond-Scale Biomolecular Dynamics","OAC","Special Initiatives, Software Institutes","10/01/2018","09/12/2018","Erik Santiso","NC","North Carolina State University","Standard Grant","Robert Beverly","09/30/2022","$599,998.00","Carol Hall, Stefano Menegatti","eesantis@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","1642, 8004","026Z, 077Z, 7923, 8004","$0.00","Computer simulation methods based on Molecular Dynamics (MD) have been used for decades to understand chemical and biochemical phenomena at the molecular level. MD is a very powerful tool that has enabled scientists to understand the behavior of molecules crucial for life such as proteins and nucleic acids. MD has also been used to understand diseases and develop new drugs. However, MD is limited in both the size of the systems that can be studied and the amount of time that can be simulated. Many complex phenomena relevant to life involve systems too large to study with MD, or require following the system for much longer times. An alternative to traditional MD called discontinuous molecular dynamics (DMD) has been shown to be much more efficient to study biomolecular processes. To date, however, the use of DMD has been limited due to its inability to take advantage of modern parallel computers. This project will develop a next-generation parallel DMD software that will enable the study of complex molecular phenomena involving larger systems and longer time scales. Detailed knowledge of such processes will considerably advance the development of new materials and drugs, and human health.  The project team combines the computational and experimental expertise to successfully develop and validate a robust parallel DMD software framework. The software and results will be actively shared both with the computational simulation community and with the scientific and engineering community at large, contributing to the capability, capacity, and cohesiveness of the national cyber-infrastructure ecosystem. Furthermore, the results of this project will be used in outreach efforts geared toward the education and inclusion of minorities traditionally underrepresented in higher STEM education.<br/><br/><br/>This project aims to develop an open software framework that enables multi-millisecond dynamic simulations of peptides and peptide-mimetics by implementing a parallel discontinuous molecular dynamics (DMD) package. Unlike current molecular dynamics (MD), which features limited simulation timescales, discontinuous molecular dynamics (DMD) assumes ballistic motion of the particles between interaction events and enables the study of phenomena across much longer time scales. To demonstrate the approach, the project will (1) develop a parallel version of existing serial DMD codes to enable extending simulation times from hundreds of microseconds to several milliseconds; (2) extend and improve the available DMD peptide force field, adding parameters for non-natural peptides and peptoids; and (3) develop software for translating interaction potentials from traditional MD to DMD. The project team possesses the complementary expertise necessary for this project, including coarse-grained models and force fields for complex polymers and peptoids, MD simulation of protein self-assembly and peptide-protein binding processes, synthesis of protein-binding peptides and peptoids, and measurement of thermodynamic and kinetic binding parameters. The tools resulting from this research will allow the scientific and engineering community to model and study very long time-scale phenomena, such as biopolymer folding, aggregation and inhibition of aggregation, fibril formation, and protein-binding. This toolbox shows great promise to not only accelerate innovation in the computational design of biomaterials, but also to impact the molecular simulation community focusing on highly complex systems, up to cell-level dynamics. Notably, this project is ideal for the National Science Foundation's Cyber-infrastructure for Sustained Scientific Innovation (CSSI), as it (i) contributes to the capability, capacity, and cohesiveness of the national cyberinfrastructure ecosystem by providing user-friendly open-source computational tools, (ii) actively engages CI experts and testers of our toolbox, who would potentially be its ultimate users, (iii) advances our current capabilities in developing bioactive peptides and peptoids, (iv) establishes plans and metrics that encourage measurement of progress and certify the quality of shared tools and results, and (iv) devise strategies to combine wide-access with long-term community-driven development and progress.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808591","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Evangelos Papalexakis","CA","University of California-Riverside","Standard Grant","Tevfik Kosar","08/31/2021","$175,263.00","","epapalex@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835272","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, Data Cyberinfrastructure","01/01/2019","06/17/2020","Laura Trouille","IL","Adler Planetarium","Standard Grant","Amy Walton","12/31/2022","$732,172.00","","ltrouille@adlerplanetarium.org","1300 S. DuSable Lake Shore Dr.","Chicago","IL","606052403","3123220325","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828163","MRI: Acquisition of Hardware for the Enhancement of the ELSA High Performance Computing Cluster to Enable Computational Research at The College of New Jersey","OAC","Major Research Instrumentation","09/01/2018","08/23/2018","Joseph Baker","NJ","The College of New Jersey","Standard Grant","Alejandro Suarez","08/31/2021","$651,032.00","Michael Ochs, Wendy Clement, Paul Wiita, Michael Bloodgood","bakerj@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","1189","026Z, 1189","$0.00","The College of New Jersey (TCNJ) will acquire equipment to significantly upgrade and enhance the Electronic Laboratory for Science and Analysis (ELSA) High Performance Computing cluster. TCNJ is a primarily undergraduate institution promoting a deep engagement of undergraduate students in research. Many of TCNJ's School of Science faculty members are working at the cutting edge of computational research in their fields, which include a broad range of areas including biochemistry/biophysics, genetics, bioinformatics, astrophysics, machine learning, and mathematical biology. In order to maintain a diverse and state of the art resource that meets the current and future computational needs of TCNJ's faculty and undergraduate students the current ELSA cluster requires targeted hardware enhancements. The new instrument will (1) enhance the research capacity and resulting scientific discovery of TCNJ's School of Science faculty members and their undergraduate research teams; (2) expose a greater number of undergraduate students and researchers to this powerful computational infrastructure through a series of newly developed High Performance Computing and data visualization short courses and workshops; and (3) improve access to the ELSA cluster for students traditionally underrepresented in STEM, as well as to researchers beyond TCNJ through a new collaboration with Open Science Grid.<br/><br/>This project will expand the research programs of more than 13 faculty members (many of whom are early career faculty) spanning all five of TCNJ's School of Science departments. The computationally intensive work that will be supported through this project includes a diverse array of scientific efforts, including studies of pilus biomechanics, estimations of cell signaling processes, methods for investigating the strength of passwords and security of password systems, and improving our understanding of the most energetic objects in the universe. Currently, the research programs of TCNJ faculty members in these and other areas are restricted by inadequate graphic processing unit (GPU) resources and by the slow speed aging central processing unit (CPU) servers part of the current instrumentation. The new ELSA cluster will allow faculty and student researchers at TCNJ to run workflows ranging from embarrassingly parallel computations, to those that necessitate high levels of parallelization over hundreds of cores, intensive GPU computations, and remote visualization of simulation results. The instrument will thus ensure that these research programs are able to reach their full potential. This project will also benefit nearly 100 undergraduate student researchers each year who are part of these labs and work directly on the cluster. As a result, in addition to improving the capacity for scientific discovery, the proposed acquisition will help TCNJ meet the demands of developing an undergraduate workforce that is ready to leverage increasingly powerful High-Performance Computing resources, now and in their future careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112558","Better Scientific Software Fellowship Program","OAC","EDUCATION AND WORKFORCE","03/01/2021","07/02/2021","Shelly Olsan","IA","Krell Institute","Standard Grant","Alan Sussman","08/31/2022","$54,266.00","","shelly@krellinst.org","1609 Golden Aspen Drive","Ames","IA","500106215","5159563696","CSE","7361","","$0.00","Addressing the scientific software challenges facing the nation requires broad collaboration to foster practices, processes, and tools to improve software developer productivity and software sustainability.  Both of those are key aspects of ensuring the integrity of computational results and increasing scientific productivity. The Better Scientific Software (BSSw) Fellowship Program, launched in 2018 with support from the U.S. Department of Energy (DOE), provides recognition and funding for leaders and advocates of high-quality scientific software.  This project will enable NSF to collaborate in sponsorship of the BSSw Fellowship Program and participate in managing the program.  The project will be piloted in the current year with NSF support of one BSSw Fellow and one Honorable Mention, with the goal of expanding in future years to 2 or more BSSw Fellows and Honorable Mentions.  The BSSw fellow will receive a stipend that can be used for various activities that promote improved scientific software, such as organizing a workshop or creating a tutorial.  Both the BSSw fellow and the honorable mention receive travel support to work with scientific community members and form a cohort.<br/><br/>The National Science Foundation (NSF) and DOE are leaders in advanced computing, pushing the growth of computational and data-enabled science and engineering as an essential driver of scientific and technological progress. Moreover, NSF and DOE researchers have developed a wide range of high-impact scientific software for advanced modeling, simulation, discovery, and analysis. The country faces a software crisis, however, due to disruptive changes in computer architectures and increasing complexity in next-generation computational science.  NSF partnership in sponsoring the BSSw Fellowship Program will enable a more robust approach toward pioneering the future of advanced computing ecosystems in support of American leadership in science and engineering. An important element of the project is exploring the feasibility to expand the program to more fully address the NSF scientific community. The BSSw Fellowship Program enhances workforce development and pathways to the NSF and DOE software communities, though nurturing a network of people who advance software practices as a fundamental aspect of increasing overall scientific productivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029278","IRNC: Core Improvement: AtlanticWave-SDX: A Distributed Experimental SDX Supporting Research, Experimental Deployments, and Interoperability Testing on Global Scales","OAC","International Res Ret Connect","12/15/2020","08/12/2021","Julio Ibarra","FL","Florida International University","Continuing Grant","Kevin Thompson","11/30/2025","$5,238,543.00","Heidi L. Morgan, Jeronimo Bezerra, Yufeng Xin","julio@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7369","","$0.00","Open Exchange Points (OXPs) serve as meet points for connecting and facilitating the exchange of data between Research and Education (R&E) networks. They are critical cyberinfrastructure in the transit of data over long geographical distances, switching data flows from one R&E network to the next, to its destination. For example, data from the Vera Rubin Observatory will transit 6 OXPs as it moves from the southern to northern hemispheres. Operationalizing the transit of data flows across OXPs is increasingly important to minimize the impact from events on network services (hardware failures and soft failures). End-to-end network paths for these data flows are not under the control of any individual organization. Multiple R&E network operators must coordinate to establish geographically distributed end-to-end paths ? an effort that can take days to weeks.<br/><br/>Florida International University (FIU), University of Southern California ? Information Sciences Institute (USC-ISI) and University of North Carolina at Chapel Hill - RENCI are furthering AtlanticWave-SDX: a distributed experimental SDX, supporting research, experimental deployments, prototyping and interoperability testing, on national and international scales. AtlanticWave-SDX leverages innovations from the initial AtlanticWave-SDX project, introducing new capabilities, enabling OXPs to react to unplanned network events by adding intelligent closed-loop control of network services powered by in-band network telemetry.<br/><br/>AWAVE-SDX is comprised of two components: An infrastructure development component with optical super channels to reframe and reprovision network capacity between R&E backbone networks; An innovation component to build a distributed intercontinental experimental SDX by leveraging OXPs in the U.S., Chile, Brazil, and S. Africa.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029295","IRNC: Testbed: COSMOS Interconnecting Continents (COSMIC)","OAC","International Res Ret Connect","10/01/2020","09/20/2021","Gil Zussman","NY","Columbia University","Continuing Grant","Kevin Thompson","09/30/2023","$2,000,000.00","Daniel Kilper, Ivan Seskar, Ethan Katz-Bassett, Tarek Saadawi","gil@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7369","","$0.00","The project enables the use of unique programmable wireless, optical, and edge-cloud network testbed infrastructure for international collaborative experiments. It builds on the interfaces of the PAWR COSMOS (NYC) and ORBIT (NJ) testbeds with the PEERING (US/International) and FABRIC (US) testbeds, and adds connections to international testbeds, including CPQD (Brazil), Kyutech/StarBED (Japan), OneLab/NITOS (EU/Greece), and CONNECT (Ireland). To support cross-layered international experiments, unique capabilities are developed, including optical and BGP extensions to the Mininet network emulator, and PEERING testbed tools for cross-layer BGP functions and controlled public Internet experimentation.<br/><br/>The project creates a powerful international experimental platform for networking research from applications down to the optical and radio physical layers. A few distinct example experiments that demonstrate various capabilities and motivate further experimentation include: (i) Artificial Intelligence for multi-layer Quality of Service over disaggregated infrastructure during remote scientific experimentation; (ii) cloud processing of latency- and capacity-sensitive mobile applications across network domains and testbeds; and (iii) interdomain routing, data security, and privacy across national boundaries.<br/><br/>A strong outreach component aiming at broadening participation in computing involves local Harlem middle and high school teachers in research and in development of educational components. Finally, the development of emulation-based sandbox capabilities for international experiments promises to make such large-scale experiments more accessible to a wide range of researchers. Therefore, and due to the availability of a unique international infrastructure, the project has the potential to support experimentation that enhances the performance of future networks and helps bridge the digital divide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031744","Supermassive Black Holes Approaching Merger: Accretion Dynamics, Jets and Electromagnetic Signals","OAC","Leadership-Class Computing","09/01/2020","05/07/2020","Manuela Campanelli","NY","Rochester Institute of Tech","Standard Grant","Edward Walker","08/31/2022","$9,994.00","Yosef Zlochower","manuela@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841475","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Laura Cadonati","GA","Georgia Tech Research Corporation","Standard Grant","William Miller","09/30/2020","$146,937.00","","cadonati@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838994","EAGER: Crowdsourcing Metadata Enhancements to Improve the Discoverability and Reusability of Scientific Data: Experimental Evaluations","OAC","NSF Public Access Initiative","10/01/2018","08/15/2018","Margaret Levenstein","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Martin Halbert","09/30/2022","$298,234.00","","maggiel@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7414","7916","$0.00","This exploratory pilot project will undertake two experiments to determine what motivates people to contribute metadata enhancements to data that have been archived, but that are not sufficiently findable, accessible, interoperable, and reusable.  Current practice in data curation relies on the efforts of data producers and professional data curators to produce and provide metadata, including variable level data descriptors, study key words, and bibliographic citations to data-related publications. These efforts are expensive and, as a result, are often undersupplied, leaving data that has been archived and shared with the scientific community of limited value for reuse. The experiments in this project will directly inform potentially transformative efforts to engage the broader community in crowdsourcing enhancements to metadata so that tools and interfaces can be designed that will induce others to participate in this valuable activity, tapping into their knowledge of and interest in data in particular domains to increase data FAIRness. <br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931368","Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering","OAC","Software Institutes","01/01/2020","08/14/2019","Amneet Pal Bhalla","CA","San Diego State University Foundation","Standard Grant","Seung-Jong Park","12/31/2024","$499,682.00","","asbhalla@sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.<br/><br/>This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827225","CC* Network Design: Network Upgrades to Improve Engagement in Science Discovery and Education","OAC","Campus Cyberinfrastructure","08/01/2018","07/25/2018","Donna Liss","MO","Truman State University","Standard Grant","Kevin Thompson","07/31/2021","$400,000.00","Jon Beck, Jim McNabb","dliss@truman.edu","100 E. Normal","Kirksville","MO","635014200","6607857245","CSE","8080","9150","$0.00","Truman State University, with the support of the Missouri Research and Education Network (MOREnet) and the University of Missouri, is upgrading the campus network and infrastructure to better enable access to, and use of, scientific data through improved data transfer capabilities for large datasets.  The enhanced infrastructure supports faculty and undergraduate research in understanding star spots, quantifying light pollution in geographical areas, understanding the inhibition mechanisms of drugs to treat global health issues, natural language processing to improve approaches in artificial intelligence, and building low-cost, real-time, soil sensors.  An expanded curriculum that includes hands-on training in cybersecurity and IPv6 technologies is also enabled by utilizing these network resources.  <br/><br/>The improvements include upgrades to the network switch and distribution technologies that result in a ten-fold increase in data transfer and access rates for faculty in STEM-related disciplines, as well as increases in the data transfer bandwidth to the campus observatory, deployment of IPv6 in support of the computer science curriculum, establishment of network performance metrics to inform continued growth, and robust and secure access (through federated identity management) to off-campus research tools and intra-institutional collaboration opportunities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925632","CC* Regional: Sun Corridor Network ? Arizona Community College Research Expansion","OAC","Campus Cyberinfrastructure","08/01/2019","07/17/2019","Steven Burrell","AZ","Northern Arizona University","Standard Grant","Kevin Thompson","06/30/2022","$690,708.00","Morgan Vigil-Hayes, Paul Trujillo","steven.burrell@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","8080","","$0.00","Northern Arizona University (NAU) is a founding member of the Sun Corridor Network (SCN) along with Arizona State University (ASU) and the University of Arizona (UA).  SCN delivers research and education networking to Arizona's research universities through high-speed connections and Internet2. Research and specialized network connectivity at Arizona community colleges are limited and generally unavailable to academic or to specialized technical programs. This project connects one of the nation's largest community college systems and the northern NAU collaborator to SCN. Network expansion to Arizona's community colleges enables research and education connectivity and support to the students and faculty of Maricopa Community College County District's Estrella Mountain Community College (Goodyear, AZ), Chandler-Gilbert Community College (Chandler, AZ), Phoenix College (Phoenix, AZ) and Coconino Community College, (Flagstaff, AZ). The network expansion advances the use of science-oriented workflows, high performance computing in undergraduate research, instrument sharing, STEM education, and homework gap connectivity and introduces wide area and campus networking capacities that align with cybersecurity academic requirements. <br/><br/>NAU and SCN will improve campus network performance, increasing external connectivity to each campus by connecting them to SCN as a regional aggregator. This leverages a strong existing regional relationship with NAU, a leader in rural and online programs and services. The proposal emphasizes outreach to determine needs and requirements, followed by design, workshops, and science network implementation. By building new collaborations and by expanding connectivity, NAU and SCN improve undergraduate science and technical instruction for over 200,000 students in the Phoenix metropolitan area and northern Arizona.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835592","Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure","01/01/2019","09/07/2018","Anthony Castronova","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Seung-Jong Park","12/31/2022","$156,168.00","","acastronova@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392267445","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931328","Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","01/03/2022","Garnet Chan","CA","California Institute of Technology","Standard Grant","Robert Beverly","09/30/2022","$450,000.00","","garnetc@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7925, 8004, 8009, 9216, 9263","$0.00","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies.  However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties.  This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.<br/><br/>This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940209","Collaborative Research: Science-Aware Computational Methods for Accelerating Data-Intensive Discovery: Astroparticle Physics as a Test Case","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Christopher Tunnell","TX","William Marsh Rice University","Continuing Grant","Vyacheslav (Slava) Lukin","09/30/2022","$345,962.00","","tunnell+nsf@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","099Y","062Z","$0.00","The rapid technological advances of the last two decades have ushered in an era of data-rich science for several disciplines.  One such discipline is astroparticle physics, where researchers aim to discover what our Universe is made of by trying to directly detect Dark Matter. This discovery can be hastened if data science tools are used to extract significant domain-specific information from data, and to reliably test scientific hypotheses at scale. The overarching goal of this two-year project is to lay the groundwork for incorporating scientific knowledge into machine learning and data science methods in the context of scientific disciplines in which discovery requires effective, efficient analysis of lots of noisy data gathered by multiple imperfect sensors. In doing so, it not only advances the state-of-the-art in data science, machine learning, and astrophysics, but it also has the potential to accelerate data-driven discoveries in other scientific disciplines where data shares similar characteristics.<br/><br/>This project will develop innovative domain-enhanced data science methods that will be based on probabilistic graphical models and graph-regularized inverse problems. Using the leading astroparticle experiment XENON as a test bed, the investigators will explore and demonstrate approaches for incorporating domain knowledge into machine learning and data science methods. In doing so, the investigators will address major data-analysis challenges in the context of dark matter identification. Additionally, the investigators will invest significant effort reaching out to other data-intensive science communities, such as materials science, oceanography, and meteorology, that can benefit from the new methods and ideas. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808576","CDS&E: Collaborative Research: Strategies for Managing Data in Uncertainty Quantification at Extreme Scales","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/28/2018","Tan Bui-Thanh","TX","University of Texas at Austin","Standard Grant","Tevfik Kosar","08/31/2022","$409,829.00","","tanbui@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8069, 8084","026Z, 8084, 9263","$0.00","The exponential increase in the quantity of measurements and data holds tremendous promise for data-driven scientific discovery and decision making.  In many cases, data-driven scientific discovery is mathematically formulated as an inverse problem.  For inverse problems that serve as a basis for discovery and decision-making for complex problems, the uncertainty in its solutions must be quantified.  Though the past decades have seen tremendous advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty (UQ) in their solutions taking big-data issues into account remains challenging.  This is largely due to computationally demanding nature of existing mathematical techniques that are unable to scale up to the amount of data being generated.  Consequently, much of the available data remains unused.  This project develops UQ algorithms that are both computationally scalable as well as datascalable for making scientific progresses in geosciences and medical imaging. In particular, the proposed methods are evaluated in the context of two challenging data-driven applications: (1) from large amount of seismograms (records of the ground motion) perform geophysical imaging to infer earth's interior structure to better understand earthquakes, and (2) from magnetic resonance (MR) cine images of patients estimate the heart's function (e.g. motion, contraction) to detect early onset of heart disease (cardiomyopathy).<br/><br/><br/>The goal of this collaborative research project is to develop an integrated research program that addresses the data management and data analytics arising from both observations and scientific simulations, with applications from diverse domains at extreme scales. The project develops innovative statistical, mathematical, and parallel computational methods to manage the large amounts of simulation data as well as the ever increasing amounts of observation data required for extreme-scale UQ problems in general and Bayesian inverse problems in particular. These methods will be of immediate practical utility to scientists  and engineers dealing with big data and large-scale UQ problems in sensing-based disciplines, geosciences, climatology, medical imaging, etc. The successful completion of the project would  provide a first step towards the development of mathematical and computational methods for a wide range of data-driven  large-scale inverse and UQ challenges that can lead to original scientific discoveries and promote the progress of science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2114580","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS","10/01/2020","01/04/2021","Pablo Laguna","TX","University of Texas at Austin","Continuing Grant","Amy Walton","08/31/2021","$56,473.00","","pablo.laguna@physics.gatech.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7244","7433, 7569, 8004, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"2031247","Study of Linear Instabilities in Laminar Hypersonic Shock-wave/Boundary-Layer Interactions using DSMC","OAC","Leadership-Class Computing","09/01/2020","05/01/2020","Deborah Levin Fliflet","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2021","$10,000.00","","deblevin@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934598","Collaborative Research: Understanding Subatomic-Scale Quantum Matter Data Using Machine Learning Tools","OAC","HDR-Harnessing the Data Revolu","09/01/2019","08/11/2021","Markus Greiner","MA","Harvard University","Continuing Grant","Daryl Hess","08/31/2021","$788,616.00","Eugene Demler","greiner@physics.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","099Y","062Z","$0.00","A central goal of modern quantum physics is to search for new systems and technological paradigms that utilize quantum mechanical aspects of matter rather than being limited by them. In particular, there is an active search for new materials that exhibit surprising physical properties because of strong interaction between individual electrons that leads to strong correlations in the motion of electrons and as a result, to strongly correlated quantum matter. The study of Strongly Correlated Quantum Matter (SCQM) has reached a tipping point through intense efforts over the last decade that have led to vast quantities of experimental data. The next breakthrough in the field will come from relating these experimental data to theoretical models using tools of data science. However, data-driven challenges in SCQM require a fundamentally new data science approaches for two reasons: first, quantum mechanical imaging is probabilistic; and second, inference from data should be subject to fundamental laws of physics. Hence the new data-driven challenges in the field of SCQM requires ""Growing Convergent Research"" and ""Harnessing the Data Revolution"", two of NSF's Ten Big Ideas. <br/><br/>The objective of the project is to develop and disseminate machine learning (ML) tools that can serve as a two-way highway connecting the data revolution in SCQM experiments at sub-atomic scale to a fundamental theoretical understanding of SCQM. The specific goals are: (1) Develop interpretable ML tools for position space image data; (2) Develop unsupervised ML tools for momentum space scattering data; (3) Design new imaging modality guided by the insight gained from ML; and (4) Integrate ML tools with in-operando human interface to the Cornell High Energy Synchrotron Source (CHESS) beamline. Goals (1) and (2) are within reach, while (3) and (4) are more ambitious visions for scaling up to a future institute that can involve more academic institutions and scattering experiment facilities nationwide. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835526","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Thomas Crawford","IL","University of Chicago","Standard Grant","Bogdan Mihaila","08/31/2021","$39,994.00","","tcrawfor@kicp.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826997","CC* Integration: Delivering a Dynamic Network-Centric Platform for Data-Driven Science (DyNamo)","OAC","CISE Research Resources","08/01/2018","07/19/2018","Anirban Mandal","NC","University of North Carolina at Chapel Hill","Standard Grant","Deepankar Medhi","07/31/2021","$1,000,000.00","Ewa Deelman, Michael Zink, Ivan Rodero, Cong Wang","anirban@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","2890","9102","$0.00","Computational science today depends on many complex, data-intensive applications operating on datasets that originate from a variety of scientific instruments and data stores. A major challenge for data-driven science applications is the integration of data into the scientist's workflow. Recent advances in dynamic, networked cloud infrastructures provide the building blocks to construct integrated, reconfigurable, end-to-end infrastructure that has the potential to increase scientific productivity. However, applications and workflows have seldom taken advantage of these advanced capabilities.<br/>Dynamo will allow atmospheric scientists and hydrologists to improve short- and long-term weather forecasts, and aid the oceanographic community to improve key scientific processes like ocean-atmosphere exchange, turbulent mixing etc., both of which have direct impact on our society.<br/> <br/>The Dynamo project will develop innovative network-centric algorithms, policies and mechanisms to enable programmable, on-demand access to high-bandwidth, configurable network paths from scientific data repositories to national CyberInfrastructure facilities, and help satisfy data, computational and storage requirements of science workflows. This will enable researchers to test new algorithms and models in real time with live streaming data, which is currently not possible in many scientific domains. Through enhanced interactions between Pegasus, the network-centric platform, and new network-aware workflow scheduling algorithms, science workflows will benefit from workflow automation and data management over dynamically provisioned infrastructure. The system will transparently map application-level, network Quality of Service expectations to actions on programmable software defined infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761810","Spokes: MEDIUM: NORTHEAST: Collaborative Research: Data Science Foundry: A Collaborative Platform for Computational Social Science","OAC","Economics, Methodology, Measuremt & Stats","09/01/2018","07/31/2018","Augustin Chaintreau","NY","Columbia University","Standard Grant","Cheryl Eavey","08/31/2021","$250,000.00","","ac3318@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1320, 1333","028Z, 8083, 9102","$0.00","This research project will develop a collaborative data science platform for computational social science called the Data Science Foundry. The collection and management of large-scale data currently is a relatively unstructured process, with data-processing decisions being made in an ad hoc fashion. Society has started to rely on data-driven science to address policy-related questions, however. The development of a collaborative platform that provides structure will allow social scientists to collaborate and validate each other's studies.  This project has the potential to transform how studies are designed and how data will be processed. The collaborative platform will result in a higher level of trust in the studies conducted via the collaborative curation of study design, procedures, and validation. The collaborative platform also will increase the number of studies that can be done in a short span of time. The platform will be developed as open-source, thereby facilitating interactions with the community and enabling different institutions to install the program.<br/><br/>This project will develop a collaborative platform that social scientists can use to collaborate and validate each other's studies. The investigative team will attempt to identify the best possible collaborative model for data-driven social science, determine how automation can most enhance the studies, and develop explicit and implicit mechanisms to establish trust in end-to-end data processing pipelines and the results they generate. To aid in the platform's development, the research team will focus on the prediction of outcomes from surveys, a specific yet widely applicable type of problem within computational social science. This class of problems involves much subjective assessment during the feature engineering state as well as copious interpretation during the data transformation stage. These unique challenges will benefit both from a collaborative workflow and from mechanisms that enable trust in the eventual results. The project will bring together three distinct teams to develop this platform: computer scientists to develop abstractions, APIs and systems; statisticians to help with methods and study design; and social scientists to help define the problems and workflow and to provide user feedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031910","Frontera Travel Grant: Study of How Membrane Properties Control Enveloped Viral Entry","OAC","Leadership-Class Computing","09/01/2020","06/03/2020","Peter Kasson","VA","University of Virginia Main Campus","Standard Grant","Edward Walker","08/31/2021","$8,075.00","","kasson@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1947440","BD Spokes: SPOKE: NORTHEAST: Collaborative: A Licensing Model and Ecosystem for Data Sharing","OAC","BD Spokes -Big Data Regional I","09/01/2019","09/23/2019","Tim Kraska","MA","Massachusetts Institute of Technology","Standard Grant","Martin Halbert","08/31/2021","$270,912.00","","kraska@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","024Y","026Z, 028Z, 7433, 8083, 9150","$0.00","Sharing of data sets can provide tremendous mutual benefits for industry, researchers and nonprofit organizations. For example, companies can profit from the fact that university researchers explore their data sets and make discoveries, which help the company to improve their business. At the same time, researchers are always on the search for real world data sets to show that their newly developed techniques work in practice. Unfortunately, many attempts to share relevant data sets between different stakeholders in industry and academia fail or require a large investment to make data sharing possible. A major obstacle is that data often comes with prohibitive restrictions on how it can be used (requiring e.g., the enforcement of legal terms or other policies, handling data privacy issues, etc.). In order to enforce these requirements today, lawyers are usually involved in negotiation the terms of each contract. It is not atypical that this process of creating an individual contract for data sharing ends up in protracted negotiations, which are both disconnected from what the actual stakeholders aim to do and fraught as both sides struggle with the implications and possibilities of modern security, privacy, and data sharing techniques. Worse, fear of missing a loophole in how the data might be (mis)used often prevents many data sharing efforts from even getting off the ground. To address these challenges, our new data sharing spoke will enable data providers to easily share data while enforcing constraints on the use of the data. This effort has two key components:(1) Creating a licensing model for data that facilitates sharing data that is not necessarily open or free between different organizations and (2) Developing a prototype data sharing software platform, ShareDB, which enforces the terms and restrictions of the developed licenses. We believe these efforts will have a transformative impact on how data sharing takes place. By moving data out of the silos of individuals and single organizations and into the hands of broader society, we can tackle many societally significant problems.<br/><br/>This new data sharing spoke will enable data providers to easily share data while enforcing constraints on the use of the data. Many services and platforms that provide access to data sets exist already today. However, these platforms generally promote completely open access and do not address the aforementioned issues that arise when dealing with proprietary data. Thus, the effort has three key components: (1) Creating a licensing model for data that facilitates sharing data that is not necessarily open or free between different organizations and (2) developing a prototype data sharing software platform, ShareDB, which enforces the terms and restrictions of the developed licenses, and (3) developing and integrating relevant metadata that will accompany the datasets shared under the different licenses, making them easily searchable and interpretable. To ensure that the developed tools and licenses are useful, the project will form the Northeast Data Sharing Group, comprising of many different stakeholders to make the licensing model widely accepted and usable in many application domains (e.g., health and finance). The intellectual merit of this proposal is to design a licensing model and a data sharing platform that is widely accepted and usable as a template in many different domains. While there exist other efforts to enable data sharing (e.g., Creative Commons), they focus on the case where the data owner is willing to openly share the data on the Internet. This licensing model and the ecosystem is different since it allows data owners to enforce certain requirements stated in a data sharing agreement (e.g., on who is allowed to access the data) and also provides tools to make data sharing of sensitive information safe. The licenses and software we propose to investigate will make it easier for organizations to open up their data to the appropriate organizations, while maintaining the ability to ensure it is protected, that access is revocable, and that access controls and audit logs are maintained."
"1917383","EAGER: Curricula Development of a Quantum Programming Class with Hardware Access","OAC","CYBERINFRASTRUCTURE","09/01/2019","08/29/2019","Frank Mueller","NC","North Carolina State University","Standard Grant","Robert Beverly","08/31/2021","$100,000.00","Gregory Byrd, Patrick Dreher","mueller@cs.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","7231","026Z, 7916, 8004","$0.00","The project aims to transform the landscape of computing by establishing and advancing quantum computing as a means of computational acceleration by developing curricular materials that uses actual quantum hardware.  The effort has the potential significantly advance the knowledge about quantum computing of the scientific community in general and within the areas of Computer Science and Engineering specifically. <br/><br/>This project will utilize free cloud-based access to a gate-based platform to provide hands-on experience with programming actual quantum hardware. Curricular material will include the fundamentals in physics and mathematics required to understand quantum computing, introductory material to the quantum field, and the programming environment for a cloud-based platform. The PIs will also develop training material suitable for tutorials at major conferences/symposia across different fields as well as online courses for faculty, staff and students. As a means to gauge success, the suitability of the material will be thoroughly evaluated statistically via surveys at the end of educational units for both classes and tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842703","DETER Research Education and Operations Mission Sustainment","OAC","Special Projects - CNS, Cybersecurity Innovation","09/01/2018","08/30/2018","Terry Benzel","CA","University of Southern California","Standard Grant","Robert Beverly","08/31/2021","$2,000,000.00","Jelena Mirkovic, Erik Kline","tbenzel@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1714, 8027","","$0.00","The project advances the sophistication, robustness and usefulness of the DETERLab testbed, a key scientific laboratory for cybersecurity research and education.  Advances in the research, educational and operational missions of the testbed directly benefit the hundreds of research projects, and thousands of classroom users currently using DETERLab. All proposed activities directly impact users, enabling them to experiment faster, more easily, with higher fidelity and repeatability, and in more sophisticated environments. The research and development efforts taken together  expand capacity for both the research and education communities and are necessary to meet the growing need for cybersecurity professionals in our country today, and the growing need for repeatability, reproducibility and reuse of testbed experiments.<br/><br/>The project is comprised of 16 tasks across three mission objectives to (1) advance  DETERLab's research mission  - these include operationalization of new experimental capabilities, and addition of new tools for users, to allocate and manage experiments, (2) activities that  advance DETERLab's education mission  - these include creation of containerized versions of existing exercises, creation of new shared materials from various sources within and outside of DETERLab, and customization of lab exercises for each student, to prevent cheating, (3) activities that advance DETERLab's operational mission  - these activities improve DETERLab's security, streamline experiment allocation and management, and allow for fast and automated handling of many errors. Many of the planned activities already have 1st generation prototypes, that are  improved, generalized and operationalized under the project. All the source and binary code developed under this proposal is released publicly. All experimental and educational materials are released via DETERLab's sharing portal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835530","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, Data Cyberinfrastructure","01/01/2019","06/17/2020","Lucy Fortson","MN","University of Minnesota-Twin Cities","Standard Grant","Amy Walton","12/31/2022","$1,134,850.00","Craig Packer, Christopher Lintott, Daniel Boley","fortson@physics.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/><br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840041","CICI: SSC: Robust and Secure Internet Infrastructure for Scientific Collaboration","OAC","Cybersecurity Innovation","09/01/2018","07/31/2019","Amir Herzberg","CT","University of Connecticut","Standard Grant","Robert Beverly","08/31/2021","$704,068.00","Richard Jones, Bing Wang","amir.herzberg@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","8027","9251","$0.00","Scientific collaboration increasingly relies on the Internet as a critical communication infrastructure. However, the Internet is vulnerable to congestion and Denial-of-Service (DoS) attacks, which can cause devastating losses of research resources and results, disrupting scientific collaboration, as well as to other services. The goal of this project is to develop and deploy defenses to facilitate robust and secure Internet communication for scientific collaboration. The developed defenses will address two major connectivity concerns: minimizing congestion by mitigating amplification DoS attacks, and preventing route hijacking, that is, circumventing devastating attacks on inter-domain routing that can prevent communication between remote sites. Throughout this research, the PIs work closely with the network operators of the University of Connecticut, the Connecticut Educational Network, and other Research and Education networks. The project generates tested tools, protocols and free open- source software, which can be directly integrated into operational networks. The protection mechanisms developed in this project significantly benefits the scientific community and the society in general.<br/><br/>This project addresses two of the biggest, long-standing challenges to Internet Infrastructure robustness and security. Firstly, the project develops new defenses against amplification bandwidth-DoS attacks; these attacks and subsequent tools are particularly relevant to university networks. These defenses combine the emerging Resource Public Key Infrastructure (RPKI) standard, with an improved Reverse Path Forwarding (RPF) technique, to effectively filter spoofed packets received from peers. Secondly, the project develops, implements and deploys an improved RPKI validator for secure inter-domain routing. This validator addresses deployment concerns and challenges, in particular concerns about false positives, and provides additional security and functionality improvements to facilitate wide deployment.  Amongst RPKI's benefits, correct deployment of RPKI prevents route hijacking, which is the most common type of attacks on Internet inter-domain routing. Route hijacking may be abused in many ways, including eavesdropping, spoofing and preventing communication between remote sites.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839868","CICI: RDP: Open Badge Researcher Credentials for Secure Access to Restricted and Sensitive Data","OAC","Cybersecurity Innovation","09/01/2018","09/27/2018","Margaret Levenstein","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Robert Beverly","08/31/2021","$881,342.00","Libby Hemphill, Florian Schaub, Andrea Thomer","maggiel@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8027","","$0.00","This project reduces the complexity in protecting research data by developing and piloting an open badge research credential system (OBRCS). Open badges are visual tokens that signal achievement, affiliation, authorization, or another trust relationship and are shareable across the web. The challenges of managing and protecting restricted data mean that data providers are often wary of sharing sensitive data or that data ends up in the wrong hands, and potential gains to society and science from using those data go unrealized. OBRCS allows researchers to present their evolving credentials openly and to record their achievements and credentials publicly and enables more collaboration, facilitates data re-use, and supports replication efforts. OBRCS benefits the scientific community by ensuring the integrity, resilience, and reliability of research data.<br/><br/>Combining and analyzing collections of data enable scientific breakthroughs. Efficient, secure data sharing and reuse facilitates collaboration and replication, leading to better science. However, managing different access policies, authenticating, and authorizing access to sensitive data is a challenge faced by all data management organizations. Unauthorized access threatens the integrity of data and the privacy of study participants and these threats can impact the conclusions researchers draw. The project achieves these goals through three main activities: (a) develops an open badge system for managing researcher credentials, (b) articulates levels of data sensitivity and risk that indicate criteria for access, and (c) identifies the right balance between openness and privacy for data users in a restricted data access system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018500","Framework: Sofware: Collaborative Research: CyberWater -An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure","01/01/2020","02/07/2020","Yang Zhang","MA","Northeastern University","Standard Grant","Seung-Jong Park","12/31/2022","$87,522.00","","ya.zhang@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7726","062Z, 077Z, 7925, 8004","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835768","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","06/02/2020","Matthew Hasselfield","PA","Pennsylvania State Univ University Park","Standard Grant","Bogdan Mihaila","08/31/2021","$40,000.00","","mhasse@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839024","EAGER: Collaborative Research: Synchronization Between Terrestrial and Aquatic Ecosystems","OAC","NSF Public Access Initiative","09/01/2018","08/13/2018","Michael Pace","VA","University of Virginia Main Campus","Standard Grant","Martin Halbert","08/31/2021","$163,673.00","","pacem@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7414","7916","$0.00","Aquatic ecosystems are closely connected to their surrounding watersheds through the flux of water, nutrients, and organic carbon.  Similarly, the flux of exogenous carbon from the landscape causes lakes to be substantial sources of greenhouse gases as well as sinks for organic carbon buried in the sediment.  There are also important fluxes from inland waters to the terrestrial ecosystems. Despite the recognition of the importance of terrestrial-aquatic coupling, synchronization (persistent relatedness) of dynamics between these ecosystems has not been broadly investigated.  Employing data reuse techniques to use data from Lake Multiscaled Geospatial and Temporal Database (LAGOS) and Global Lake Ecological Observatory Network (GLEON) databases, the PIs will apply new analysis tools to answer questions related to the synchrony of lakes and their surrounding watersheds.<br/> <br/>To quantify synchronization between terrestrial and aquatic habitats wavelet coherence will be used to measure the strength of synchronization between terrestrial and lake ecosystems, as well as phase relationships, describing time lags. Time lags are hypothesized to reflect mechanisms of synchrony and may be related to lake size, water residence time, trophic status, and watershed area. These factors are hypothesized to affect the degree of aquatic-terrestrial synchrony. Random forest regression will be applied to test this idea, leveraging the range of lake and watershed properties. Multiple regression for wavelet transforms will be used to determine the fraction of aquatic-terrestrial synchrony that can be explained by climate drivers and assess synchrony between terrestrial ecosystems and lake nutrient fluctuations for improved inference into non-climate mechanisms of synchrony.<br/> <br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760052","Spokes: MEDIUM: NORTHEAST: Collaborative Research: Data Science Foundry: A Collaborative Platform for Computational Social Science","OAC","BD Spokes -Big Data Regional I","09/01/2018","07/31/2018","Matthew Salganik","NJ","Princeton University","Standard Grant","Cheryl Eavey","08/31/2021","$250,000.00","","mjs3@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","024Y","028Z, 8083, 9102","$0.00","This research project will develop a collaborative data science platform for computational social science called the Data Science Foundry. The collection and management of large-scale data currently is a relatively unstructured process, with data-processing decisions being made in an ad hoc fashion. Society has started to rely on data-driven science to address policy-related questions, however. The development of a collaborative platform that provides structure will allow social scientists to collaborate and validate each other's studies.  This project has the potential to transform how studies are designed and how data will be processed. The collaborative platform will result in a higher level of trust in the studies conducted via the collaborative curation of study design, procedures, and validation. The collaborative platform also will increase the number of studies that can be done in a short span of time. The platform will be developed as open-source, thereby facilitating interactions with the community and enabling different institutions to install the program.<br/><br/>This project will develop a collaborative platform that social scientists can use to collaborate and validate each other's studies. The investigative team will attempt to identify the best possible collaborative model for data-driven social science, determine how automation can most enhance the studies, and develop explicit and implicit mechanisms to establish trust in end-to-end data processing pipelines and the results they generate. To aid in the platform's development, the research team will focus on the prediction of outcomes from surveys, a specific yet widely applicable type of problem within computational social science. This class of problems involves much subjective assessment during the feature engineering state as well as copious interpretation during the data transformation stage. These unique challenges will benefit both from a collaborative workflow and from mechanisms that enable trust in the eventual results. The project will bring together three distinct teams to develop this platform: computer scientists to develop abstractions, APIs and systems; statisticians to help with methods and study design; and social scientists to help define the problems and workflow and to provide user feedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103596","Elements: An Infrastructure for Software Quality and Security Issues Detection and Correction","OAC","Software Institutes","08/15/2021","08/23/2021","Marouane Kessentini","MI","Regents of the University of Michigan - Dearborn","Standard Grant","Tevfik Kosar","07/31/2024","$599,999.00","Birhanu Mekuria Eshete","kessentini@oakland.edu","4901 EVERGREEN RD","Dearborn","MI","481282406","7347636438","CSE","8004","077Z, 7923, 8004, 9102","$0.00","Research into more effective software development has the potential to make the infrastructure on which so many aspects of society depend less costly and more secure in the scientific community, industry and government agencies. In particular, the scientific community is proposing millions of scientific software prototypes to enable reproducibility of research results in almost every domain. Scientists may frequently introduce security and quality issues into existing scientific software via their code changes due to their limited experience in software quality and security and the lack of tools for quality and security assessments that can be easily used and integrated in programming environments. Thus, several existing scientific software projects are difficult to 1) extend by scientists due to their poor quality and 2) deploy by industry due to the likelihood of security vulnerabilities and the bad development practices used. Without a unified and easy-to-integrate framework for detecting, fixing, and documenting vulnerability and quality issues in scientific projects, the reusability, extendibility, safe deployment, and technology transfer of scientific projects will remain limited. This project builds a sustainable, community-driven software security and quality analysis framework. These tools enable more scientists to build better software and to transfer their prototypes to industry by following the best software development practices. Its integrated education plan will bring undergraduate and graduate computer science students more awareness and expertise in the evolution of software systems, including security and quality issues.<br/><br/>This project develops a framework for detecting, fixing, and documenting security and quality issues. It will continuously monitor the software repository to identify security vulnerabilities and quality issues based on static and dynamic analyses, and then find the best sequence of code changes to prioritize and fix them. The developers can review the recommendations and their impacts in a detailed report and select the code changes that they want to apply. The framework includes a visualization support of the quality and security changes over the evolution of the project. Furthermore, non-expert programmers from the scientific community can use the automatically generated documentation by the framework to understand the severity of the detected issues and necessary code changes to fix them. The project has the potential to revolutionize how developers monitor the evolution of their systems in continuous integration environments by unifying security and quality issues detection and correction and enabling their automated documentation. All tools and methodologies will be empirically evaluated in collaboration with scientists from various domains. These tools will enable more scientists to build better software and transfer their prototypes to industry by following best development practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039822","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","09/01/2019","10/14/2020","Rayid Ghani","PA","Carnegie-Mellon University","Standard Grant","Wendy Nilsen","08/31/2021","$46,222.00","","rayid@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","024Y","028Z, 8083","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities. (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925717","CC* Compute:  A high-performance GPU cluster for accelerated research","OAC","Campus Cyberinfrastructure","10/01/2019","09/12/2019","Kris Delaney","CA","University of California-Santa Barbara","Standard Grant","Kevin Thompson","09/30/2021","$394,804.00","Tresa Pollock, M. Scott Shell, Yufei Ding","kdelaney@mrl.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","8080","","$0.00","The exponential growth of computing power and the emergence of high-performance computing paradigms has revolutionized all fields of science and engineering. Graphics processing unit (GPU) hardware, a type of highly parallel co-processor originally designed for generating 3D scenes in video games, has been increasingly leveraged over the last decade to dramatically accelerate scientific computing workloads. This project is for the acquisition of a GPU compute cluster consisting of 24 state-of-the-art NVIDIA Tesla V100 32 GB GPUs with fast inter-GPU communication. The resource is housed at the University of California, Santa Barbara (UCSB), and is accessible to researchers across campus and externally through a connection to the Pacific Research Platform/Nautilus federated systems network.<br/><br/>Initial research activities on the facility span the computational realm, including: a new type of multi-scale molecular simulation for predicting structural and thermodynamic properties of complex polymeric solution formulations; a materials characterization thrust involving crystal orientation indexing with real-time instrument feedback control; and the development of a scalable Neural Architecture Search framework for automatic generation of Deep Neural Network models for scientific applications of machine learning. The cluster provides a significant resource for educating the next generation of computational scientists in the latest GPU-computing techniques. Undergraduates, high-school students, and K-12 teachers will also have access via existing campus-sponsored programs: Research Experience for Teachers (RET), California Alliance for Minority Participation (CAMP), and the Center for Science and Engineering Partnerships (CSEP). These programs serve to provide training and increase the number of under-represented students in STEM fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835661","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","XC-Crosscutting Activities Pro, Data Cyberinfrastructure","01/01/2019","08/27/2018","Diego Melgar","OR","University of Oregon Eugene","Standard Grant","Amy Walton","12/31/2022","$405,494.00","","dmelgarm@uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7222, 7726","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839018","Optimizing  Openness in Human Participants Research: Harmonizing Standards for Consent Agreements and Data Management Plans to Empower the Reuse of Sensitive Scientific Data","OAC","NSF Public Access Initiative","10/01/2018","08/15/2018","Colin Elman","NY","Syracuse University","Standard Grant","Martin Halbert","09/30/2022","$299,787.00","Margaret Levenstein, Diana Kapiszewski, Lynette Hoelter","celman@maxwell.syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7414","7916","$0.00","Recent technological advances allow for an increase in the amount of sensitive human participants data that can be safely shared. Institutional Review Boards' (IRBs) traditional reluctance to allow the sharing of such data unnecessarily constrains their long-term reuse. This project seeks to close the gap between modern options for safely sharing sensitive data and IRB practices by establishing socio-technical infrastructure to support a sustained dialogue and productive partnerships between data repositories and IRBs with the objective to pilot new consensus guidance, protocols, and templates on the sharing and long-term re-use of sensitive data generated through research with human participants.<br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835792","Collaborative Research: Elements: Software: NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Tobin Isaac","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2021","$299,136.00","","tisaac@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales. The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments. This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses. The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis. This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability. The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization. The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840069","CICI: RDP: SAMPRA: Scalable Analysis, Management, and Protection of Research Artifacts","OAC","Cybersecurity Innovation","09/01/2018","06/16/2020","Patrick Bridges","NM","University of New Mexico","Standard Grant","Robert Beverly","08/31/2021","$598,594.00","Vince Calhoun, Vincent Clark, Kevin Comerford, Jonathan Wheeler","patrickb@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","8027","9150","$0.00","Current computing systems that support research on sensitive data, such as personally identifiable information, are frequently single-purpose and rely on ad-hoc approaches to data protection and management.  This project develops system called SAMPRA: Scalable Analysis, Management, and Protection of Research Artifacts.  SAMPRA's goal is to provide a compliant research computing platform that supports diverse, inter-disciplinary, collaborative research on protected data.  SAMPRA leverages modern virtualization technology to enable the decentralized management of protected computing enclaves that can be customized to the needs of each specific research project. In addition, the project trains researchers and students on best practices for managing and analyzing protected data, and technical staff on how to customize environments to the needs of individual research groups.<br/><br/>SAMPRA investigates multiple techniques to meet these goals, with the overall technical goal of understanding the technical and administrative tradeoffs between isolating and sharing protected research infrastructure services. First, SAMPRA systematically virtualizes hardware, software, and network resources to provide a flexible system architecture that supports research computing with varying analysis, management, and protection needs. SAMPRA also provides virtual data transfer nodes to interface protected environments with external data acquisition systems, with the goal of supporting modern data-intensive research projects using central institutional resources. The project develops exemplar computing, data analysis, and data management virtual environments, and integrating these with institutional systems for managing protected data. These exemplar systems are also the examples used in workshops that train researchers on the use of SAMPRA to support research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837847","Accelerating Public Access to Research Data by Research Universities","OAC","NSF Public Access Initiative","09/01/2018","06/11/2018","Kacy Redd","DC","Association of Public and Land-Grant Universities","Standard Grant","Alejandro Suarez","08/31/2021","$49,687.00","James Reecy, Tobin Smith","kredd@aplu.org","1220 L St., NW","Washington","DC","200050000","2024786084","CSE","7414","7556","$0.00","Creating a robust system for public access to research data and findings requires the active engagement of researchers and their institutions, research sponsors, community data repositories, disciplinary societies, and others.  Building on a widely recognized AAU/APLU Public Access Working Group report, this workshop will convene 20-30 teams of university leaders and researchers to learn about current data sharing practices and emerging tools, evaluate options for supporting and promoting public access to research data on their campuses, and begin cross-institutional discussions to create an interoperable system that accelerates progress in supporting public access to research data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761805","Spokes: SMALL: NORTHEAST: Collaborative: Building the Community to Address Data Integration of the Ecological Long Tail","OAC","BD Spokes -Big Data Regional I","09/15/2018","09/06/2018","Kevin Rose","NY","Rensselaer Polytechnic Institute","Standard Grant","Martin Halbert","08/31/2021","$20,126.00","","rosek4@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","024Y","028Z, 8083","$0.00","Frequently research on data integration carried out by computer scientists and resulting tools must be modified to fit the needs of domain practitioners (ecologists in this case). This challenge is a socio-technical, collective action problem that can be addressed through a combination of tools and incentives. The project proposes to holding a series of workshops along with proofs-of-concept implementations. These workshops will result in approaches to decentralize the sharing of data in the long tail, through socio-technical approaches that appropriately incentivize and facilitate data integration by smaller labs. Such an interdisciplinary community will provide crucial real-world input to computer science researchers, which will give their research into tools the potential for larger impact in ecological practice and will yield better tools for ecologists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761812","Spokes: MEDIUM: NORTHEAST: Collaborative Research: Data Science Foundry: A Collaborative Platform for Computational Social Science","OAC","BD Spokes -Big Data Regional I","09/01/2018","07/31/2018","Devavrat Shah","MA","Massachusetts Institute of Technology","Standard Grant","Cheryl Eavey","08/31/2021","$500,000.00","Munther Dahleh, Alberto Abadie, Kalyan Veeramachaneni","devavrat@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","024Y","028Z, 8083, 9102","$0.00","This research project will develop a collaborative data science platform for computational social science called the Data Science Foundry. The collection and management of large-scale data currently is a relatively unstructured process, with data-processing decisions being made in an ad hoc fashion. Society has started to rely on data-driven science to address policy-related questions, however. The development of a collaborative platform that provides structure will allow social scientists to collaborate and validate each other's studies.  This project has the potential to transform how studies are designed and how data will be processed. The collaborative platform will result in a higher level of trust in the studies conducted via the collaborative curation of study design, procedures, and validation. The collaborative platform also will increase the number of studies that can be done in a short span of time. The platform will be developed as open-source, thereby facilitating interactions with the community and enabling different institutions to install the program.<br/><br/>This project will develop a collaborative platform that social scientists can use to collaborate and validate each other's studies. The investigative team will attempt to identify the best possible collaborative model for data-driven social science, determine how automation can most enhance the studies, and develop explicit and implicit mechanisms to establish trust in end-to-end data processing pipelines and the results they generate. To aid in the platform's development, the research team will focus on the prediction of outcomes from surveys, a specific yet widely applicable type of problem within computational social science. This class of problems involves much subjective assessment during the feature engineering state as well as copious interpretation during the data transformation stage. These unique challenges will benefit both from a collaborative workflow and from mechanisms that enable trust in the eventual results. The project will bring together three distinct teams to develop this platform: computer scientists to develop abstractions, APIs and systems; statisticians to help with methods and study design; and social scientists to help define the problems and workflow and to provide user feedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2047127","CAREER: Forward and Inverse Uncertainty Quantification of Cardiovascular Fluid-Structure Dynamics via Multi-fidelity Physics-Informed Bayesian Geometric Deep Learning","OAC","CAREER: FACULTY EARLY CAR DEV","07/15/2021","07/09/2021","Jian-Xun Wang","IN","University of Notre Dame","Continuing Grant","Alan Sussman","06/30/2026","$324,896.00","","jwang33@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1045","026Z, 075Z, 079Z, 1045","$0.00","Image-based computational models of the cardiovascular system play an increasingly important role in advancing the fundamental understanding of cardiovascular physiology and supporting clinical diagnosis and treatment planning. However, traditional models are primarily based on well-posed physics that are solved numerically, and their reliability is limited because of unknown or uncertain modeling conditions. On the other hand, sparse and noisy data have become increasingly available thanks to the rapid development of medical imaging techniques (e.g., flow MR images), which can be utilized for model inference and uncertainty reduction. Hence, forward uncertainty quantification and inverse data assimilation in cardiovascular simulations are of paramount importance to enhancing predictive confidence and prompting clinical translation efforts. This project will develop computational cyberinfrastructure for data-enabled forward and inverse stochastic cardiovascular modeling by leveraging recent advances in scientific machine learning. The project aims to establish a novel paradigm of data-augmented cardiovascular fluid-structure simulations, which could help transform personalized cardiovascular diagnostics/therapeutics, leading to higher quality of life. Moreover, this research program will also try to address long-standing challenges in effectively engaging students in STEM education across K-12, undergraduate, and graduate education by promoting an interactive and inclusive learning strategy. In particular, the PI will (1) design pedagogical software using physics-informed transfer learning for rapid interactive fluid simulation based on hand-drawn sketches; (2) develop new modules on Artificial Intelligence & Mechanics for U.S. Department of Education TRiO programs to engage K-12 students from low-income families in emerging interdisciplinary STEM fields. <br/><br/>The overarching goal of this CAREER program is to pioneer a scalable and transformative computational cyberinfrastructure for forward and inverse uncertainty quantification (UQ) of cardiovascular modeling based on physics-informed Bayesian geometric deep learning, leveraging physics/physiological knowledge to enable efficient probabilistic learning with sparse and noisy data. This project tackles the fundamental challenges faced by the traditional paradigm of modeling cardiovascular fluid-structure interaction (FSI) dynamics. In the proposed framework, geometric deep learning models will be constructed based on both (partially) known physics and sparse measurement data in a Bayesian manner, enabling efficient forward and inverse FSI simulations with quantified uncertainties. Specifically, the PI will (1) formulate a variational PDE-informed, discretization-based learning framework using graph convolutional networks and use a reduced basis to constrain the dimension of the solution space, facilitating network training; (2) enable high-dimensional UQ capability of the proposed learning framework based on scalable variational Bayesian inference; (3) establish a multi-fidelity meta-learning strategy to parameterize solutions in the physical parameter space for rapid surrogate modeling, on the path to real-time cardiovascular simulations. The fast inference speed, strong expressibility, and GPU parallelization of deep learning models will be exploited to enable large-scale stochastic FSI simulations with patient-specific geometries. This project will build a solid foundation for developing the next-generation computational cyberinfrastructure of cardiovascular FSI modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2121896","EAGER: Collaborative Research: Synchronization Across Terrestrial and Aquatic Ecosystems","OAC","NSF Public Access Initiative","07/01/2021","06/24/2021","Grace Wilkinson","WI","University of Wisconsin-Madison","Standard Grant","Martin Halbert","02/28/2022","$30,455.00","","gwilkinson@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7414","7916","$0.00","Aquatic ecosystems are closely connected to their surrounding watersheds through the flux of water, nutrients, and organic carbon.  Similarly, the flux of exogenous carbon from the landscape causes lakes to be substantial sources of greenhouse gases as well as sinks for organic carbon buried in the sediment.  There are also important fluxes from inland waters to the terrestrial ecosystems. Despite the recognition of the importance of terrestrial-aquatic coupling, synchronization (persistent relatedness) of dynamics between these ecosystems has not been broadly investigated.  Employing data reuse techniques to use data from Lake Multiscaled Geospatial and Temporal Database (LAGOS) and Global Lake Ecological Observatory Network (GLEON) databases, the PIs will apply new analysis tools to answer questions related to the synchrony of lakes and their surrounding watersheds.<br/> <br/>To quantify synchronization between terrestrial and aquatic habitats wavelet coherence will be used to measure the strength of synchronization between terrestrial and lake ecosystems, as well as phase relationships, describing time lags. Time lags are hypothesized to reflect mechanisms of synchrony and may be related to lake size, water residence time, trophic status, and watershed area. These factors are hypothesized to affect the degree of aquatic-terrestrial synchrony. Random forest regression will be applied to test this idea, leveraging the range of lake and watershed properties. Multiple regression for wavelet transforms will be used to determine the fraction of aquatic-terrestrial synchrony that can be explained by climate drivers and assess synchrony between terrestrial ecosystems and lake nutrient fluctuations for improved inference into non-climate mechanisms of synchrony.<br/> <br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018873","CC* Team: CAREERS: Cyberteam to Advance Research and Education in Eastern Regional Schools","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","09/21/2021","Andrew Sherman","CT","Yale University","Continuing Grant","Kevin Thompson","06/30/2023","$1,238,420.00","Christopher Carothers, Gaurav Khanna, Wayne Figurelle, Karlis Kaugars, Galen Collier","andrew.sherman@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7231, 8080","","$0.00","Given the pivotal role of data and cyberinfrastructure (CI) in scientific discovery and teaching, it is essential that all small and mid-sized institutions be empowered to fully exploit them. Access to physical infrastructure is certainly required, but researchers also need access to ?Research Computing Facilitators? (RCFs) possessing the mix of technical knowledge and interpersonal skills required to help them use CI resources effectively. This poses challenges for smaller institutions, since RCFs are in short supply and are difficult to recruit and retain. Moreover, institutions can often afford only one or two, making it challenging to support diverse scientific disciplines.<br/> <br/>This project is developing a sustainable distributed approach to address these challenges in six Eastern states, facilitated by the Eastern Regional Network, a nascent, but growing collaboration among this project?s seven anchor institutions, other institutions in the Eastern US, and the area?s regional network providers. The project strategy has two principal legs: (1) expanding the RCF talent pipeline by engaging students at smaller institutions in nearly 70 project-based mentored experiential learning opportunities; and (2) developing a regional RCF pool providing CI facilitation across institutional and geographic boundaries.<br/> <br/>Success of this project directly enhances scientific research at smaller institutions. The regional RCF pool enables researchers to access appropriate expertise without the costs and delays of building an institutional RCF team. The experiential training approach exposes a relatively large, diverse group of students to the RCF profession, yielding opportunities to encourage trainees, especially in underrepresented groups, to pursue RCF careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934725","DELTA: Descriptors of Energy Landscape by Topological Analysis","OAC","PROJECTS","09/01/2019","10/20/2020","Aurora Clark","WA","Washington State University","Continuing Grant","Rebecca Peebles","02/28/2022","$1,600,000.00","Markus Pflaum, Yang Zhang, Ravishankar Sundararaman, Henry Adams","auclark@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","099y, 1978","062Z, 8037, 9216, 9263","$0.00","Twenty years ago, machine learning (ML) began a trajectory of theoretical/algorithmic improvements that have led to advanced materials for energy efficiency and molecular machines that synthesize molecules in ways unfathomable by the human hand. Those key advances were based upon a foundation of statistical methods that now mirror the field of topological data analysis (TDA) - which combines algebraic topology with computational methods to extract new knowledge by characterizing the global shape of data. Professor Clark at Washington State University, Professor Adam at Colorado State, Professor Pflaum at University Colorado Boulder, Professor Sundararaman at Rensselaer Polytechnic Institute, and Professor Zhang at University of Illinois Urbana Champagne are developing the Institute for Data-Intensive Research in Science and Engineering - Frameworks entitled ""Descriptors of Energy Landscapes Using Topological Data Analysis"" (DELTA).  They are working on advancing TDA for the study of intensive and complex data sets found in Chemistry by focusing upon the development of methods and software tools that characterize the function that describes energy flow during chemical transformations, known as the energy landscape. Scalable and extensible TDA tools are used to extract new information from the energy landscape, understanding how it changes under different applied conditions and supporting a new paradigm in Chemistry, including the long-standing challenge of real-time optimization and control of chemical systems. At the intersection of Math, Data Science, and Chemistry, students trained under DELTA and its collaborative partners develop the skills and the foundation for a new community of practice.<br/><br/>Chemists generally do not know how the underlying energy landscape of transformation changes as a function of system conditions, nor are there quantifiable relationships between intra- and intermolecular interactions and its topological features. Topological data analysis (TDA) is uniquely poised to extract new information from the energy landscape (EL), as it combines algebraic topology with computational methods to characterize its global shape of data. The Descriptors of Energy Landscapes Using Topological Data Analysis (DELTA) Institute Frameworks adapts TDA for chemistry applications, invoking persistent homology, Morse theory, catastrophe theory, and other topological descriptors and creating new software tools that are accessible by domain experts. Tackling the 3N-dimensional energy surface necessitates scalable and extensible tools that first reduce its dimensionality (Objective 1), then yield geometric and topological descriptors that quantify the way in which the EL is perturbed under different chemical conditions (Objective 2). This provides the basis for new predictive methods that accelerate sampling of large regions of the EL and have have learned how to optimize landscape topology to control the fate of reacting molecules and phase behavior (Objective 3). <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity. The effort is jointly funded by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817573","NSCI: Advancing U.S. Competitiveness through Public-Private Partnerships for Advanced Computing","OAC","Software Institutes","05/01/2018","04/23/2018","Charles Evans","DC","Council on Competitiveness","Standard Grant","Tevfik Kosar","04/30/2020","$453,997.00","","cevans@compete.org","900 17th ST NW","Washington","DC","200062515","2026824292","CSE","8004","026Z, 8004, 8005","$0.00","Advanced computing is a foundational technology that has an enormous and growing impact on America's science, security and economic interests - all of which are interrelated. This project aims to: identify strategic public-private research areas for American leadership in advanced computing, optimize the way the public and private sector collaborate so research advances are deployed in meaningful ways that serve America's security and prosperity, and engage many stakeholder groups and help them prepare a diverse community of Americans to lead this shift in computing. By helping the United States field a more competitive advance computing ecosystem, the project would also advance scientific knowledge that underpins that same security and prosperity, and build the resulting industry of the future - along with the accompanying jobs - in the United States.<br/><br/>High performance computing represents a foundational technology and business asset for improving United States? competitiveness. However, the United States, a long-time leader in the supercomputing realm, has been falling behind other nations in the development of advanced computing resources potentially ceding leadership in this space and as the global innovation leader to other countries investing in the resources to power innovation. This project will improve and expand collaboration between public and private sector groups to use the limited resources available to the high-performance computing community effectively, and strengthen the ability of firms to leverage advanced computing for economic advantage by developing skills within the American workforce, new software partnerships, and greater access to advanced computing resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761839","Spokes: MEDIUM: SOUTH: Collaborative: Integrating Biological Big Data Research into Student Training and Education","OAC","BD Spokes -Big Data Regional I, IUSE","10/01/2018","02/26/2019","Hong Qin","TN","University of Tennessee Chattanooga","Standard Grant","Earnestine Psalmonds","09/30/2022","$549,888.00","Joey Shaw, Hope Klug, Yu Liang, Jennifer Boyd, Azad Hossain","hong-qin@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","024Y, 1998","028Z, 8083, 9102","$0.00","The project is a collaborative effort among the University of Tennessee Chattanooga, Tuskegee University, Spelman College, and West Virginia University to integrate and automate biological big data into student training and education. Leveraging the team's expertise in computer science and ecology, the project will offer training workshops on using network models to integrate heterogeneous genomic big data and heterogeneous ecological big data to address life sciences questions. The team will engage faculty and students in developing a protocol to automate field data collection. The team also will prototype automated methods to enhance plant digitization, leveraging the collection of digitized plant images and meta-information at the Southeast Regional Network of Expertise and Collections, as well as the ecological datasets in collaboration with the Encyclopedia of Life.<br/><br/>The project objectives are to (1) enhance faculty expertise in big biological data through summer workshops; (2) catalyze interdisciplinary collaboration on big biological data research and education through hackathons, working groups, and community-building via a Video Education Faculty Network; and (3) develop hands-on, constructively peer-evaluated learning modules incorporating high-quality video tutorials. The proposed activities will address challenges surrounding the integration and automation of big biological data into education and training at predominantly undergraduate institutions and Historically Black Colleges and Universities. The project will help bridge the gaps between big biological data and the fields of systems biology, ecology and evolution, and environmental sciences. Overall, the project will catalyze collaborations among diverse institutions and disciplines while increasing diversity in big data. <br/><br/>This award is co-funded by the Improving Undergraduate STEM Education: Education and Human Resources (IUSE): EHR Program (NSF 17-590). IUSE supports projects that are designed to improve student learning through development of new curricular materials and methods of instruction and development of new assessment tools to measure student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103936","Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","06/01/2021","05/24/2021","Carlos Fernandez Granda","NY","New York University","Standard Grant","Varun Chandola","05/31/2024","$299,926.00","","cfg3@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1712, 7726, 8004","054Z, 077Z, 094Z, 095Z, 7923, 8004, 8396, 8397","$0.00","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.<br/><br/>Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931525","Collaborative Research: Element: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","08/22/2019","Yang Wang","PA","Carnegie-Mellon University","Standard Grant","Varun Chandola","09/30/2022","$269,950.00","","ywg@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 8004, 9216, 9263","$0.00","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. <br/><br/>The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.<br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934612","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Robert Nowak","WI","University of Wisconsin-Madison","Continuing Grant","Amy Walton","08/31/2022","$399,992.00","Stephen Wright","rdnowak@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","099Y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2128607","Collaborative Research: CICI: Secure and Resilient Architecture: SciGuard: Building a Security Architecture for Science DMZ Based on SDN and NFV Technologies","OAC","Cybersecurity Innovation","04/01/2021","03/29/2021","Hongxin Hu","NY","SUNY at Buffalo","Standard Grant","Robert Beverly","03/31/2022","$35,088.00","","hongxinh@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8027","9150","$0.00","As data-intensive science becomes the norm in many fields of science, high-performance data transfer is rapidly becoming a standard cyberinfrastructure requirement. To meet this requirement, an increasingly large number of university campuses have deployed Science DMZs. A Science DMZ is a portion of the network, built at or near the edge of the campus or laboratory's network, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose computing. This project develops a secure and resilient architecture called SciGuard that addresses the security challenges and the inherent weaknesses in Science DMZs. SciGuard is based on two emerging networking paradigms, Software-Defined Networking (SDN) and Network Function Virtualization (NFV), both of which enable the granularity, flexibility and elasticity needed to secure Science DMZs. <br/><br/>Two core security functions, an SDN firewall application and a virtual Intrusion Detection System (IDS), coexist in SciGuard for protecting Science DMZs. The SDN firewall application is a software-based, in-line security function running atop the SDN controller. It can scale well without bypassing the firewall using per-flow/per-connection network traffic processing.  It is also separated from the institutional hardware-based firewalls to enforce tailored security policies for the science-only traffic sent to Science DMZs. The virtual IDS is an NFV-based, passive security function, which can be quickly instantiated and elastically scaled to deal with attack traffic variations in Science DMZs, while significantly reducing both equipment and operational costs. In addition to these functions, the researchers also design a cloud-based federation mechanism for SciGuard to support security policy automatic testing and security intelligence sharing. The new mechanisms developed in this project are robust, scalable, low cost, easily managed, and optimally provisioned, therefore substantially enhancing the security of Science DMZs. This research encourages the diversity of students involved in the project by active recruitment of women and other underrepresented groups for participation in the project. The project has substantial involvement of graduate students in research, and trains promising undergraduate students in the implementation and experiments of the proposed approach. Moreover, the project enhances academic curricula by integrating the research findings into new and existing courses."
"1835747","Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework","OAC","EDUCATION AND WORKFORCE, Software Institutes","11/01/2018","05/14/2020","Marouane Kessentini","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Seung-Jong Park","10/31/2022","$378,451.00","","kessentini@oakland.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7361, 8004","026Z, 077Z, 7923, 8004, 9179, 9251","$0.00","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. <br/><br/>This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761880","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","09/01/2018","08/27/2018","Simon Lin","OH","The Research Institute at Nationwide Children's Hospital","Standard Grant","Wendy Nilsen","08/31/2021","$120,000.00","","Simon.Lin@NationwideChildrens.org","700 Childrens Drive","Columbus","OH","432052664","","CSE","024Y","028Z","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities. (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2038682","IDIEA-DC: An Infrastructure for Distributed Intelligence Experimentation and Architectures in the Digital Continuum: from IoTs to the Cloud","OAC","CYBERINFRASTRUCTURE","08/15/2020","08/05/2020","Tarek El-Ghazawi","DC","George Washington University","Standard Grant","Varun Chandola","07/31/2022","$299,905.00","","tarek@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7231","7231, 7916","$0.00","This project sets the stage for leveraging the concurrent and synergistic advances in digital domains such as machine learning, high-performance computing, big data, clouds, and smart and connected devices to create new breads of applications that were hardly possible to conceive and deploy before. This can be productively accomplished by presenting this plethora of different domains to application developers as one integrated system, which is referred to here as the digital continuum.  This project creates a testbed infrastructure for such digital continuum with capabilities for application deployments, task distributions, performance monitoring and optimizations, and data sets for experimentation with emphasis on distributed intelligence. The concepts and access to such testbed are shared with the community in order to open new doors for innovative system and application research in the digital continuum as a system.  <br/><br/>Convergence research is identified as one of the big ideas for future progress and investments.  The digital continuum is an example of such converged systems.  This project creates an experimental infrastructure, IDIEA-DC that can mimic the digital continuum from the edge devices to the data center. IDEA-DC also provides productive interfaces for exploring and testing research ideas in support of launching distributed machine learning applications on the digital continuum as a system, such as federated learning.   It supports monitoring and control for intelligent and dynamic cross-layer optimizations with respect to many different parameters including performance and bandwidth availability, energy and quality of service.  Two instances of such infrastructure will be created.  One will be a heavily instrumented cluster devoted to emulate such digital continuum while providing the opportunity for more accurate measurements and closer control.  The second will be a cloud-based deployment.  The infrastructure will be augmented with relevant data sets, as well as libraries for measurements and control.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931367","Collaborative Research:Element: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","08/22/2019","Hanna Terletska","TN","Middle Tennessee State University","Standard Grant","Varun Chandola","09/30/2022","$135,000.00","","Hanna.Terletska@mtsu.edu","1301 E. Main","Murfreesboro","TN","371320001","6154947848","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 8004, 9216, 9263","$0.00","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. <br/><br/>The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.<br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1929893","Regional Cybersecurity/Cyberinfrastructure Workshop Targeting Minority Serving Institutions: Low-Cost/High-Impact Cyberdefense and Cyberinfrastructure Resources","OAC","Cybersecurity Innovation","06/01/2019","05/27/2021","Tracy Futhey","NC","Duke University","Standard Grant","Robert Beverly","10/31/2021","$66,886.00","Jean Davis, Eva Kraus, Tracy Doaks","futhey@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8027","","$0.00","The CyberRISK (Regional Information Security Knowledge-sharing) workshop brings together twenty Minority-Serving Institutions (MSIs) with diverse characteristics and academic missions, but with a shared need for training, resources, and peer networking on  Because cybersecurity postures arise from policy and technical designs, attending institutions bring two participants: one policy lead (CIO or senior leader with policy-setting authority) and one technical lead (security or networking resource responsible for security protections). Workshop programs follow policy and technical tracks, emphasizing their coordination for effectiveness and coherence.  Pre-workshop surveys shape workshop content so it is relevant for the diverse institutional circumstances, priorities, and levels of cybersecurity maturity.<br/><br/>National and regional cyber experts, network operators, and research university peers participate to broaden and diversify insight into best practices and relevant service offerings, and to promote interaction around campus cyberinfrastructure, science use cases, and regional / national cyberinfrastructure resources.<br/><br/>This workshop's intellectual merit stems from information exchanged at the event, which leads to new ideas, relationships and collaborations among the diverse participants: colleges and universities, regional and national cyber experts, and relevant consortia.  Evaluative instruments employed before, during, and after the event shed light on changes and actions arising from the workshop that improve participants' cybersecurity postures.  Broader impact of this workshop begins with the dissemination of workshop materials, presentations, and plans that are publicly available for adaptation and reuse; it extends further by providing a framework and model for future regional workshops, with the results of evaluative instruments available to guide improvements of those workshops.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844565","CAREER: Spatial Network Database approach for Emergency Management Information Systems","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","03/15/2019","10/15/2020","KwangSoo Yang","FL","Florida Atlantic University","Continuing Grant","Alan Sussman","02/29/2024","$375,143.00","","yangk@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","1045, 7231","1045","$0.00","Emergency Management Information Systems (EMIS) are an increasingly important tool for understanding, managing, and governing transportation-related systems, as well as for testing the stability or vulnerability of these systems against interference. Recently, EMIS have benefitted from both volunteer geographic information (VGI) and crowdsourcing as powerful methods of collecting user-generated datasets. However, these data sources are challenging due to their very large size, variety, and update rates required to ensure the timely and accurate delivery of useful emergency information and response for disastrous events. Developing fundamental data processing components for advanced relevant queries which can clearly and succinctly deliver critical information in the case of an emergency is critically important and challenging. This research focuses on three interrelated domains: 1) evacuation route planning 2) resource assignment, and 3) transportation resilience. This research investigates innovative queries in these three domains in the context of emergency management. The outcome of this project has potential benefit to a wide range of societal applications, such as transportation management, logistics, public safety, resource assignment, and service delivery and thus aligns well with the NSF mission: to promote the progress of science; to advance the national health, prosperity and welfare . Educational objectives of this project include broadening participation of Hispanic women, increasing undergraduate research opportunities including research-intensive course development, and promotion of team science skills. <br/><br/>The goals of this project are to identify promising solutions for addressing the challenge of EMIS and to develop an advanced spatial query processing platform that clearly and succinctly delivers critical information in emergencies. First, this project designs and develops the problem-solving framework that can integrate different technical components, including geometry, topology, graph theory, and optimization techniques. Second, this project investigates multiple inherent constraints for spatial networks and identifies main bottlenecks for query processing. Third, this project develops fast and scalable query processing mechanisms that overcome these bottlenecks and produce simple and concise information for emergency management. A key research challenge is to identify structural patterns or optimal substructures of the spatial network optimization problem that can enhance the scalability and efficiency of spatial network query processing. The components of the query processing framework include frequent suffix tree mining, graph simplification, bi-partite graph clustering, minimum polygon covering, graph partitioning, spectral method, random walk, and expander graph mining. These components are integrated to develop fast and scalable spatial network queries and to provide simple and concise information for EMIS. The outcomes of this project include data processing tools, spatial and spatial network optimization algorithms, queries, and visualization tools. This project validates the performance of new spatial network queries using historical and real-time datasets and provides a web-based educational system to enhance student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835885","Elements: Software: Multidimensional Fast Fourier Transforms on the Path to Exascale","OAC","Software Institutes","10/01/2018","08/13/2018","Dmitry Pekurovsky","CA","University of California-San Diego","Standard Grant","Robert Beverly","04/30/2022","$477,460.00","","dmitry@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","This project will contribute to software available freely to the public, providing solution of an important class of problems in computational science, namely the multidimensional Fast Fourier transforms (FFTs). This software will enable scientists to run certain classes of computer experiments on powerful state-of-the-art supercomputers, known as Exascale machines. A wide range of science fields will benefit from such public software; thus, it will contribute to innumerable discoveries related to fundamental understanding of nature, protecting the environment, efficient energy and transportation, designing new materials and improving medicine. It will also contribute to the computational ecosystem by virtue of being a building block that can be used and reused by other programs. This project also includes an educational effort in training a wide range of supercomputer users, from undergraduate interns to graduate/postdoctoral researchers and faculty. Effort will be made to reach out to underrepresented groups when recruiting the interns.<br/><br/>The National Strategic Computing Initiative (NSCI), in particular Strategic Objective 4 ""An Enduring National HPC Ecosystem"", outlines the crucial need for development of foundational algorithms and software for next generation supercomputers, and for easing access to the next-generation compute resources to wide classes of users. Fast Fourier Transforms (FFT) is a ubiquitous tool in scientific simulations, from Computational Fluid Dynamics to plasma physics, astrophysics, ocean modeling, materials research, medical imaging, molecular dynamics and many others. This project will fill the gap in highly efficient software for multidimensional FFTs for use on Exascale platforms. While running full FFT at exa-scale appears prohibitive due to strong interconnect bandwidth dependence, steps must be taken towards this goal due to the importance of the algorithm. Building on the previous work with P3DFFT, an open-source numerical library used by many applications in diverse science fields, this proposal aims to push the envelope in terms of adapting multidimensional FFT to new architectures and aggressively scale its performance, without losing the portability and the practical ease of use. The project will employ both novel and proven state-of-the-art tools, such as auto-tuning, overlap of communication with computation and GPU implementation, which will help reduce the bandwidth bottleneck. In addition, features will be added that will make the software appealing to a wider user base. The software will become an integral part of the National Cyberinfrastructure and will aid in numerous scientific and engineering advances, including in areas such as environmental research, energy, efficient transportation, new drugs and new materials. This project includes training of a new generation of scientific software developers through XSEDE training webinars, conference training workshops and supervision of undergraduate interns. The project plan includes reaching out to underrepresented groups when recruiting undergraduate interns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112356","Category II: ACES - Accelerating Computing for Emerging Sciences","OAC","Innovative HPC","10/01/2021","12/29/2021","Honggao Liu","TX","Texas A&M University","Cooperative Agreement","Robert Chadduck","09/30/2026","$5,999,998.00","Shaowen Wang, Timothy Cockerill, Lisa Perez, Dhruva Chakravorty","honggao@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","7619","7619, 9102","$0.00","The ever-growing complexity of Science and Engineering (S&E) workflows and expectations of Open Science have encouraged researchers to adopt new technologies, such as containerization, virtualization and composability, that enable them to respond to an increasingly complex cyberinfrastructure (CI) landscape while producing shareable, and reproducible results. ACES (Accelerating Computing for Emerging Sciences), an innovative advanced computational prototype to be developed by Texas A&M University, tries to answer a fundamental question: how does one effectively offer a holistic computing platform that can simultaneously meet the needs of a continuum of users in diverse research communities with varying levels of computing adoption? The project will allow researchers to creatively develop new programming models and workflows that utilize these architectures while simultaneously advancing HPC (High Performance Computing) and data science projects.<br/><br/>The ACES platform removes significant bottlenecks in advanced computing by introducing the flexibility to aggregate various components (i.e., processors, accelerators and memory) on an as-needed basis to solve problems that were previously not addressable. By letting researchers switch and run on accelerators best suited for their workflows, ACES will benefit many research and development projects in the fields of artificial intelligence and machine learning (AI/ML), cybersecurity, health population informatics, genomics and bioinformatics, human and agricultural life sciences, oil & gas simulations, de novo materials design, climate modeling, molecular dynamics, quantum computing architectures, imaging, smart and connected societies, geosciences, and quantum chemistry. Toward facilitating researcher use, ACES will offer avenues for interactive computing, portals, and cloud connectivity. ACES will support the national research community through coordination systems supported by the National Science Foundation (NSF).  Finally, ACES will also leverage existing efforts that promote science and broaden participation in computing at the K-12, collegiate, and professional levels to have a transformative impact nationally by focusing on training, education and outreach. ACES activities are designed to expand the participation of traditionally underrepresented groups in computing and STEM (Science, Technology, Engineering and Mathematics), particularly at minority-serving institutions. ACES will offer fellowships to students, continue efforts to support teacher programs, and offer a number of formal and informal courses, whose materials will be offered to the national community for use free-of-charge.<br/><br/>This project funds the development of a dynamically composable high-performance data analysis and computing platform, named ACES.  AI and ML are integrated with traditional simulation and modeling approaches in the pursuit of innovation. Edge-computing and instrumental probes have pushed the need to verify, process, store, analyze, and query vast amounts of unstructured data in real time. The coupling of analytics with closely-situated data on highly-usable web-based technologies connected to a compute backend have led to a paradigm shift in expectations from research computing environments. The ACES innovative composable hardware platform helps accelerate transformative changes in research areas that can leverage novel High Bandwidth Memory (HBM) processors and accelerators for analytics and computing. ACES leverages Liqid?s composable framework via PCIe (Peripheral Component Interconnect express) Gen5 on Intel?s HBM Sapphire Rapid processors to offer a rich accelerator testbed consisting of Intel Ponte Vecchio GPUs (Graphics Processing Units), Intel FPGAs (Field Programmable Gate Arrays), NEC Vector Engines, NextSilicon co-processors, Graphcore IPUs (Intelligence Processing Units). The accelerators are coupled with Intel Optane memory and DDN Lustre storage interconnected with Mellanox NDR 400Gbps (gigabit-per-second) InfiniBand to support workflows that benefit from optimized devices. ACES will enable applications and workflows to dynamically integrate the different accelerators, memory, and in-network computing protocols to glean new insights by rapidly processing large volumes of data, and provide researchers with a unique platform to produce complex hybrid programming models that effectively supports calculations that were not feasible before.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104105","Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","06/01/2021","05/24/2021","Peter Crozier","AZ","Arizona State University","Standard Grant","Varun Chandola","05/31/2024","$300,068.00","","crozier@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","1712, 7726, 8004","054Z, 077Z, 094Z, 095Z, 7923, 8004, 8396, 8397","$0.00","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.<br/><br/>Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931445","Collaborative Research: Elements: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/19/2019","Ka Ming Tam","LA","Louisiana State University","Standard Grant","Varun Chandola","09/30/2022","$194,898.00","","kmtam@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 8004, 9150, 9216, 9263","$0.00","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. <br/><br/>The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.<br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1941443","EAGER: Reproducibility and Cyberinfrastructure for Computational and Data-Enabled Science","OAC","CYBERINFRASTRUCTURE","09/01/2019","08/19/2019","Victoria Stodden","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","08/31/2021","$300,000.00","Michela Taufer","stodden@usc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7231","7916","$0.00","This project seeks to improve understanding of how the scientific community can adapt to the increasing use of computing and large-scale data resources. One challenge is ensuring that computational results - such as those from simulations - are ""reproducible,"" that is, the same results are obtained when one re-uses the same input data, methods, software and analysis conditions. In 2019, the National Academies of Science, Engineering, and Medicine (NASEM) issued a report on ""Reproducibility and Replication in Science"" with a series of recommendations. The project will assess the implications of these recommendations on the scientific discovery process for computationally- and data-enabled research.<br/><br/>The following research questions will guide this study: (1) what reproducibility issues are surfaced by the NASEM recommendations and what constraints and requirements do they imply for computational infrastructure?; and (2) what are the implications of these issues and constraints on the computational infrastructure ecosystem as a whole? To explore and illustrate answers to these questions, we will employ diverse scientific use cases chosen to cover different ways researchers interact with computational infrastructure. Formalisms will also be applied to the use cases to articulate the role of computational infrastructure in enabling transparency and reproducibility, and to elucidate how computational infrastructure can conform to the NASEM report recommendations. The overall aim is to articulate avenues for future research at the intersection of transparency, reproducibility and computational infrastructure that supports scientific discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839010","EAGER: Preserve/Destroy Decisions for Simulation Data in Computational Physics and Beyond","OAC","NSF Public Access Initiative","08/15/2018","08/08/2018","Victoria Stodden","IL","University of Illinois at Urbana-Champaign","Standard Grant","Bogdan Mihaila","08/31/2021","$300,000.00","Darko Marinov","stodden@usc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7414","7916","$0.00","The scientific research community has been increasingly developing ways to share and re-use research data, thereby allowing more discoveries to be made from previous research investments. Much of the focus on sharing and reusability has been on experimental and observational data. This project addresses the equally vexing challenge of how to make best use and re-use of the massive data produced in computational simulations. Important research questions guiding this project include: the degree to which simulation results can be replicated; the advantages of storing the simulation data itself for others to reuse as compared to providing the computational software so that others can re-run the simulations; and understanding which software testing practices can facilitate the replication/reuse of simulation data and the simulation software that produces those data. The principal investigators will address these questions by performing extensive replication and software code testing on a set of computational physics simulation datasets and software code that they had gathered through a previous study. The project will produce publicly available, fully reproducible computational physics works as examples for publishing results in a way that the data and code are effectively reusable.<br/><br/>The principal investigators aim to improve understanding of, and increase, the reusability of the code and data associated with simulation-based research.  This project specifically aims to better inform data destroy/preservation decisions in the simulation context, toward improving the reusability and interoperability of simulation data and code. The project will also consider important questions such as how software engineering testing practices relate to computational physics practices, and how changes in computational environments affect code execution and the regeneration of simulation data. Ultimately, the results of this work are intended to guide the research community on how to best produce and disseminate research code. It is anticipated that the results for computational physics can be extended to develop general guidelines for simulation data and code sharing for other communities, the appropriate code testing to do so, and best practices for development of associated cyberinfrastructure and tools. <br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032661","EAGER: Sharing Knowledge, Building Community:  Introducing a Journal Editors' Discussion Interface (JEDI)","OAC","NSF Public Access Initiative","09/01/2020","07/17/2020","Colin Elman","NY","Syracuse University","Standard Grant","Martin Halbert","08/31/2022","$299,735.00","Margaret Levenstein, Diana Kapiszewski, Merce Crosas, Thu-Mai Christian","celman@maxwell.syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7414","7916","$0.00","A partnership of social science domain repositories proposes to create a ""Journal Editors' Discussion Interface"" (JEDI).  JEDI will establish and maintain an online community of social science journal editors and data professionals, encouraging and facilitating continuous communication and learning. With a long track record of engaging with social science journal editors, including organizing a well-attended series of annual workshops for editors focused on open science practices, the partner repositories are well placed to design, launch, lead and participate in this endeavor. By offering journal editors the opportunity to draw on the expertise of other editors and data services personnel, this pioneering, light-weight, high-risk / high-payoff project will augment the readiness of the social science publishing community to generate and adopt consensual best practices for open science.<br/><br/>JEDI will combine features of an online forum and a traditional email listserv. It will initially focus on the aspects of the editorial process that concern data and code, and their management, citation, and accessibility; other aspects of research transparency; and reproducibility, replication, and verification. Editors, in turn, will be empowered to encourage and help authors to capitalize on developments in the socio-technical infrastructure supporting open science. With consequential developments in that infrastructure on the horizon ? such as the expansion of the National Science Foundation?s Public Access Repository (NSF PAR) ? there is a pressing need for immediate action.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835897","ELEMENTS: DATA: HDR: SWIM to a Sustainable Water Future","OAC","Data Cyberinfrastructure","01/01/2019","04/21/2020","Natalia Villanueva Rosales","TX","University of Texas at El Paso","Standard Grant","Amy Walton","12/31/2022","$615,451.00","Josiah Heyman, Deana Pennington","nvillanuevarosales@utep.edu","ADMIN BLDG RM 209","El Paso","TX","799680001","9157475680","CSE","7726","062Z, 077Z, 7923, 9251","$0.00","The project develops a system that automates ingestion of data into models, the integration of decoupled models, and the dynamic generation and interpretation of models.  The focus is on water resources.  The team leverages software development from an ongoing USDA-funded project, and the expertise of an interdisciplinary, international team of scientists and students who are investigating future scenarios of water availability and use in the Middle Rio Grande valley of southern New Mexico, west Texas, and northern Chihuahua (Mexico).<br/><br/>This project advances water sustainability research capabilities by creating a Sustainable Water through Integrated Modeling (SWIM) framework that automates ingestion of data into models, facilitates integration of decoupled models, and supports dynamic generation and interpretation of models. The four objectives are to: <br/>1) foster use of water models by stakeholders (non-modelers) through direct participation enabled by a web-based interface and provenance capture; <br/>2) enable seamless model-to-model integration through service-driven data exchange and transformation; <br/>3) develop data- and technology-enabled approaches for reasoning with biophysical and social models; and <br/>4) engage data providers, modelers and stakeholders in conceiving and testing the framework.<br/>The research and products of this project contribute to advanced research capabilities on water sustainability.  By providing seamless integration of scientific data and models, and generating provenance data to create dynamic user interfaces, the project instills trust in the models generated through participatory analysis.   This approach is built around a strong appreciation of the value of stakeholder engagement and alignment to achieving the described goals.  The research is carried out by a diverse and experienced team, and will contribute to understanding of how to more effectively conduct convergent research with researchers and stakeholders.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939892","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Chris Myers","UT","University of Utah","Standard Grant","Peter McCartney","08/31/2021","$204,296.00","","chris.myers@colorado.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","099Y, 7231","1165, 7231","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849519","Promoting International Collaboration on Developing Scalable, Portable & Efficient HPC Software for Modern HPC Platforms","OAC","Information Technology Researc, EDUCATION AND WORKFORCE","10/01/2018","09/15/2018","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Alan Sussman","09/30/2021","$27,750.00","","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","1640, 7361","026Z, 7556, 9179","$0.00","Supercomputers are used to power discoveries and to reduce the time-to-results in a wide variety of disciplines such as engineering, physical sciences, and healthcare. Scalable and efficient software is required for optimally using the large-scale supercomputing platforms, and thereby, effectively leveraging NSF investments in the nation's advanced CyberInfrastructure (CI). With the rapid advancement in the computer architecture discipline, the complexity of the processors that are used in the supercomputers is also increasing, and, in turn, the task of developing efficient software for supercomputers is further becoming challenging and complex. To mitigate such challenges, there is a need for forums that brings together different stakeholders - the researchers and practitioners from the areas of software engineering and supercomputing. To provide such a platform, the second workshop on ""Software Challenges to Exascale Computing (SCEC)"" is being organized in India. This project funds the participation of ten US students from diverse backgrounds in the workshop and will provide them the opportunities to develop their skills in the area of ""software for supercomputing platforms"". The SCEC workshop will not only inform the participants about the challenges in large-scale software development for supercomputers but will also steer them in the direction of building international collaborations for finding solutions to those challenges. The project provides opportunities for workforce development, and serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. <br/><br/>The workshop participants will learn about the two NSF funded supercomputers - Stampede2 and Comet - and will get practical experience at developing scalable, efficient, and portable software on those systems through tutorials and ""bring your own code"" sessions. The SCEC workshop will provide a forum through which hardware vendors and software developers can communicate with each other and influence the architecture of the next generation supercomputing systems and the supporting software stack. By fostering cross-disciplinary associations, the SCEC workshop will serve as a stepping-stone towards innovations in the future. The workshop will benefit researchers, students, and practitioners in supercomputing by providing them an opportunity to disseminate their results to the public, and find potential collaborators. The students funded through this project will get an opportunity to network with professionals, faculties and researchers working in the areas of supercomputing and software engineering, and will get to learn about the opportunities for internships, jobs, and higher education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828083","MRI: Acquisition of a Next Generation, Data-Centric Supercomputer","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","09/01/2018","08/24/2018","Christopher Carothers","NY","Rensselaer Polytechnic Institute","Standard Grant","Alejandro Suarez","08/31/2021","$999,000.00","Mark Shephard, Kristin Bennett, Mohammed Zaki, George Slota","chrisc@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1189, 7231","1189","$0.00","The project will support the acquisition of a data centric supercomputer at Rensselaer Polytechnic Institute (RPI). This instrument will lead to significant advancements in science and engineering problems currently being tackled at RPI's Center for Computational Innovations (CCI) for applications including: the definition of new designed materials, applying active flows control for energy savings and microbiological systems modeling for medical treatment planning. The research will also include the development of new extreme-scale simulation technologies, graph analysis algorithms and the construction of entirely new simulation workflows. Hundreds of researchers and students from over 20 universities, 5 DOE national laboratories, 3 major industrial research centers (Corning, GE and IBM), 50 faculty, 4 start-ups across 11 U.S. states will take advantage of this proposed cyberinstrument to continue making a deep impact on their research. Student participation has been key to CCI's current success and national interest is anticipated not only due to the instrument's ability to advance current research but also due to its potential as a prototype model for future exascale systems.  Students engaged in projects supported by the instrument will become the next generation of compute and data intensive experts.<br/><br/>The new instrument integrates IBM POWER9 CPUs with next generation NVIDIA Volta GPUs into a hardware accelerated unified memory system (e.g., cache coherent). Additionally, all compute nodes are augmented with non-volatile memory storage, and a subset of the nodes include FPGA acceleration. The system will be used by faculty, students and CCI collaborators to address current barriers caused by the need to interact with massive data volumes that are used in and produced by next generation simulation tools. The cyberinstrument and algorithmic developments to be carried out will enable a new level of understanding and enhance our ability to solve many key challenges including: the accurate diagnosis of breast cancer directly from large-scale image datasets; semantic integration of the abundance of heterogeneous, multimodal, and multiscale data to improve personal health; modeling plasmas in fusion reactors; modeling active flow control devices that will greatly increase the weather conditions under which wind turbines will produce electricity; and combined biological data and model integration on molecular, cellular, and organ levels to understand organism-level phenomena and gain predictive understanding in systems biology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920147","MRI: Acquisition of a GPU-Accelerated Research Cluster","OAC","Major Research Instrumentation, OFFICE OF MULTIDISCIPLINARY AC","10/01/2019","09/12/2019","Pengyu Hong","MA","Brandeis University","Standard Grant","Alejandro Suarez","09/30/2022","$349,983.00","Michael Hagan","hongpeng@brandeis.edu","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","1189, 1253","075Z, 1189","$0.00","Large scale data analysis capability has become the key factor that is driving rapid progress in all fields of science. This project will significantly expand the high-performance computing resources at Brandeis University, by acquiring state of the art GPU computing machines that enable big data driven research. The new capability will support integrated research projects across a variety of disciplines, including the areas of Biology, Biochemistry, Chemistry, Computer Science, Math, Physics, and Psychology. It will also provide a platform at Brandeis University for training the next generation workforce to develop and apply deep learning techniques, thus accelerating discoveries in basic research and technological innovations. The project will enable Brandeis researchers and instructors to utilize modern computing techniques to broaden participation in science, technology, engineering, and mathematics (STEM) fields.  <br/><br/>Specifically, the new resources will include 16 GPU nodes and 1 storage node on a 10Gbit network fabric to allow rapid internode communications. This project will enable Brandeis researchers to conduct big data driven convergence research that enhances our understanding of the Rules of Life in areas ranging from neuroscience to virus assembly, and addresses the fundamental problems underlying societal needs (e.g., green energy). The emerging interdisciplinary research activities will also create an inclusive environment for developing novel and more powerful big data driven techniques. The project will enable courses and workshops that train faculty, postdocs, graduate students, and undergraduate students to effectively use state of the art GPU computing and deep learning techniques. It will allow Brandeis to further integrate its education and research via a current NSF funded REU site at Brandeis Materials Research Science and Engineering Research Center.  In addition, it will enhance Brandeis' ability to broaden participation in STEM, especially by women and underrepresented minorities, through several existing programs, including the NSF funded REU site, the Transitional Year Program for economically disadvantaged students, the local Society for Advancement of Chicanos/Hispanics and Native Americans in Science chapter at Brandeis, the Brandeis Science Posse Program, and the Brandeis Scientists in the Classroom Workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934714","Collaborative Research: Understanding Subatomic-Scale Quantum Matter Data Using Machine Learning Tools","OAC","HDR-Harnessing the Data Revolu","09/01/2019","08/23/2021","Eun-Ah Kim","NY","Cornell University","Continuing Grant","Daryl Hess","08/31/2022","$1,211,285.00","Kilian Weinberger, Kilian Weinberger, Andrew Wilson, Christopher De Sa","eun-ah.kim@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","099Y","062Z","$0.00","A central goal of modern quantum physics is to search for new systems and technological paradigms that utilize quantum mechanical aspects of matter rather than being limited by them. In particular, there is an active search for new materials that exhibit surprising physical properties because of strong interaction between individual electrons that leads to strong correlations in the motion of electrons and as a result, to strongly correlated quantum matter. The study of Strongly Correlated Quantum Matter (SCQM) has reached a tipping point through intense efforts over the last decade that have led to vast quantities of experimental data. The next breakthrough in the field will come from relating these experimental data to theoretical models using tools of data science. However, data-driven challenges in SCQM require a fundamentally new data science approaches for two reasons: first, quantum mechanical imaging is probabilistic; and second, inference from data should be subject to fundamental laws of physics. Hence the new data-driven challenges in the field of SCQM requires ""Growing Convergent Research"" and ""Harnessing the Data Revolution"", two of NSF's Ten Big Ideas. <br/><br/>The objective of the project is to develop and disseminate machine learning (ML) tools that can serve as a two-way highway connecting the data revolution in SCQM experiments at sub-atomic scale to a fundamental theoretical understanding of SCQM. The specific goals are: (1) Develop interpretable ML tools for position space image data; (2) Develop unsupervised ML tools for momentum space scattering data; (3) Design new imaging modality guided by the insight gained from ML; and (4) Integrate ML tools with in-operando human interface to the Cornell High Energy Synchrotron Source (CHESS) beamline. Goals (1) and (2) are within reach, while (3) and (4) are more ambitious visions for scaling up to a future institute that can involve more academic institutions and scattering experiment facilities nationwide. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835780","Collaborative Research: NSCI Framework. Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","Special Initiatives, Leadership-Class Computing, Software Institutes","01/01/2019","04/12/2019","Peter Kasson","VA","University of Virginia Main Campus","Standard Grant","Bogdan Mihaila","12/31/2022","$766,115.00","","kasson@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1642, 7781, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835863","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","Climate & Large-Scale Dynamics, Data Cyberinfrastructure","10/01/2018","08/13/2018","Michael Pritchard","CA","University of California-Irvine","Standard Grant","Amy Walton","09/30/2022","$289,409.00","","mspritch@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","5740, 7726","062Z, 077Z, 4444, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939505","Collaborative Research: Biology-guided neural networks for discovering phenotypic traits","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","10/15/2020","Ali Maga","WA","Seattle Children's Hospital","Continuing Grant","Peter McCartney","09/30/2022","$592,152.00","","ali.maga@seattlechildrens.org","4800 Sand Point Way NE","Seattle","WA","981053901","2069872005","CSE","099Y, 7231","1165, 7231","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835632","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, Data Cyberinfrastructure","01/01/2019","08/27/2018","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Amy Walton","12/31/2022","$63,538.00","","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103741","Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED)","OAC","Software Institutes","09/01/2021","08/24/2021","Felix Waldhauser","NY","Columbia University","Standard Grant","Tevfik Kosar","08/31/2025","$641,284.00","","felixw@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004","077Z, 7925, 8004","$0.00","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. <br/><br/>The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940330","Collaborative Research: Converging Genomics, Phenomics, and Environments Using Interpretable Machine Learning Models","OAC","ICB: Infrastructure Capacity f, HDR-Harnessing the Data Revolu","10/01/2019","12/07/2021","Anne Thessen","OR","Oregon State University","Continuing Grant","Peter McCartney","06/30/2022","$494,580.00","Pankaj Jaiswal, Anne Thessen","annethessen@gmail.com","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","085Y, 099Y","062Z, 1165","$0.00","Mitigating the effects of climate change on public health and conservation calls for a better understanding of the dynamic interplay between biological processes and environmental effects. The state-of-the-art, which has led to many important discoveries, utilizes numerical or statistical models for making predictions or performing in silico experimentation, but these techniques struggle to capture the nonlinear response of natural systems. Machine learning (ML) methods are better able to cope with nonlinearity and have been used successfully in biological applications, but several barriers still exist, including the opaque nature of the algorithm output and the absence of ML-ready data. This project seeks to significantly advance technologies in ML and create a new interdisciplinary field, computational ecogenomics. This will be accomplished by designing ML techniques for encoding heterogeneous genomic and environmental data and mapping them to multi-level phenotypic traits, reducing the amount of necessary training data, and then developing interactive visualizations to better interpret ML models and their outputs.  These advances will responsibly and transparently inform policy to maximize resources during this crucial window for planetary health, while revealing underlying biological mechanisms of response to stress and evolutionary pressure.<br/><br/>The long-term vision for this project is to develop predictive analytics for organismal response to environmental perturbations using innovative data science approaches and change the way scientists think about gene expression and the environment. The goal for this two-year award is to develop a proof-of-concept for an institute focused on predicting emergent properties of complex systems; an institute that would itself foster the development of many new sub-disciplines.  The core of this activity is developing a machine learning framework capable of predicting phenotypes based on multi-scale data about genes and environments.  Available data, ranging from simple vectors to complex images to sequences, will be ingested into this framework by applying proven semantic data integration tools and algorithmic data transformation methods.  The central hypothesis of this research is that deep learning algorithms and biological knowledge graphs will predict phenotypes more accurately across more taxa and more ecosystems than do current numerical and traditional statistical modeling methods.  The rationale for this project is that a timely investment in data science will push through a bottleneck in life science, accelerating discovery of gene-phenotype-environment relationships, and catalyzing a new computational discipline to uncover the complex ""rules of life.""<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916585","BD Hubs: NORTHEAST: The Northeast Big Data Innovation Hub","OAC","BD Spokes -Big Data Regional I, HDR-Harnessing the Data Revolu","06/01/2019","06/22/2021","Jeannette Wing","NY","Columbia University","Cooperative Agreement","Martin Halbert","05/31/2023","$3,070,454.00","James Hendler, Vasant Honavar, Andrew McCallum, Florence Hudson, Rene Baston","WING@COLUMBIA.EDU","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","024Y, 099Y","062Z, 8083","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The Northeast Big Data Innovation Hub serves as a uniquely neutral entity within this ecosystem, harnessing the data revolution by building strategic partnerships that advance innovative solutions to a broad range of societal, scientific, and industry challenges. This vision is empowered and strengthened through the Hub's collaboration with a diverse community of partners, including underserved populations, world-class institutions, and people of all backgrounds who rely on or are impacted by big data.<br/> <br/>Leveraging the distinctive characteristics and challenges of the northeastern United States, the Northeast Hub will design and facilitate multi-disciplinary, community-led activities and initiatives such as: <br/><br/>- Aggregating and helping to develop best practices for responsible data science; <br/>- Creating frameworks for data fluency; <br/>- Fostering better management of data security and privacy; <br/>- Integrating health data from traditional and novel sources; <br/>- Improving education through big data; and <br/>- Reducing barriers for data sharing within and between different sectors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841588","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Zsuzsanna Marka","NY","Columbia University","Standard Grant","William Miller","09/30/2020","$27,600.00","","zsuzsa@astro.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835784","Elements: Data: HDR: Collaborative Research: Developing an On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","Polar Cyberinfrastructure, Data Cyberinfrastructure","01/01/2019","05/07/2021","Hongjie Xie","TX","University of Texas at San Antonio","Standard Grant","Amy Walton","12/31/2022","$291,505.00","Alberto Mestas-Nunez","hongjie.xie@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","5407, 7726","062Z, 077Z, 7923, 9251","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835512","Collaborative Research:  Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","Data Cyberinfrastructure","01/01/2019","05/06/2021","Xin Miao","MO","Missouri State University","Standard Grant","Amy Walton","12/31/2022","$187,966.00","","XinMiao@missouristate.edu","901 South National","Springfield","MO","658970027","4178365972","CSE","7726","062Z, 077Z, 7923, 9251","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750865","CAREER:  GPU-Accelerated Framework for Integrated Modeling and Biomechanics Simulations of Cardiac Systems","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","03/01/2018","05/07/2021","Adarsh Krishnamurthy","IA","Iowa State University","Continuing Grant","Alan Sussman","02/28/2023","$516,000.00","","adarsh@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1045, 7231","026Z, 062Z, 1045, 9251","$0.00","Cardiovascular diseases, such as heart failure, are one of the leading cause of death in the U.S. and pose a severe burden to the healthcare system. Most current treatments for cardiovascular diseases are based on rough estimates of outcomes from the results of clinical trials, which might not apply to individual patients due to patient-specific variations. Computational models of the cardiovascular system, developed from patient-specific clinical data, can help refine the diagnosis and personalize the treatment, significantly improving patient care and reducing mortality. The current patient-specific methods for cardiovascular diseases have been demonstrated mainly in simple, isolated examples. For widespread adoption of personalized medicine, a flexible and easy-to-use framework for integrating patient data and simulating cardiac biomechanics needs to be developed. This project focuses on creating an integrative framework with simulation, analysis, and visualization tools that will significantly advance the state-of-the-art in personalized medicine, ultimately improving patient care and treatment outcomes. Results from this research will benefit the U.S. healthcare system, society, and economy, while supporting the NSF mission to promote the progress of science and advance the national health. The tools developed as a part of this research involves several disciplines including computer science, bioengineering, and mechanical engineering. The multidisciplinary components of the project is being integrated into a larger educational effort that offers the students a solid foundation in developing computational tools and algorithms, while also broadening the participation of underrepresented groups in research.<br/><br/>The primary objective of this research is the advancement of the state-of-the-art in translational medicine with the help of computational modeling and interactive analysis tools to improve the basic understanding of the cardiac muscle and personalize treatment of cardiovascular diseases in patients. The research focuses on creating a novel computational framework to automate biomechanics finite-element simulation and analysis of patient-specific cardiac systems. Further, it aims to advance the knowledge of disease and therapeutic mechanisms by developing advanced multiscale methods to model muscle contraction and growth. Some of the key computational tools and methods proposed as part of this framework include: (1) a geometric mesh generation tool for systematic generation of patient-specific finite element meshes from clinical data; (2) an algorithm for accelerating high-order finite-element simulations using the graphics processing unit (GPU) for fast tuning of model parameters to match the patients' baseline cardiac function; (3) new methods for multiphysics simulations of cardiac systems to model multi-scale muscle mechanics and tissue growth; and (4) new visualization and virtual reality tools to enable animated volume rendering and visual analytics of the results of the cardiac simulations. Successful development of these open-source tools will enable faster adoption of patient-specific computational models by the research community to understand therapeutic mechanisms. This framework can significantly advance the state-of-the-art in personalized medicine, ultimately improving patient care and treatment outcomes. The multidisciplinary components of the project is being integrated into a larger educational effort to offer students a solid foundation in combining biomedical engineering with scientific computing. The education and outreach plans of this research can inform the community about the crucial role of computational models in improving patient-care.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117247","MRI: Acquisition of a High Performance Hybrid Computer Cluster for Computational Modeling","OAC","Major Research Instrumentation","10/01/2021","11/26/2021","Gerardo Cisneros","TX","University of North Texas","Standard Grant","Alejandro Suarez","09/30/2024","$686,389.00","Thomas Cundari, Jincheng Du, Oliviero Andreussi, Hao Yan","andres@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1189","102Z, 1189, 9102","$0.00","This project will permit the purchase, installation, and operation of a new high-performance computing (HPC) resource, called CRUNTCH4, to be deployed at the University of North Texas? (UNT) Center for Advanced Scientific Computing and Modeling (CASCaM). This much needed HPC resource combines different computing architectures and significant amounts of data storage, all connected via a high-speed communications fabric. This computing resource will provide the means for CASCaM investigators to continue research on a broad range of topics including quantum chemistry, materials design, biomolecular simulations, machine-learning based chemical discovery, and bioinformatics, among others. CASCaM hosts a robust computational modeling program composed of computational and experimental researchers involving development and application of computational tools in an interdisciplinary environment encompassing Chemistry, Physics, Biology, Materials Science and Engineering. UNT?s CASCaM program has a strong emphasis on research and research training of early career scientists ? Ph.D., undergraduate, and high school students. The CASCaM researchers comprise 40% women and 20% minoritized peoples. Additionally, UNT Chemistry has one of the longest-running NSF-REU summer scholar programs in Chemistry, with many participants using CASCaM computing resources. <br/><br/> <br/>The HPC resource design and capabilities will enable current and future CASCaM researchers to conduct state-of-the-art simulations, addressing a diverse array of important scientific problems. The hybrid architecture of CRUNTCH4 is composed of a 54-node HPC system coupled to a large storage device and medium-term backup storage device with high-speed interconnects. This system comprises 43 general purpose Intel Xeon CPU nodes (36 core, 196 GB mem/node), 7 Nvidia Ampere graphics processing unity (GPU) nodes (4 GPU/node with NVLink), 2 large memory CPU nodes (48 core,1.5 TB mem/node), two administration nodes, one 1 PB ExaScaler Network-attached Storage (NAS), and one 670 TB block storage array node, all connected via HDR100 InfiniBand fabric, resulting in over 484 TFLOPS of peak theoretical performance. The heterogeneous architecture of this resource will enable the investigation of a broad range of scientific topics, including high-level quantum chemistry, hybrid quantum mechanics/molecular mechanics, materials design, biomolecular simulations, bioinformatics, machine-learning based chemical discovery, to name a few. The diversity of career experience and research topics in CASCaM makes for an exhilarating scientific environment, and thus the impact of the equipment across different scientific fields and career stages will be tremendous.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939067","CAREER: Organizational Capacity and Capacity Building for Cyberinfrastructure Diffusion","OAC","CAREER: FACULTY EARLY CAR DEV","06/01/2019","09/20/2019","Kerk Kee","TX","Texas Tech University","Standard Grant","Alan Sussman","09/30/2021","$241,926.00","","kerk.kee@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","1045","1045, 9179","$0.00","The vision behind advanced cyberinfrastructure (CI) is that its development, acquisition, and provision will transform science and engineering in the 21st century. However, CI diffusion is full of challenges, because the adoption of the material objects also requires the adoption of a set of related behavioral practices and philosophical ideologies. Most critically, CI-enabled virtual organizations (VOs) often lack the full range of organizational capacity to effectively integrate and support the complex web of objects, practices, and ideologies as a holistic innovation.<br/><br/>This project examines the various manifestations of CI related objects, practices, and ideologies, and the ways they support CI implementation in scientific VOs. Using grounded theory analysis of interviews and factor analysis of survey data, this project will develop and validate a robust framework/measure of organizational capacity for CI diffusion. The project's empirical focus will be the NSF-funded Extreme Science and Engineering Discovery Environment (XSEDE; https://www.xsede.org/), a nationwide network of distributed high-performance computing resources. Interviews and surveys will solicit input from domain scientists, computational technologists, and supercomputer center administrators (across e-science projects, institutions, and disciplines) who have experience with adopting and using CI tools within the XSEDE ecosystem. The project will generate a series of capacity building strategies to help VOs increase the organizational capacity necessary to fully adopt CI. Findings will help NSF and other federal agencies to improve existing and future CI investments. This project may also have implications for open-source and commercial technologies that harness big data for complex simulations, modeling, and visualization analysis."
"1850012","CRII: Algorithms and Methodologies for Real-Time Decision-Making of Mission-Critical Structures Experiencing High-Rate Dynamics","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","03/15/2019","05/12/2020","Austin Downey","SC","University of South Carolina at Columbia","Standard Grant","Alan Sussman","02/28/2023","$191,000.00","","austindowney@sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","026Y, 9150","8228, 9150, 9251","$0.00","This project focuses on investigations into methodologies to enable real-time decision-making for mission-critical structural systems experiencing high-rate dynamics. Examples of mission-critical structures that experience high-rate dynamics include hypersonic vehicles, space crafts, ballistics packages, and active blast mitigation. Enabling real-time decision-making for these structures increases mission success rates by enhancing the structure's survivability, providing on-time guidance corrections, and adopting the mission goals/outcome to changing conditions. Additionally, the methodologies developed by this project increase the robustness, safety, and commercial viability of structures operating in extreme dynamic environments. The development of algorithms and methodologies for structures experiencing high-rate dynamics serves the national interest and fulfills the NSF's mission:  to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.  This project provides research experience and mentors a diverse and inclusive group of students, and introduces a graduate level class on surrogate modeling of complex systems.<br/><br/>A structural system operating in a high-rate dynamic environment can experience sudden and unmodeled plastic deformation of the structure that may further lead to damaged electronics, sensors, and/or delicate payloads. This research focuses on enabling a real-time decision-making module to take corrective actions. To achieve this goal, a parallelization framework is developed that enables a structural system to be decomposed into its constituent components where each component can be monitored, modeled, and extrapolated into the future. Once the degradation trajectories for each component have been estimated, they are recombined into a single system-level model to be used for real-time decision making.  The project is organized into two research thrusts and one experimental design challenge. Thrust 1 investigates surrogate modeling techniques with the goal of developing component-level models that can converge within the required time constraints. These surrogate models use data obtained from dense sensor networks to generate data-driven damage-sensitive features that can be used as the degradation parameters for component-level prognostics. Thrust 2 explores and formulates methodologies for real-time component-level prognostics. These component-level predictions are then recombined into a single structural model that is used to develop potential corrective actions. Lastly, the experimental design challenge validates the developed algorithms and methodologies using a high-rate dynamic test bench.<br/><br/>This project is jointly funded by Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931516","Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering","OAC","Special Initiatives, Software Institutes","01/01/2020","08/14/2019","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Seung-Jong Park","12/31/2024","$1,284,328.00","M. Gregory Forest","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1642, 8004","026Z, 028E, 077Z, 7925, 8004, 8084, 9263","$0.00","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.<br/><br/>This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835712","Elements:Software:Open-Source Robust Geometry Toolkit for Black-Box Finite Element Analysis","OAC","Data Cyberinfrastructure","09/01/2018","09/06/2018","Daniele Panozzo","NY","New York University","Standard Grant","Robert Beverly","08/31/2022","$599,967.00","Denis Zorin","panozzo@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7726","026Z, 077Z, 7923, 8004","$0.00","The numerical solution of partial differential equations (PDEs) is ubiquitous in science and engineering applications, including simulation of elastoplastic deformations, fluids, and light scattering. The finite element method (FEM) is the most commonly used discretization of PDEs, especially in the context of structural and thermal analysis, due to its generality and rich selection of off-the-shelf commercial implementations. Ideally, a PDE solver should be a ``black box'': the user provides as input the domain boundary, boundary conditions, and the governing equations, and the code computes the value of the solution at a set of user-specified points of the input domain. This is surprisingly far from being the case for all existing open-source or commercial software, despite the research efforts in this direction and the large academic and industrial interest. To a large extent, this is due to treating meshing and FEM basis construction as two disjoint problems, often exposing the user to the technical issues of interfacing the meshing software with FEM basis construction, both of which, strictly speaking, are technical issues internal to the solver. This state of matters presents a fundamental problem for applications that require fully automatic, robust processing of large collections of meshes of varying sizes, an increasingly common situation as large collections of geometric data become available. This proposal introduces an integrated pipeline, considering meshing and element design as a single challenge, and developing a software platform to enable black box analysis on complex geometric models represented as point clouds, triangle meshes, or CAD (Computer Aided Design) models, opening the door to new shape design technique to a wide range of new applications in sciences and engineering.<br/><br/>This project proposes to develop a set of software components based on a set of novel approaches the investigators have developed combined with ""filtered"" use of rational or multi-precision numerical representations to handle robustness problems while maintaining practical performance. The proposed set of geometry processing techniques, while slower than existing ones, are fully robust in a sense of always produce a valid result with minimal assumptions on the input. The geometric toolkit will allow to automatically convert geometrical data in the form of range scans, CAD models, or voxel grids into a surface or volumetric representation, directly usable in widely used open-source finite element method (FEM) packages. It will include mesh generation, in addition to tetrahedral meshes, for other common types of discretizations: hexahedral meshes, and hex-dominant hybrid meshes. The key innovation is to achieve numerical robustness with minimal added algorithmic complexity by carefully mixing higher precision representations for the critical part, while relying on standard fixed-precision floating point representation for the rest and designing algorithms amenable to this approach. As in overwhelming majority of cases higher accuracy is needed for a vanishingly small fraction of computation, this approach allows the users to achieve sensible running time while ensuring output validity and algorithmic correctness on imperfect, real world data. Secondly, the invetigators will integrate FEM basis construction with meshing decoupling accuracy from mesh quality. The software toolkit developed in this proposal has potential for a major impact in all domains that require computational simulation of physical phenomena in complex geometries, enabling the automation of data acquisition, reconstruction, and simulation pipelines. The expectation of this project is that the outcome will not only be a reduction in human time, but the opportunity to fully automate this pipeline will open new research venues. The release of all the software with a MPL2 license will facilitate integration of the results of the work into commercial software, in addition to academic/non-profit research use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027518","Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations.","OAC","COVID-19 Research","05/15/2020","05/11/2020","Sirish Namilae","FL","Embry-Riddle Aeronautical University","Standard Grant","Seung-Jong Park","04/30/2022","$50,000.00","","namilaes@erau.edu","Sponsored Research Adm","Daytona Beach","FL","321143910","3862267695","CSE","158Y","077Z, 096Z, 7914, 8004","$0.00","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.<br/><br/>We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of  COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931483","Collaborative:Elements:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel","OAC","Software Institutes","11/01/2019","06/11/2020","Sirish Namilae","FL","Embry-Riddle Aeronautical University","Standard Grant","Robert Beverly","10/31/2022","$163,500.00","","namilaes@erau.edu","Sponsored Research Adm","Daytona Beach","FL","321143910","3862267695","CSE","8004","026Z, 075Z, 077Z, 7923, 8004, 9229, 9251","$0.00","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.<br/><br/>This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039794","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","HDR-Harnessing the Data Revolu","07/01/2020","10/14/2020","Yusu Wang","CA","University of California-San Diego","Standard Grant","Alexis Lewis","09/30/2022","$357,730.00","","yusuwang@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","099Y","062Z","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004839","Collaborative Research: Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples","OAC","XC-Crosscutting Activities Pro, Sedimentary Geo & Paleobiology, Software Institutes, EarthCube","08/15/2020","10/12/2021","Kerstin Lehnert","NY","Columbia University","Standard Grant","Alan Sussman","07/31/2024","$1,277,628.00","Sarah Ramdeen","lehnert@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7222, 7459, 8004, 8074","077Z, 7925, 8004","$0.00","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.<br/><br/>The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031891","Frontera Travel Grant: Flow Induced Phenomena in Entangled Polymeric Fluids","OAC","Leadership-Class Computing","10/01/2020","05/12/2020","Bamin Khomami","TN","University of Tennessee Knoxville","Standard Grant","Edward Walker","08/31/2022","$9,804.00","","bkhomami@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940203","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","Special Initiatives","10/01/2019","09/14/2019","WaiChing Sun","NY","Columbia University","Standard Grant","Giovanna Biscontin","09/30/2022","$418,000.00","","wsun@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","099y, 1642","062Z, 085E, 8021","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940125","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","","10/01/2019","09/14/2019","Yusu Wang","OH","Ohio State University","Standard Grant","Giovanna Biscontin","10/31/2020","$387,579.00","","yusuwang@ucsd.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","099y","062Z","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835769","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","Data Cyberinfrastructure, EarthCube","10/01/2018","08/13/2018","Pierre Gentine","NY","Columbia University","Standard Grant","Amy Walton","09/30/2022","$307,426.00","","pg2328@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7726, 8074","062Z, 077Z, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004512","Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning","OAC","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","10/01/2020","10/15/2020","M Joan Alexander","WA","NorthWest Research Associates, Incorporated","Standard Grant","Alan Sussman","09/30/2025","$1,061,352.00","","alexand@nwra.com","45th Street Plaza Bldg","Seattle","WA","981054696","2065568151","CSE","5740, 8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid.  They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify.  Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices.  These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.<br/><br/>The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2050943","REU Site: Online Interdisciplinary Big Data Analytics in Science and Engineering","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2021","05/21/2021","Jianwu Wang","MD","University of Maryland Baltimore County","Standard Grant","Alan Sussman","03/31/2024","$293,710.00","Matthias Gobbert","jianwu@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1139","8083, 9250","$0.00","This REU Site program will provide 8-week summer online research experiences to undergraduates on how to utilize modern data science and high-performance computing (HPC) techniques to process and analyze big data in many science and engineering disciplines such as Atmospheric Science, Mechanical Engineering, and Medicine. The REU Site program will be conducted purely online to allow students to conduct research without traveling and to work with experts nationwide. In recent years, the astronomical growth of available datasets in many science and engineering disciplines often requires big data analytics techniques to efficiently and effectively process the large datasets and obtain knowledge and insight from them. The program will help students identify frontier research challenges encountered when facing big data in science and engineering, and guide students to conduct research to tackle those challenges using advanced cyberinfrastructure software technologies (big data, distributed machine/deep learning, HPC, etc.) and hardware resources (including big data clusters, CPU clusters and GPU clusters). The program will develop the national workforce in areas of critical need on ?Data + Computing + X?. The project thus serves the national interest, as stated by NSF's mission, to promote the progress of science and advance the national prosperity and welfare. <br/><br/>By having three phases of training, namely formal instruction, team-based research and dissemination, this REU Site program will i) ignite students' interest in how data science and high-performance computing techniques can aid in the scientific discovery process via interdisciplinary research projects; ii) provide interdisciplinary team-based research experiences via guidance from research mentors, graduate assistants, and interdisciplinary collaborators in the application domain area of each project; and iii) provide integrated training on crucial professional skills (including skills in collaboration, communication, presentation and writing, and experience in scientific paper preparation and presentation). This REU Site will provide a unique and comprehensive program integrating the following elements under the guidance of the faculty team, who have extensive experience in interdisciplinary research and online education: 1) frontier research connecting advanced cyberinfrastructure techniques with big data challenges in science and engineering, 2) online research experience that leverages modern communication tools, 3) team-based interdisciplinary research and communication experience, 4) complementary professional development activities (such as graduate school preparation, invited talks from established researchers, and interaction with university administers including college deans and department chairs), 5) educational research on online undergraduate training experiences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032628","EAGER: Open Science Data in Semantic Web Research","OAC","GVF - Global Venture Fund, NSF Public Access Initiative","09/01/2020","08/23/2021","Pascal Hitzler","KS","Kansas State University","Standard Grant","Martin Halbert","08/31/2022","$321,809.00","Krzysztof Janowicz, Cogan Shimizu","hitzler@ksu.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","054Y, 7414","5936, 7916, 9150, 9251","$0.00","Establishing Open Science as the default practice in the academic community is critical. Reporting a research result is more than just the manuscript. It is the actual process undertaken, the data (and metadata), the implemented software (and environment), and more. It is also important to ask questions pertaining to transparency and reproducibility, such as those regarding their reviews, publishing metadata, and, in particular, Where are the data and software that underlie their reported results? By making the connections between these artifacts through Open Science practices, it democratizes research by lowering the barrier of entry to understanding (and replicating) cutting edge research, while simultaneously accelerating novel research by reducing the need to re-implement software or re-collect data. Connecting these components and data in a useful and understandable way is a core mission of Semantic Web research. <br/><br/>The PIs propose to leverage the leadership and reputation of the Semantic Web journal by IOS Press to demonstrate that the open sharing of data underlying publications can be done without significant overhead and provides significant added value. The PIs will, for this journal, require authors to sustainably publish the data and software underlying their results, include software and data provision as part of the peer review assessment, and publish rich metadata for the supplementary data and software.  The PIs will develop technology in support of open science and will work with the community to refine the software and policies that enhance open science for the Semantic Web journal. The approach can serve as a model for other journal editors who seek a blueprint for adopting similar Open Science Data practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835692","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","Data Cyberinfrastructure, EarthCube, Big Data Science &Engineering","01/01/2019","08/27/2018","Ivan Rodero","NJ","Rutgers University New Brunswick","Standard Grant","Amy Walton","12/31/2022","$899,139.00","Juan Jose Villalobos","irodero@cac.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7726, 8074, 8083","062Z, 077Z, 7925, 8083","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835566","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","EarthCube","01/01/2019","08/27/2018","Kristy Tiampo","CO","University of Colorado at Boulder","Standard Grant","Amy Walton","12/31/2022","$432,326.00","","kristy.tiampo@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8074","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835509","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Greg Bryan","NY","Columbia University","Standard Grant","Bogdan Mihaila","08/31/2022","$439,350.00","","gbryan@astro.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835507","Collaborative Research: Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","Data Cyberinfrastructure, Software Institutes, EarthCube","01/01/2019","03/08/2021","Chaowei Yang","VA","George Mason University","Standard Grant","Amy Walton","12/31/2022","$372,765.00","","cyang3@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7726, 8004, 8074","019Z, 062Z, 077Z, 7923, 9251","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017564","CyberTraining: Implementation: Small: Collaborative and Integrated Training on Connected and Autonomous Vehicles Cyber Infrastructure","OAC","CyberTraining - Training-based","08/01/2020","07/16/2020","Qing Yang","TX","University of North Texas","Standard Grant","Alan Sussman","07/31/2023","$499,438.00","Song Fu","qing.yang@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","044Y","","$0.00","In response to the quick development and adoption of connected and autonomous vehicles (CAVs), U.S. cities and states have recently started building CAV cyberinfrastructure. However, there is not an adequate supply of skilled research workforce in this field. A key obstacle to such workforce development is the lack of well-structured training programs for utilizing CAV cyberinfrastructure to enable and potentially transform fundamental CAV research. To address this issue, a project-oriented training program will be developed in this CyberTraining project to enable scientific research workforce development for CAV cyberinfrastructure. It is estimated that the adoption of CAVs would lead to nearly $800 billion in annual social and economic benefits by 2050, therefore, it is important for the nation to invest in CAV cyberinfrastructure research workforce training programs. The proposed training program targets students and early-stage researchers who are interested in CAVs, including participants with a broad diversity in academic level and in experience level with CAVs. It is expected that more than 100 trainees will participate every year in the training program, including researchers from various domains such as cyber-physical systems, edge computing, wireless networking, deep learning, computer vision, and big data. A longstanding collaboration with the trainees and/or their advisors will be built to ensure a broad adoption of CAV cyberinfrastructure by the research community to catalyze major research advances. The long-term goal of this project is to develop a first of its kind open CAV cyberinfrastructure, an integrated training and research hub, to accelerate research and education in CAVs.<br/><br/>The goal of this project is to develop a collaborative and integrated training program to enable scientific research work force development for Connected and Autonomous Vehicle CyberInfrastructure (CAV-CI) and foster broad adoption of CAV-CI to advance fundamental CAV related research. To achieve these goals, the project will leverage existing partnerships with relevant stakeholders to create tailored, high-impact, engaging, collaborative, and integrated training modules for CAV-CI research workforce development. With the aim of enhancing trainees design and implementation capabilities, problem-solving skills, and critical thinking ability, the proposed training program will result in: (1) a project-oriented short course plus long-term coaching and support, (2) hands-on training modules on the perception, network, and application layers in CAV-CI, (3) an annual research workshop that disseminates research results and receives feedback on the training  program from the research and industrial communities, and (4) research projects for students supported through NSF's Research Experiences for Undergraduates (REU)program and capstone projects for senior undergraduates. During the training workshops, project-oriented training will be offered to actively engage trainees in learning and solving real-world problems. Three sample research projects will be designed, allowing trainees to develop complete research skills, i.e., competency to solve authentic problems. Following every sample research project, with each having a strong practical relevance and meaningfulness, two versions of training modules will be developed to reach a broader trainee group: a fundamental training module for undergraduate students and community college educators, and a research-intensive training module for graduate students and postdocs. By taking either the fundamental or the research-intensive training modules, trainees will enhance their problem-solving skills, improve their creative and independent thinking ability, as well as gaining enthusiasm and confidence in conducting CAV-CI enabled research.  The CAV-CI education, research and training activities include specific goals to train individuals from underrepresented groups and the broader STEM workforce.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931354","Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems","OAC","Software Institutes","11/01/2019","07/23/2019","William Barth","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","10/31/2022","$383,161.00","Zhao Zhang","bbarth@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 075Z, 077Z, 7925, 8004","$0.00","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)<br/><br/>The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762378","Spokes: MEDIUM: SOUTH: Collaborative: Enhanced 3-D Mapping for Habitat, Biodiversity, and Flood Hazard Assessments of Coastal and Wetland Areas of the Southern US","OAC","BD Spokes -Big Data Regional I","08/15/2018","08/16/2018","James Gibeaut","TX","Texas A&M University Corpus Christi","Standard Grant","Alejandro Suarez","07/31/2022","$142,000.00","","james.gibeaut@tamucc.edu","6300 Ocean Dr.","Corpus Christi","TX","784125844","3618252730","CSE","024Y","8083","$0.00","The risk to coastal populations and infrastructure from flooding due to sea level rise, severe storms, and river discharge will increase for U.S. southern states. The vision of this project is that communities occupying low-lying coastal areas of the southern US will be protected and develop in a sustainable manner through planning based on knowledge, conservation, and wise use of sensitive lands. Researchers from the University of South Florida's College of Marine Science and the School of Geosciences, Texas A&M University Corpus Christi, and Google Earth Engine are collaborating with the South Big Data Hub through this project to develop more accurate, ultra-high resolution topographic, land cover, and urban environment geospatial products. The project examines in detail areas that were directly impacted by Hurricanes Harvey and Irma in 2017, and identifies flood-prone areas across the region. The 3D maps show habitat diversity, needed to plan for conservation and development in these important ecosystems.<br/><br/>This project will develop the improved topographic and land cover maps of the south States within 50 Km of the coast from Texas to Florida (an area >220,000 square Km). The maps will be constructed using a Big Data approach, using detailed historical airborne LiDAR (Light Detection and Ranging) data collected from airplanes merged with high spatial resolution (<2 m pixel) multispectral commercial satellite imagery. The project will also include research into detailed 3D mapping of urban areas using Structure-from-Motion (SfM) methods; specifically the project will map portions of Houston/Corpus Christi in Texas, and Tampa/Saint Petersburg in Florida, using Kite Photography and light aircraft. The production of land cover maps and digital elevation models requires the fusion of very large amounts of disparate data and efficient, automated techniques. The project will develop the strategies to aggregate these data into useful products using Google Earth Engine and a High Performance Computing cluster. The project will distribute all products openly via NOAA's Digital Coastal portal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106661","Collaborative Research: OAC Core: ScaDL: New Approaches to Scaling Deep Learning for Science Applications on Supercomputers","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","08/31/2021","Zhao Zhang","TX","University of Texas at Austin","Standard Grant","Alan Sussman","09/30/2024","$226,442.00","","zzhang@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","090Y","026Z, 079Z, 7923","$0.00","Today's deep learning (DL) revolution is enabled by efficient deep neural network (DNN) training methods that capture important patterns within large quantities of data in compact, easily usable DNN models. DL methods are applied routinely to tasks like natural language translation and image labeling--and, in science and engineering, to problems as diverse as drug design, environmental monitoring, and fusion energy. Yet as data sizes increase and DL methods grow in sophistication, the time required to train new models often emerges as a major challenge. The Scalable Deep Learning (ScaDL) project will address this challenge by making it possible to use specialized high-performance computing (HPC) systems to train bigger models more rapidly. Efficient use of the thousands of powerful processors in modern HPC systems for DNN training has previously been stymied by communication costs that grow rapidly with the number of processors used. ScaDL will overcome this obstacle by developing new DNN training methods that reduce communication requirements by performing additional computation, by validating the effectiveness of these new methods in a range of scientific applications that use DL in different ways, and by integrating the new methods into scalable DL software for use by domain scientists, computer scientists, and engineers supporting DL application in HPC centers. By permitting the use of powerful HPC systems to train DNN models thousands of times faster than on a single computer, ScaDL will enable advances in many areas of science and engineering. The project will also contribute to educational outcomes by engaging PhD students in project goals, by using ScaDL tools in a new DL systems engineering class at the University of Chicago, and by enlisting participants in summer schools at the Texas Advanced Computing Center (TACC) and U. Chicago, both of which target recruitment of students from underserved communities at the graduate, undergraduate, and high-school levels, to apply the tools to scientific problems. ScaDL's focus on science applications and education aligns the project with NSF's mission of promoting the progress of science.<br/><br/>The ScaDL project contributes to science in two ways. First, it explores new techniques for enhancing the speed and scalability of commonly used optimization methods without losing model performance, by: 1) exploiting scalable algorithms for second-order information approximation; 2) developing methods for adapting to different computer hardware by tuning computation and communication to maximize training speed; 3) exploring compression techniques to reduce communication overheads; 4) using well-known benchmark applications to evaluate the convergence of ScaDL; and 5) applying its new algorithms and systems to science applications.  Second, it will release an open-source implementation of the proposed algorithms and system. The implementation will be available on a variety of hardware platforms and capable of choosing the ratio of computation and communication required to make efficient use of the computation and communication hardware on a particular HPC system. The resulting algorithms and system will help disseminate ScaDL research results to a wide spectrum of research domains and users, and promote the adoption of the new methods in practical settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2008388","Collaborative Research: OAC Core: Small: Efficient and Policy-driven Burst Buffer Sharing","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","05/20/2020","Zhao Zhang","TX","University of Texas at Austin","Standard Grant","Robert Beverly","09/30/2022","$292,863.00","LEI HUANG","zzhang@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","090Y","026Z, 7923","$0.00","Modern scientific research heavily relies on supercomputers. Supercomputing applications, such as traditional numerical simulations (HPC), data intensive applications (Big Data), and most recently, deep learning (DL) applications, are increasingly run on supercomputers to obtain timely results and explore new research methods that combine multiple application types. However, a bottleneck in their design reduces the potential performance of modern supercomputers. This project, bbThemis, addresses this problem by enabling efficient and policy-driven sharing of an intermediate storage layer known as a ""burst buffer"", so that more scientists and applications can leverage state-of-the-art storage techniques to significantly reduce their runtime and enhance the productivity of their research. This project will deliver substantial gains to almost every research area that uses HPC resources, leading to improved science and engineering methods and products in all fields. This research will have an immediate and significant impact on existing scientific applications and on deriving guidelines for next-generation HPC system design, deployment, and utilization. The project will also contribute to educational outcomes. In addition to students working directly on project goals, results developed in the project will be used in tutorial and training sessions at Texas Advanced Computing Center?s summer institute in deep learning and other major conferences, and in University of Illinois Urbana-Champaign student projects. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC.<br/><br/>This project, bbThemis (https://github.com/bbThemis), leverages a suite of technologies, such as disassociation of I/O processing from control logic, time-sliced intra I/O node sharing, function interception for low overhead POSIX I/O, and metadata and data placement for optimal individual application performance. It is investigating how to best apply these technologies, by: 1) Identifying optimal burst buffer configurations for a suite of representative supercomputing applications; 2) Proposing, prototyping, and verifying different design options to address intra-node and inter-node I/O performance sharing; and 3) Designing and evaluating a set of sharing policies, such as fair sharing and priority sharing, with real applications and I/O traces. This project will dramatically increase the sharing capacity of existing burst buffers and enhance domain scientists? productivity at a large scale.  It explores various sharing policies that permit efficient sharing of I/O resources and that meet the requirements of computing centers. The results will enable the provisioning of I/O resources, where users can request specific IOPS or bandwidth for a period of time.  The prototype burst buffer sharing framework will immediately increase the capacity of existing supercomputers with enhanced I/O performance. The lessons learned will guide next-generation I/O system design for large scale systems. The general improvement of HPC, Big Data, and DL applications will also increase the coherence of the hardware and software used for data analytics computing and modeling and simulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828380","MRI: Acquisition of Cloud Computing Infrastructure for Science and Engineering Research Innovations","OAC","Major Research Instrumentation","09/01/2018","09/07/2021","Ajay Katangur","TX","Texas A&M University Corpus Christi","Standard Grant","Alejandro Suarez","08/31/2022","$517,657.00","Saman Desilva, Dulal Kar, Scott King, Ning Zhang, Jinha Jung, Michael Starek","ajaykatangur@missouristate.edu","6300 Ocean Dr.","Corpus Christi","TX","784125844","3618252730","CSE","1189","026Z, 1189","$0.00","This award provides for the acquisition of the Island Science and Engineering Research Cloud (ISERC). ISERC will support major innovative research at Texas A&M University-Corpus Christi (TAMUCC) and other institutions in South Texas. ISERC is an ideal base to tackle a multitude of research problems of regional, national and global significance by providing high capacity computing resources to empower science and engineering research. ISERC enabled projects include geospatial analytics, coastal resiliency, ecology, genomics, bio-informatics, environmental monitoring, health, geology, water resource management, remote sensing, unmanned systems, agriculture, and climate prediction. This research will have impact on the environmental sustainability, health and economic well-being of coastal communities across the globe. The instrument will be used by approximately 100 researchers comprising faculty, graduate and undergraduate students. It is estimated that, each year, more than 300 students enrolled in STEM related courses would also benefit from it.<br/><br/>ISERC supports heterogeneous computing environments with low-latency, high-bandwidth, high computational power, and storage for large data sets. It provides users with a dynamically scalable, robust, heterogeneous, and virtualized architecture. By allowing confidential and secure execution of guest virtual machines, it ensures the privacy and security of data. The new cloud instrument can handle a wide variety of services simultaneously, including ones for real-time, compute-bound, as well as data intensive applications. Some of the major projects that are poised to commence include: hyper-spatial 3D data streams for coastal resiliency, high throughput phenotyping for agriculture, social media data analytics for security, cloud data migration, seagrass genetics, biomimetic catalysis, climate impact on water resources, genome sequencing, and predicting environmental factors for respiratory diseases. ISERC will propel the academic community's knowledge generation and provide research opportunities that will enable many further follow-up projects, including multi-institutional and cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103794","Elements: Open-Source Cyberinfrastructure as a Decision Engine for Socioeconomic Disaster Risk (DESDR)","OAC","Software Institutes","10/01/2021","09/01/2021","Daniel Osgood","NY","Columbia University","Standard Grant","Tevfik Kosar","09/30/2024","$599,998.00","Eugene Wu, Lydia Chilton","deo@iri.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004","077Z, 7923, 8004","$0.00","Natural disasters can have long-lasting financial consequences for vulnerable populations such as farmers, but disparate disaster impacts make the accurate and timely deployment of limited resources such as disaster funding and relief very difficult. This project will develop an open-source suite of cyberinfrastructure tools, called the Decision Engine for Socioeconomic Disaster Risk (DESDR), to fill data voids by collecting and cleaning disaster risk data directly from affected populations. As well, the project will combine those data with satellite data to provide farmers and others with data and tools to make better informed decisions about disaster risk management. <br/><br/>Existing approaches for disaster detection, mapping, and prediction primarily rely on satellite and sensor data that are susceptible to errors and do not measure how disasters directly affect regions and individuals. This project will develop open-source software infrastructure employing data from rural populations, and then use the data to build more accurate disaster risk models. The cyberinfrastructure, DESDR, will provide a scalable, customizable data collection platform using mobile messaging services that take local incentives and community norms into account, and will be used to gather data on from affected communities. A data visualization and cleaning platform will enable local partners and researchers to cross-reference the data with satellite and sensor data sources, and to identify and clean data errors. A database architecture will store the data in an interoperable format. And, a web-based interface will provide government agencies, policy makers, researchers, and other stakeholders the ability to interactively create and back-test disaster risk models. DESDR will be disseminated as open-source, ready-to-deploy software to a user community of governmental meteorological agencies, humanitarian program officers, insurers and affiliated agricultural and social scientists. By combining these tools into an integrated, extensible process cyberinfrastructure, DESDR will directly involve vulnerable populations in the design of solutions, enabling disaster risk managers to scale up critical relief programs to reach multitudes of farmers and others while providing an unprecedented voice to the project beneficiaries.<br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, and by the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931321","Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/07/2019","Timothy Berkelbach","NY","Columbia University","Standard Grant","Robert Beverly","09/30/2022","$450,000.00","","tcb2112@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7925, 8004, 8009, 9216, 9263","$0.00","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies.  However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties.  This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.<br/><br/>This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835778","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","Data Cyberinfrastructure","11/01/2018","08/23/2018","Ryan Abernathey","NY","Columbia University","Standard Grant","Amy Walton","10/31/2022","$254,103.00","","ra2697@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7726","062Z, 077Z, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940175","Collaborative Research: Accelerating the Discovery of Electronic Materials through Human-Computer Active Search","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","09/17/2019","Remco Chang","MA","Tufts University","Standard Grant","Daryl Hess","09/30/2022","$231,838.00","","remco@cs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","099Y, 1712","054Z, 062Z, 8249, 8396, 8611","$0.00","The overarching goal of this project is to accelerate the discovery of  materials with tailored electronic properties through human-computer active search. These efforts will lay the groundwork for accelerating materials discovery, and advance the capability to control electronic properties in materials with the potential for profound societal impact. The thermoelectric and photocatalytic materials predicted, synthesized, and characterized in this research can realize societal advances in the space of energy and solar fuels. High-efficiency thermoelectric materials can revolutionize how heat sources are transformed into electrical power by eliminating the traditional intermediate mechanical energy conversions.  Earth-abundant  light-responsive catalysts are emerging as  an alternative to costly, rare metal catalysts to store solar energy as  portable liquid fuels, like ethanol. These green reactions  are enabling low-cost, carbon-neutral fuels.  The team brings together expertise in materials science, chemistry, machine learning, visualization, metadata, and knowledge frameworks to develop multi-fidelity, expert-guided active search strategies within materials science and chemistry.  Resonances among the team's existing outreach programs will broaden inclusion of students from underrepresented groups and be moderated via the Alliance for Diversity in Science and Engineering.  The work will provide cross-disciplinary training to graduate students and postdocs in all aspects of material informatics, including participating in and leading team efforts, co-mentorship of Ph.D.  and postdoctoral researchers, inclusive symposia at national conferences, and a summer workshop focused on the intersection of visualization, machine learning, ontological engineering and materials science. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative. <br/><br/>An interdisciplinary team will create a search framework for scientific discovery that leverages recent advances in material databases, machine learning, visualization, human-machine interaction, and knowledge structures. To broadly assess the efficacy of this approach, the search effort will span the electronic behavior of both molecules and crystalline materials:  (i) new organic photocatalysts for solar fuels production and (ii) new thermoelectric materials for electricity generation.  Central to this effort is the engagement of domain experts and associated feedback in a human-in-the-loop active search process. Dynamic visualizations will enable the user to (i) understand the underlying reasons why the materials are being suggested and (ii) provide a user steering capability to identify and annotate specific aspects of the explored search space. Domain-expert annotations and feedback will be parsed against a suite of ontologies, further aiding the search process by providing relational insight between features. New molecules and materials will be explored through a combination of first principles calculations and high-throughput, automated experimentation; these results will be incorporated into a continually growing open-access database. Efficiently integrating and directing evolving data-streams from experiment, computation, and human steering during the search will be achieved with a multi-fidelity active search policy. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative.  <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019077","MRI: Acquisition of a High-Performance Computing System for Scientific Research and Education at NDSU","OAC","Major Research Instrumentation, Data Cyberinfrastructure, EPSCoR Co-Funding","10/01/2020","07/21/2020","Bakhtiyor Rasulev","ND","North Dakota State University Fargo","Standard Grant","Alejandro Suarez","09/30/2023","$884,596.00","Dinesh Katti, Svetlana Kilina, Khang Hoang, Dana Skow","bakhtiyor.rasulev@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","CSE","1189, 7726, 9150","1189, 7726, 9150","$0.00","This award to North Dakota State University (NDSU) funds the acquisition and commissioning of a high-performance computing (HPC) instrument that will significantly expand and update the resources at NDSU and in the state of North Dakota for scientific research and education. The HPC system will provide a state-wide regional resource giving faculty and students of the North Dakota University System (NDUS) appropriate infrastructure to engage efficiently in challenging research that requires parallel computing. It will facilitate hands-on training in HPC techniques for large-scale compute- and data-intensive analyses. The new computing facility is essential to the growing spectrum of research and training activities and will be used to foster collaborative relationships with research and education partners within North Dakota as well as nationally and internationally. At NDSU and beyond, the new HPC system will enable cutting-edge research in multiple areas, including fluid dynamics, biomedical engineering, physics, chemistry, materials science and engineering, precision agriculture, plant sciences and plant pathology, artificial intelligence, health care, and financial and business analytics. Beyond NDSU, the new instrument will provide HPC resources to researchers and students at the tribal colleges (TCs) in the state of North Dakota and the primarily undergraduate institutions (PUIs) and Master?s colleges/universities (MCUs) within NDUS. Continuing present outreach activities is planned to stimulate interest in science and engineering within the state.<br/><br/>The project will provide a new instrument that consists of a fast, tiered storage subsystem with a parallel file system and a distributed memory hybrid HPC cluster (CPUs, GPUs, big-memory nodes, and high speed interconnect) specifically designed to efficiently process massive amounts of data as well as handle compute-intensive applications. The compute nodes will be capable of a combined theoretical peak performance of 98 TFLOPS whilst using 64-bit double precision GPUs will lead to a combined theoretical peak of 140 TFLOPS. This gives a total peak performance of 238 TFLOPS for the compute cluster. The research projects undertaken by the research groups will contribute to multiple fields, including those listed above. Consequently, more than 15 NDSU lead computational researchers and their groups (100+ scientists and students) have joined in this project to use an extensible HPC instrument architected for enhanced support for modeling, data collection, generation, analysis, storage, provenance, curation, and sharing. The PIs of the project will integrate students into their research projects and incorporate HPC into several graduate and undergraduate courses. Training workshop series and internship opportunities to train students and research staff in computational research using HPC will also be provided and supported by the commissioning and operations of this instrument. This will provide an excellent opportunity to train the next generation of HPC systems specialists. <br/><br/>This award by the Office of Advanced Cyberinfrastructure (OAC) is jointly funded with the Division of Materials Research (DMR), part of the Mathematical and Physical Sciences Directorate, and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940335","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","HDR-Harnessing the Data Revolu","10/01/2019","09/14/2019","Hendrik Heinz","CO","University of Colorado at Boulder","Standard Grant","Giovanna Biscontin","09/30/2022","$417,999.00","","hendrik.heinz@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","099Y","062Z","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940114","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","Special Initiatives","10/01/2019","09/14/2019","Wei Chen","IL","Illinois Institute of Technology","Standard Grant","Giovanna Biscontin","09/30/2022","$387,881.00","","wchen66@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","1642","062Z, 085E, 8021","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931421","Elements: MATPOWER for Integrated Smart Grid Research and Education","OAC","EPCN-Energy-Power-Ctrl-Netwrks, Software Institutes","11/01/2019","09/03/2019","Ray Zimmerman","NY","Cornell University","Standard Grant","Alexis Lewis","10/31/2022","$600,000.00","","rz10@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7607, 8004","077Z, 155E, 7923","$0.00","The electric power system is one of the most fundamental and critical infrastructures underlying modern society, and the economic, environmental and societal impacts of advances in its planning and operations are potentially enormous. MATPOWER is a set of open-source scientific software elements for electric power system simulation, analysis and optimization. A version of MATPOWER is widely used, especially for research and education. This Cyberinfrastructure for Sustained Scientific Innovation (CSSI) Elements award supports the research needed to transition MATPOWER to a new flexible internal modeling architecture.  The new capabilities will enable MATPOWER to simulate and optimize integrated smart grid networks consisting of both transmission (balanced) and distribution (unbalanced) systems. These capabilities will serve to expand the scope of MATPOWER's future impact as a successful research tool for designing and analyzing the power systems of the future. As power grids evolve toward more sustainable, economically efficient and environmentally friendly systems, positioning MATPOWER to expand its role as a flexible research and educational tool in this arena of innovation and change has the potential for far reaching and transformative impact both nationally and globally.<br/><br/>MATPOWER addresses some of the most fundamental classes of problems in power systems analysis, namely the power flow, optimal power flow, and related problems used to determine the steady state voltages, currents and power flows arising from the interactions among system conditions, operator control actions and the laws of physics. The aim of this work is to broaden and extend MATPOWER's reach as a research-enabling tool for tackling future power systems problems in two specific ways. The first is to restructure the MATPOWER internals around a new unified MATPOWER element model, vastly expanding the range of devices and controls modeled by MATPOWER and further increasing its customizability. The second is to implement the modeling of multiphase unbalanced systems and integrated balanced and unbalanced models. This will expand the scope of MATPOWER?s usefulness to many current and emerging research areas related to the modern smart grid, with its proliferation of decentralized control and distributed energy resources (DER), such as solar PV, plug-in hybrid electric vehicles, local energy storage, and various kinds of actively managed demand.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839159","RAISE TAQS: Very Large Scale Integrated Electronics and Phontonics Platform for Scaleable Quantum Information Processing","OAC","OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/11/2018","Dirk Englund","MA","Massachusetts Institute of Technology","Standard Grant","Bogdan Mihaila","09/30/2022","$999,000.00","Isaac Chuang, Ruonan Han","englund@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253","026Z, 049Z, 057Z, 7203","$0.00","The world of quantum mechanics holds enormous potential to address unsolved problems in communications, computation, and precision measurements. Efforts are underway across the globe to develop such technologies in various types of quantum memories, such as photons or atoms. One of the most challenging problems in building quantum computers and the envisioned ""quantum internet"" concerns the question of how to efficiently connect large numbers of quantum memories. While proof-of-concept experiments are possible with today's technology, scaling quantum systems to tens, hundreds, or thousands of individually controllable quantum memories requires a new generation of electronic and photonic components, systems, and algorithms. The goal of this NSF project is to develop the underlying photonic and electronic chips, as well as control and algorithms, that will make it possible to translate today?s proof-of-concept demonstrations out of the laboratory and into viable quantum technologies. <br/><br/>This NSF project addresses the core architectural challenges -- in hardware and algorithms -- needed for scaling atomic quantum processing platforms. At the core of the envisioned quantum architecture is the development of a chip architecture that combines complementary metal-oxide semiconductor electronics with a photonic integrated circuit layer. This chip will serve as a scalable chip-based platform to control large numbers of quantum memories. The program will also develop error correction thresholds as well as a new class of heralded two-qubit gates to approach fault-tolerant thresholds despite lossy and decohering channels connecting our logical qubits. The envisioned architecture will be developed for trapped ions and atom-like emitters in diamond, though the core quantum computing architecture will also inform other modular quantum computing or quantum repeater architectures based on atomic or atom-like quantum memories. The program will also include a strong outreach effort to inform the general public about the underlying concepts and the promise of quantum information processing, quantum computing algorithms, and large-scale opto-electronic circuits.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835321","Elements: Software. icepack: an open-source glacier flow modeling library in Python","OAC","Polar Cyberinfrastructure, EarthCube","11/01/2018","08/15/2018","Daniel Shapero","WA","University of Washington","Standard Grant","Seung-Jong Park","10/31/2022","$388,314.00","Ian Joughin","shapero@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","5407, 8074","026Z, 062Z, 072Z, 077Z, 1079, 7923, 8004","$0.00","This project supports the development of a software package named ""icepack"", that will enable simulations of how glaciers, such as those in Greenland, Antarctica, and mountain ranges around the world, will flow in response to the environment around them. Glaciologists use software tools to run simulations so that they can make predictions of how large the Greenland and Antarctic ice sheets will be in the future. With these predictions, scientists can give policy-makers and the public better predictions on the sea level rise in the coming decades. While the ability to run simulations is essential for advancing our understanding of science, doing so requires a significant programming and scientific expertise. The goal of this project is to lower this barrier to entry. Led by an early career scientist, the team, from University of Washington will develop a tool that is easier to use for researchers and students, whether they are experts or novices. The software applications will be freely available and an open source license.<br/><br/>icepack allows for estimating parameters, such as a basal friction or internal rheology, that are not observable via remote sensing. Glaciologists use simulation tools like icepack for (1) exploring aspects of the physics of ice sheets that are not completely understood, (2) drawing inferences from observational data, and (3) making predictions of the future state of the ice sheets in order to estimate future sea-level rise. While modeling is an essential tool for practicing glaciologists, it is still a complex endeavor. In addition to supporting development of more features and improvements to icepack, we will create an extensive set of tutorial materials for a workshop aimed at graduate students and early-career researchers on how to use icepack. Additionally, the investigators will implement novel algorithms for parameter estimation and uncertainty quantification in icepack. These will allow the investigators to leverage the entire time series of observations of the ice sheets, while current algorithms are limited in how much data they can use, and to get a better idea of the statistical spread on estimates of the current and future states of the ice sheets.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940291","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Rajesh Gupta","CA","University of California-San Diego","Standard Grant","Amy Walton","09/30/2022","$303,027.00","","gupta@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835669","Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems","OAC","Software Institutes","11/01/2018","09/12/2018","Jian Peng","IL","University of Illinois at Urbana-Champaign","Standard Grant","Seung-Jong Park","10/31/2022","$150,000.00","","jianpeng@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use  effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that <br/>will significantly accelerate I/O performance. <br/><br/>This project  proposes to extend HDF technologies with the Hermes design. Hermes is new, and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adaptprefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934641","HDR IDEAS^2 Institute: Data-Driven Frameworks for Materials Discovery","OAC","Special Initiatives","09/01/2019","10/15/2020","Samantha Daly","CA","University of California-Santa Barbara","Continuing Grant","Giovanna Biscontin","08/31/2022","$2,000,000.00","Tresa Pollock, Bangalore Manjunath, Yu-Xiang Wang, Christos Thrampoulidis","samdaly@engineering.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","099y, 1642","062Z, 8021","$0.00","The discovery and development of new materials with unique properties and functionalities has revolutionized entire industries, including aviation, space, communication, biomedical, and automotive. Materials design has been traditionally experimentally and computationally intensive. However, advances in data-driven approaches, computational power, and experimental capabilities have created a tipping point for targeted and efficient materials design.  This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) Frameworks award supports conceptualization of an Institute to advance data-intensive research in Materials Science and Engineering. The IDEAS^2 (Integrated Data Environment for Accelerated Stochastic Science) Institute for Materials Discovery will provide a platform for the development of experimental and computational frameworks for materials advancement, that encourages collaboration and the sharing of data-driven approaches among research communities. The Data Science methods are intrinsically interoperable, and this program will engage diverse research communities in the collaborative development of large data frameworks that are applicable across a wide range of disciplines. The IDEAS^2 Institute will be structured to lower the barrier for domain scientists to work with data scientists through a variety of mechanisms including biannual ""Teach the Teacher"" workshops, an annual IDEAS^2 Symposium, visiting faculty positions at UCSB, and a range of other community engagement activities. Students working on this program will gain valuable multidisciplinary research and educational opportunities.<br/><br/>First-principle calculations of thermodynamic and kinetic properties and information from microstructurally-based, high throughput models will be integrated into the design of data structures and the analyses of the developed techniques. The developed frameworks will be grounded in machine learning approaches that are fundamentally-based, computationally and statistically tractable, and incorporate domain knowledge and simulation results. The frameworks and data developed in the Institute - such as those to predict processing advancements from first principles, model these advancements in a high-throughput fashion, enable high-throughput experimentation, align the resulting experimental data (chemical, microstructure, deformation, etc.), and efficiently mine the resultant high-dimensional datasets - will be integrated with an open-source platform (BisQue) to facilitate both internal and external collaboration on their development for a broad range of materials applications. The computational infrastructure and parallelization of calculations through the BisQue platform enables the screening of very large datasets, with a hierarchical workflow requiring minimal software requirements (only a web browser is needed) and minimal domain knowledge of the user in modeling of materials. The focus of this program is on a research area with major and broad implications on numerous scientific and technological fields, and it also represents a unique training opportunity with acquired skills that will propel its graduates to the forefront of the emerging, critical field of data-driven science, as well as its many application areas within various scientific disciplines and high-tech industry sectors. This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931430","Element: Computational Toolkit to Discover Peptides that Self-assemble into User-selected Structures","OAC","Special Initiatives, Software Institutes","10/01/2019","09/06/2019","Carol Hall","NC","North Carolina State University","Standard Grant","Alan Sussman","09/30/2022","$600,000.00","Anant Paravastu, Xingqing Xiao","hall@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","1642, 8004","026Z, 077Z, 7923, 8004, 9102","$0.00","Peptides are short chains (sequences) of naturally-occurring amino acids. They are found in all living cells and tissues, where they perform vital biological functions. Peptides are now being considered for use in nanotechnology as they are able to assemble to form a variety of nanostructures - nanofibers, nanosheets, and nanoparticles. Such structures have potential applications in a wide variety of fields including medicine, electronics, enzyme catalysis and drug release.  The goal of this project is to develop an open software toolkit that enables the identification of peptide sequences that are capable of assembling into user-selected fiber-like structures.  Users will be able to screen potentially thousands of peptide sequences that assemble into the nanostructure of their choosing, and rank order them according to their stability. An algorithm, PepAD (Peptide Assembly Design) will be developed that searches for sequences that assemble into structures specified by the user. An accompanying software package will allow further analysis of the relative speed at which a large number of these peptide sequences form the desired structure. To establish efficacy and a basis for future improvement of computational tools, selected designs will be validated experimentally using advanced biophysical characterization techniques and solid-state nuclear magnetic resonance spectroscopy. PepAD will be open source and easy to run. Its use by the developers and by members of the scientific and engineering communities should lead to the ability to design the next generation of complex nanostructures. The toolkit, which will be the first of its kind for these types of assemblies, will be available on GitHub and on the NSF-sponsored Molecular Simulation and Design Framework (MoSDeF).<br/><br/>Many peptides are known to adopt beta strand conformations and assemble spontaneously into a variety of nanostructures--- nanofibers, nanosheets, nanoparticles, etc. - with applications in a wide variety of fields including nanomedicine, electronics, drug release, and hydrogels. The goal of this project is to develop an open software toolkit that enables the identification of peptide sequences that are capable of assembling into user-selected beta-sheet-based structures. An algorithm, PepAD (Peptide Assembly Design) will be developed that searches for sequences that assemble into structurers specified by the user. PepAD will allow users to screen potentially thousands of peptide sequences that assemble spontaneously into the structure of their choosing, and rank order them according to their stability. Discontinuous molecular dynamics (DMD) simulation software along with the PRIME20 force field will also be made available to enable analysis of the designed structures? assembly kinetics. To establish efficacy and a basis for future improvement of computational tools, selected designs will be validated experimentally using biophysical characterization techniques and solid-state nuclear magnetic resonance (ssNMR) spectroscopy. There are four objectives: (1) develop an algorithm, PepAD, that identifies short peptide sequences that are capable of self-assembling into user-determined amyloid structures; (2) perform DMD/PRIME20 simulations to examine assembly kinetics, (3) synthesize and test the peptide designs using biophysical characterization experiments and ssNMR, and (4) community test and refine the PepAD software and then install it on GitHub and on MoSDeF as a plugin. The toolkit, which will be the first of its kind for beta-sheet assemblies, will be open source and easy to use. Successful implementation of this software will pave the way for the computational design of nanostructures that self-assemble:  (a) in response to a trigger such as a change in temperature, pH, or specific ions, and (b) when the peptides are conjugated to functionalities like small molecules, recognition elements, fluorophores or enzymes. Outreach activities include the creation of a video for general audiences that describes how molecular-level computer simulations can be used in the design of new materials and an iPad app that allows users to computationally design model proteins and then watch movies of them as they fold.  The project will use the concept of harnessing self-assembly and related ideas to design educational activities for undergraduate STEM students. The project will work to broaden opportunities for women and minorities, and to increase science awareness in K-12 students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1930025","Assessment and Evaluations of NSF OAC-Funded Program Impact on the Scientific Community","OAC","CYBERINFRASTRUCTURE, Software Institutes","09/01/2019","01/24/2020","Changwon Suh","MD","Nexight Group LLC","Standard Grant","Alexis Lewis","06/30/2020","$52,458.00","","csuh@nexightgroup.com","8405 Colesville Road, 4th Floor","Silver Spring","MD","209103317","2406677636","CSE","7231, 8004","7231","$0.00","Cyberinfrastructure (CI) is essential to the advancement and transformation of science and engineering. Over the last decade, the CI community has focused on developing secure, advanced, scalable, and global CI resources, tools, and services, creating an interoperable and collaborative CI ecosystem. In order to move forward strategically, it is critical to understand the impact that CI programs have had on the scientific research community. This award supports an effort to develop an understanding of this impact, by working with the Cyberinfrastructure community to understand and enhance the impact of programs such as NSF's Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program and its predecessors. To this end, the PIs will gather relevant information, conduct a targeted survey, and prepare a report describing findings and mapping out the future directions of CI research.<br/><br/>This work will provide new understanding of the impact of recent Data and Software Programs, through a systematic cross-cutting survey-based assessment. The assessment will seek to evaluate the impact of the activities funded under the Data Infrastructure Building Blocks (DIBBs), Software Infrastructure for Sustained Innovation (SI2), and Cyberinfrastructure for Sustained Scientific Innovation (CSSI) programs. The approach is to (1) determine the scope of the activity and establish a framework for design of the survey (2) design and administer the survey and (3) analyze the results and synthesize the findings into a report. The outcomes will benefit the scientific research community broadly, by providing insights into the most effective methods of research support in Cyberinfrastrucutre, and broadening the impact of advanced Cyberinfrastructure research on the science and engineering domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019012","CC* Integration-Large: N-DISE: NDN for Data Intensive Science Experiments","OAC","CISE Research Resources, Campus Cyberinfrastructure","10/01/2020","06/29/2020","Edmund Yeh","MA","Northeastern University","Standard Grant","Deepankar Medhi","09/30/2022","$875,000.00","Harvey Newman, Jason Cong, Lixia Zhang, Susmit Shannigrahi","eyeh@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","2890, 8080","9102","$0.00","The project, N-DISE (Named Data Networking for Data Intensive Science Experiments), aims to accelerate the pace of breakthroughs and innovations in data-intensive science fields such as the Large Hadron Collider (LHC) high energy physics program and the BioGenome and human genome projects. Based on Named Data Networking (NDN), a data-centric architecture, N-DISE will deploy and commission a highly efficient and field-tested petascale data distribution, caching, access and analysis system serving major science programs.<br/><br/>The N-DISE project will design and develop high-throughput caching and forwarding methods, containerization techniques, hierarchical memory management subsystems, congestion control mechanisms, integrated with Field Programmable Gate Arrays (FPGA) acceleration subsystems, to produce a system capable of delivering LHC and genomic data over a wide area network at throughputs approaching 100 Gbits per second, while significantly decreasing download time. In addition, N-DISE will utilize NDN's built-in data security support to ensure data integrity and provenance tracing. N-DISE will leverage existing infrastructure and build an enhanced testbed with four additional high performance NDN data cache servers at participating institutions.<br/><br/>N-DISE will provide a field-tested working prototype of a multi-domain data distribution and access system offering fast access and low cost, as well as data integrity and provenance, to many data-intensive science and engineering fields. The project plans to hold annual workshops and hackathons to train students, postdocs, and other researchers on NDN architectural design, algorithms, as well as implementation methodologies for specific data-intensive science environments. The project will undertake initiatives for actively involving under-represented groups, and for educational outreach to K-12 students.<br/><br/>N-DISE will maintain a GitHub repository at https://github.com/neu-yehlab/n-dise. The repository will host up-to-date publications, code, data, results, and simulators. The repository will be maintained by the team for at least three years beyond the duration of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939945","Collaborative Research: Converging Genomics, Phenomics, and Environments Using Interpretable Machine Learning Models","OAC","ICB: Infrastructure Capacity f, HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Remco Chang","MA","Tufts University","Continuing Grant","Peter McCartney","09/30/2022","$299,520.00","","remco@cs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","085Y, 099Y","062Z, 1165","$0.00","Mitigating the effects of climate change on public health and conservation calls for a better understanding of the dynamic interplay between biological processes and environmental effects. The state-of-the-art, which has led to many important discoveries, utilizes numerical or statistical models for making predictions or performing in silico experimentation, but these techniques struggle to capture the nonlinear response of natural systems. Machine learning (ML) methods are better able to cope with nonlinearity and have been used successfully in biological applications, but several barriers still exist, including the opaque nature of the algorithm output and the absence of ML-ready data. This project seeks to significantly advance technologies in ML and create a new interdisciplinary field, computational ecogenomics. This will be accomplished by designing ML techniques for encoding heterogeneous genomic and environmental data and mapping them to multi-level phenotypic traits, reducing the amount of necessary training data, and then developing interactive visualizations to better interpret ML models and their outputs.  These advances will responsibly and transparently inform policy to maximize resources during this crucial window for planetary health, while revealing underlying biological mechanisms of response to stress and evolutionary pressure.<br/><br/>The long-term vision for this project is to develop predictive analytics for organismal response to environmental perturbations using innovative data science approaches and change the way scientists think about gene expression and the environment. The goal for this two-year award is to develop a proof-of-concept for an institute focused on predicting emergent properties of complex systems; an institute that would itself foster the development of many new sub-disciplines.  The core of this activity is developing a machine learning framework capable of predicting phenotypes based on multi-scale data about genes and environments.  Available data, ranging from simple vectors to complex images to sequences, will be ingested into this framework by applying proven semantic data integration tools and algorithmic data transformation methods.  The central hypothesis of this research is that deep learning algorithms and biological knowledge graphs will predict phenotypes more accurately across more taxa and more ecosystems than do current numerical and traditional statistical modeling methods.  The rationale for this project is that a timely investment in data science will push through a bottleneck in life science, accelerating discovery of gene-phenotype-environment relationships, and catalyzing a new computational discipline to uncover the complex ""rules of life.""<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934292","HDR: I-DIRSE-FW: Accelerating the Engineering Design and Manufacturing Life-Cycle with Data Science","OAC","CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Magdalena Balazinska","WA","University of Washington","Continuing Grant","Giovanna Biscontin","08/31/2022","$2,000,000.00","W. James Pfaendtner, David Beck, Ariel Rokem","magda@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","099y, 7231","062Z, 7231","$0.00","The manufacturing life cycle begins with the discovery of new molecules and materials. This first step is often initiated through computer simulations that explore the space of possible molecules and materials, and identify promising candidates that can later be tested in laboratories. As simulations have grown in scale and complexity, this step has become a critical bottleneck. New data-driven approaches present the opportunity to increase the speed and accuracy of such predictions, with broad potential impact on the US Manufacturing sector. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) Frameworks award brings together Engineers and Data Scientists to conceptualize a new Engineering Data Science Institute where these tools can be applied for new discovery. The effort will develop new data science approaches to accelerate the engineering life cycle: design, characterization, manufacturing, and operation. This life cycle starts with the discovery of new molecules and materials, followed by advanced characterization with high throughput methods augmented by machine learning. Then, efficient manufacturing and operation of systems that use these materials can be designed and developed. By focusing on this holistic lifecycle, the researchers will build a broadly applicable foundation in Engineering Data Science methods. The new Institute will seek to create an Engineering Data Science environment that supports engineers and scientists (students, postdoctoral researchers, and faculty) through a synergistic set of collaboration and education activities.<br/><br/><br/>This collaborative effort follows three thrusts. The first focuses on the reduction of the experimental design space with data science tools targeting the discovery of new molecules and polymers. The research develops a new, formal framework for pairing accurate predictive simulations with data-driven models to create a scalable and transferable workflow that can be deployed across multiple examples of molecular engineering applications. The second thrust addresses a manifold of cross-cutting needs at the intersection of image data analytics and characterization of materials and systems. It also builds community cyberinfrastructure through open-source software resources with support for execution in public clouds. The final thrust focuses on improving manufacturing, optimization, and control. It further enhances cyberinfrastructure resources through a suite of open-source software solutions to systematically develop digital twin models for complex engineering and manufacturing systems, and apply them for optimization and control. This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2204115","EAGER: SciDatBench: Principles and Prototypes of Science Data Benchmarks","OAC","CYBERINFRASTRUCTURE","01/01/2022","11/26/2021","Geoffrey Fox","VA","University of Virginia Main Campus","Standard Grant","Tevfik Kosar","10/31/2022","$260,242.00","","gcfexchange@gmail.com","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7231","7231, 7916","$0.00","Analysis of large scientific data sets requires new research in both the data analysis methods and the information technology hardware and software to use in the analysis. This project is investigating and prototyping a new set of science data benchmarks, dubbed SciDatBench. It establishes a new collection of important and representative big scientific datasets together with typical software implementations of the machine learning algorithms that are needed for best practice analysis. The SciDatBench collection is accompanied by documentation allowing it to be used in the training of researchers in the rapidly evolving Big Data analysis techniques. The project has a potential to impact a broad range of scientific disciplines including eventually material sciences, environmental sciences, life sciences including epidemiology, fusion, particle physics, astronomy, earthquake, and earth sciences, with more than one representative problem from each of these domains.<br/><br/>SciDatBench generates particular instances of big data analysis benchmarks and establishes a sustainable process for maintaining and enhancing them. This collection includes both standalone examples and end-to-end examples needing multiple components that are seen in the analysis of many science experiments. SciDatBench is affiliated as an approved Science Data working group with the very successful MLPerf activity with 80 organizational members looking at Industry machine learning benchmarks. The state-of-the-art examples in SciDatBench are contributing to progress in scientific discovery that advances the national health, prosperity, and welfare, as stated by NSF's mission. The project is proactively involving under-represented communities in its activities. SciDatBench supports comparative studies and identifies requirements for future cyberinfrastructure to support scientific data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103905","Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science","OAC","Software Institutes","10/01/2021","09/17/2021","C Michael Barton","AZ","Arizona State University","Continuing Grant","Alan Sussman","09/30/2026","$1,939,262.00","Marco Janssen, Allen Lee, Sean Bergin, Kenneth Buetow","Michael.barton@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8004","077Z, 7925, 8004","$0.00","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges.  The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.<br/><br/>The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the  Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.<br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042084","CDS&E: Collaborative Research: HyLoC: Objective-driven Adaptive Hybrid Lossy Compression Framework for Extreme-Scale Scientific Applications","OAC","CDS&E","08/01/2020","10/14/2020","Dingwen Tao","WA","Washington State University","Standard Grant","Tevfik Kosar","07/31/2023","$270,802.00","","dingwen.tao@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","8084","026Z, 8084","$0.00","Today's extreme-scale scientific simulations and instruments are producing huge amounts of data that cannot be transmitted or stored effectively. Lossy compression, a data compression approach leading to certain data distortion, has been considered as a promising solution, because it can significantly reduce the data size while maintaining high data fidelity. However, the existing lossy compression methods may not always work effectively on all datasets used in specific applications because of their distinct and diverse characteristics. Moreover, the user objectives in compression quality and performance may vary with applications, datasets or circumstances. This project aims to develop a hybrid lossy compression framework to automatically construct the best-fit compression for diverse user objectives in data-intensive scientific research. Educational and engagement activities are provided to develop new curriculum related to scientific data compression and promote research collaborations with national laboratories.<br/><br/>Designing an efficient, adaptive, hybrid framework that can always choose the best-fit compression strategy is nontrivial, since existing state-of-the-art lossy compression methods are developed with distinct principles. The project has a three-stage research plan. First, the project decouples the state-of-the-art error-bounded lossy compression approaches into multiple stages and effectively models the working efficiency (e.g., compression ratio, error, speed) of particular approaches in each stage. Second, the project develops a loosely-coupled framework to aggregate the decoupled compression stages together and also explores as many compression pipelines composed of different stages as possible, to optimize the classic compression efficiency, including compression quality and performance. Third, the project optimizes the synthetic data-movement performance regarding the external devices and resources, such as I/O performance. The team evaluates the proposed framework on multiple extreme-scale scientific applications, including cosmological simulations, light source instrument data analytics, quantum circuit simulations, and climate simulations. The project may create technologies that can increase the storage availability and improve the performance for extreme-scale scientific applications, opening opportunities for new discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2038007","EAGER: SciDatBench: Principles and Prototypes of Science Data Benchmarks","OAC","CYBERINFRASTRUCTURE","08/15/2020","08/05/2020","Geoffrey Fox","IN","Indiana University","Standard Grant","Tevfik Kosar","12/31/2021","$296,877.00","","gcfexchange@gmail.com","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7231","7231, 7916","$0.00","Analysis of large scientific data sets requires new research in both the data analysis methods and the information technology hardware and software to use in the analysis. This project is investigating and prototyping a new set of science data benchmarks, dubbed SciDatBench. It establishes a new collection of important and representative big scientific datasets together with typical software implementations of the machine learning algorithms that are needed for best practice analysis. The SciDatBench collection is accompanied by documentation allowing it to be used in the training of researchers in the rapidly evolving Big Data analysis techniques. The project has a potential to impact a broad range of scientific disciplines including eventually material sciences, environmental sciences, life sciences including epidemiology, fusion, particle physics, astronomy, earthquake, and earth sciences, with more than one representative problem from each of these domains.<br/><br/>SciDatBench generates particular instances of big data analysis benchmarks and establishes a sustainable process for maintaining and enhancing them. This collection includes both standalone examples and end-to-end examples needing multiple components that are seen in the analysis of many science experiments. SciDatBench is affiliated as an approved Science Data working group with the very successful MLPerf activity with 80 organizational members looking at Industry machine learning benchmarks. The state-of-the-art examples in SciDatBench are contributing to progress in scientific discovery that advances the national health, prosperity, and welfare, as stated by NSF's mission. The project is proactively involving under-represented communities in its activities. SciDatBench supports comparative studies and identifies requirements for future cyberinfrastructure to support scientific data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829704","CyberTraining: CIC: CyberTraining for Students and Technologies from Generation Z","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Alan Sussman","11/30/2021","$492,283.00","Gregor von Laszewski, Douglas Swany","gcfexchange@gmail.com","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Information technology is playing a dramatically increasing role in society, industry, and research. This includes design and use of large databases, simulations and artificial intelligence applications hosted on clouds and supercomputers with convergent technologies. Correspondingly, there is an increasing need for research workforce job skills in these and related areas. This project takes freshly created Indiana University Engineering course material on this cyberinfrastructure and adapts it for training with an emphasis on the needs of under-represented communities. The techniques of the successful open source software movement are used to create sustainable communities around the course curriculum and software. The project is creating new technologies to enable this for today's generation of students. Skills in core cloud computing, big data, supercomputing and artificial intelligence are exemplified by applications in the life science and nanotechnology areas. This project enables the future research workforce to contribute effectively using advanced cyberinfrastructure, promoting the progress of science and advancing the national health, prosperity, and welfare, which serves the national interest, as stated by NSF's mission. <br/><br/>The future economic progress and research leadership of the U.S. is dependent on having a research workforce that is capable of making use of advanced cyberinfrastructure (CI) resources as articulated by the National Strategic Computing Initiative (NSCI). This requires a curriculum that changes and integrates modern concepts and practices for the new generation of students aiming at a ""data-enabled computational science and engineering"" expertise. This project takes what Indiana University has learned from a brand new four-year undergraduate engineering curriculum designed ab initio and taught so far to its first two undergraduate classes, and invests it into developing active training modules. The innovative curriculum integrates big data, simulations, clouds and high performance computing systems presented in a uniform framework. The course material is customized for communities of cyberinfrastructure researchers nucleated, built, and sustained via the dynamic use of GitHub and enhanced by innovative tools to build a novel learning management system optimized for cyberinfrastructure-intensive classes. The project modules include Cloud Computing, Big Data Applications and Analytics, Networking, High-Performance Computing, Artificial Intelligence/Machine Learning, and Information Visualization. There are residential sessions, with a call for participants, and purely online courses and these have both ""teach the student"" and ""teach the teacher"" modes; the latter enables easy spread of the classes. Hands-on learning with research projects built around the class material is fully supported. The project offers CyberTraining with all of the popular approaches used by the Apache Software Foundation, including Meetups and Hackathons. Modules for domain scientists and engineers, e.g., the cyberinfrastructure users that exploit advanced CI methods for research in nanoengineering and bioengineering are included. Both students and teachers contribute to the course improving the text, the software, including a unique set of examples and the project aims to show that one can build both learning and sustainability communities by using the proven techniques of the open source software community. The project uses proactive measures to enhance the involvement of under-represented communities in its activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835720","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","Special Initiatives, Software Institutes","01/01/2019","09/06/2018","Michael Shirts","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","12/31/2022","$286,504.00","","michael.shirts@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1935984","Mid-Scale RI-1: SAGE: A Software-Defined Sensor Network","OAC","Information Technology Researc, CYBERINFRASTRUCTURE","10/01/2019","12/15/2021","Peter Beckman","IL","Northwestern University","Standard Grant","Kevin Thompson","09/30/2022","$9,026,927.00","Eugene Kelly, Charles Catlett, Ilkay Altintas, Scott Collis","beckman@uchicago.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","1640, 7231","","$0.00","Distributed, intelligent sensor networks that can collect and analyze data are essential to scientists seeking to understand the impacts of global urbanization, natural disasters such as flooding and wildfires, and climate change on natural ecosystems and city infrastructure. Sage is a pilot project that assembles sensor nodes to support machine learning frameworks and deploy them for rigorous testing in environmental testbeds in California, Colorado, and Kansas and in urban environments in Illinois and Texas. The reusable cyberinfrastructure running on these testbeds will give climate, traffic, and ecosystem scientists new data for building models to study these coupled systems.  The software components developed in Sage are open source and provide an open architecture that will enable scientists from a wide range of fields to build their own intelligent sensor networks. The toolkit also extends the current educational curriculum used in Chicago and will inspire young people - with an emphasis on women and minorities -- to pursue science, technology, and mathematics careers by providing a platform for students to explore measurement-based science questions related to the natural and built environments. <br/><br/>The Sage project designs and builds new reusable software components and cyberinfrastructure services to enable deployment of intelligent environmental sensors.  Geographically distributed sensor systems that include cameras, microphones, and weather and air quality stations can generate such large volumes of data that fast and efficient analysis is best performed by an embedded computer connected directly to the sensor. This project explores new techniques for applying machine learning algorithms to data from such intelligent sensors and builds reusable software that can run programs within the embedded computer and transmit the results over the network to central computer servers. The Sage project maintains links to computer source code, open hardware design documents, and sensor specifications, as well as both the raw and calibrated sensor data collected from all the testbed nodes at the website http://wa8.gl.  The data is also be hosted in the cloud to facilitate easy data analysis. All project data is maintained for five years after the project ends.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004441","Collaborative Research: Elements: EdgeVPN: Seamless Secure Virtual Networking for Edge and Fog Computing","OAC","Software Institutes","06/01/2020","04/29/2020","Renato Figueiredo","FL","University of Florida","Standard Grant","Robert Beverly","05/31/2023","$519,581.00","","renato@acis.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","8004","077Z, 7923","$0.00","Edge computing encompasses a variety of technologies that are poised to enable new applications across the Internet that support data capture, storage, processing and communication near the edge of the Internet. Edge computing environments pose new challenges, as devices are heterogeneous, widely distributed geographically, and physically closer to end users, such as mobile and Internet-of-Things (IoT) devices. This project develops EdgeVPN, a software element that addresses a fundamental challenge of networking for edge computing applications: establishing Virtual Private Networks (VPNs) to logically interconnect edge devices, while preserving privacy and integrity of data as it flows through Internet links. More specifically, the EdgeVPN software developed in this project addresses technical challenges in creating virtual networks that self-organize into scalable, resilient systems that can significantly lower the barrier to entry to deploying a private communication fabric in support of existing and future edge applications. There are a wide range of applications that are poised to benefit from EdgeVPN; in particular, this project is motivated by use cases in ecological monitoring and forecasting for freshwater lakes and reservoirs, situational awareness and command-and-control in defense applications, and smart and connected cities. Because EdgeVPN is open-source and freely available to the public, the software will promote progress of science and benefit society at large by contributing to the set of tools available to researchers, developers and practitioners to catalyze innovation and future applications in edge computing.<br/><br/>Edge computing applications need to be deployed across multiple network providers, and harness low-latency, high-throughput processing of streams of data from large numbers of distributed IoT devices. Achieving this goal will demand not only advances in the underlying physical network, but also require a trustworthy communication fabric that is easy to use, and operates atop the existing Internet without requiring changes to the infrastructure. The EdgeVPN open-source software developed in this project is an  overlay virtual network that allows seamless private networking among groups of edge computing resources, as well as cloud resources. EdgeVPN is novel in how it integrates: 1) a flexible group management and messaging service to create and manage peer-to-peer VPN tunnels grouping devices distributed across the Internet, 2) a scalable structured overlay network topology supporting primitives for unicast, multicast and broadcast, 3) software-defined networking (SDN) as the control plane to support message routing through the peer-to-peer data path, and 4) network virtualization and integration with virtualized compute/storage endpoints with Docker containers to allow existing Internet applications to work unmodified. EdgeVPN self-organizes an overlay topology of tunnels that enables encrypted, authenticated communication among edge devices connected across disparate providers in the Internet, possibly subject to mobility and constraints imposed by firewalls and Network Address Translation, NATs. It builds upon standard SDN interfaces to implement packet manipulation primitives for virtualization supporting the ubiquitous Ethernet and IP-layer protocols.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1942714","CAREER: Big Data Climate Causality Analytics","OAC","CAREER: FACULTY EARLY CAR DEV","04/15/2020","05/19/2021","Jianwu Wang","MD","University of Maryland Baltimore County","Continuing Grant","Alan Sussman","03/31/2025","$358,710.00","","jianwu@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1045","019Z, 026Z, 1045, 9179","$0.00","A fundamental problem in climate science is climate causality analysis that studies the cause-effect relationship among climate variables, such as temperature and humidity. By studying how the climate system works from a causality perspective, the findings could be used for many research areas including climate variability, climate dynamics, climate simulation, and extreme climate prediction. Nowadays, climate causality study faces many computing challenges, such as processing very large and high-dimensional datasets, and the complexity of modern computing resources.  To tackle these challenges, this project targets novel causality discovery algorithms and related scalable computing techniques. The project is expected to greatly aid Earth System scientists and climate scientists to explore new hypotheses and use cases related to climate causality. The project includes an integrated program of research, education and outreach to help better understand and evaluate climate simulation, fostering workforce development for a multidisciplinary research community on ""Data + Computing + Climate Science"", and raising interest in both IT technology and climate studies among K-12 students, and various underrepresented groups. The project thus serves the national interest, as stated in NSF's mission, by promoting the progress of science and advancing national prosperity and welfare.<br/><br/>The goal of this CAREER project is to study efficient and reproducible causality analytics for large-scale climate data, so that climate scientists can easily test their causal hypotheses, reproduce existing studies and compare different causality analytics results. To handle the increasing dimensionality and resolution of spatiotemporal climate datasets, the project will study incremental causality discovery algorithms for large-scale climate datasets and parallel causality discovery for spatiotemporal climate data. To address the variety of both causal discovery algorithms and climate simulation/observation datasets, the project will study how to effectively measure climate causality results from different causality algorithms and different climate datasets, and integrate causality results through ensemble techniques. To cope with difficulties in conducting and reproducing causality analytics with large-scale climate datasets, the project will study cloud computing for big data climate analytics pipeline construction and execution optimization. The project will be evaluated from two perspectives. From the computing perspective, the research will be evaluated in terms of algorithm computation complexity, algorithm accuracy and algorithm scalability.  From the climate perspective, the applicability of the research will be evaluated by collaborating with climate scientists in their specific research programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934464","Collaborative Research: Framework for Integrative Data Equity Systems","OAC","CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Julia Stoyanovich","NY","New York University","Continuing Grant","Sylvia Spengler","08/31/2022","$550,000.00","","stoyanovich@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","099y, 7231","062Z, 7231, 9102","$0.00","Data Science continues to have a transformative impact on Science and Engineering, and on society at large, by enabling evidence-based decision making, reducing costs and errors, and improving objectivity. The techniques and technologies of data science also have enormous potential for harm if they reinforce inequity or leak private information.  As a result, sensitive datasets in the public and private sector are restricted from research use, slowing progress in those areas that have the most to gain: human services in the public sector.  Furthermore, the misuse of data science techniques and technologies will disproportionately harm underrepresented groups across race, gender, physical ability, sexual orientation, education, and more. These data equity issues are pervasive, and represent an existential risk for the use of data-driven methods in science and engineering. This project will establish a  Framework for Integrative Data Equity Systems (FIDES): an Institute for the study of systems that enable research on sensitive data while preventing misuse and misinterpretation. <br/><br/><br/>FIDES will enable interdisciplinary community convergence around data equity systems, with an initial study in critical domains such as mobility, housing, education, economic indicators, and government transparency, leading to the development of a novel data analytics infrastructure that supports responsibility in integrative data science.  Towards this goal, the project will address several technically challenging problems: (1) To be able to use data from multiple sources, risks related to privacy, bias, and the potential for misuse must be addressed. This project will develop principled methods for dataset processing to overcome these concerns.  (2) Individual datasets are difficult to integrate for use in advanced multi-layer network models.  This project considers methods to create pre-trained tensors over large collections of spatially and temporally coherent datasets, making them easier to incorporate while controlling for fairness and equity.  (3) Any dataset or model must be equipped with sufficient information to determine fitness for use, communicate limitations, and describe underlying assumptions.  This project will develop tools and techniques to produce ""nutritional labels"" for data and models, formalizing and standardizing ad hoc metadata approaches to provenance, specialized for equity issues. In addition to supporting methodological innovation in data science, the Institute will become a focal point for sharing expertise in data equity systems.  It will do so by establishing interfaces for interaction between data science and domain experts to promote expertise development and sharing of best practices, and by consistently supporting efforts on diversity and equity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835443","Framework: Software: Next-Generation Cyberinfrastructure for Large-Scale Computer-Based Scientific Analysis and Discovery","OAC","Software Institutes","01/01/2019","11/23/2020","Alan Edelman","MA","Massachusetts Institute of Technology","Standard Grant","Robert Beverly","12/31/2023","$3,498,560.00","Juan Pablo Vielma Centeno","EDELMAN@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Recent revolutions in data availability have radically altered activities across many fields within science, industry, and government. For instance, contemporary simulations medication properties can require the computational power of entire data centers, and recent efforts in astronomy will soon generate the largest image datasets in history. In such extreme environments, the only viable path forward for scientific discovery hinges on the development and exploitation of next-generation computational cyberinfrastructure of supercomputers and software. The development of this new computational infrastructure demands significant engineering resources, so it is paramount to maximize the infrastructure's potential for high impact and wide adoption across as many technical domains as possible. Unfortunately, despite this necessity, existing development processes often produce software that is limited to specific hardware, or requires additional expertise to use properly, or is overly specialized to a specific problem domain. Such ""single-use"" software tools are limited in scope, leading to underutilization by the wider scientific community. In contrast, this project seeks to develop methods and software for computer-based scientific analysis that are sufficiently powerful, flexible and accessible to (i) enable domain experts to achieve significant advancements within their domains, and (ii) enable innovative use of advanced computational techniques in unexpected scientific, technological and industrial applications. This project will apply these tools to a wide variety of specific scientific challenges faced by various research teams in astronomy, medicine, and energy management. These teams plan on using the proposed work to map out new star systems, develop new life-saving medications, and design new power systems that will deliver more energy to a greater number of homes and businesses at a lower cost than existing systems. Finally, this project will seek to leave a legacy of sustained societal benefit by educating students and practitioners in the broader scientific and engineering communities via exposure to state-of-the-art computational techniques. <br/><br/>Through close collaboration with research teams in statistical astronomy, pharmacometrics, power systems optimization, and high-performance computing, this project will deliver cyberinfrastructure that will effectively and effortlessly enable the next generation of computer-based scientific analysis and discovery. To ensure the practical applicability of the developed cyberinfrastructure, the project will focus on three target scientific applications: (i) economically viable decarbonization of electrical power networks, (ii) real-time analysis of extreme-scale astronomical image data, and (iii) pharmacometric modeling and simulation for drug analysis and discovery. While tackling these specific problems will constitute an initial stress test of the proposed cyberinfrastructure, it is the ultimate goal of the project that the developed tools be sufficiently performant, accessible, composable, flexible and adaptable to be applied to the widest possible range of problem domains. To achieve this vision, the project will build and improve various software tools for computational optimization, machine learning, parallel computing, and model-based simulation. Particular attention will be paid to the proposed cyberinfrastructure's composability with new and existing tools for scientific analysis and discovery. The pursuit of these goals will require the design and implementation of new programming language abstractions to allow close integration of high-level language features with low-level compiler optimizations. Furthermore, maximally exploiting proposed cyberinfrastructure will require research into new methods that combine state-of-the-art techniques from optimization, machine learning, and high-performance computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017337","CyberTraining: Implementation: Small: Using Problem-Based Learning for Vocational Training in Cyberinfrastructure Security at Community Colleges","OAC","CyberTraining - Training-based, Advanced Tech Education Prog","10/01/2020","10/20/2020","Irfan Ahmed","VA","Virginia Commonwealth University","Standard Grant","Alan Sussman","09/30/2023","$250,000.00","","iahmed3@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","044Y, 7412","1032, 9178","$0.00","Professionals in the cybersecurity field are in high demand. Many sectors, including the research, military, civil law enforcement, corporate, and private sector communities, all benefit from a larger, highly skilled cybersecurity workforce, particularly considering the increasing number and sophistication of cyber attacks. Challenges for academia include producing diverse and high quality professionals, and unfortunately, the demand for cybersecurity professionals far exceeds the supply of students. Community colleges can play a crucial role in meeting the demand for cybersecurity professionals. This project uses problem based learning, an innovative approach to teaching cybersecurity, to bring community college students into the high rewarding cybersecurity field. The educational methods and products developed through the project will be broadly applicable, beyond community colleges, and will contribute to a better trained and more diverse cybersecurity and research workforce. Ultimately, the project will contribute to the health, safety, and economic well being of society by protecting the nation's cyberinfrastructure.<br/><br/>The overall goal of the project is to provide vocational training in cyberinfrastructure security to community college students, to address the shortage of technical staff in cybersecurity. Community colleges have a significant student population of low income, diverse, first generation college students that have the potential for enabling a new stream of cybersecurity professionals. These colleges, however, face significant challenges to teaching advanced cybersecurity skills for cyberinfrastructure. To address these challenges, the project will develop vocational training modules on cyberinfrastructure security. Each module will consist of multiple micro-modules and target the skill sets required for entry level jobs in the cybersecurity field. Specifically, the approach involves three major tasks. The first task involves creating training modules on several offensive/defensive topics on cyberinfrastructure, including network penetration testing and digital forensics. The contents of the modules support problem based learning with a strong hands on components. The second task involves developing CRICE (Cyber Range Infrastructure for Cybersecurity Education) on NSFCloud to support problem based learning. NSFCloud is an NSF funded public cloud service available for research and training purposes. This will help community colleges to utilize the training modules effectively without requiring their own expensive computing infrastructure. The third task involves integrating the modules into the curriculum of two community colleges and evaluating the effectiveness of the problem based learning and CRICE in terms of cost, ease of adoption, student learning gain, and attitudinal survey on the students' experience. The project research contribution lies in exploring the effectiveness of problem based learning for vocational training in cybersecurity in the context of community colleges in the United States. The project will show that problem based learning is effective in providing both conceptual and practical understanding of cybersecurity topics. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.  The Advanced Technological Education (ATE) program in NSF's Division of Undergraduate Education is providing co-funding for this project in recognition of its contribution to education in community colleges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117429","MRI: Acquisition of a High-Performance Computing Cluster for Research and Teaching at Rutgers University-Newark","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2021","09/18/2021","Michele Pavanello","NJ","Rutgers University Newark","Standard Grant","Alejandro Suarez","09/30/2024","$559,288.00","James von Oehsen, Neepa Maitra, Patrick Shafto, Michael Cole","m.pavanello@rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","CSE","1189, 7231","1189, 9102","$0.00","This award to Rutgers University-Newark supports the acquisition and deployment of a High-Performance Computing (HPC) cluster (named PRICE) dedicated to research, teaching and societal outreach efforts. PRICE will have 60 general compute (CPU) nodes and one graphical processing unit (GPU) node as well as storage appropriate for the planned usage over the lifetime of the machine. The enabled research develops along three main directions: atomistic modeling, neuroscience, and data science. Some atomistic models enabled by PRICE will study the structure and dynamics of proteins to address questions related to diseases such as Alzheimer?s. New materials modeling and design is enabled both by PRICE and by the development of new quantum simulation methods. The enabled simulations will also regard new materials design by way of genetic algorithms. The enabled neuroscience research regards computational analysis of experimental data to understand brain function, connectivity, and human behavior. Enabled data science research includes the formulation of novel cooperative artificial intelligence (AI) algorithms that will improve the outcome of machine learning (ML) models of broad applicability. In addition to enabling new science, the project realizes several societal broader impacts including broadening HPC literacy of underrepresented minorities and training the future NJ workforce using HPC in the classroom and development of new undergraduate and graduate curricula.<br/><br/>PRICE will comprise 60 compute nodes (52 cores/node), 700 TB of redundant storage and one GPU node (4 GPUs/node) to be housed at Rutgers University-Newark. PRICE will enable several additional research projects carried out by the PI, co-PIs, and major users at Rutgers-Newark and NJIT. The GPU portion enables state-of-the-art molecular dynamics simulations that elucidate structure and dynamics of proteins for the understanding of diseases, such as Alzheimer?s. GPUs also enable the efficient and timely execution of cooperative AI algorithms aimed at improving predictivity. The CPU nodes will enable quantum simulations aimed at materials engineering through density-functional theory calculations. These simulations facilitate the development of quantum models based on density functional theory, its subsystem formulation (which parallelizes efficiently over PRICE?s low-latency network) as well as quantum mechanical frameworks based on the exact factorization of the Schrödinger equation for multicomponent systems. CPU and GPU nodes will enable data analysis associated with neuroscience experiments aimed at uncovering how the brain modulates behavior and vision-related tasks as well as the study of neuron connectivity to understand brain function. Data from these experiments is growing exponentially due to increased instrument data flow from new fMRI units and improved technology allowing recordings of local field potentials from hundreds on neurons. The project also enables new science as well as the realization of societal broader impacts, such as broadening high-performance computing literacy of underrepresented minorities, training the future NJ workforce and recruitment of new faculty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931473","Collaborative Research: Elements: Flexible & Open-Source Models for Materials and Devices","OAC","OFFICE OF MULTIDISCIPLINARY AC, Chem Thry, Mdls & Cmptnl Mthds, Software Institutes","11/01/2019","08/22/2019","Michele Pavanello","NJ","Rutgers University Newark","Standard Grant","Seung-Jong Park","10/31/2022","$238,562.00","","m.pavanello@rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","CSE","1253, 6881, 8004","026Z, 054Z, 077Z, 7923, 8004, 8005, 9216","$0.00","The project will develop first principles materials modeling software that can approach multiple length and time scales (multiscale). This software will be capable of modeling systems as complex as entire devices and materials of mesoscopic sizes. Over the course of the project the principal investigators plan to develop an open-source python-based software aimed at standardizing and generalizing multiscale simulations methods.  This will enable the use of computer modeling in the design of new compounds, materials and devices. The goals are to render multiscale simulations reproducible and accessible by the broader community. In that context, the project will address the notion of ""lab 2.0"", by which computer simulations replace laboratory experiments in tasks such as materials design and costly combinatorial searches for viable chemical processes. The software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will be promoted by direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.<br/><br/>An approach that leverages the long-range multiscale capabilities of continuum models with accurate short-range atomistic descriptions of specific interactions, and that exploits the ideal scalability of quantum-embedding techniques, will be investigated. The main driver of the proposed implementation will be a Python codebase which will carry out the part of current software that is not computationally heavy, but instead is code heavy where many lines of code are needed in typically non-object-oriented languages. This is key to obtain the desired cluster-topology-agnostic workflows. Longstanding problems related to computational scalability and code stiffness will addressed in a three-pronged approach aimed at developing (1) modular tools implementing modules with highly object-oriented codes (e.g., quantum, classical atomistic, and continuum solvers), (2) hybrid tools implementing combinations of modular tools in a way that best exploits high-performance computing architectures, and (3) hyper tools implementing a high-level data-enabled optimization strategy that generates optimal workflows combining several hybrid tools, thereby making the software of broad applicability and accessible to nonexperts.  These goals will render multiscale simulations reproducible and accessible by the broader community. The project will address the ""lab 2.0"" paradigm, by which computer simulations replace laboratory experiments in tasks such as materials design and combinatorial searches for viable chemical processes. The resultant software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will include the direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940107","Collaborative Research: I-AIM: Interpretable Augmented Intelligence for Multiscale Material Discovery","OAC","Special Initiatives","10/01/2019","09/14/2019","Yanxun Xu","MD","Johns Hopkins University","Standard Grant","Giovanna Biscontin","09/30/2022","$388,000.00","","Yanxun.xu@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1642","062Z, 085E, 8021","$0.00","The ability to model, predict, and improve the mechanical performance of engineering materials such as polymers, composites, and alloys can have a significant impact on manufacturing, with important economic and societal benefits. As advanced computational algorithms and data science approaches become available, they can be harnessed to disrupt the current approaches to materials modeling, and allow for the design and discovery of new high-strength, high-performance materials for manufacturing. Bringing together multidisciplinary teams of researchers can maximize the impact of these new tools and techniques. This Harnessing the Data Revolution Institutes for Data-Intensive Research in Science and Engineering (HDR-I-DIRSE) award supports the conceptualization of an Institute to develop novel data science methods, address fundamental scientific questions of Materials Engineering and Manufacturing, and build such multidisciplinary teams. The project will apply novel data science methods to advance the analysis of large sets of structural data of composite materials and alloys from the atomic scale to correlate with and predict mechanical properties. The methods are based on machine learning techniques and uncertainty quantification, and will help uncover underlying structural features in the materials that determine the properties and performance. The methods and results will help accelerate the development of ultra-high strength and lightweight carbon-based composites for aerospace applications, and multi-element superalloys for more durable engine parts, by navigating in the large possible design space and providing faster predictions than experiments and traditional simulation methods. The project will also lead to new methods and computational algorithms that will become publicly available. The investigators will train graduate and undergraduate students from various disciplines with a focus on engaging women and minorities in STEM fields, develop short courses that integrate novel Materials Science and Engineering applications and Data Science methods, and foster vertical integration of interdisciplinary research from undergraduate students to senior scientists.<br/><br/>This project aims at building an effective and interpretable learning framework for materials data across scales to solve a major challenge in current data-driven materials design. The combined Materials Science and Data Science approaches will synergistically contribute to the development and use of interpretable and physics-informed data science methodologies to gain new understanding of mechanical properties of polymer composites and alloys, with the potential to be expanded into different property sets and different systems. The PIs will utilize available data efficiently through combination with physical rules and prior knowledge, to develop an interpretable augmented intelligent system to learn principles behind the association of input structures with material properties with uncertainty quantification. The interconnected tasks involve the (1) collection and curation of large amounts of computational and experimental data for polymer/carbon nanotube composites and alloys from open data sources and targeted calculations and experiments, (2) the development of geometric and topological methods incorporating physical principles to generate a better, more sensitive low-dimensional representation of the multidimensional data and characterize the parameter space related to mechanical properties, (3) the development of a Bayesian deep reinforcement learning framework to generate interpretable knowledge graphs that depict the relational knowledge among physical quantities with uncertainty quantification, and (4) the prediction of mechanical properties to reveal design principles to improve materials performance, evaluate and validate the methods, and develop software for dissemination. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity and is co-funded by the Division of Civil, Mechanical and Manufacturing Innovation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017371","CyberTraining: Implementation: Small: Using Problem-Based Learning for Vocational Training in Cyberinfrastructure Security at Community Colleges","OAC","CyberTraining - Training-based, Advanced Tech Education Prog","10/01/2020","10/20/2020","Sajal Bhatia","CT","Sacred Heart University","Standard Grant","Alan Sussman","09/30/2023","$249,032.00","","bhatias@sacredheart.edu","5151 Park Avenue","Fairfield","CT","068251000","2033968241","CSE","044Y, 7412","1032, 9178","$0.00","Professionals in the cybersecurity field are in high demand. Many sectors, including the research, military, civil law enforcement, corporate, and private sector communities, all benefit from a larger, highly skilled cybersecurity workforce, particularly considering the increasing number and sophistication of cyber attacks. Challenges for academia include producing diverse and high quality professionals, and unfortunately, the demand for cybersecurity professionals far exceeds the supply of students. Community colleges can play a crucial role in meeting the demand for cybersecurity professionals. This project uses problem based learning, an innovative approach to teaching cybersecurity, to bring community college students into the high rewarding cybersecurity field. The educational methods and products developed through the project will be broadly applicable, beyond community colleges, and will contribute to a better trained and more diverse cybersecurity and research workforce. Ultimately, the project will contribute to the health, safety, and economic well being of society by protecting the nation's cyberinfrastructure.<br/><br/>The overall goal of the project is to provide vocational training in cyberinfrastructure security to community college students, to address the shortage of technical staff in cybersecurity. Community colleges have a significant student population of low income, diverse, first generation college students that have the potential for enabling a new stream of cybersecurity professionals. These colleges, however, face significant challenges to teaching advanced cybersecurity skills for cyberinfrastructure. To address these challenges, the project will develop vocational training modules on cyberinfrastructure security. Each module will consist of multiple micro-modules and target the skill sets required for entry level jobs in the cybersecurity field. Specifically, the approach involves three major tasks. The first task involves creating training modules on several offensive/defensive topics on cyberinfrastructure, including network penetration testing and digital forensics. The contents of the modules support problem based learning with a strong hands on components. The second task involves developing CRICE (Cyber Range Infrastructure for Cybersecurity Education) on NSFCloud to support problem based learning. NSFCloud is an NSF funded public cloud service available for research and training purposes. This will help community colleges to utilize the training modules effectively without requiring their own expensive computing infrastructure. The third task involves integrating the modules into the curriculum of two community colleges and evaluating the effectiveness of the problem based learning and CRICE in terms of cost, ease of adoption, student learning gain, and attitudinal survey on the students' experience. The project research contribution lies in exploring the effectiveness of problem based learning for vocational training in cybersecurity in the context of community colleges in the United States. The project will show that problem based learning is effective in providing both conceptual and practical understanding of cybersecurity topics. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.  The Advanced Technological Education (ATE) program in NSF's Division of Undergraduate Education is providing co-funding for this project in recognition of its contribution to education in community colleges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1910539","OAC Core: Small: Scalable Non-linear Dimensionality Reduction Methods to Accelerate Scientific Discovery","OAC","OAC-Advanced Cyberinfrast Core","05/01/2019","09/16/2021","Varun Chandola","NY","SUNY at Buffalo","Standard Grant","Seung-Jong Park","04/30/2022","$499,814.00","Jaroslaw Zola, Olga Wodo, Nils Napp","chandola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","090Y","026Z, 9179","$0.00","The progress in science and engineering increasingly depends on our ability to analyze massive amounts of observed and simulated data. The vast majority of this data, coming from high-performance high-fidelity simulations, high-resolution sensors, or Internet connected devices, arise from physical processes that, while complex and nonlinear, depend on only few parameters. However, these low-dimension parameters are often hidden in the deluge of high-dimensional data, and are frequently impossible to discover, and thus reason about, by the existing methods. This project will develop new efficient methods to help scientists and engineers, especially in manufacturing and robotics, to simplify complex data such that dynamic processes underlying the data can be better represented, understood and controlled. By leveraging nation?s advanced cyberinfrastructure, these methods will accelerate pace of materials design, reduce the cost and time-to-market of tailored devices, and aid the design, control, and operation of new complex robotic systems. The research outcomes of the project are closely integrated with the educational components, to train the next generation of scientists and engineers on these new technologies, resulting in a skilled and globally competent workforce, especially in the high-priority areas of Artificial Intelligence, Data Science, and Scientific Computing. This project thus promotes advancement of science, welfare and prosperity, as stated by NSF's mission.<br/><br/>This multidisciplinary research project aims at developing scalable end-to-end non-linear dimensionality reduction based solutions to accurately learn the dynamic behavior of complex systems. To this end the project introduces new parallel primitives and algorithmic innovations to enable deployment of non-linear spectral dimensionality reduction (NLSDR) and manifold learning methods on the next generation extreme scale computing systems. The project is based on the following key components: i) development of novel locality-aware data distribution and task scheduling strategies for individual NLSDR building blocks taking into account their inter-dependencies when executing in distributed memory environments such as Message Passing Interface and Map/Reduce clusters of multi-core processors, ii) design of new algorithmic strategies to manage data influx while maintaining crucial properties of the sub-manifold characterized by the data, and, iii) development of end-to-end solutions for two transformative example applications pertaining to advanced manufacturing and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920430","CICI: CCoE: Trusted CI: Advancing Trustworthy Science","OAC","Cybersecurity Innovation","10/01/2019","12/14/2021","Von Welch","IN","Indiana University","Cooperative Agreement","Robert Beverly","09/30/2024","$12,599,443.00","Barton Miller, James Basney, James Marsteller, Sean Peisert, Dana Brunson, Kelli Shute","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8027","041Z, 7556, 8027","$0.00","The National Science Foundation funds over seven billion dollars of research annually, nearly all of which relies heavily on information technology. The digital data produced and computing systems used by that research are subject to risks from cyber attacks, in some cases similar to other data and computing systems on the Internet and in other cases particular to their research mission. Cybersecurity appropriate to the research mission is necessary both to make today's scientific discoveries possible and to ensure that the science is trustworthy. Appropriate cybersecurity must respect the collaborative environments that span organizational and national boundaries, the high-performance technologies used to manage large scale data and computation, and the dynamic information technology demands common in NSF research. Different science domains also have varying requirements for data confidentiality, availability, and integrity. These aspects challenge the use of traditional cybersecurity paradigms and technologies to manage the risks to trustworthy, reproducible research.<br/><br/>As the NSF Cybersecurity Center of Excellence, Trusted CI brings together experts in cybersecurity, knowledgeable and experienced in the NSF research endeavor, who provide the NSF community with the leadership and support necessary to tackle the cybersecurity challenges to NSF research. Trusted CI directly supports individual NSF cyberinfrastructure projects and Major Facilities through collaborative engagements that address specific project needs. Trusted CI engagement activities include (but are not limited to) security reviews, security architecture design, identity and access management, and software assurance. Trusted CI provides cybersecurity situational awareness to the NSF cyberinfrastructure community through timely advisories and notices. Trusted CI organizes the annual NSF Cybersecurity Summit for Large Facilities and Cyberinfrastructure, providing the community with the opportunity to share lessons learned, attend practical training sessions, and collaborate on solving common challenges. Trusted CI performs outreach and dissemination of best practices via the Trusted CI website (http://trustedci.org), blog posts, email lists, and online chats, as well as providing cybersecurity training in person and via online courses. Leadership activities of Trusted CI include developing a cybersecurity framework for NSF science and fostering the transition to practice of NSF cybersecurity research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835890","Framework: Software: HDR Globus Automate: A Distributed Research Automation Platform","OAC","Software Institutes","11/01/2018","08/27/2018","Ian Foster","IL","University of Chicago","Standard Grant","Robert Beverly","10/31/2022","$2,000,000.00","Kyle Chard, Blase Ur","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Rapid increases in data volumes and velocities are overwhelming finite human capabilities. Continued progress in science and engineering demands that we automate a broad spectrum of currently manual research data manipulation tasks, from transfer and sharing to acquisition, publication, indexing, analysis, and inference. To address this need, which arises across essentially all scientific disciplines, this project will work with scientists in astronomy, engineering, geosciences, materials science, and neurosciences to develop and apply Globus Automate, a distributed research automation platform. Its purpose is to increase productivity and research quality across many science disciplines by allowing scientists to offload the management of a broad range of data acquisition, manipulation, and analysis tasks to a cloud-hosted distributed research automation platform. By thus enabling scientists to hand off responsibility for managing frequently performed tasks, such as acquiring, analyzing, and storing data, Globus Automate will increase the productivity of scientific instruments and the scientists that use them.<br/><br/>This project will expand the capabilities and reach of the highly successful Globus research data management platform. Globus combines a professionally operated cloud-hosted management service with Globus Connect software deployed on more than 12,000 storage system endpoints, spanning most research universities, NSF-funded compute facilities, and NSF disciplines. Users employ Globus web interfaces and APIs to drive data movement, synchronization, and sharing tasks at and among endpoints. This ability to hand off responsibility for such tasks to cloud-hosted management logic has enabled substantial increases in data management efficiency, and spurred development of a wide range of innovative data management applications. Globus Automate will extend Globus capabilities to produce a full-featured distributed research automation platform that will enable the reliable, secure, and efficient automation of a wide range of research data management and manipulation activities. It will extend intuitive trigger-action programming models, suitable for non-programming users, to enable the specification and execution of a series of actions. It will provide for the detection of data events both at Globus storage system endpoints (e.g., creation or modification of new data files, extraction of new metadata) and at other sources (e.g., completion or failure of Globus transfer tasks); the propagation of such events to a cloud-hosted orchestration engine for reliable, efficient, and secure processing; and the invocation of remote actions on Globus endpoints and other resources. The project will leverage these basic event mechanisms to implement solutions to challenging science problems associated with partner science projects, and create a library of automation flows, both general-purpose (e.g., data publication and data replication) and domain-specific (e.g., feature detection in experimental data). These data event mechanisms will be made available on all storage systems relevant to research (Globus already supports most on-premises and cloud systems) and integrated with the Python language and JupyterLab environment that have become popular in science, so that researchers can define and share data automation behaviors as simple Python programs. A quantitative and qualitative research agenda will analyze the usability and adoption of both the platform and the research automation paradigm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2145742","CAREER: Efficient and Reliable Data Transfer Services for Next Generation Research Networks","OAC","CAREER: FACULTY EARLY CAR DEV","06/01/2022","12/13/2021","Engin Arslan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Continuing Grant","Seung-Jong Park","05/31/2027","$316,936.00","","earslan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","1045","1045, 9150","$0.00","Research networks are crucial for data intensive, distributed, and collaborative science projects as they provide high speed connectivity between research and education institutions. However, users of research networks are unable to efficiently utilize available resources as existing transfer applications suffer from scalability issues at high speeds. This project designs and develops a scalable and robust data transfer framework for next-generation research networks to improve their utilization. Enhanced network performance in research networks allows seamless execution of next generation distributed science applications, thereby paving the way for breakthrough discoveries to be made swiftly. This project also promotes collaboration between scientists at geographically separated institutions by means of reducing the time it takes to share data. In addition to research contributions, this project has strong education plan tightly integrated into its research plan. The plan involves trainings for scientists to help them better utilize advanced cyberinfrastructure resources when dealing with large scale data, game development for middle and high school students to teach networking concepts, and summer schools for high school students for underrepresented groups to teach programming and networking. <br/><br/>As trend towards data intensive distributed science continues, it is becoming increasingly important to develop data transfer services that can scale to next generation terabit per second networks and beyond. To achieve this goal, this project focuses four key research directions: First, it innovates a modular file transfer architecture to separate I/O operations from network transfers to enable dynamic and component specific tuning. Second, it implements Quality of Service support for delay sensitive distributed workflows to meet their stringent performance requirements. Third, it develops scalable, secure, and low overhead integrity verification for file transfers through in network caching/computing and probabilistic error checking mechanisms. Fourth, it integrates the developed algorithms to commonly used workflow management systems to increase its adoption by a broader science community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104024","Collaborative Research: Elements: ROCCI: Integrated Cyberinfrastructure for In Situ Lossy Compression Optimization Based on Post Hoc Analysis Requirements","OAC","Software Institutes","09/15/2021","08/31/2021","Dingwen Tao","WA","Washington State University","Standard Grant","Amy Walton","08/31/2024","$280,000.00","","dingwen.tao@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","8004","077Z, 7923, 8004","$0.00","Today?s simulations and advanced instruments are producing vast volumes of data, presenting a major storage and I/O burden for scientists. Error-bounded lossy compressors, which can significantly reduce the data volume while controlling data distortion with a constant error bound, have been developed for years. However, a significant gap still remains in practice. On the one hand, the impact of the compression errors on scientific research is not well understood, so how to set an appropriate error bound for lossy compression is very challenging. On the other hand, how to select the best fit compression technology and run it automatically in scientific application codes is non-trivial because of strengths and weaknesses of different compression techniques and diverse characteristics of applications and datasets. This project aims to develop a Requirement-Oriented Compression Cyber-Infrastructure (ROCCI) for data-intensive domains such as astrophysics and materials science, which can select and run the best fit lossy compressor automatically at runtime, in terms of user's requirement on their post hoc analysis.<br/><br/>The overarching goal of this project is to offer a complete series of automatic functions and services allowing users to transparently run the best fit compressor at runtime during the scientific simulations or data acquisition. This project advances knowledge and understanding with three key thrusts: (1) it builds an efficient layer to interoperate with different lossy compressors and diverse post hoc analysis requirements on data fidelity by leveraging an existing compression adaptor library (LibPressio) and compression assessment library (Z-checker); (2) it develops an efficient engine to determine the best fit compressor with optimized settings based on user?s post-hoc analysis requirements; and (3) it develops a user-friendly infrastructure that integrates compression optimization and execution via the HDF5 dynamic filter mechanism. This project particularly targets cosmology and materials science applications and their specific requirements of using lossy compressors in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003624","CDS&E: Collaborative Research: HyLoC: Objective-driven Adaptive Hybrid Lossy Compression Framework for Extreme-Scale Scientific Applications","OAC","CDS&E","08/01/2020","07/22/2020","Dingwen Tao","AL","University of Alabama Tuscaloosa","Standard Grant","Tevfik Kosar","10/31/2020","$270,802.00","","dingwen.tao@wsu.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","8084","026Z, 8084","$0.00","Today's extreme-scale scientific simulations and instruments are producing huge amounts of data that cannot be transmitted or stored effectively. Lossy compression, a data compression approach leading to certain data distortion, has been considered as a promising solution, because it can significantly reduce the data size while maintaining high data fidelity. However, the existing lossy compression methods may not always work effectively on all datasets used in specific applications because of their distinct and diverse characteristics. Moreover, the user objectives in compression quality and performance may vary with applications, datasets or circumstances. This project aims to develop a hybrid lossy compression framework to automatically construct the best-fit compression for diverse user objectives in data-intensive scientific research. Educational and engagement activities are provided to develop new curriculum related to scientific data compression and promote research collaborations with national laboratories.<br/><br/>Designing an efficient, adaptive, hybrid framework that can always choose the best-fit compression strategy is nontrivial, since existing state-of-the-art lossy compression methods are developed with distinct principles. The project has a three-stage research plan. First, the project decouples the state-of-the-art error-bounded lossy compression approaches into multiple stages and effectively models the working efficiency (e.g., compression ratio, error, speed) of particular approaches in each stage. Second, the project develops a loosely-coupled framework to aggregate the decoupled compression stages together and also explores as many compression pipelines composed of different stages as possible, to optimize the classic compression efficiency, including compression quality and performance. Third, the project optimizes the synthetic data-movement performance regarding the external devices and resources, such as I/O performance. The team evaluates the proposed framework on multiple extreme-scale scientific applications, including cosmological simulations, light source instrument data analytics, quantum circuit simulations, and climate simulations. The project may create technologies that can increase the storage availability and improve the performance for extreme-scale scientific applications, opening opportunities for new discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2034169","CRII: OAC: An Efficient Lossy Compression Framework for Reducing Memory Footprint for Extreme-Scale Deep Learning on GPU-Based HPC Systems","OAC","CRII CISE Research Initiation","05/12/2020","05/07/2021","Dingwen Tao","WA","Washington State University","Standard Grant","Alan Sussman","04/30/2022","$189,593.00","","dingwen.tao@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","026Y","026Z, 8228, 9251","$0.00","Deep learning (DL) has rapidly evolved to a state-of-the-art technique in many science and technology disciplines, such as scientific exploration, national security, smart environment, and healthcare. Many of these DL applications require using high-performance computing (HPC) resources to process large amounts of data. Researchers and scientists, for instance, are employing extreme-scale DL applications in HPC infrastructures to classify extreme weather patterns and high-energy particles. In recent years, using Graphics Processing Units (GPUs) to accelerate DL applications has attracted increasing attention. However, the ever-increasing scales of DL applications bring many challenges to today?s GPU-based HPC infrastructures. The key challenge is the huge gap (e.g., one to two orders of magnitude) between the memory requirement and its availability on GPUs. This project aims to fill this gap by developing a novel framework to reduce the memory demand effectively and efficiently via data compression technologies for extreme-scale DL applications. The proposed research will enhance the GPU-based HPC infrastructures in broad communities for many scientific disciplines that rely on DL technologies. The project will connect machine learning and HPC communities and increase interactions between them. Educational and engagement activities include developing new curriculum related to data compression, mentoring a selected group of high school students in a year-long  research project for a regional Science Fair competition, and increasing the community's understanding of leveraging HPC infrastructures for DL technologies. The project will also encourage student interest in research related to DL technologies on HPC environment and promote research collaborations with multiple national laboratories.<br/><br/>Existing state-of-the-art GPU memory saving methods for training extreme-scale deep neural networks (DNNs) suffer from high performance overhead and/or low memory footprint reduction. Error-bounded lossy compression is a promising approach to significantly reduce the memory footprint while still meeting the required analysis accuracy. This project will explore how to leverage error-bounded lossy compression on DNN intermediate data to reduce the memory footprint for extreme-scale DNN training. The project has a three-stage research plan. First, the team will comprehensively investigate the impacts of applying error-bounded lossy compression to DNN intermediate data on both validation accuracy and training performance, using different error-bounded lossy compressors, compression modes, and error bounds on the targeted DNNs and datasets. Second, the team will optimize the compression quality of suitable error-bounded lossy compressors on different intermediate data based on the impact analysis outcome, and design an efficient scheme to adaptively apply a best-fit compression solution. Finally, the team will optimize the compression performance on the proposed lossy compression framework for state-of-the-art GPUs. The team will evaluate the proposed framework on high-resolution climate analytics and high-energy particle physics applications and compare it with existing state-of-the-art techniques based on both the memory footprint reduction ratio and training performance improvements (e.g., throughput, time, epoch number).  The project will enable scientists and researchers to train extreme-scale DNNs with a given set of computing resources in a fast and efficient manner, opening opportunities for new discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2002649","Collaborative Research: CyberTraining: Conceptualization: Planning a Sustainable Ecosystem for Incorporating Parallel and Distributed Computing into Undergraduate Education","OAC","CyberTraining - Training-based","09/20/2019","11/21/2019","Sushil Prasad","TX","University of Texas at San Antonio","Standard Grant","Almadena Chtchelkanova","02/28/2022","$423,921.00","","Sushil.prasad@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","044Y","026Z","$0.00","In this era of pervasive multicore machines, GPUs, cloud services, big data, machine learning, and the Internet of Things, there is a critical need for an institute to create a sustainable, discipline-wide ecosystem for incorporating parallel and distributed computing (PDC) into undergraduate computing curricula. Such an institute would support the community of educators, students, and other stakeholders, with the goal of developing a workforce that is ready to meet the challenges of working with current and future computing fabrics. The investigators propose planning for such an institute (iPDC) that can help eliminate the longstanding barrier of the sequential computing paradigm such that, analogous to the establishment of the object oriented paradigm, the PDC paradigm is naturally integrated into Computer Science (CS) and Computer Engineering (CE) curricula across various institutions as recommended by the 2013 ACM/IEEE Computer Science Curricula and now by ABET.<br/><br/>Through the network of funded and unfunded collaborators, established contacts with instructors at institutions serving underrepresented groups, and outreach efforts, the project will robustly engage with stakeholder communities through four well-structured planning workshops, weekly teleconferences, and feedback and dissemination activities to formulate the key attributes of the institute.Broadening PDC education will further enable advances in science and engineering, which depend increasingly on PDC systems, by providing the next generation of practitioners and researchers with the necessary skills and knowledge to effectively exploit them. The curriculum standards, adoption, and dissemination activities will have synergistic international components. Overall, this project will facilitate a rich exchange of ideas within the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940199","Collaborative Research: Accelerating the Discovery of Electronic Materials through Human-Computer Active Search","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","09/17/2019","Eric Toberer","CO","Colorado School of Mines","Standard Grant","Daryl Hess","10/31/2022","$419,937.00","","etoberer@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","099Y, 1712","054Z, 062Z, 8249, 8396, 8611","$0.00","The overarching goal of this project is to accelerate the discovery of  materials with tailored electronic properties through human-computer active search. These efforts will lay the groundwork for accelerating materials discovery, and advance the capability to control electronic properties in materials with the potential for profound societal impact. The thermoelectric and photocatalytic materials predicted, synthesized, and characterized in this research can realize societal advances in the space of energy and solar fuels. High-efficiency thermoelectric materials can revolutionize how heat sources are transformed into electrical power by eliminating the traditional intermediate mechanical energy conversions.  Earth-abundant  light-responsive catalysts are emerging as  an alternative to costly, rare metal catalysts to store solar energy as  portable liquid fuels, like ethanol. These green reactions  are enabling low-cost, carbon-neutral fuels.  The team brings together expertise in materials science, chemistry, machine learning, visualization, metadata, and knowledge frameworks to develop multi-fidelity, expert-guided active search strategies within materials science and chemistry.  Resonances among the team's existing outreach programs will broaden inclusion of students from underrepresented groups and be moderated via the Alliance for Diversity in Science and Engineering.  The work will provide cross-disciplinary training to graduate students and postdocs in all aspects of material informatics, including participating in and leading team efforts, co-mentorship of Ph.D.  and postdoctoral researchers, inclusive symposia at national conferences, and a summer workshop focused on the intersection of visualization, machine learning, ontological engineering and materials science. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative. <br/><br/>An interdisciplinary team will create a search framework for scientific discovery that leverages recent advances in material databases, machine learning, visualization, human-machine interaction, and knowledge structures. To broadly assess the efficacy of this approach, the search effort will span the electronic behavior of both molecules and crystalline materials:  (i) new organic photocatalysts for solar fuels production and (ii) new thermoelectric materials for electricity generation.  Central to this effort is the engagement of domain experts and associated feedback in a human-in-the-loop active search process. Dynamic visualizations will enable the user to (i) understand the underlying reasons why the materials are being suggested and (ii) provide a user steering capability to identify and annotate specific aspects of the explored search space. Domain-expert annotations and feedback will be parsed against a suite of ontologies, further aiding the search process by providing relational insight between features. New molecules and materials will be explored through a combination of first principles calculations and high-throughput, automated experimentation; these results will be incorporated into a continually growing open-access database. Efficiently integrating and directing evolving data-streams from experiment, computation, and human steering during the search will be achieved with a multi-fidelity active search policy. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative.  <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931352","Elements: PASSPP: Provenance-Aware Scalable Seismic Data Processing with Portability","OAC","Geophysics, Software Institutes, EarthCube","11/01/2019","09/06/2019","Yinzhi Wang","TX","University of Texas at Austin","Standard Grant","Robert Beverly","10/31/2022","$232,770.00","","iwang@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1574, 8004, 8074","026Z, 075Z, 077Z, 4444, 7923","$0.00","Most of what we know about the Earth's deep interior comes from the analysis of ground motion data recording seismic waves produced by large earthquakes from instruments around the entire planet.  Seismologists have developed a long list of methods to process modern seismic data to ?image? the Earth?s interior.  Much of our understanding of Earth's interior has been limited by the resolution of the tools available to construct these ""images"".  At present, the massive increase in data volume has pushed the data processing infrastructure of seismology to the breaking point.  The inability to handle data of this scale has imposed significant barrier to scientific discoveries, especially for the smaller research groups with limited resources.  Aiming to help improve this situation, this project introduces a new data management and processing system that is portable and scalable to run on any platforms from a personal computer to a large-scale supercomputer.  By leveraging and integrating sophisticated tools from cloud computing and high-performance computing (HPC) communities, the system can fill in the widening gap between the massive data made available by data centers and the inadequacy of data management and processing capability provided with current tools.  Seamless discovery, access, transfer, and processing of data and metadata outside of data centers will become possible for the community.  This project will also serve as the foundation to enable novel research utilizing massive data to change the way we study the structure, composition, and evolution of the Earth.<br/> <br/>This project aims to develop a seismic data management and processing system that is composed of a scalable parallel processing framework based on dataflow computation model, a NoSQL database system centered on document store, and a container-based virtualization environment.  The scalable processing component will be based on the iterative map-reduce model using Apache Spark to handle scheduling and flow of data through systems of different scales.  The provenance-aware data management will be enabled by managing all data created during processing with MongoDB, including process generated metadata, processed waveform data, processing parameters, and the log outputs.  All these core components as well as a script to configure and deploy the framework on different systems will be containerized with Singularity to provide portability.  All these components serve the two primary goals of the project: produce a system that will allow common seismology algorithms to run effectively on modern HPC platforms; and provide the means for seismologists with average experience in programming to implement their own algorithms to extend the system.  The system will serve as the infrastructure to make data intensive research such as deep learning possible for smaller research groups that usually don't have the necessary manpower to manage and process massive data in a sustainable fashion.  By enabling the ability to process massive data collected by increasing number of instruments, it will facilitate the transition of the field into data-intensive paradigm of science discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835421","Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/31/2018","Bryna Hazelton","WA","University of Washington","Standard Grant","Bogdan Mihaila","08/31/2022","$344,613.00","Miguel Morales","brynah@phys.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed.  These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The investigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.<br/><br/>One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849625","Toward a common digital continuum platform for big data and extreme-scale computing (BDEC2)","OAC","CYBERINFRASTRUCTURE","10/01/2018","08/28/2018","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Alan Sussman","09/30/2020","$203,406.00","Daniel Reed, Geoffrey Fox, Peter Beckman","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7231","026Z, 062Z","$0.00","By the end of this decade, the world's store of digital data is projected to reach 40 zetabytes (10 to the power 21 bytes), while the number of network-connected devices (sensors, actuators, instruments, computers, and data stores) is expected to reach 20 billion. While these devices vary dramatically in their capabilities and number, taken collectively they represent a vast ""digital continuum"" of computing power and prolific data generators that scientific, economic, social, governmental and military concerns of all kinds will want and need to utilize. The diverse set of powerful forces propelling the growth of this digital continuum are prompting calls from various quarters for a next generation network computing platform - a digital continuum platform (DCP) - for creating distributed services in a world permeated by devices and saturated by digital data. But experience shows how challenging the creation of such a future-defining platform is likely to be, especially if the goal is to maximize its acceptance and use, and thereby the size of the community of interoperability it supports. Focusing on the strategically important realm of scientific research, broadly conceived, this project is staging six international workshops (two each in the United States, Europe, and Asia over a two-year period) to enable transnational research communities in a wide range of disciplines to converge on a common DCP to meet this challenge. Building on a decade of leadership in cyberinfrastructure planning, the Big Data and Extreme-scale Computing (BDEC2) community is attacking this problem by pursuing three complementary objectives: <br/>1. Draft a design for a ""digital continuum platform"" (DCP) to serve as shared software infrastructure for the growing continuum of computing devices and data sources on which future science will rely; <br/>2. Organize and plan an international demonstration of the feasibility and potential of the DCP; and<br/>3. Develop a corresponding ""shaping strategy"" that addresses all relevant stakeholders and moves the community toward convergence on a standard DCP specification. <br/>Thus, the project serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense.<br/><br/>Creating a common digital continuum platform represents a grand challenge problem for the global cyberinfrastructure community. To address this monumental challenge and achieve its objectives, the BDEC2 community is organizing around four distinct but complementary activities:<br/>1. Surveying and analyzing the spectrum of application/workflow needs across diverse research and engineering communities who will use the digital continuum; <br/>2. Developing a reference design for a DCP system architecture that is able to manage the trade offs involved in using a widely shared infrastructure to satisfy diverse application community requirements; <br/>3. Strengthening cooperative and crosscutting efforts among stakeholders (e.g., research communities, commercial vendors, software developers, resource providers) in the ""cyber ecosystem"" of science and engineering; and <br/>4. Formulating a strategy for building community consensus on a common digital continuum within this same collection of stakeholders.<br/>To help researchers converge on critical problems for important user communities, foster and focus collaboration to solve those problems, and better coordinate software research and data sharing, the BDEC2 community process engages both international software research and data science communities. The process also includes inter-meeting working groups (e.g., for application/workflow analysis and DCP architecture). Combined with the work of the international meetings themselves, these working groups are intended to accelerate community-wide discussion and collaborative activities needed to address the multi-dimensional challenges of the emerging digital continuum. By achieving its goals, this project intends to supply the different stakeholder communities with the kind of well-defined vision and consensus building strategy necessary to realize a common, open, and interoperable DCP for digital continuum era. The project actively promotes participation by talented young scientists (up to 15 across the series) drawn from the academic community and with special attention to women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829744","Collaborative Research: CYBER Training: CIU: Data Streams, Model Workflows, and Educational Pipelines for Hydrologic Sciences","OAC","CyberTraining - Training-based","09/01/2018","07/09/2018","Anthony Castronova","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Alan Sussman","08/31/2022","$53,540.00","","acastronova@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392267445","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Studies of water and environmental systems are becoming increasingly complex and require integration of knowledge across multiple domains. At the same time, technological advances have enabled the collection of massive quantities of data for studying earth system changes. Fully leveraging these datasets and software tools requires fundamentally new approaches in the way researchers store, access and process data. The project serves the national interest by motivating a culture shift within the hydrologic and more broadly earth science communities toward open and reproducible software practices that will enhance interdisciplinary collaboration and increase capacity for addressing complex science challenges around the availability, risks and use of water. Project's CyberTraining approach provides virtual learning experiences throughout an academic year, with online learning modules oriented around a one-week in-person workshop (WaterHackWeek) that will focus on hands-on real-world research projects. These research projects are designed to serve the national interest by preparing for natural hazards such as floods, hurricanes and climate change, and to advance the nation's health by making tools and data accessible to health researchers, local governments, and citizens.<br/><br/>New cyberinfrastructure that emphasizes data sharing and open, reproducible software practices is currently in development, but requires a mode of knowledge transfer, or CyberTraining, that extends beyond currently available university curriculum.  Project's aim is to ensure successful use of community cyberinfrastructure to 1) publish large datasets, 2) run numerical models, 3) organize collaborative research projects, and 4) meet journal requirements to follow open data standards. The activities take advantage of HydroShare, a National Science Foundation funded cyberinfrastructure platform, operated by the Consortium of Universities Allied for Hydrologic Sciences (CUAHSI), for sharing hydrologic data and models.  The short-term goals are to develop new CyberTraining modules; the long-term goals are to have an annually recurring WaterHackWeek, to distribute curriculum CUAHSI to more than 130 member universities, and advance cyberinfrastructure education for the broader geoscience community. The use of the hackweek educational model extends the use of cyberinfrastructure to promote the progress of science by including a specific emphasis on graduate student training as instructors, training coordinators, and building research networks with data providers who are stakeholders outside of academia. For example, case studies include data and resource management by Native American tribal governments, Hurricane Maria data archive for research in Puerto Rico, improving flood forecasting, and tool-building using complex numerical models such as the National Water Model.  This project allows to test the educational model in the water research community, in addition to connecting team's research and curriculum to annually recurring hackweeks in neuro, astro, ocean, and geo sciences. The team of researchers is actively engaged in experimenting with this new model, and in testing its efficacy through robust evaluation metrics. The proposed activities encourage collaboration and support for use of cyberinfrastructure at all stages of the educational pipeline and provides participants with opportunities for networking, career development, community building and design of open-source software tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039932","Data CI Pilot: NCAR and NEON Cyberinfrastructure Collaborations to Enable Convergence Research Linking the Atmospheric and Biological Sciences","OAC","CESER-Cyberinfrastructure for, NFS Special Programs, MacroSysBIO & NEON-Enabled Sci","11/01/2020","10/14/2020","Gordon Bonan","CO","University Corporation For Atmospheric Res","Standard Grant","Bogdan Mihaila","10/31/2022","$599,996.00","Michael SanClements, David Durden, Dawn Lenz","bonan@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","7684, 7791, 7959","020Z, 4444, 7569","$0.00","Numerical models of weather and climate such as those developed at the National Center for Atmospheric Research (NCAR) are essential research tools to advance atmospheric science. At the same time, the National Ecological Observatory Network (NEON) provides long-term, comprehensive data on the state of the terrestrial ecosystem at sites distributed across the US. Ecologists use these data to develop a predictive understanding of ecosystems at large spatial scales. NCAR models are highly complementary to NEON data products and can inform data quality, the types of data to collect, and the frequency of data collection. As these models have matured, the role of terrestrial ecosystems in climate processes has come to the forefront. To maximize the benefits to atmospheric science and ecology, this project will deliver new cyberinfrastructure tools to bring together NEON observational data products and NCAR modeling capabilities. As a result, scientific discovery and innovation will be advanced at the confluence of the geosciences and biological sciences, through more seamless and rapid integration of NCAR models and NEON observations, and the ability to provide model outputs to NEON for use by the ecological community. <br/><br/>This project builds upon NCAR?s modeling of the land surface and its terrestrial ecosystems and NEON's observations and monitoring of terrestrial ecosystems to develop synergies between two heretofore independent research communities. We will link NCAR?s modeling capabilities in terms of the Community Earth System Model (CESM) and its Community Land Model (CLM) with NEON's measurement network through two cyberinfrastructure activities: First, we will create a portable containerized computing environment that allows university researchers to use and scientifically develop the model in conjunction with NEON data, with seamless transition across a variety of computational platforms from high performance supercomputers to laptops to cloud computing. The container image will be further enhanced by the addition of a built-in JupyterLab environment for visualization and analysis, and a collection of common notebooks that provide in-line examples and documentation of analysis methods, model configurations, and additional data transformations.  The modeling system will be enhanced with automated data acquisition and preprocessing for standard configurations, enabling scientists to focus on the research rather than the technical details.  Second, we will develop an automated operational analysis mode to allow for near-real time modeling of terrestrial ecosystems across the NEON network, both to verify model simulations and to supplement NEON measurements with model output, as well as validate NEON input data. We will use a cloud-based version of CESM/CLM in conjunction with cyberinfrastructure that monitors NEON outputs to spin up a configurable set of analysis runs whenever new data are available. The operational analysis will include site-specific runs focused on NEON measurement locations, giving regular feedback to modeling and observational teams, improving the quality of both. The ability to ingest near real time data is a novel capability, in contrast with standard datasets for model evaluation that are typically several years old.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2200409","CyberTraining: CIC: CyberTraining for Students and Technologies from Generation Z","OAC","CyberTraining - Training-based","12/01/2021","11/05/2021","Geoffrey Fox","VA","University of Virginia Main Campus","Standard Grant","Alan Sussman","10/31/2022","$105,683.00","","gcfexchange@gmail.com","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Information technology is playing a dramatically increasing role in society, industry, and research. This includes design and use of large databases, simulations and artificial intelligence applications hosted on clouds and supercomputers with convergent technologies. Correspondingly, there is an increasing need for research workforce job skills in these and related areas. This project takes freshly created Indiana University Engineering course material on this cyberinfrastructure and adapts it for training with an emphasis on the needs of under-represented communities. The techniques of the successful open source software movement are used to create sustainable communities around the course curriculum and software. The project is creating new technologies to enable this for today's generation of students. Skills in core cloud computing, big data, supercomputing and artificial intelligence are exemplified by applications in the life science and nanotechnology areas. This project enables the future research workforce to contribute effectively using advanced cyberinfrastructure, promoting the progress of science and advancing the national health, prosperity, and welfare, which serves the national interest, as stated by NSF's mission. <br/><br/>The future economic progress and research leadership of the U.S. is dependent on having a research workforce that is capable of making use of advanced cyberinfrastructure (CI) resources as articulated by the National Strategic Computing Initiative (NSCI). This requires a curriculum that changes and integrates modern concepts and practices for the new generation of students aiming at a ""data-enabled computational science and engineering"" expertise. This project takes what Indiana University has learned from a brand new four-year undergraduate engineering curriculum designed ab initio and taught so far to its first two undergraduate classes, and invests it into developing active training modules. The innovative curriculum integrates big data, simulations, clouds and high performance computing systems presented in a uniform framework. The course material is customized for communities of cyberinfrastructure researchers nucleated, built, and sustained via the dynamic use of GitHub and enhanced by innovative tools to build a novel learning management system optimized for cyberinfrastructure-intensive classes. The project modules include Cloud Computing, Big Data Applications and Analytics, Networking, High-Performance Computing, Artificial Intelligence/Machine Learning, and Information Visualization. There are residential sessions, with a call for participants, and purely online courses and these have both ""teach the student"" and ""teach the teacher"" modes; the latter enables easy spread of the classes. Hands-on learning with research projects built around the class material is fully supported. The project offers CyberTraining with all of the popular approaches used by the Apache Software Foundation, including Meetups and Hackathons. Modules for domain scientists and engineers, e.g., the cyberinfrastructure users that exploit advanced CI methods for research in nanoengineering and bioengineering are included. Both students and teachers contribute to the course improving the text, the software, including a unique set of examples and the project aims to show that one can build both learning and sustainability communities by using the proven techniques of the open source software community. The project uses proactive measures to enhance the involvement of under-represented communities in its activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017590","Collaborative Research:CyberTraining:Implementation:Medium: Broadening Adoption of Parallel and Distributed Computing in Undergraduate Computer Science and Engineering Curricula","OAC","CyberTraining - Training-based","10/15/2020","07/22/2020","Sushil Prasad","TX","University of Texas at San Antonio","Standard Grant","Almadena Chtchelkanova","09/30/2023","$701,772.00","Anshul Gupta","Sushil.prasad@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","044Y","026Z","$0.00","This collaborative project represents a multi-faceted effort to shift computer science and engineering education toward ensuring that students can use 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly designed around a single processor, executing a sequence of operations. But this century is characterized by widespread deployment of multi-core, graphics, and AI tensor processors, as well as a shift to cloud servers, and the internet of things, all of which depend on the much different PDC approach to problem solving and programming. Financial, technical, scientific, engineering and medical companies, government labs, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the old model. Yet most students continue to learn the old approach due to significant inertia in academia. To turn the tide toward infusing PDC into the early stages of computer science and engineering education, this project will guide curricula and accreditation standards, prepare teachers, and foster a strong PDC education community. It will thus strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) is preparing the 2020 update of their 2013 curriculum guidelines for introducing parallel and distributed computing (PDC) into early undergraduate courses. This project will engage in four areas of activity to foster adoption of the curriculum, and extend it, with the goal of modernizing computer science and engineering workforce development.<br/>One major thrust is running summer training workshops for teachers, to learn both PDC concepts and experimental course evaluation methodology. The discipline is still in a phase of discovery with respect to PDC education approaches, and must encourage a diverse set of well-designed experiments to test and evaluate a broad range of pedagogical hypotheses. The workshop participants will be drawn from a diverse pool of educators, and given curriculum development grants in support of experimental course offerings and evaluation, leading to conference or journal publications, as well as contributions of exemplars to the CDER online course materials repository. <br/>A second effort is to help ABET/CSAB to formulate core PDC requirements and to inform/train ABET/CSAB evaluators (CSAB is the lead society within ABET for accreditation of degree programs in computer science). <br/>A third effort is expanding the curriculum guidelines to explicitly address adding PDC to computer engineering programs, which present novel curricular opportunities. <br/>Lastly, it will continue CDER's successes in organizing PDC education workshops in conjunction with major conferences, publishing PDC education books and journal special issues, maintaining and curating an online repository of PDC education resources, and providing free access to a publicly available PDC education cluster system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018846","CC* Compute: Accelerating Advances in Science and Engineering at The University of Alabama Through HPC Infrastructure","OAC","Campus Cyberinfrastructure","07/01/2020","05/14/2020","Jeffrey Carver","AL","University of Alabama Tuscaloosa","Standard Grant","Kevin Thompson","06/30/2022","$399,995.00","David Dixon, Xiaoyan Hong, Dingwen Tao","carver@cs.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","8080","9150","$0.00","This project augments the University of Alabama?s (UA) computing infrastructure to support the increase in computational science and engineering needed to study diverse, interesting problems including: reliable computational chemistry predictions, properties of engineered materials, applied mathematics for image analysis and signal processing, bioinformatics of complex cellular systems, and hydrological simulations. The new high-performance computing (HPC) infrastructure removes bottlenecks in local UA resources caused by an increasing number of users and increasingly larger computational solutions to more realistic problems. This infrastructure enables UA researchers to make scientific and engineering advances not possible with previous UA HPC machines. To provide broader impacts, the infrastructure is also available to regional institutions of higher education, including HBCUs and private institutions who lack adequate HPC access, and to similar institutions across the nation through the Open Science Grid.<br/><br/>This project augments the UA HPC system by (1) doubling computing capacity, (2) adding two large-memory nodes for large-scale data analysis and mining, (3) adding ten GPUs for massively data-parallel computations, (4) significantly increasing storage node bandwidth, and (5) shifting from a ?condo? model to a general use, shared model. This infrastructure provides compelling new research and educational opportunities for students, staff, and faculty at UA and other regional and national institutions (including HBCUs and private institutions). In terms of broader impacts, the infrastructure allows undergraduate students to perform state-of-the-art computational research, thereby attracting more diverse STEM participants. The infrastructure provides a platform for educating the next generation of computational and computer scientists in cutting-edge HPC techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840034","CICI: CSRC: Research Security Operations Center (ResearchSOC)","OAC","Cybersecurity Innovation","10/01/2018","12/13/2021","Von Welch","IN","Indiana University","Standard Grant","Robert Beverly","09/30/2022","$5,148,720.00","Kenneth Goodwin, James Marsteller, Inna Kouper, Richard Biever, Susan Sons","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8027","5205, 8027","$0.00","The Research Security Operations Center (ResearchSOC) provides the research and education (R&E) community with cybersecurity services, training, and information sharing needed to make scientific computing resilient to cyberattacks and capable of supporting trustworthy, productive research. The R&E community has particular challenges regarding cybersecurity: a large span of size and autonomy, the use of diverse infrastructure (scientific instruments, sensor networks, sequencers, etc.), the highly collaborative and dynamic nature of scientific work, and acquiring the scarce domain expertise expertise needed to support cybersecurity in the research context.<br/><br/>To address these challenges, the ResearchSOC leverages existing cybersecurity services from Indiana University, Duke University, the Pittsburgh Supercomputing Center, and the University of California San Diego. It tailors these service to the needs of the R&E community and combines these operational services with the establishment of a community of practice for sharing knowledge and expertise, and operational intelligence. The ResearchSOC also offers outreach and training targeted at  educating research project teams and the higher education information security community regarding information security for research. The ResearchSOC is engaging with the cybersecurity research community to discern how it can advance research activities and make data available for research purposes in order to improve the overall state of the art in information security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106446","OAC Core: SMALL: DeepJIMU: Model-Parallelism Infrastructure for Large-scale Deep Learning by Gradient-Free Optimization","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","11/19/2020","Liang Zhao","GA","Emory University","Standard Grant","Alan Sussman","09/30/2023","$498,609.00","","liang.zhao@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","090Y","075Z, 079Z, 7923","$0.00","In recent years, the use of deep neural networks (DNNs) has been increasing to obtain useful insights for scientific explorations, business management, security, and healthcare. The constant improvement of DNN model performance has been accompanied by an increase in their complexity and size, which indicate a clear trend toward larger and deeper models. Such a trend is especially the case for numerous important application domains, such as remote sensing where super-high-resolution geospatial image processing is required. Such applications lead to a huge challenge for the training of very large models to fit on a single computing device (e.g., a graphics processing unit, GPU), and hence raises urgent demands for partitioning such models across multiple computing devices and parallelizing the training process (i.e., model parallelism). However, until now model parallelism for DNNs has been poorly explored and is very difficult due to the inherent bottleneck from the backpropagation algorithm, where the training of one layer closely depends on input from all the previous layers. To overcome these challenges, this project aims a radically new pathway toward model parallelism infrastructure for large-scale DNNs based on optimization methods that do not rely on backpropagation for training. This project plans to address the challenges of training very large and very deep neural network models that require huge amounts of high-dimensional data. The project will develop new optimization techniques and distributed DNN training software infrastructure to enable wider applications and deployment of model parallel deep learning training. The project includes educational and engagement activities that will greatly increase the community's understanding of distributed machine learning algorithms and systems. Those activities include teaching and training students and peers, providing graduate and undergraduate students with new courses, and research and internship opportunities, as well as broadening participation of underrepresented groups and students at local high schools.<br/><br/>This project brings together researchers in machine learning algorithms, distributed computing systems, remote sensing, and spatial data science, to boost the performance and scalability of deep learning applications enhanced by model parallelism. Specifically, this project focuses on proposing and developing a suite of new model parallelism optimization algorithms and system infrastructure for training large-scale DNNs, especially for image processing of massive datasets for geospatial scientific research. To enable model parallelism in the training, new gradient-free optimization methods are proposed to break down the whole problem of DNN optimization into subproblems, which can then be solved separately in parallel (by many workers) with high efficiency. The products of this project include new theories and algorithms for model parallelism, along with an efficient gradient-free DNN training framework with new scheduling and work balancing techniques. Specifically, this project has the following research thrusts: 1) Develop new gradient-free methods for training various types of DNNs; 2) Designing an algorithmic and theoretical framework of model parallelization based on gradient-free optimization; and 3) Building a scalable and efficient distributed training framework for a broad range of model parallel DNN training applications, such as deep learning for large graphs and very deep convolutional neural networks for image processing. This project also involves both theoretical and experimental comparison between the new techniques and current state-of-the-art methods, including those using gradient-based optimizations and pipeline parallelism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940239","Collaborative Research: Accelerating the Discovery of Electronic Materials through Human-Computer Active Search","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","09/17/2019","Jane Greenberg","PA","Drexel University","Standard Grant","Daryl Hess","09/30/2022","$211,328.00","","janeg@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158956342","CSE","099Y, 1712","054Z, 062Z, 8249, 8396, 8611","$0.00","The overarching goal of this project is to accelerate the discovery of  materials with tailored electronic properties through human-computer active search. These efforts will lay the groundwork for accelerating materials discovery, and advance the capability to control electronic properties in materials with the potential for profound societal impact. The thermoelectric and photocatalytic materials predicted, synthesized, and characterized in this research can realize societal advances in the space of energy and solar fuels. High-efficiency thermoelectric materials can revolutionize how heat sources are transformed into electrical power by eliminating the traditional intermediate mechanical energy conversions.  Earth-abundant  light-responsive catalysts are emerging as  an alternative to costly, rare metal catalysts to store solar energy as  portable liquid fuels, like ethanol. These green reactions  are enabling low-cost, carbon-neutral fuels.  The team brings together expertise in materials science, chemistry, machine learning, visualization, metadata, and knowledge frameworks to develop multi-fidelity, expert-guided active search strategies within materials science and chemistry.  Resonances among the team's existing outreach programs will broaden inclusion of students from underrepresented groups and be moderated via the Alliance for Diversity in Science and Engineering.  The work will provide cross-disciplinary training to graduate students and postdocs in all aspects of material informatics, including participating in and leading team efforts, co-mentorship of Ph.D.  and postdoctoral researchers, inclusive symposia at national conferences, and a summer workshop focused on the intersection of visualization, machine learning, ontological engineering and materials science. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative. <br/><br/>An interdisciplinary team will create a search framework for scientific discovery that leverages recent advances in material databases, machine learning, visualization, human-machine interaction, and knowledge structures. To broadly assess the efficacy of this approach, the search effort will span the electronic behavior of both molecules and crystalline materials:  (i) new organic photocatalysts for solar fuels production and (ii) new thermoelectric materials for electricity generation.  Central to this effort is the engagement of domain experts and associated feedback in a human-in-the-loop active search process. Dynamic visualizations will enable the user to (i) understand the underlying reasons why the materials are being suggested and (ii) provide a user steering capability to identify and annotate specific aspects of the explored search space. Domain-expert annotations and feedback will be parsed against a suite of ontologies, further aiding the search process by providing relational insight between features. New molecules and materials will be explored through a combination of first principles calculations and high-throughput, automated experimentation; these results will be incorporated into a continually growing open-access database. Efficiently integrating and directing evolving data-streams from experiment, computation, and human steering during the search will be achieved with a multi-fidelity active search policy. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative.  <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835717","Elements: Software: HDR: A knowledge base of deep time to facilitate automated workflows in studying the co-evolution of the geosphere and biosphere","OAC","XC-Crosscutting Activities Pro, CESER-Cyberinfrastructure for, Software Institutes, EarthCube","12/01/2018","05/26/2020","Xiaogang Ma","ID","Regents of the University of Idaho","Standard Grant","Seung-Jong Park","11/30/2022","$620,375.00","","xgmachina@gmail.com","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","7222, 7684, 8004, 8074","026Z, 062Z, 077Z, 7923, 8004, 9150, 9251","$0.00","This project will result in the creation of a software that will support research in the Earth's deep time history. The co-evolution of the geosphere and biosphere is one of the fundamental questions for the 21st century Earth science. The multi-disciplinary characteristics of the research questions on co-evolution are reflected in the various subjects of datasets that need to be integrated. In the past decades, many open data facilities have been built through the support from NSF and other sources. However, the shortage of efficient methods for accessing and synthesizing multi-source datasets hamper the data-intensive co-evolution research. Geologic time is an essential topic in the co-evolving geosphere and biosphere, and can be used as a common reference to connect various parameters among the data silos. This project will improve the machine readability and alignment of various global, local and regional geologic time standards and build a knowledge base of deep time and its service on the Web. All the deliverables will be well-documented and offered under open-access to promote a national cyberinfrastructure ecosystem. The planned tasks and activities will leverage the usage of existing data facilities, facilitate executable and reproducible workflows, generate best practices of cross-disciplinary data science, generate state-of-the-art materials to education programs, and engage the participation of female and underrepresented groups. Shared in the national cyberinfrastructure, the knowledge base built in the project will be able to support a broad range of research, education and outreach programs, which will benefit not only science and engineering but also the society at large.<br/><br/>The research question to be addressed is the heterogeneity of geologic time concepts that hamper the data synthesis among multiple data facilities. Accordingly, the objective of this project is to build a knowledge base of deep time to automate geoscience data access and integration in the open data environment, and to support data synthesis in executable workflows for data-intensive scientific discovery. The development approach will include both top-down and bottom-up tracks to leverage previous works on geologic time ontologies and address end user needs through use case analyses. With carefully designed activities and work plan, deliverables from this project will include a machine-readable knowledge base of aligned geologic time standards, services and packages for accessing and querying the knowledge base, and best practices of data synthesis in workflow platforms for studying the co-evolution. The developed knowledge base of deep time will provide powerful support to co-evolution researchers to tackle data heterogeneity issues. Robust services the knowledge base will be built to support automated data synthesis in workflow platforms to advance the co-evolution research. The source code and metadata of the knowledge base will be released on GitHub and registered on community repositories to enable reuse and adaptation. <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2148729","BD Spokes: SPOKE: MIDWEST: Collaborative: Advanced Computational Neuroscience Network (ACNN)","OAC","BD Spokes -Big Data Regional I, IntgStrat Undst Neurl&Cogn Sys","11/01/2021","09/13/2021","Franco Pestilli","TX","University of Texas at Austin","Standard Grant","Alejandro Suarez","06/30/2023","$66,460.00","","pestilli@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","024Y, 8624","028Z, 7433, 8083, 8089, 8091","$0.00","Novel neuroscience tools and techniques are necessary to enable insight into the building blocks of neural circuits, the interactions between these circuits that underpin the functions of the human brain, and modulation of these circuits that affect our behavior. To leverage rapid technological development in sensing, imaging, and data analysis new ground breaking advances in neuroscience are necessary to facilitate knowledge discovery using data science methods. To address this societal grand challenge, the project will foster new interdisciplinary collaborations across computing, biological, mathematical, and behavioral science disciplines together with partnerships in academia, industry, and government at multiple levels. The Big Data Neuroscience Spoke titled Midwest: Advanced Computational Neuroscience Network (ACNN) is strongly aligned with the national priority area of neuroscience and brings together a diverse set of committed regional partners to enable the Midwest region to realize the promise of Big Data for neuroscience. The ACNN Spoke will build broad consensus on the core requirements, infrastructure, and components needed to develop a new generation of sustainable interdisciplinary Neuroscience Big Data research. ACNN will leverage the strengths and resources in the Midwest region to increase innovation and collaboration for the understanding of the structure, physiology, and function of the human brain through partnerships and services in education, tools, and best practices. <br/><br/>The ACNN will design, pilot and support powerful neuroscientific computational resources for high-throughput, collaborative, and service-oriented data aggregation, processing and open-reproducible science. The ACNN Spoke framework will address three specific problems related to neuroscience Big Data: (1) data capture, organization, and management involving multiple centers and research groups, (2) quality assurance, preprocessing and analysis that incorporates contextual metadata, and (3) data communication to software and hardware computational resources that can scale with the volume, velocity, and variety of neuroscience datasets. The ACNN will build a sustainable ecosystem of neuroscience community partners in both academia and industry using existing technologies for collaboration and virtual meeting together with face-to-face group meetings. The planned activities of the ACNN Spoke will also allow the Midwest Big Data Hub to disseminate additional Big Data technologies resources to the neuroscience community, including access to supercomputing facilities, best practices, and platforms.<br/><br/>This award received co-funding from CISE Divisions of Advanced Cyberinfrastructure (ACI) and Information and Intelligent Systems (IIS)."
"2018927","CC* CRIA: The Eastern Regional Network","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","James von Oehsen","NJ","Rutgers University New Brunswick","Continuing Grant","Kevin Thompson","06/30/2022","$250,000.00","Vasant Honavar, Bruce Segee, John Goodhue, Sharon Pitt","barr.vonoehsen@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7231, 8080","","$0.00","This Eastern Regional Network (ERN) CI-Research Alignment project organizes six information-gathering events: four workshops and two All Hands Meetings. These events are aimed at building and sustaining regional partnerships that simplify multi-campus collaborations to advance the frontiers of research, pedagogy, and innovation. With a deeper understanding gained through the four workshops, augmented by wider discussions at the two All Hands meetings, the ERN is better positioned to ensure impact across broad institutional types, research disciplines and pedagogical approaches, and well versed in relevant technical and administrative challenges and opportunities.<br/><br/>Two workshops focus on learning from the Cryo-EM and Materials Discovery communities about their research workflows and their computational, storage, and network requirements. These workshops also cover data management plans, types of collaborations (national and international), and the pain points associated with sharing data and resources between institutions. A third workshop brings together university Vice Presidents of Research, Chief Information Officers, General Counsel, Institutional Review Board directors, and other ERN representatives to discuss the CI Sharing Policies needed to simplify sharing of data, infrastructure, and expertise between universities and across state lines. The final workshop, ?Broadening the Reach,? includes researchers, educators, and senior administrators from under-represented colleges and universities, including MSIs, HSIs, and HBCUs. This workshop ensures that these institutions can develop strategic and attainable CI plans and successful, sustainable CC* funded initiatives that leverage the regional ERN collaboration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004323","Collaborative Research: Elements: EdgeVPN: Seamless Secure VirtualNetworking for Edge and Fog Computing","OAC","Software Institutes","06/01/2020","04/29/2020","Cayelan Carey","VA","Virginia Polytechnic Institute and State University","Standard Grant","Robert Beverly","05/31/2023","$80,387.00","","cayelan@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","077Z, 7923","$0.00","Edge computing encompasses a variety of technologies that are poised to enable new applications across the Internet that support data capture, storage, processing and communication near the edge of the Internet. Edge computing environments pose new challenges, as devices are heterogeneous, widely distributed geographically, and physically closer to end users, such as mobile and Internet-of-Things (IoT) devices. This project develops EdgeVPN, a software element that addresses a fundamental challenge of networking for edge computing applications: establishing Virtual Private Networks (VPNs) to logically interconnect edge devices, while preserving privacy and integrity of data as it flows through Internet links. More specifically, the EdgeVPN software developed in this project addresses technical challenges in creating virtual networks that self-organize into scalable, resilient systems that can significantly lower the barrier to entry to deploying a private communication fabric in support of existing and future edge applications. There are a wide range of applications that are poised to benefit from EdgeVPN; in particular, this project is motivated by use cases in ecological monitoring and forecasting for freshwater lakes and reservoirs, situational awareness and command-and-control in defense applications, and smart and connected cities. Because EdgeVPN is open-source and freely available to the public, the software will promote progress of science and benefit society at large by contributing to the set of tools available to researchers, developers and practitioners to catalyze innovation and future applications in edge computing.<br/><br/>Edge computing applications need to be deployed across multiple network providers, and harness low-latency, high-throughput processing of streams of data from large numbers of distributed IoT devices. Achieving this goal will demand not only advances in the underlying physical network, but also require a trustworthy communication fabric that is easy to use, and operates atop the existing Internet without requiring changes to the infrastructure. The EdgeVPN open-source software developed in this project is an  overlay virtual network that allows seamless private networking among groups of edge computing resources, as well as cloud resources. EdgeVPN is novel in how it integrates: 1) a flexible group management and messaging service to create and manage peer-to-peer VPN tunnels grouping devices distributed across the Internet, 2) a scalable structured overlay network topology supporting primitives for unicast, multicast and broadcast, 3) software-defined networking (SDN) as the control plane to support message routing through the peer-to-peer data path, and 4) network virtualization and integration with virtualized compute/storage endpoints with Docker containers to allow existing Internet applications to work unmodified. EdgeVPN self-organizes an overlay topology of tunnels that enables encrypted, authenticated communication among edge devices connected across disparate providers in the Internet, possibly subject to mobility and constraints imposed by firewalls and Network Address Translation, NATs. It builds upon standard SDN interfaces to implement packet manipulation primitives for virtualization supporting the ubiquitous Ethernet and IP-layer protocols.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940307","Collaborative Research: Accelerating the Discovery of Electronic Materials through Human-Computer Active Search","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","06/02/2020","Steven Lopez","MA","Northeastern University","Standard Grant","Daryl Hess","09/30/2022","$591,000.00","Semion Saikin","s.lopez@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","099Y, 1712","054Z, 062Z, 8249, 8396, 8611","$0.00","The overarching goal of this project is to accelerate the discovery of  materials with tailored electronic properties through human-computer active search. These efforts will lay the groundwork for accelerating materials discovery, and advance the capability to control electronic properties in materials with the potential for profound societal impact. The thermoelectric and photocatalytic materials predicted, synthesized, and characterized in this research can realize societal advances in the space of energy and solar fuels. High-efficiency thermoelectric materials can revolutionize how heat sources are transformed into electrical power by eliminating the traditional intermediate mechanical energy conversions.  Earth-abundant  light-responsive catalysts are emerging as  an alternative to costly, rare metal catalysts to store solar energy as  portable liquid fuels, like ethanol. These green reactions  are enabling low-cost, carbon-neutral fuels.  The team brings together expertise in materials science, chemistry, machine learning, visualization, metadata, and knowledge frameworks to develop multi-fidelity, expert-guided active search strategies within materials science and chemistry.  Resonances among the team's existing outreach programs will broaden inclusion of students from underrepresented groups and be moderated via the Alliance for Diversity in Science and Engineering.  The work will provide cross-disciplinary training to graduate students and postdocs in all aspects of material informatics, including participating in and leading team efforts, co-mentorship of Ph.D.  and postdoctoral researchers, inclusive symposia at national conferences, and a summer workshop focused on the intersection of visualization, machine learning, ontological engineering and materials science. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative. <br/><br/>An interdisciplinary team will create a search framework for scientific discovery that leverages recent advances in material databases, machine learning, visualization, human-machine interaction, and knowledge structures. To broadly assess the efficacy of this approach, the search effort will span the electronic behavior of both molecules and crystalline materials:  (i) new organic photocatalysts for solar fuels production and (ii) new thermoelectric materials for electricity generation.  Central to this effort is the engagement of domain experts and associated feedback in a human-in-the-loop active search process. Dynamic visualizations will enable the user to (i) understand the underlying reasons why the materials are being suggested and (ii) provide a user steering capability to identify and annotate specific aspects of the explored search space. Domain-expert annotations and feedback will be parsed against a suite of ontologies, further aiding the search process by providing relational insight between features. New molecules and materials will be explored through a combination of first principles calculations and high-throughput, automated experimentation; these results will be incorporated into a continually growing open-access database. Efficiently integrating and directing evolving data-streams from experiment, computation, and human steering during the search will be achieved with a multi-fidelity active search policy. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative.  <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1938914","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","10/15/2020","Prodromos Daoutidis","MN","University of Minnesota-Twin Cities","Continuing Grant","Sylvia Spengler","09/30/2022","$268,815.00","","daout001@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","099Y, 1640","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1948447","CRII: OAC: An Efficient Lossy Compression Framework for Reducing Memory Footprint for Extreme-Scale Deep Learning on GPU-Based HPC Systems","OAC","CRII CISE Research Initiation","05/01/2020","04/24/2020","Dingwen Tao","AL","University of Alabama Tuscaloosa","Standard Grant","Alan Sussman","06/30/2020","$174,593.00","","dingwen.tao@wsu.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","026Y","026Z, 8228","$0.00","Deep learning (DL) has rapidly evolved to a state-of-the-art technique in many science and technology disciplines, such as scientific exploration, national security, smart environment, and healthcare. Many of these DL applications require using high-performance computing (HPC) resources to process large amounts of data. Researchers and scientists, for instance, are employing extreme-scale DL applications in HPC infrastructures to classify extreme weather patterns and high-energy particles. In recent years, using Graphics Processing Units (GPUs) to accelerate DL applications has attracted increasing attention. However, the ever-increasing scales of DL applications bring many challenges to today?s GPU-based HPC infrastructures. The key challenge is the huge gap (e.g., one to two orders of magnitude) between the memory requirement and its availability on GPUs. This project aims to fill this gap by developing a novel framework to reduce the memory demand effectively and efficiently via data compression technologies for extreme-scale DL applications. The proposed research will enhance the GPU-based HPC infrastructures in broad communities for many scientific disciplines that rely on DL technologies. The project will connect machine learning and HPC communities and increase interactions between them. Educational and engagement activities include developing new curriculum related to data compression, mentoring a selected group of high school students in a year-long  research project for a regional Science Fair competition, and increasing the community's understanding of leveraging HPC infrastructures for DL technologies. The project will also encourage student interest in research related to DL technologies on HPC environment and promote research collaborations with multiple national laboratories.<br/><br/>Existing state-of-the-art GPU memory saving methods for training extreme-scale deep neural networks (DNNs) suffer from high performance overhead and/or low memory footprint reduction. Error-bounded lossy compression is a promising approach to significantly reduce the memory footprint while still meeting the required analysis accuracy. This project will explore how to leverage error-bounded lossy compression on DNN intermediate data to reduce the memory footprint for extreme-scale DNN training. The project has a three-stage research plan. First, the team will comprehensively investigate the impacts of applying error-bounded lossy compression to DNN intermediate data on both validation accuracy and training performance, using different error-bounded lossy compressors, compression modes, and error bounds on the targeted DNNs and datasets. Second, the team will optimize the compression quality of suitable error-bounded lossy compressors on different intermediate data based on the impact analysis outcome, and design an efficient scheme to adaptively apply a best-fit compression solution. Finally, the team will optimize the compression performance on the proposed lossy compression framework for state-of-the-art GPUs. The team will evaluate the proposed framework on high-resolution climate analytics and high-energy particle physics applications and compare it with existing state-of-the-art techniques based on both the memory footprint reduction ratio and training performance improvements (e.g., throughput, time, epoch number).  The project will enable scientists and researchers to train extreme-scale DNNs with a given set of computing resources in a fast and efficient manner, opening opportunities for new discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1917117","CICI: SSC: Proactive Cyber Threat Intelligence and Comprehensive Network Monitoring for Scientific Cyberinfrastructure: The AZSecure Framework","OAC","Cybersecurity Innovation","07/01/2019","09/21/2020","Hsinchun Chen","AZ","University of Arizona","Standard Grant","Robert Beverly","06/30/2022","$998,014.00","Mark Patton, Peter Troch, Edwin Skidmore, Sagar Samtani","hchen@eller.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8027","","$0.00","The rapid growth of computing technologies in scientific instruments has increased the rate of discovery. Some recent examples include the discovery of new fundamental particles and the first-ever images of a black hole. Unfortunately, the same technologies contributing to these high-impact discoveries are also being targeted by hackers to steal ideas or for profit. These attacks threaten the privacy, integrity, and ability to access valuable scientific data and events. The risks to scientific facilities and how they can be attacked have still not been properly mapped. This project will use novel Artificial Intelligence to (1) study hackers in the international and ever-evolving Dark Web and identify and categorize hundreds of thousands of risks and (2) link those risks to possible attacks on two large-scale science community facilities. One of them is a facility funded by the National Science Foundation offering advanced computing resources for Life Sciences. The other uses a network of sensors around the globe to collect detailed and timely data for Earth Sciences. Studying these valuable targets enables investigation of current and emerging threats that present risk to scientific discovery.<br/><br/>Led by the Hispanic Serving Institution (HSI) University of Arizona (UA), this project designs an innovative, holistic, and proactive Cyber Threat Intelligence (CTI) framework with two synergistic research streams. The first builds upon advanced topic modelling and text classification approaches from our NSF Secure and Trustworthy Cyberspace (SaTC) research to systematically collect and explore multi-million record Dark Web hacker forums for scientific cyberinfrastructure exploits. The second designs novel banner data feature extraction, text analytics, and custom vulnerability scanning integrating state-of-the-art tools to comprehensively categorize and assess the vulnerabilities within CyVerse?s (life sciences) and LEO?s (earth sciences) diverse instruments, data, hardware, and software. Exploit and vulnerability assessment results are linked via a novel deep learning-based Exploit Vulnerability Deep Structured Semantic Model (EV-DSSM) based on word embedding. UA?s National Security Agency-designated Center of Academic Excellence in Cyber Defense, Research, and Operations, NSF Scholarship-for-Service (SFS) Cyber-Corps, and Master?s in Cybersecurity programs position the project for synergy with teaching and research. Techniques developed in this project will advance knowledge not only CTI, but network analysis, deep learning, and text analytics across multiple disciplines. Findings from this research will be disseminated to 75+ SFS partner institutions and operational intelligence for the larger scientific community (e.g., NSF Cybersecurity Summits of Large Facilities and Cyberinfrastructure).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835446","Elements: Software: Towards Efficient Embedded Data Processing","OAC","Software Institutes","01/01/2019","08/24/2018","Jignesh Patel","WI","University of Wisconsin-Madison","Standard Grant","Robert Beverly","12/31/2022","$599,800.00","","jignesh@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Embedded databases are ubiquitous, though that fact may not be widely-recognized. Every smart phone has many embedded databases, which implies that billions of people worldwide carry dozens of databases in their phones/pockets every day. Many of these databases are powered by data processing technology that has not kept up with the pace with which the underling hardware in phones have evolved. As a result, data processing is slow, and consumes more energy than needed. The focus of this proposal is on developing new data processing technology for mobile devices that targets a 10X efficiency and performance improvements. The aims of the project go beyond more efficient data processing on phones to also include more efficient processing in embedded environments, which also includes databases running on laptops. Thus, the project aims for a broad impact on database across a spectrum of mobile devices.<br/><br/>The technical contributions of this project are in recognizing that modern hardware, even at the ?low-end? which includes mobile phones and laptops, now have multiple processing cores, relatively large amounts of memory, and flash storage. There is a critical need for a new class of embedded data processing systems that can work efficiently, and effectively on such modern mobile platforms. This project aims to build a system, called Hustle, to address this need. The project will design, develop and implement a range of data processing methods, which include predicate-based concurrency control mechanisms, query processing methods that inherently expose and exploit opportunities for intra and inter-operator parallelism, and query optimization methods that target embedded settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916805","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/01/2019","Madhav Marathe","VA","University of Virginia Main Campus","Standard Grant","Robert Beverly","10/31/2023","$2,880,000.00","","mvm7hz@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835598","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Jurij Leskovec","CA","Stanford University","Standard Grant","Robert Beverly","10/31/2023","$540,000.00","","jure@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940096","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Info Integration & Informatics","10/01/2019","08/26/2021","Catherine Stamoulis","MA","Children's Hospital Corporation","Continuing Grant","Sylvia Spengler","09/30/2022","$561,086.00","","caterina.stamoulis@childrens.harvard.edu","300 LONGWOOD AVENUE","Boston","MA","021155737","6179192729","CSE","099Y, 7364","062Z, 9102, 9251","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931387","Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA)","OAC","Software Institutes","11/01/2019","07/19/2019","Robert Harrison","NY","SUNY at Stony Brook","Standard Grant","Seung-Jong Park","10/31/2024","$1,099,300.00","","robert.harrison@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers.  Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research.  The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.<br/><br/>Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835441","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Albert Esterline","NC","North Carolina Agricultural & Technical State University","Standard Grant","Robert Beverly","10/31/2023","$40,000.00","","esterlin@ncat.edu","1601 E. Market Street","Greensboro","NC","274110001","3363347995","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848641","EAGER:Experimental Deployment of the ARTEMIS BGP Hijacking Detection Prototype in Research and Educational Networks","OAC","Cybersecurity Innovation","09/01/2018","08/16/2018","Alberto Dainotti","CA","University of California-San Diego","Standard Grant","Robert Beverly","08/31/2019","$184,985.00","Alistair King","adainotti6@gatech.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8027","7916","$0.00","The Internet suffers from a design flaw affecting the security of global routing, exposing it to a class of large-scale attacks that can cause significant portions of network traffic to be diverted across the globe.  The potential impact of these attacks - known as BGP hijacking - can range from massive eavesdropping to identity spoofing or selective content modification. In addition, executing such attacks does not require access or proximity to the affected links and networks, posing increasing risks to national security. Recently, researchers at CAIDA, UC San Diego, proposed a novel methodology (called ARTEMIS) to detect and mitigate BGP hijacking attacks that is radically different from the state of the art but that network operators find promising and more in line with their needs.  In this project, researchers carry out a first experimental assisted deployment of ARTEMIS in few selected operator networks supporting education, research and other no-profit activities in the US.  This project significantly contributes to transitioning from a prototype mainly based on theoretical foundations and controlled experiments into a real platform that can be practically used by operators to secure their networks.  The project also contributes to improving the security of networks participating in the project, with an immediate positive effect on the security of infrastructure critical to our society. <br/><br/>This project is a collaboration among researchers and selected research and education network operators to carry out an experimental deployment of ARTEMIS detection and mitigation techniques in their networks.  The main goals of such experimentation are: i) to evaluate the ARTEMIS methods and their implementation in real operational scenarios; ii) to learn and address real deployment technical issues; iii) to train operators in the use of the tool and contribute to improve the security of their networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939934","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","10/15/2020","David Rand","NY","SUNY at Albany","Continuing Grant","Sylvia Spengler","01/31/2021","$398,311.00","David Rand","drand@mit.edu","1400 Washington Ave MSC 312","Albany","NY","122220100","5184374974","CSE","099Y, 1640","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841586","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Adam Brazier","NY","Cornell University","Standard Grant","William Miller","09/30/2020","$23,301.00","","abrazier@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829771","CyberTraining:CIC: DeapSECURE: A Data-Enabled Advanced Training Program for Cyber Security Research and Education","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Hongyi Wu","VA","Old Dominion University Research Foundation","Standard Grant","Alan Sussman","08/31/2022","$500,000.00","Masha Sosonkina, Wirawan Purwanto","h1wu@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","As the volume and sophistication of cyber-attacks grow, cybersecurity researchers, engineers and practitioners heavily rely on advanced cyberinfrastructure (CI) techniques such as big data, machine learning, and parallel programming, as well as advanced CI platforms, e.g., cloud and high-performance computing to assess cyber risks, identify and mitigate threats, and achieve defense in depth. However, advanced CI techniques have not been widely introduced in undergraduate and graduate cybersecurity curricula. This lack creates a hurdle for many senior undergraduates and early-stage graduate cybersecurity students who are keen to conduct cutting-edge cybersecurity research and/or participate in advanced industrial cybersecurity projects. This project introduces a unique Data-Enabled Advanced Training Program for Cyber Security Research and Education (DeapSECURE), aimed to prepare undergraduate and graduate students with advanced CI techniques and teach them to use CI resources, tools, and services to succeed in cutting-edge cybersecurity research and industrial cybersecurity projects. The project responds to the urgent need for well-prepared cybersecurity workforce in the Hampton Roads metropolitan region, the Commonwealth of Virginia, and the Nation. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/>This project develops six new CI training modules which emphasize the practical use of the advanced CI techniques, especially the tools that implement them, in the context of cybersecurity research. Each training module includes three sections: (1) an overview presented by an invited cybersecurity faculty about his/her research, concluding with a research problem that heavily depends on CI techniques; (2) an introduction of corresponding CI skills, tools and platforms; (3) a hands-on lab session where students will apply the CI techniques to solve the research problem formerly introduced by the cybersecurity faculty. The modules will be delivered via two distinct means: monthly workshops and summer institutes. Six monthly workshops are conducted during academic year, primarily targeting students enrolled at Old Dominion University (ODU). The summer institutes present these six modules to students from local community colleges, Research Experiences for Undergraduates program at ODU, and other Virginia universities; they also include special activities such as field trips, open house for K-12 students, Cyber Night events, cybersecurity career panels, and student competitions. Complementing the workshops and summer institutes, an online continuous learning community is created, which includes a virtual computer lab and a student forum, as a place for students to continue their learning engagement after the face-to-face sessions. Archived workshop materials, as well as additional learning materials are also posted on this online platform as open educational resources, to be made available to the cybersecurity research and education communities. The open-source style development of the learning modules facilitates a wide-range of adoption, adaptations, and contributions in an efficient manner. The project leverages existing and new partnerships to ensure broad participation, and accordingly broaden the adoption of advanced CI techniques in the cybersecurity community. The project employs a rigorous assessment and evaluation plan rooted in diverse metrics of success to improve the curricula and demonstrate its effectiveness. The metrics, which are based on the students' outcomes and exit surveys, are assessed by an independent evaluator. The adoption of the learning modules outside of the training program is also considered as a metric of success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940224","Collaborative Research: Accelerating the Discovery of Electronic Materials through Human-Computer Active Search","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","09/17/2019","Roman Garnett","MO","Washington University","Standard Grant","Daryl Hess","09/30/2022","$305,855.00","","garnett@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","099Y, 1712","054Z, 062Z, 8249, 8396, 8611, 9150","$0.00","The overarching goal of this project is to accelerate the discovery of  materials with tailored electronic properties through human-computer active search. These efforts will lay the groundwork for accelerating materials discovery, and advance the capability to control electronic properties in materials with the potential for profound societal impact. The thermoelectric and photocatalytic materials predicted, synthesized, and characterized in this research can realize societal advances in the space of energy and solar fuels. High-efficiency thermoelectric materials can revolutionize how heat sources are transformed into electrical power by eliminating the traditional intermediate mechanical energy conversions.  Earth-abundant  light-responsive catalysts are emerging as  an alternative to costly, rare metal catalysts to store solar energy as  portable liquid fuels, like ethanol. These green reactions  are enabling low-cost, carbon-neutral fuels.  The team brings together expertise in materials science, chemistry, machine learning, visualization, metadata, and knowledge frameworks to develop multi-fidelity, expert-guided active search strategies within materials science and chemistry.  Resonances among the team's existing outreach programs will broaden inclusion of students from underrepresented groups and be moderated via the Alliance for Diversity in Science and Engineering.  The work will provide cross-disciplinary training to graduate students and postdocs in all aspects of material informatics, including participating in and leading team efforts, co-mentorship of Ph.D.  and postdoctoral researchers, inclusive symposia at national conferences, and a summer workshop focused on the intersection of visualization, machine learning, ontological engineering and materials science. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative. <br/><br/>An interdisciplinary team will create a search framework for scientific discovery that leverages recent advances in material databases, machine learning, visualization, human-machine interaction, and knowledge structures. To broadly assess the efficacy of this approach, the search effort will span the electronic behavior of both molecules and crystalline materials:  (i) new organic photocatalysts for solar fuels production and (ii) new thermoelectric materials for electricity generation.  Central to this effort is the engagement of domain experts and associated feedback in a human-in-the-loop active search process. Dynamic visualizations will enable the user to (i) understand the underlying reasons why the materials are being suggested and (ii) provide a user steering capability to identify and annotate specific aspects of the explored search space. Domain-expert annotations and feedback will be parsed against a suite of ontologies, further aiding the search process by providing relational insight between features. New molecules and materials will be explored through a combination of first principles calculations and high-throughput, automated experimentation; these results will be incorporated into a continually growing open-access database. Efficiently integrating and directing evolving data-streams from experiment, computation, and human steering during the search will be achieved with a multi-fidelity active search policy. Through enabling the acceleration of the discovery of new materials, this project supports the goals of the Materials Genome Initiative.  <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126253","CC* Compute: RAPTOR - Reconfigurable Advanced Platform for Transdisciplinary Open Research","OAC","Campus Cyberinfrastructure","10/01/2021","10/19/2021","Jason Liu","FL","Florida International University","Standard Grant","Kevin Thompson","09/30/2023","$400,000.00","Julio Ibarra, Keqi Zhang, Cassian D'Cunha, Jayantha Obeysekera, Yuepeng Li","liux@cis.fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","8080","9102","$0.00","Building resilient, sustainable, livable, and environmentally safe dynamic systems (natural or human built) requires on-demand computing resources, facilitating machine learning, data processing, and data analytics. These systems rely on computation to support simulation, modeling, and analyses to enable discovery, facilitate understanding, and make decisions. This project implements RAPTOR (Reconfigurable Advanced Platform for Transdisciplinary Open Research), a reconfigurable compute environment to address dynamic and diverse computing needs of science drivers??coastal resilience, sustainable environmental research, and systems biology.<br/><br/>The goal of RAPTOR is to increase research production by enhancing computing capabilities at Florida International University (FIU), both at the campus level and through participation in a resource-sharing federated distributed computing community. For that, RAPTOR integrates with the Chameleon Cloud Infrastructure for on-demand resource allocation and the Open Science Grid (OSG) for opportunistic preemptible resource sharing allocation. The result is a platform capable of connecting with a rapidly deployable sampling system that can assimilate and transmit data in real-time to facilitate actionable intelligence and drive adaptive environmental monitoring decisions, and respond as conditions change. The integration of knowledge, tools, and modes of computation proposed by RAPTOR builds upon the expertise of domain researchers, computer scientists, and IT practitioners towards a ?smart cyberinfrastructure? to foster and develop transdisciplinary approaches. RAPTOR not only introduces new computing modalities at FIU to support its researchers, but also benefits a broader federated community of researchers and scholars sharing resources through OSG and Chameleon Cloud.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019085","CC* Integration-Large: mGuard: A Secure Real-time Data Distribution System with Fine-Grained Access Control for mHealth Research","OAC","CISE Research Resources","10/01/2020","06/30/2020","Lan Wang","TN","University of Memphis","Standard Grant","Deepankar Medhi","09/30/2022","$825,000.00","Lixia Zhang, Santosh Kumar","lanwang@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","2890","9102","$0.00","The mGuard project aims to address two major data access challenges in sharing mobile health (mHealth) data among researchers who investigate a wide range of health and wellness issues: (1) because wearable sensor data may expose privacy-sensitive information about a user, they should be accessed only by authorized users; currently this access control is largely handled manually, incurring high overhead and subject to human errors; and (2) to enable real-time intervention for certain medical conditions, researchers need to retrieve and process the sensor data in real-time, which is not supported at this time.<br/><br/>mGuard tackles the above challenges by utilizing the results from the NSF-supported Named Data Networking (NDN) initiative, in particular the solutions that automate the cryptographic key management for data access control (name-based access control, or NAC) and the solutions that enable real-time synchronization among distributed datasets (NDN Sync). First, mGuard utilizes and extends NDN NAC to automate fine-grained access control of confidential data to authorized researchers.  Second, it utilizes NDN Sync to provide real-time data production notification; based on this, it enables applications to publish and subscribe to data in real time by directly using MD2K data names. These new capabilities will be deployed in the MD2K cyberinfrastructure.<br/>    <br/>This effort enables the Mobile Sensor Data-to-Knowledge (MD2K) Center (supported by the National Institute of Health) to share its data securely and in real time with a large number of mHealth researchers. The transformative potential of mGuard thus extends across many types of digital interventions and many health domains. mGuard also encourages researchers in other areas of data-intensive applications to explore NDN?s data-centric solutions. To train the next generation, mGuard is creating undergraduate and graduate education materials including concrete examples and hands-on exercises, as well as training and outreach activities through online seminars, conference tutorials, mHealth training institute, and summer camps.  <br/><br/>The mGuard project website is https://mguard.md2k.org.  Published papers are maintained through the mGuard website, NDN website (https://named-data.net/), MD2K website (https://md2k.org), and publishers? websites.  All of the software produced by the project is maintained in the NDN GitHub (https://github.com/named-data) and mHealthHUB (https://mhealth.md2k.org/).   Curriculum materials, program documentations, and user manuals are maintained on the mGuard project website. The trace data, simulation code, and evaluation results will be maintained on mGuard internal servers for a period of at least 10 years. Investigators will archive data, samples, and other research products, and preserve access to them at least five years beyond the project?s end date, subject to resource availability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925482","CC* Integration: Rutgers University Next-Generation Edge Testbed (RU-NET)","OAC","CISE Research Resources, Campus Cyberinfrastructure","10/01/2019","04/14/2020","James von Oehsen","NJ","Rutgers University New Brunswick","Standard Grant","Deepankar Medhi","09/30/2022","$1,015,014.00","Thu Nguyen, Richard Martin, Ivan Seskar, Srinivas Narayana","barr.vonoehsen@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2890, 8080","9102","$0.00","The technological landscape of computing is changing at a phenomenal rate with recent advances in big data, deep learning, data mining, cloud services, and cybersecurity, along with many novel consumer and scientific devices. Research universities, especially IT support, are challenged to be agile in this landscape, as they must keep pace with the needs of the research and user community. To bolster researchers working on these technologies, this project seeks to support two key network-centric requirements: incorporating user-owned devices at the edge of the network and moving bulk data between data collection nodes and data processing nodes over the network.<br/><br/>This project will design, implement, test, and utilize a novel research testbed, the Rutgers University Next-Generation Edge Testbed (RU-NET). RU-NET is a campus-wide platform designed to simplify the deployment of new and emerging edge technologies, while preserving the stability of the campus network. RU-NET will allow integrating user-owned devices in laboratories at the edge of the network with Rutgers's three campus networks into a unified 100 Gbit/s network managed using programmable switches and host devices. The RU-NET design is multi-layered, constituting researchers' applications, RU-NET-owned physical resources, and orchestrator software mapping applications to resources and isolating different research applications.<br/><br/>RU-NET will serve as a catalyst to position Rutgers University researchers at the forefront of scientific discovery and to improve compute and network infrastructure that benefits the University community as a whole. The project plans to develop course material to educate undergraduate and graduate students about novel network-centric technologies, training students from Rutgers's diverse student body. The project also plans to share best practices with academic institutions and enterprise communities that are planning similar distributed computing environments, with the possibility of this project serving as a template for similar future efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143249","CAREER: Synergistic physics-based and deep learning cardiovascular flow modeling","OAC","CAREER: FACULTY EARLY CAR DEV","07/01/2022","12/08/2021","Amirhossein Arzani","AZ","Northern Arizona University","Continuing Grant","Seung-Jong Park","06/30/2027","$299,170.00","","Amir.Arzani@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","1045","079Z, 1045","$0.00","Accurate quantification of blood flow across different scales is crucial to our fundamental understanding of cardiovascular disease and clinical decision making. While computational and experimental blood flow modeling has seen tremendous progress, we still have difficulty generating reliable data. Low resolutions and unknown parameters overburden high fidelity modeling. Additionally, blood flow dynamics near the wall where disease localizes are hard to quantify due to thin boundary layers and challenges in near-wall transport modeling. Finally, the large datasets that blood flow models produce are difficult to efficiently store and interpret. This project will develop software that synergistically integrates novel deep learning and traditional physics-based modeling to address these issues. The software will promote the progress of scientific cardiovascular disease and fluid flow modeling research and ultimately advance national health. The project will create new education programs integrated with research to promote fluid flow computer modeling education by blending data analysis, visualization, and computer modeling. The education program will be integrated with regional initiatives to promote STEM participation in underrepresented groups.<br/><br/><br/>This CAREER program is built on four overarching goals. First, physics informed neural network (PINN) models will be used to overcome inherent blood flow modeling limitations. Second, the models will be used for gaining a physical understanding of blood flow patterns across different scales. Auxiliary PINN models will be defined to bridge blood flow patterns away and near the vessel wall and understand the minimal data collection necessary for near-wall blood flow modeling. Subsequently, a hybrid computational fluid dynamics (CFD) and PINN model will be developed for on-the-fly CFD and deep learning modeling. The goal is to compress and store the wealth of information that is generated and often ignored by CFD solvers and tackle difficult multiscale problems that challenge traditional CFD approaches. Finally, an education program called FAST (Fluids, Art, and StoryTelling!) will be developed to generate enthusiasm for integrated computer modeling and engineering education. The goal is to leverage the art of visualization and storytelling to show the hidden beauty in fluid mechanics computer modeling. This CAREER program will build a foundation for hybrid and complementary deep learning and physics-based modeling approaches that advance fundamental and transformative blood flow modeling research and will enable lifetime leadership in integrating research with education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1944873","Collaborative Research: Examining Impact and Fostering Academic Support for Open Science Products","OAC","Sociology, Project & Program Evaluation, NSF Public Access Initiative","10/01/2019","12/08/2021","Felice Levine","DC","American Educational Research Association","Standard Grant","Martin Halbert","12/31/2022","$74,526.00","","flevine@aera.net","1430 K Street, NW","Washington","DC","200057820","2022383200","CSE","1331, 7261, 7414","9179, SMET","$0.00","Open Science has the potential to significantly advance STEM knowledge beyond individual academic research silos. This project will support a conference entitled, ""Examining Impact and Fostering Academic Support for Open Science Products."" The meeting is jointly organized by the American Educational Research Association (AERA) and the Council of Graduate Schools (CGS).  The objective of the conference is to examine the products of open science practices and explore what counts as productivity and quality in contributions that are non-traditional research outcomes. Non-research outcomes have attendant activities that include data sharing; the development of multi-user data sets, harvesting and archiving big data, replication studies, registered reports, data management plans, and use as well as citation to persistent identifiers (DOIs).<br/><br/>The conference attendees are higher education leaders, education researchers, and related scientists, together with collective expertise in scientific productivity, science professions, higher education institutions.  The conference will engage attendees in a policy conversation and development of performance metrics and assessments on non-traditional research outcomes beyond publication citations in highly ranked journals. This several-day conference, scheduled for Summer 2020, will be held at the AERA Conference Center in Washington DC and will involve about 28 participants. This conference has the potential to support change and foster experimentation in the very institutions where the next generation of researchers are being trained, where science is organized, and where open science products are produced.<br/><br/>This award is co-funded by the NSF Research Traineeship (NRT) Program. The NRT Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1938985","CYBER-INSIGHT: Evaluating Cyberinfrastructure Total Cost of Ownership","OAC","XD-Extreme Digital","04/09/2019","07/23/2019","Daniel Reed","UT","University of Utah","Standard Grant","Edward Walker","07/31/2021","$246,761.00","","dan.reed@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7476","7916","$0.00","Academic research computing technologies is rapidly changing.   In addition, new methods in gleaning insight from the deluge of data that is becoming available in science and society are fueling demand for computing infrastructure, and motivating commercial companies to provision infrastructure similar to those found at academic environments.  These factors, as well as others, are putting pressure on academic institutions to find cost-effective approaches to providing computing infrastructure to maximize scientific benefit while minimizing cost.  This project seeks to understand the cost-effectiveness of different methods of delivering High Performance Computing capabilities to the academic research community.   Specifically, the project proposes to build, publish, and regularly update comparisons of the evolving total cost of ownership (TCO) of on premise HPC clusters and data storage systems relative to NSF-supported facilities and commercial cloud services such as Amazon AWS, Microsoft Azure, and Google Cloud Platform.  The results of the project will have broad applicability to the national research community, institutional resource providers, and funding agencies. In addition, the project is collaborating with many organizations to promote long-term maintenance and data sharing of the project outcomes, with the goal of creating a sustainable infrastructure for long-term analysis.<br/><br/>Concretely, the team proposes to build and host a Jupyter notebook that will enable the cyberinfrastructure (CI) community to add, modify, and explore total cost of ownership (TOC) models based on a variety of usage patterns and performance expectations. This will allow the project to explore questions such as when is it cost-effective to use commercial clouds compared to university clusters to service researcher needs? How do staff costs, hardware utilization, capital depreciation, replacement costs, the value of money, return on investment, power and cooling costs, and bandwidth charges affect TCO?  As science gateways hide resources behind web interfaces, when and where should such queries execute?  In addition to investigating these important questions, the project is also expected to facilitate community building, shared experimentation and comparisons.  The ultimate goal of the project is to develop comparative models that allow funding agencies, institutional resource providers, and individual users to evaluate the costs and benefits of specific choices and subsidies. Finally, the project will collaborate with the Coalition for Academic Scientific Computation (CASC), the Association of American Universities (AAU) and the Association of Public and Land Grant Universities (APLU) senior research officers (SROs) and CIOs to promote long-term maintenance and data sharing of the project outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829721","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Anand Padmanabhan","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alan Sussman","07/31/2021","$22,375.00","","apadmana@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829718","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Forrest Bowlick","MA","University of Massachusetts Amherst","Standard Grant","Alan Sussman","07/31/2021","$40,515.00","","fbowlick@umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827186","CC* Networking Infrastructure: Building a high-performance, flexible DMZ backbone at the University of Nevada, Reno","OAC","Campus Cyberinfrastructure","07/01/2018","06/25/2018","Graham Kent","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Kevin Thompson","06/30/2021","$495,255.00","Jeffrey LaCombe, Scotty Strachan, Jeff Springer","gkent@seismo.unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","8080","9150","$0.00","Advances in science and engineering and the use of science-driven technology are transforming the scale and complexity of research at the University of Nevada, Reno (UNR). Fundamental to supporting these shifts in data volume and research workflows are core networking and computing infrastructures maintained by the central campus Office of Information Technology. As part of a strategic plan to increase technology support of research activity at the university, a collaboration of faculty and IT professionals are building a dedicated, high-speed research network backbone across campus with connections to off-campus high-performance-compute facilities, remote sensor networks, and national peering points. The primary tasks of this project are to: 1) increase external connectivity to Internet2 and Pacific Wave from 10G to 100G; 2) extend a set of 40G dedicated research network paths with data transfer management and monitoring to geographically distributed locations on campus to serve critical research efforts; 3) implement a ""scienceDMZ"" network architecture for scientific and research workflows across the UNR network and as a model for Nevada's higher education community; and 4) augment key wide-area network end-to-end research workflows impacting rural areas all the way to UNR's high-performance research computing colocation partner Switch, a new state-of-the-art world-class datacenter.<br/><br/>This project solves current and near-future connectivity problems for science-focused researchers at UNR by enabling larger-scale science, saving time, and increasing the rate of discovery, and creating the capability for the institution to participate in the emerging Pacific Wave and National Research Platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835865","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","02/04/2021","Julian Borrill","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","08/31/2022","$481,001.00","Nathan Whitehorn, Akito Kusaka","borrill@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934757","Collaborative Research: Advancing Science with Accelerated Machine Learning","OAC","CYBERINFRASTRUCTURE","09/01/2019","05/06/2021","Zhizhen Zhao","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Amy Walton","08/31/2022","$600,311.00","Volodymyr Kindratenko, Mark Neubauer, Zhizhen Zhao","zhizhenz@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","099y, 7231","062Z, 7231","$0.00","In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.<br/><br/>In this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152085","Collaborative Research: OAC CORE: Large-Scale Spatial Machine Learning for 3D Surface Topology in Hydrological Applications","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/16/2021","Zhe Jiang","FL","University of Florida","Standard Grant","Alan Sussman","09/30/2024","$261,161.00","","zhe.jiang@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","090Y","075Z, 079Z, 7923, 9150","$0.00","Rapid advances in sensing technology and computer simulation have generated vast amounts of 3D surface data in various scientific domains, from high-resolution geographic terrains to electrostatic surfaces of proteins. Analyzing such emerging 3D surface big data provides scientists an opportunity to study problems that were not possible before, such as mapping detailed surface water flow and distribution for the entire continental US. Despite its vast transformative potential, machine learning tools to analyze large volumes of 3D surface data are not readily available. The project aims to fill this gap by designing a novel parallel spatial machine learning framework for 3D surface topology and implementing the system in a distributed computing environment. The system can produce high-quality observation-based flood inundation maps derived from satellite images. In collaboration with federal agencies (e.g., U.S. Geological Survey, NOAA), the project will enhance situational awareness for flood disaster response and improve flood forecasting capabilities of the NOAA National Water Model by filling in the gap of lacking observations in model calibration and validation. The proposed software tools will be open-source to enhance the research infrastructure for the broad geoscience communities. Educational activities include curriculum development, mentoring a group of high school students in data science seminars at K-12 Summer Camps, and year-long projects for selected high school students in regional Science Fair competitions. <br/><br/>The project will transform spatial machine learning research by enhancing terrain awareness through modeling large-scale 3D surface topology. Specifically, the project will bring about the following cyberinfrastructure innovations. First, the project will design a topography-aware spatial probabilistic model called hidden Markov contour forest, which advances existing machine learning tools by incorporating physical constraints of heterogeneous 3D terrains into zonal tree structures in the model representation. Second, the project will investigate a parallel inference framework by decomposing both intra-zone dependency and inter-zone dependency. Finally, the project will implement the proposed parallel learning framework in a distributed computing environment by addressing challenges related to task partitioning, load balancing, and dynamic task scheduling. The proposed system will be deployed for real-world rapid flood disaster response and the validation and calibration of the National Water Model through collaboration with the U.S. Geological Survey and NOAA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919452","MRI Acquisition of a High Performance Large Memory Computing Cluster for Large Scale Data-Driven Research","OAC","Major Research Instrumentation","10/01/2019","09/11/2019","Peter Ramadge","NJ","Princeton University","Standard Grant","Alejandro Suarez","09/30/2022","$499,000.00","Ryan Adams, Bridgett vonHoldt, Barbara Engelhardt, Prateek Mittal","ramadge@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1189","075Z, 1189","$0.00","This project will acquire a state-of-the-art High Performance Computing (HPC) cluster to support large scale, data-driven research. The instrument will support a variety of projects from computer science, electrical engineering, ecology, evolutionary biology, neuroscience and genomics. In neuroscience, the cluster will allow the use of advanced statistical techniques at scale to identify and connect anatomical and functional brain-imaging features of diseased and healthy subjects with specific underlying genetic profiles. In computer science, using machine learning algorithms deployed on the instrument, researchers will to seek new ways to protect the security and privacy of users in large-scale networked systems. Finally, the cluster will also enable research that will improve our understanding of evolutionary history and the molecular complexities of traits through the analysis of multi-animal, large-scale genomic datasets. In addition, through short courses and multiday boot-camps, the instrument will provide valuable opportunities for training postdoctoral fellows, graduate students, and advanced undergraduates in large-scale computational data science. The instrument will also be a valuable asset for certificate programs in statistics and machine learning (one for undergraduate students, the other for graduate students) and for a certificate program in computational science, all of which will support broadening participation of groups underrepresented in STEM. The research and training enabled by the instrument is expected to help improve our understanding of human health and well-being, help create new knowledge that will aid economic competitiveness, and help maintain the country's leadership in science and engineering. <br/><br/>The computing cluster will be formed of by nodes with very large memory. The system complements the institution's investments in research cyberinfrastructure and will be managed by the Princeton Institute for Computational Science and Engineering (PICSciE) and the Office of Information Technology (OIT). The instrument would initially be used by five research groups, part of the Center for Statistics and Machine Learning (CSML), which will leverage existing programs and partnerships to increase participation in data science. The initial five specific projects are united under a common theme: machine learning will be used for analyzing big data sets that may not be easily broken into smaller pieces for processing. Specifically, they will examine the following: 1) the use of probabilistic models for large-scale scientific analysis and de novo design in applications areas such as mechanical metamaterials and mixed-signal circuit development; 2) statistical machine learning in genomics, biomedicine, and health biostatistics including the analysis of hospital records to aid doctors in taking early action to improve patient outcomes, the heritability of neuropsychiatric diseases and drug responses, and statistical and experimental examination of cardiovascular disease risk;  3) security and privacy challenges in networked systems using machine learning techniques to detect and isolate attackers in networked systems such as social media; 4) large-scale machine learning for neuroscience such as joint analysis of many large-scale, multi-subject fMRI datasets where the size and number of the datasets; 5) evolutionary genomic and epigenome analyses through collection and analysis of large datasets to investigate the evolutionary history and molecular complexities of traits. Collectively, these research groups are composed of forty graduate students, ten postdocs, and include, on average, thirteen undergrad research projects per year. The instrument will also be used by other researchers engaged in large-scale, data-driven research across a wide variety of disciplines. Hence both the capacity and the capability aspects of the proposed instrument will be highly utilized and will enable the continued advancement of research at the University.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835660","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Madhav Marathe","VA","Virginia Polytechnic Institute and State University","Standard Grant","Robert Beverly","09/30/2019","$2,880,000.00","Edward Fox, Naren Ramakrishnan, Catherine Amelink, Christopher Kuhlman","mvm7hz@virginia.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925541","CC* Compute: Integrating Georgia Tech into the Open Science Grid for Multi-Messenger Astrophysics","OAC","Campus Cyberinfrastructure","07/01/2019","06/10/2019","Mehmet Belgin","GA","Georgia Tech Research Corporation","Standard Grant","Kevin Thompson","06/30/2021","$399,883.00","Laura Cadonati, Ignacio Taboada, Nepomuk Otte, Semir Sarajlic","mehmet.belgin@oit.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8080","","$0.00","Studying the Universe with all possible sources of information is the objective of multi-messenger astrophysics. Groundbreaking gravitational wave observations with LIGO, high-energy neutrino observations with IceCube, and very high-energy gamma-ray observations with VERITAS enable multi-messenger astrophysics. These and future instruments like CTA enable a complete view of the most violent and energetic phenomena in the Universe, such as the merger of black holes and neutron stars or the processes near the supermassive black holes at the center of large galaxies.  All these efforts are computationally intensive, both in data analysis and in simulations. The Open Science Grid (OSG) infrastructure and services are an ideal platform to meet the computational requirements of Multi-messenger Astrophysics. This project acquires cyber infrastructure resources to connect Georgia Tech to the OSG and integrate these resources into computational efforts of the previously-mentioned NSF funded facilities.<br/><br/> The High Throughput Computing Cluster acquired under this award includes 12 compute nodes with 40-core Intel Skylakes with 192 GB memory to support LIGO project requirements and others. IceCube and LIGO are supported with 4 GPU nodes, each equipped with 16-core Intel Skylakes and 4 NVIDIA TeslaV100 GPUs with 96GB memory. A special OSG StashCache has 432TB of storage in direct support of projects such as CTA. The system is 10Gbps connected in the Georgia Tech data center which in turn has 100Gbps+ to Southern Crossroads R&E exchange point. This project's resources serve as a catalyst for Georgia Tech's long-term integration into the OSG, as a standard service offered to all researchers on campus. An important component of this proposal is a significant number of Graphical Processing Units, used to accelerate simulations. This project makes Atlanta the first StashCache provider in the Southeast, as a service that enables fast access to distributed OSG datasets by regional institutions that participate in this national grid.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827227","CC* Network Design: The Western South Dakota Research and Education Network","OAC","Campus Cyberinfrastructure","07/01/2018","01/24/2020","Ryan Johnson","SD","University of South Dakota Main Campus","Standard Grant","Kevin Thompson","06/30/2021","$697,922.00","Kara Keeter, Cynthia Anderson, Kenneth Benjamin, Paul Hinker","Ryan.Johnson@usd.edu","414 E CLARK ST","vermillion","SD","570692307","6056775370","CSE","8080","9150","$0.00","High-speed connectivity to remote state and national cyberinfrastructure resources such as advanced computing systems, data stores, and scientific instrumentation is crucial in states like South Dakota. For this project, the South Dakota School of Mines and Technology (SDSM&T) and Black Hills State University (BHSU) are collaborating with the University of South Dakota (USD) to transform the way campus and state networks are designed and operated by addressing the unique challenges of deploying accessible cyberinfrastructure in a rural state. Specifically, SDSM&T and BHSU are building out campus network infrastructure in support of multidisciplinary research and education including: advanced imaging and microscopy, underground science, weather prediction, and high throughput genetic sequencing.<br/><br/>With a system architecture conceived and designed in year 1 and deployed in year 2 this project aims to increase STEM research and education productivity in western South Dakota by re-architecting campus networks and regional aggregation points so that they are optimized for current and future research and education applications. Architectural characteristics include aspects of the Science DMZ approach to streamline academic data flows; improved wired and wireless connections within and between science buildings and the network core; network measurement leveraging existing campus edge PerfSONAR nodes as well as new nodes at each campus core; data transfer nodes based on the Flash IO Network Appliance; and Globus middleware for data movement. With guidance from state, regional, and national advisors this collaboration is establishing an aggregation model for accessible cyberinfrastructure in rural regions where under-resourced institutions are the norm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925641","CC* Networking Infrastructure: Claflin Research Network","OAC","Campus Cyberinfrastructure","08/01/2019","07/17/2019","Joey Brenn","SC","Claflin University","Standard Grant","Kevin Thompson","07/31/2021","$352,505.00","Nicholas Panasik, Bijoy Dey, Arezue Boroujerdi, Deidra Morrison","jbrenn@claflin.edu","400 Magnolia Street","Orangeburg","SC","291154498","8035355540","CSE","8080","9150","$0.00","This award supports Claflin University's research capabilities through a coordinated information technology expansion plan called the ""Claflin Research Network"". Claflin University is building a 10Gbps Science DMZ supporting Big Data research projects through a re-designed campus border prioritizing science data flows. This new component enables the transfer of very large datasets over long distances in a friction-free path. Claflin's collaborators include the Medical University of South Carolina (MUSC), Clemson University, and many domestic institutions as well as several international institutions. This project greatly increases Claflin's ability to share data between research collaborators in geographically dispersed institutions around the world.<br/><br/>This project will provide Claflin with the ability to be more competitive and makes Claflin a stronger research partner with the innovation ecosystem. In addition, this project allows for greater research and collaboration possibilities for Claflin's undergraduate and graduate student researchers giving them access to data and technology not currently available at our institution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925596","CC* Compute: Accelerating Computational Research for Engineering and Science (ACRES) at Clarkson University, A Campus Cluster Proposal","OAC","Campus Cyberinfrastructure","07/01/2019","06/10/2019","Joshua Fiske","NY","Clarkson University","Standard Grant","Kevin Thompson","06/30/2021","$396,950.00","Brian Helenbrook","jfiske@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","8080","","$0.00","Clarkson University is building a computational cluster (ACRES: Accelerating Computation Research for Engineering and Science) to support data and computationally intensive projects aligned with Clarkson's four interdisciplinary research themes: Data Analytics, Healthy World Solutions, Advanced Materials Development, and Next Generation Healthcare. ACRES facilitates the conduct of high-impact, collaborative research that requires access to high-performance computing (HPC) resources, enables research currently not practical/feasible, and also supports student-learning opportunities through credit-bearing courses, undergraduate research, and an existing NSF REU site focusing on HPC. As a campus resource, ACRES is made available to any faculty member or student at the University according to queueing policies implemented to ensure fair-access. And, ACRES supports Clarkson's increased focus on computational research and a cluster hire of computationally active faculty. <br/><br/>The ACRES compute cluster replaces an existing, five-year-old high-performance compute cluster whose computational capacity provided 1.05M core-h/yr. Research need for computational capacity has grown to an identified total of 8.5M core-h/yr. ACRES is sized to meet current demands and modest near-term growth with unused computational capacity being shared via the Open Science Grid (OSG) to benefit the broader scientific community. This new computational resource provides 9.8M core-h/year through 1120 cores, high-speed Infiniband interconnect, four NVIDIA Tesla V100 GPUs, and 40 TB of scratch storage.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925267","CC* Compute: Improved Computing for Advanced Research and Education (ICARE)","OAC","Campus Cyberinfrastructure","07/01/2019","06/03/2019","Zachary Rossmiller","MT","University of Montana","Standard Grant","Kevin Thompson","06/30/2021","$394,895.00","Erin Landguth, Jeffrey Good, Travis Wheeler, Hilary Martens","zachary.rossmiller@mso.umt.edu","32 CAMPUS DRIVE","Missoula","MT","598120001","4062436670","CSE","8080","9150","$0.00","University researchers and educators require access to high performance computing, high bandwidth connections, sharable storage, and high-throughput data transfers. Unfortunately, institutional cyberinfrastructures can be silos of dissimilar and unconnected computing resources, with limited functionality, and inconsistent service delivery. The Improved Computing for Advanced Research and Education (ICARE) team addresses these issues and advances discovery and understanding by serving as a shared high-throughput computing site. Information Technology (IT) professionals and the scientific research community work together to coordinate campus-level cyberinfrastructure improvements based on project-driven needs. These include scientific research projects that improve natural disaster forecasting, produce innovative modeling tools used for maintaining biodiversity, impact the design of nuclear receptor drugs used to treat many diseases, develop deep learning approaches to DNA sequencing, and explore genomic responses to climate change.<br/><br/>The main feature of ICARE is the UM Shared Computing Cluster (UMSCC). UMSCC is configured with 8 computing nodes, 3 GPU nodes, and 2 storage nodes. Open source software solutions and participation in the Open Science Grid (OSG) facilitates data intensive research and extends computing resources to internal and external research groups, graduate and undergraduate researchers, and the nation. This effort strengthens collaboration between diverse groups of researchers and provides a vehicle for ongoing discussion and planning between IT professionals, educators, and researchers. A special aspect of this project is the direct relationship between IT professionals and the campus scientific research community, which is providing sound IT solutions that are critical for future competitiveness in grant funding and academic contribution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2033468","Preliminary Design Planning for the Leadership-Class Computing Facility","OAC","Leadership-Class Computing","10/01/2020","09/17/2020","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","12/31/2022","$3,500,000.00","Omar Ghattas, John West","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","","$0.00","Over the past decades, the role of computational and data science in advancing discovery has increased tremendously.  NSF has played a central role in this transformation of scientific research as a funder of pioneering research activities and as a leader in enabling the use of High-Performance Computing (HPC), with Computation-based methods now becoming central to research in diverse areas of Science and Engineering (S&E).  The NSF leadership-class computing facility (LCCF) program is intended to provide unique services and resources to enable discoveries in the largest and most computationally intensive S&E research frontiers that could not advance otherwise.  This award supports the design and planning activities associated with the Preliminary Design (PD) of LCCF after successful completion of Conceptual Design (CD). <br/><br/>The LCCF project proposes an advanced computing facility to enable discoveries in all fields of S&E requiring leadership computing capabilities, delivering a facility that will not only deal with the very largest challenges in S&E simulation, but also in Artificial Intelligence (AI), as well as the analysis of data from experimental and observational facilities.  To deliver this goal, the LCCF project will provide a large-scale computing platform, called Horizon, with a ten-fold increase in the time-to-solution capability of the NSF's current largest system (Frontera), as well as a complete environment for managing data throughout the research lifecycle. The LCCF will also provide a software environment, supporting services, and training to ensure that researchers can effectively use the facility for maximum impact to enhance the scientific discovery process.  Extensive plans for Education and Public Outreach are also planned which includes developing existing high impact formal learning programs for students, teachers, and parents, as well as construction for an informal learning environment that will be part of LCCF called the Computarium.  The PD process will continue many of the activities from CD, but with a focus on improving the project plans and estimates sufficiently to make important decisions moving forward.  Important PD goals are to make the final site selection for the facility, refine the science use cases into specific application targets with defined Figure of Merit performance increases, and choose the general direction of the Horizon system architecture.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107530","Collaborative Research: OAC CORE: Large-Scale Spatial Machine Learning for 3D Surface Topology in Hydrological Applications","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/01/2021","Zhe Jiang","AL","University of Alabama Tuscaloosa","Standard Grant","Alan Sussman","10/31/2021","$261,161.00","","zhe.jiang@ufl.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","090Y","075Z, 079Z, 7923, 9150","$0.00","Rapid advances in sensing technology and computer simulation have generated vast amounts of 3D surface data in various scientific domains, from high-resolution geographic terrains to electrostatic surfaces of proteins. Analyzing such emerging 3D surface big data provides scientists an opportunity to study problems that were not possible before, such as mapping detailed surface water flow and distribution for the entire continental US. Despite its vast transformative potential, machine learning tools to analyze large volumes of 3D surface data are not readily available. The project aims to fill this gap by designing a novel parallel spatial machine learning framework for 3D surface topology and implementing the system in a distributed computing environment. The system can produce high-quality observation-based flood inundation maps derived from satellite images. In collaboration with federal agencies (e.g., U.S. Geological Survey, NOAA), the project will enhance situational awareness for flood disaster response and improve flood forecasting capabilities of the NOAA National Water Model by filling in the gap of lacking observations in model calibration and validation. The proposed software tools will be open-source to enhance the research infrastructure for the broad geoscience communities. Educational activities include curriculum development, mentoring a group of high school students in data science seminars at K-12 Summer Camps, and year-long projects for selected high school students in regional Science Fair competitions. <br/><br/>The project will transform spatial machine learning research by enhancing terrain awareness through modeling large-scale 3D surface topology. Specifically, the project will bring about the following cyberinfrastructure innovations. First, the project will design a topography-aware spatial probabilistic model called hidden Markov contour forest, which advances existing machine learning tools by incorporating physical constraints of heterogeneous 3D terrains into zonal tree structures in the model representation. Second, the project will investigate a parallel inference framework by decomposing both intra-zone dependency and inter-zone dependency. Finally, the project will implement the proposed parallel learning framework in a distributed computing environment by addressing challenges related to task partitioning, load balancing, and dynamic task scheduling. The proposed system will be deployed for real-world rapid flood disaster response and the validation and calibration of the National Water Model through collaboration with the U.S. Geological Survey and NOAA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1928224","Category I. Computing without Boundaries: Cyberinfrastructure for the Long Tail of Science","OAC","CYBERINFRASTRUCTURE, Innovative HPC","10/01/2019","12/06/2021","Michael Norman","CA","University of California-San Diego","Cooperative Agreement","Robert Chadduck","09/30/2024","$24,499,999.00","Amitava Majumdar, Ilkay Altintas, Shawn Strande, Mahidhar Tatineni","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7231, 7619","7231","$0.00","Science and engineering rely upon an increasingly complex and integrated ecosystem of advanced computing and data systems, scientific software, and expertise to conduct research that leads to new knowledge and discovery, and improves the Nation's competitiveness and the health and welfare of its citizens. From the building blocks of life on earth to the deepest mysteries of the universe, researchers use this cyberinfrastructure (CI) to carry out computation and analysis at ever larger scales and complexity. Computing without Boundaries: Cyberinfrastructure for the Long Tail of Science is a transformational  project from the San Diego Supercomputer Center at the University of California, San Diego, designed to address these challenges. The centerpiece of the project is the acquisition, deployment, and operation of Expanse, a powerful supercomputer that will complement and extend NSF's Innovative High-Performance Computing (HPC) program. Expanse will: 1) increase the capacity and performance for thousands of users of batch-oriented and science gateway computing; and 2) provide new capabilities that will enable research increasingly dependent upon heterogeneous and distributed resources composed into integrated and highly usable CI. Expanse will feature innovations in system software, operations, and support that extend its capabilities far beyond the limits of the physical system. Through its integration with the public cloud and the Open Science Grid, collaboration with the Science Gateway Community Institute, and support for composable systems, Expanse will become part of a more inclusive national CI. The long tail of computing reflects diversity in science, researchers, their institutions, and those who support and operate CI. The project will reach out to underserved and under-resourced communities, through new initiatives like HPC@MSI, which will allocate a portion of Expanse to Minority Serving Institutes via a rapid access allocation to help them quickly use this powerful new resource.<br/> <br/>Projected to have a peak speed of 5 Petaflop/s, Expanse will feature next-generation Intel Central Processing Units (CPUs), NVIDIA Graphics Processing Units (GPUs), and a Mellanox InfiniBand network. It will be composed of 13 SDSC Scalable Compute Units (SSCUs), each of which contains 56 CPU nodes and 4 GPU nodes along with over 60 TB of distributed non-volatile memory storage for user scratch. The SSCUs will be integrated with a 12-PB Lustre parallel file system and 7 PB of object storage. SDSC, along with its partners Dell and Aeon Computing, will deploy Expanse in SDSC's energy-efficient data center on the UCSD campus. Expanse will be connected to multiple high-performance research and education networks at 100 Gbps and reach thousands of users who require high performance, but modest-scale resources. Allocation and usage policies will be tailored to achieve fast turnaround and responsiveness. Experts in computational science, data-intensive computing, scientific workflows, and large-scale systems operations will support Expanse at the highest levels of utilization, reliability, and usability. Through integration with national CI, Expanse will enable new models of computing and research that require the full complement of HPC systems and simulation, experimental data analysis, and computational expertise to support and facilitate breakthrough science. Knowledge gained through the project will lead to improvements in algorithms, software, and systems management tools, as well as a better understanding of how integrated CI can be configured to support emerging research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842042","Pilot Study for a Cyberinfrastructure Center of Excellence","OAC","CYBERINFRASTRUCTURE, MacroSysBIO & NEON-Enabled Sci","10/01/2018","05/19/2021","Ewa Deelman","CA","University of Southern California","Continuing Grant","Kevin Thompson","09/30/2021","$3,884,346.00","Valerio Pascucci, Anirban Mandal, Jaroslaw Nabrzyski, Robert Ricci","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7231, 7959","020Z, 062Z","$0.00","NSF's major multi-user research facilities (large facilities) are sophisticated research instruments and platforms - such as large telescopes, interferometers and distributed sensor arrays - that serve diverse scientific disciplines from astronomy and physics to geoscience and biological science. Large facilities are increasingly dependent on advanced cyberinfrastructure (CI) - computing, data and software systems, networking, and associated human capital - to enable broad delivery and analysis of facility-generated data. As a result of these cyber infrastructure tools, scientists and the public gain new insights into fundamental questions about the structure and history of the universe, the world we live in today, and how our plants and animals may change in the coming decades.  The goal of this pilot project is to develop a model for a Cyberinfrastructure Center of Excellence (CI CoE) that facilitates community building and sharing and applies knowledge of best practices and innovative solutions for facility CI.<br/><br/>The pilot project will explore how such a center would facilitate CI improvements for existing facilities and for the design of new facilities that exploit advanced CI architecture designs and leverage establish tools and solutions. The pilot project will also catalyze a key function of an eventual CI CoE  - to provide a forum for exchange of experience and knowledge among CI experts. The project will also gather best practices for large facilities, with the aim of enhancing individual facility CI efforts in the broader CI context.  The discussion forum and planning effort for a future CI CoE will also address training and workforce development by expanding the pool of skilled facility CI experts and forging career paths for CI professionals.  The result of this work will be a strategic plan for a CI CoE that will be evaluated and refined through community interactions: workshops and direct engagement with the facilities and the broader CI community.<br/> <br/>This project is being supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Emerging Frontiers in the Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835640","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","PHYSICAL OCEANOGRAPHY, Data Cyberinfrastructure, EarthCube","11/01/2018","08/23/2018","Thomas Haine","MD","Johns Hopkins University","Standard Grant","Amy Walton","10/31/2022","$1,850,538.00","Alexander Szalay, Gerard Lemson, Renske Gelderloos","Thomas.Haine@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1610, 7726, 8074","062Z, 077Z, 1324, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835618","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","Data Cyberinfrastructure","11/01/2018","08/23/2018","Christopher Hill","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","10/31/2022","$728,217.00","Paul O'Gorman","cnh@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7726","062Z, 077Z, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939459","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","08/06/2020","Jie Gao","NY","SUNY at Stony Brook","Continuing Grant","Sylvia Spengler","07/31/2021","$218,888.00","","jg1555@cs.rutgers.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","099Y, 1640","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031619","Heating and Particle Energization in Quasi-Perpendicular Shocks","OAC","Leadership-Class Computing","08/01/2020","05/04/2020","Jason TenBarge","NJ","Princeton University","Standard Grant","Edward Walker","07/31/2021","$9,558.00","","jason.tenbarge@gmail.com","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829804","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Karen Kemp","CA","University of Southern California","Standard Grant","Alan Sussman","07/31/2021","$44,090.00","","kakemp@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762039","Spokes: MEDIUM: MIDWEST: Collaborative: An Integrated Big Data Framework for Water Quality Issues in the Upper Mississippi River Basin","OAC","BD Spokes -Big Data Regional I, Hydrologic Sciences, EarthCube","08/01/2018","07/29/2018","Jong Lee","IL","University of Illinois at Urbana-Champaign","Standard Grant","Martin Halbert","07/31/2021","$300,000.00","Richard Warner","jonglee1@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","024Y, 1579, 8074","062Z, 1579, 8083","$0.00","This project will develop a cyberinfrastructure framework to facilitate research on the efficient management of agricultural practices and their impact on water resources in the Upper Mississippi River Basin (UMRB).  Large-scale data acquisition, integration, analysis, and visualization using data-enabled information technologies will accelerate the dissemination of knowledge, experience, and shared resources (e.g., technology, equipment, and people) among communities and partners.  The key element of the project is a new cyber platform, the Upper Mississippi Information System (UMIS), which will provide water quality data within a rich spatio-temporal hydrologic context.  The UMIS directly addresses three of the Grand Challenges for Engineering identified by the National Academy of Engineering: i) provide access to clean drinking water; ii) manage the nitrogen cycle; and iii) engineer the tools of scientific discovery.  The UMIS will immediately begin facilitating data access, integration, and scientific discovery for water quality challenges in the UMRB. <br/><br/>UMIS will offer internet-based open access to water quality information in its meteorological, hydrological, and geographical context, providing almost endless potential benefits for stakeholders.  For example, the experimental design of the UMIS will enable researchers to study spatial scaling, efficiency of various land use and agricultural practices to improve water quality, and the impact of climate change on land management and water quality.  Decision-makers, producers, and extension staff will be able to assess the relative efficacy of local (e.g., best management practices) versus system-level (e.g., state programs) solutions designed to reduce pollution, optimize the use of resources, and evaluate tradeoffs among competing objectives.  For all stakeholders, the UMIS will support partnerships and collaborations, increase dissemination of information about a critical natural resource to empower stakeholders at all levels, and set new standards in the communication of scientific data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1929279","Travel Support: Student Program for Practice and Experience in Advanced Research Computing Conference (PEARC19)","OAC","EDUCATION AND WORKFORCE","05/01/2019","05/07/2019","Thomas Furlani","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","04/30/2020","$19,994.00","John Towns, Jay Alameda, Semir Sarajlic, Alana Romanella","furlani@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7361","026Z, 7556, 9179","$0.00","Computing and in particular advanced computing plays a vital role in virtually every aspect of society today, including the U.S. economy, manufacturing, science and engineering. Access to and an understanding of advanced computational resources by researchers and practitioners from a wide variety of fields from the traditional sciences and engineering to medical research and the humanities/social sciences is no longer a luxury but rather a necessity if they hope to stay at the forefront of their respective field, advance knowledge, and improve the quality of life. In order for this to happen, it is critical that there exists a robust pipeline of experts with the requisite skills needed to leverage advanced computing resources. The Practice and Experience in Advanced Research Computing (PEARC) Conference series goal is to provide a forum for discussing current practices, challenges, and opportunities across the entire spectrum of research computing practitioners, including computational scientists, end-users, students and support staff, as well as industry and government agency representatives. Building on the success of past conferences, this project aims to provide travel support to students to attend the PEARC19 conference and participate in the Student Program. This project, thus, serves the national interest as stated by NSF's mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The PEARC conference (now in its 3rd year), follows the five-year NSF supported the Extreme Science and Engineering Discovery Environment (XSEDE) conference series. Students, including importantly, students from underrepresented groups and economically disadvantaged sectors are recruited to attend PEARC19. The proposed PEARC19 Student Program provides the students with practical hands-on training in modeling, machine learning, scientific computing, data analytics and many other tools in the modern researcher?s toolkit that are needed to facilitate today's multidisciplinary research programs. The aim is to expose students to as broad a range of topics and practices in advanced research computing as possible with the goal of helping to recruit and train the next generation of research and computational scientists. Students participating in the student program receive an opportunity to contribute to the technical program by submitting a student paper and/or poster, sharing their research efforts and gaining feedback, insights and inspiration from like-minded experts at the conference. In collaboration with XSEDE Campus Champions program, students have the option to participate in the Student Champions program that provides them resources and a community of experts whom they may consult, which enables an opportunity to begin building a community around PEARC19 Student Program efforts that extends well beyond the one week in July when the students attend the conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931560","Collaborative:Elements:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel","OAC","Software Institutes","11/01/2019","06/11/2020","Matthew Scotch","AZ","Arizona State University","Standard Grant","Robert Beverly","10/31/2022","$115,497.00","Anuj Mubayi","Matthew.Scotch@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8004","026Z, 075Z, 077Z, 7923, 8004, 9229, 9251","$0.00","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.<br/><br/>This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931363","Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research","OAC","Special Initiatives, Software Institutes","10/01/2019","03/23/2020","Shrideep Pallickara","CO","Colorado State University","Standard Grant","Seung-Jong Park","09/30/2024","$2,016,338.00","F. Jay Breidt, Sudipto Ghosh, Sangmi Pallickara, Mazdak Arabi","shrideep@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 9251","$0.00","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. <br/><br/>SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835414","Software Elements: NUPACK: Molecular Programming in the Cloud","OAC","SSA-Special Studies & Analysis, Software Institutes","11/01/2018","09/07/2018","Niles Pierce","CA","California Institute of Technology","Standard Grant","Robert Beverly","10/31/2022","$600,000.00","","niles@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1385, 8004","026Z, 077Z, 7923, 8004","$0.00","Life is orchestrated by programmable biomolecules (DNA, RNA, and proteins) that interact within complex molecular machines and biological circuits to grow, regulate, and repair organisms. These biological proofs-of-principle inspire diverse engineering efforts within the new fields of molecular programming, nucleic acid nanotechnology, and synthetic biology. Over the coming decades, these fields are poised to generate transformative programmable molecular and cellular technologies addressing challenges to science and society ranging from neuroscience and development, to diagnosis and treatment, and from renewable energy to sustainable manufacturing. To support these engineering efforts, the PI is engaged in a multi-decade effort to develop NUPACK (Nucleic Acid Package), a growing software suite for analyzing and designing nucleic acid structures, devices, and systems. Launched in 2007, NUPACK usage has grown to the point where the NUPACK compute resource is frequently overwhelmed by the research community. With the proposed work, the NUPACK web application will be re-architected from the ground up to run in the cloud, enabling the resource to scale dynamically in response to spikes in researcher demand and to growth year-over-year. The NUPACK user interface will be substantially expanded to allow users to harness next-generation analysis and design tools. Additionally, the re-architected web application will benefit from a complete re-write of the NUPACK scientific code base (moving from NUPACK 3.2 to 4.0) to achieve dramatic computational speed-ups and exploit enhanced physical models. With NUPACK in the cloud, users will be able to perform calculations far beyond current capabilities both in terms of scale and scientific scope, enabling exploration of a growing frontier of programmable molecular technologies.<br/><br/>NUPACK is a growing software suite for the analysis and design of nucleic acid structures, devices, and systems serving the needs of researchers in the emerging disciplines of molecular programming, nucleic acid nanotechnology, and synthetic biology. NUPACK algorithms are unique in treating complex and test tube ensembles containing arbitrary numbers of interacting strand species, providing crucial tools for capturing concentration effects essential to analyzing and designing the intermolecular interactions that are a hallmark of these new fields. Usage has increased to the point where the NUPACK compute cluster is frequently overwhelmed. With the proposed work, the NUPACK web application will be re-architected to enable deployment on the cloud, containerizing the dozens to thousands of jobs that are launched by a single click, and enabling the scale of the resource to vary dynamically minute-to-minute and year-over-year. To move to a sustainable model for NUPACK compute hardware and engineering support, NUPACK user accounts will be created that enable users to view and retrieve old jobs, to seamlessly pay for the cloud compute cycles that are used for their jobs, and to provide incremental support for the NUPACK Software Engineer proportional to their usage of this non-profit academic resource. The user interface will be substantially expanded to allow users to harness the new capabilities of the enhanced NUPACK backend, including kinetic analysis for complex and test tube ensembles, kinetic design for test tube ensembles, equilibrium design for large-scale pseudo-knotted structures in test tube ensembles, and use of new computationally parameterized physical models generated for custom experimental conditions. The re-architected web application will also benefit from a complete re-write of the NUPACK scientific code base, featuring improved implementations, reduced-complexity algorithms, overflow-safe evaluation algebras, and expanded physical models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839201","EAGER: Towards the Web of Biodiversity Knowledge: Understanding Data Connectedness to Improve Identifier Practices","OAC","NSF Public Access Initiative","10/01/2018","07/31/2018","Jose Fortes","FL","University of Florida","Standard Grant","Martin Halbert","09/30/2021","$299,973.00","","fortes@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7414","7916","$0.00","Biodiversity research investigates the variety and variability of life on Earth. This field of science crosses many research disciplines such as genetics, studies of organisms, plants and animals, habitats and ecosystems, and their interactions. A long-standing challenge for biodiversity researchers is to find, access, ""mine"", and integrate complex and diverse information from those disciplines. New approaches have now become possible with the increasing availability of ""big data"" techniques and infrastructure. This project will explore and employ such advanced techniques for retrieval and mining of a wide range of available open biodiversity data sources, with the aim of generating an improved holistic picture or ""knowledge graph"" of Earth's biodiversity. The project will also identify the data practices and discovered relationships that were needed to accomplish this graph-building task, with the aim of informing the development of future data systems and training on these techniques. <br/><br/>Many attempts have been made to link together biodiversity knowledge using linked identifiers coupled with data standards and taxonomies, but satisfactory results with such ""exact matching"" approaches have been elusive. This project aims to develop new methods of relating records across datasets that do not rely on matching identifiers but instead employ inferred rather than explicit relationships between data records. This is an experimental approach that has not yet been attempted at scale.  Linkages between publicly available biodiversity, genetic, literature, and other data will be explored; and software infrastructure will be developed to combine and link multiple biodiversity datasets. Another goal is to quantify the relationship between identifier practices and the ability to construct links between available biodiversity, genetic, literature, and other data. This project will draw on and complement other large ongoing collaborative efforts that contribute to broad integration of biodiversity knowledge, data science, and infrastructure such as the Encyclopedia of Life (EOL) and the NSF-supported iDigBio project. The ultimate aim is to understand which data practices provide the most value to the biodiversity community and thereby inform policy, standards, and training on identifiers. This, in turn, can enable the exploration of new fundamental and cross-disciplinary research questions, and potentially improve practices of a wide range of US and international data aggregators and data producers. <br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005572","Category I: Crossing the Divide Between Today's Practice and Tomorrow's Science","OAC","Innovative HPC","10/01/2020","08/18/2021","William Gropp","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Robert Chadduck","09/30/2026","$22,500,000.00","Gregory Bauer, Brett Bode, Timothy Boerner, Amy Schuele","wgropp@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7619","","$0.00","Advances in scientific applications and user interfaces have democratized the availability and accessibility of advanced research computing systems, allowing a broader range of researchers to pursue answers to ever more complex problems. Further, advances in computational and data-intensive research techniques are employed in a growing number of fields, both as primary methods of investigation and as complements to traditional modeling and simulation methods. Underpinning these advances is an accelerating shift in many research applications away from traditional processors to the vastly parallel floating-point processors available in modern computer graphics hardware. The National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign will deploy and operate Delta, an advanced computing and data resource that will capture the essence of change in technology and practice that NCSA, as a global leader in advanced research computing, pursues in its mission to advance human understanding.<br/><br/>Delta will advance the productivity and usability of advanced computing resources by deploying modern interfaces, such as science gateways and other web-based interfaces that will both improve usability and provide results to researchers in real-time. Federation across the national cyberinfrastructure ecosystem will enable integration as a component of multi-site workflows that include campus, national, and commercial cloud resources. NCSA will partner with the Science Gateways Community Institute (SGCI) to implement the science gateway infrastructure that will enable broad communities of researchers to take advantage of the advanced computing and data analytics capabilities of the system. Significant development will take place to evaluate and advance the accessibility of Delta by individuals with disabilities and to disseminate these advances throughout the broader research computing and data ecosystem. NCSA will take an active role in identifying applications and research teams that have the greatest potential for application acceleration through the adoption of GPU computing, thus greatly expanding the adoption of GPU-based computing as a growing norm and reducing time-to-solution for a broad range of research endeavors.<br/><br/>The Delta system will be a unique addition to the NSF-funded ecosystem of advanced computing resources due to its balanced mixture of a substantial GPU capability alongside more traditional CPU processing capabilities. Taking advantage of next-generation processor architectures and NVIDIA graphics processors, Delta will include what will be, at its launch, the most performant GPU computing resource in the National Science Foundation portfolio with its 800+ graphics processors.<br/><br/>The compute elements of Delta will employ an advanced network interconnect, delivering at least 100 gigabits per second of bandwidth to each compute node for application communications and access to an innovative storage environment. Data-intensive research will be accelerated by a nearly 10 petabyte high-performance storage subsystem, which incorporates three petabytes of flash-based, relaxed-POSIX file system which NCSA will use to demonstrate the effectiveness of modern file systems in advancing the performance of data-intensive and high-performance computing applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931511","Collaborative:Elements:RUI:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel","OAC","Software Institutes","11/01/2019","06/11/2020","Ashok Srinivasan","FL","University of West Florida","Standard Grant","Robert Beverly","10/31/2022","$366,000.00","Brian Eddy","asrinivasan@uwf.edu","11000 UNIVERSITY PKWY","PENSACOLA","FL","325145750","8504742825","CSE","8004","026Z, 075Z, 077Z, 7923, 8004, 9229, 9251","$0.00","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.<br/><br/>This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931335","Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research","OAC","Software Institutes","10/01/2019","08/17/2019","Amir AghaKouchak","CA","University of California-Irvine","Standard Grant","Seung-Jong Park","09/30/2024","$259,632.00","Kuolin Hsu, Phu Nguyen","amir.a@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. <br/><br/>SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940062","Collaborative Research: Converging Genomics, Phenomics, and Environments Using Interpretable Machine Learning Models","OAC","ICB: Infrastructure Capacity f, HDR-Harnessing the Data Revolu","10/01/2019","08/11/2020","Patrick Heidorn","AZ","University of Arizona","Continuing Grant","Peter McCartney","07/31/2022","$483,022.00","Tyson Swetnam","heidorn@u.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","085Y, 099Y","062Z, 1165","$0.00","Mitigating the effects of climate change on public health and conservation calls for a better understanding of the dynamic interplay between biological processes and environmental effects. The state-of-the-art, which has led to many important discoveries, utilizes numerical or statistical models for making predictions or performing in silico experimentation, but these techniques struggle to capture the nonlinear response of natural systems. Machine learning (ML) methods are better able to cope with nonlinearity and have been used successfully in biological applications, but several barriers still exist, including the opaque nature of the algorithm output and the absence of ML-ready data. This project seeks to significantly advance technologies in ML and create a new interdisciplinary field, computational ecogenomics. This will be accomplished by designing ML techniques for encoding heterogeneous genomic and environmental data and mapping them to multi-level phenotypic traits, reducing the amount of necessary training data, and then developing interactive visualizations to better interpret ML models and their outputs.  These advances will responsibly and transparently inform policy to maximize resources during this crucial window for planetary health, while revealing underlying biological mechanisms of response to stress and evolutionary pressure.<br/><br/>The long-term vision for this project is to develop predictive analytics for organismal response to environmental perturbations using innovative data science approaches and change the way scientists think about gene expression and the environment. The goal for this two-year award is to develop a proof-of-concept for an institute focused on predicting emergent properties of complex systems; an institute that would itself foster the development of many new sub-disciplines.  The core of this activity is developing a machine learning framework capable of predicting phenotypes based on multi-scale data about genes and environments.  Available data, ranging from simple vectors to complex images to sequences, will be ingested into this framework by applying proven semantic data integration tools and algorithmic data transformation methods.  The central hypothesis of this research is that deep learning algorithms and biological knowledge graphs will predict phenotypes more accurately across more taxa and more ecosystems than do current numerical and traditional statistical modeling methods.  The rationale for this project is that a timely investment in data science will push through a bottleneck in life science, accelerating discovery of gene-phenotype-environment relationships, and catalyzing a new computational discipline to uncover the complex ""rules of life.""<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934578","The Stanford Data Science Collaboratory","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Emmanuel Candes","CA","Stanford University","Continuing Grant","Sylvia Spengler","08/31/2022","$2,000,000.00","Fiorenza Micheli, Chiara Sabatti, Jurij Leskovec","candes@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","099Y","062Z","$0.00","Data-driven inquiry is key to all aspects of science and discovery, and data-based decisions are becoming integral to society. The challenges and the importance of meeting them are especially critical when the goal is to obtain relevant, valid, reproducible scientific insights. The Stanford Data Science Collaboratory will confront these challenges by creating a community of faculty, postdoctoral scholars, students, and research fellows that leverage data science methods and domain knowledge to tackle pressing problems.  In the Collaboratory, data scientists will work closely with scholars from other fields who rely on large, accurate, dependable datasets and data science techniques.  The Collaboratory will foster the work of researchers who study the ethical issues related to data collection and use, and will use data to solve societal and scientific problems. A hallmark will be thorough validation of data and a careful statistical calibration of the evidence to avoid misinterpretations that could have adverse consequences.  A second major goal of the Collaboratory is the growth of a citizenry literate in data science: universities have an obligation to ensure the next generation understands how to interpret and learn from data, and how to collect and manage it.<br/><br/>The Collaboratory identifies a set of five high-profile, high-impact projects that domain scientists deem important, and where they believe they are unable to make progress without a paradigm shift in the way they approach data sets. The first two concern a sustainable relation between humans and the environment, namely, (1) the problem of managing coral reefs in a changing climate and (2) reducing illegal fishing and forced labor in tuna supply chains. To make progress, the project will leverage new data sources: satellite remote sensing, ground monitoring stations based on soundscape, and genomic measurements that track biodiversity and evolution.  The other three projects are about fractures in society and steps towards a sustainable one: (3) How to understand the determinants of poverty in the U.S.; (4) How to detect and track political framing in digital media; and (5) How to develop data science tools that support equitable treatment between individuals.  Public data streams (e.g., social media apps, Wikipedia and Wikidata and moderate-resolution satellite imagery) as well as private-sector data (e.g., cell phone records, Facebook activity, internet search queries, drone imagery and fine-resolution satellite data) will inform understanding of the mechanisms causing poverty. To meet the research goals, the Collaboratory will incentivize faculty, students and postdocs to come together to find new data science solutions by supporting collaborative research teams and brainstorming working groups. The Collaboratory will also engage the undergraduate community by providing hands-on guided scientific research experience. To enlarge collaboration beyond Stanford, the Collaboratory will host outside visitors and invite scientists to campus for an annual symposium.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840191","CICI: SSC: TrOnto - A Community-Based Ontology for a Trustworthy and ResiliCent Scientific Cyberspace","OAC","Cybersecurity Innovation","09/01/2018","08/27/2018","Raul Aranovich","CA","University of California-Davis","Standard Grant","Robert Beverly","08/31/2022","$640,000.00","Matt Bishop, Premkumar Devanbu, Vladimir Filkov, Kenji Sagae","raranovich@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8027","","$0.00","Scientific research relies on an infrastructure of networked computers (cyberinfrastructure), which need to be secured against malicious threats and intrusions. For instance, the privacy of clinical trial subjects, the integrity of medical data, and the safety of tissue samples and cultures can be compromised if the cyberinfrastructure of a university hospital is breached. Information Security Officers (ISOs) are in charge of maintaining a secure cyberinfrastructure for research, but their task is complicated by the speed at which new threats emerge, and by the proliferation of software (often written in different languages) and obsolete or specialized hardware among labs and research centers. ISOs look for answers in their quest to fight intrusions and vulnerabilities in documentation and on-line discussion forums, but this is a time-consuming process. To streamline the process, an interdisciplinary team of researchers are developing a computer system to automatically and continuously harvest all that knowledge from online sources, organizing it into a network of concepts and relations that can be queried and reasoned upon to keep ISOs a step ahead of malicious attacks. <br/><br/>The project develops a number of interconnected modules: 1) A sub-system that identifies concepts and relations in software documentation, advisory bulletins, and on-line technical forums, and then retrieves that information using state-of-the-art natural language processing techniques; 2) An ontology for cybersecurity, which is a knowledge representation system that organizes the retrieved concepts and relations into a logical network, allowing for implicit knowledge to be extracted by means of automatic reasoning algorithms, and;  3) A querying interface, which allows ISO staff to access the knowledge represented in the ontology to find answers to their questions about cybersecurity. This innovative approach to cybersecurity extends the use of ontologies in the biomedical field, leveraging the metaphor of vulnerabilities in information systems as viruses or infections. Even though the initial stages in the creation of the ontology will involve curation by human experts, the researchers expect that the system can itself automatically thanks to the use of information retrieval techniques, therefore overcoming one of the known bottlenecks in the usefulness of ontologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931384","Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA)","OAC","Software Institutes","11/01/2019","07/19/2019","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","10/31/2024","$1,995,685.00","Thomas Herault","bosilca@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers.  Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research.  The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.<br/><br/>Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839223","RAISE-TAQS: Randomness Expansion Using a Loophole-Free Bell Test","OAC","OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","11/02/2018","Lynden Shalm","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","09/30/2022","$999,975.00","Paul Kwiat, Peter Bierhorst","lynden.shalm@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1253","026Z, 049Z, 057Z, 7203","$0.00","Randomness plays a central role in many cryptographic systems and protocols. If a random number generator gives outputs that aren't truly random, but rather can be predicted, this can lead to a catastrophic security breakdown. Currently, the only known way to generate true and certifiable random bits is through using entangled quantum particles in what is known as a loophole-free Bell test. However, it took over half a century to overcome the technological challenges before the first generation of loophole-free Bell tests could be carried out in 2015. Recently, the investigators of this project were able to demonstrate the creation of truly certifiable randomness from such a Bell test, but the process can only create around 1024 bits every 10 minutes, and the equipment must be separated by more than 100 meters. This project will develop a second-generation entanglement system that will produce random bits 3-4 orders of magnitude faster while requiring separations of less than 5 meters. This system will lead to new fundamental tests of physics and quantum mechanics, and lead to a more viable source of trusted randomness for use in public and private cryptographic applications. The first demonstration will be to integrate our entangled quantum random number generator into a public beacon of randomness that will help provide tamper-resistant video time-stamping capabilities for applications like police-body camera verification.<br/><br/>Performing a loophole-free Bell test is a technological milestone. It requires the successful simultaneous integration of numerous components that each must meet stringent technical requirements. In 2015 the investigators of this project were able to perform one of the three landmark loophole-free Bell tests and remain the only US-based group with such capabilities. This first generation of loophole-free Bell tests may have settled the debate on whether hidden-variables govern nature, but they also represent the beginning of a new class of quantum information systems and devices that are device-independent. This is fertile ground to begin probing the fundamental limits on the security of quantum networks, random number generators, and other new protocols. By building a second-generation loophole-free Bell test that can operate orders of magnitude faster, new regimes of exploration open that are inaccessible to the current crop of devices. A loophole-free Bell test requires measurements on entangled particles that are space-like separated. This means that the two particles are sent to distant measurement stations that are far enough apart so that information traveling at the speed of light about the measurement on one particle does not have time to reach and possibly influence the measurement of its entangled partner. Additionally, more than 66.7% of the particles must be detected to avoid opening a detection loophole. Currently, Pockels cells based on electro-optic effects are used in photonic Bell tests to make the necessary measurements as they can operate with losses of less than 1%. However, the Pockels cells only operate at speeds of approximately 100 kHz and require the measurement stations to be separated by more than 100 meters to maintain space-like separation. This large size makes the system hard to maintain and hinders it from being incorporated into practical applications requiring trusted randomness. By developing a new class of all-optical switches that can operate in the 1GHz range with ultra low-losses, the investigators will be able to shrink the experiment down to a 5m optical table. This will lead to an increase in randomness generation rates of 3-4 orders of magnitude and will enable other device-independent quantum protocols that are currently not technologically possible. Finally, the development of an ultrafast, ultra-low-loss polarization switch will impact all other photonic quantum information processing systems, allowing them to run with unprecedented efficiencies (compared to any waveguide device) and 100-1000X faster than current bulk Pockels cell methods permit. Such a device will be an enabling technology and serve as a fundamental building block in quantum optical networks and processors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826994","CC* NPEO: The Research and Science Engagement Center: A Production Platform for Operations, Applied Training, Monitoring, and R&E Support","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2018","09/18/2019","Jennifer Schopf","IN","Indiana University","Continuing Grant","Kevin Thompson","06/30/2022","$3,500,000.00","David Jent, Jason Zurawski","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7231, 8080","9102","$0.00","The scientific community has experienced an unprecedented shift in the way<br/>research is performed and how discoveries are made. Highly sophisticated experimental instruments are<br/>creating massive datasets for diverse scientific communities and hold the potential for new insights that<br/>will have long-lasting impacts on society. However, scientists cannot make effective use of this data if<br/>they are unable to move, store, and analyze it. This project establishes the Research and Science<br/>Engagement Center (ReSEC) as a collaborative focal point for operational expertise and analysis.<br/>This project will assist scientists in routinely, reliably, and robustly transferring their data.<br/>ReSEC will deliver end-to-end user support and network engineering solutions,<br/>and become a central community hub ready to provide personalized expertise and assistance on an ongoing basis.<br/><br/>ReSEC proposes four primary execution thrusts: 1) a Roadside Assistance center to reactively<br/>respond to immediate problems with science data transfers; 2) proactive network observation<br/>using tools such as perfSONAR and NetSage; 3) assistance with design & deployment of campus<br/>networking assets such as Science DMZs; and 4) training for campus network administrators.<br/>ReSEC will scale operations broadly by relying on Regional Network, Infrastructure and Science Community partners.<br/>ReSEC will deliver expertise and assistance on a sustainable, ongoing basis, with a particular emphasis<br/>on serving educational institutions with relatively limited local network administrative resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2135874","EAGER: Completing the Lifecycle: Developing Evidence Based Models of Research Data Sharing","OAC","NSF Public Access Initiative","08/01/2021","07/09/2021","Cynthia Vitale","DC","Association of Research Libraries","Standard Grant","Martin Halbert","07/31/2023","$297,019.00","","cvitale@arl.org","21 Dupont Circle, NW","Washington","DC","200361118","2022962296","CSE","7414","7916, 9102","$0.00","Effective data sharing practices are vital for the success of scientific research, especially in the emerging context of Open Science principles.  This project will conduct a detailed analysis of data sharing activities in several specific institutional and organizational contexts in order to better understand the patterns, practices, costs, barriers, and workflows that support public access to research data.  The project is being undertaken with coordination between several major associations which are key supporters of scientific research in the United States. Many academic institutions provide infrastructure to support researchers in data sharing activities. These services and infrastructures are often distributed across the institution, housed in various administrative units, such as campus IT, the university libraries, and the research office, among others. Given this distributed and often undocumented ad hoc nature, the true costs of public access to research data is not well understood. This project is designed to illuminate many of the currently unknown factors about the institutional landscape for public access to research data.  <br/><br/>The technical aspects of this analysis project are as follows.  An assessment of research data repository use patterns will be accomplished by analyzing metadata harvested by means of the DataCite application programming interface (API). A retrospective study of discipline-specific and format-specific research data practices will be accomplished utilizing a NIST-based institutional functional model for public access to research data applied to a series of institutions and sub-disciplines.  Finally, financial data will be collected and analyzed from the institutions using a combination of NAS and NIST cost models to consistently compare costs of data sharing activities and thereby produce a generalizable understanding of institutional expenditures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925476","CC* Integration: NetBASILISK: NETwork Border At Scale Integrating and Leveraging Individual Security Components","OAC","CISE Research Resources","10/01/2019","08/09/2019","Eric Boyd","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Deepankar Medhi","09/30/2022","$999,715.00","Shawn McKee, J. Alex Halderman","ericboyd@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","2890","9102","$0.00","The NetBASILISK (NETwork Border At Scale Integrating and Leveraging Individual Security Components) project enables researchers and network engineers at the University of Michigan to introduce the next level of security and privacy protection scaled to the vast volume of generated research data. As attackers develop more sophisticated tools to acquire student and faculty private data, institutional financial information, and proprietary, often classified, research information, it is imperative for information technology  staff to detect and stop these attacks. By observing patterns of network traffic, NetBASILISK will accomplish these goals with a minimal impact on the speed or volume of network traffic.<br/><br/>Current threat prevention systems do not scale to the growing volume of research data at an affordable cost. The University of Michigan solution, a novel framework combining open source and proprietary components, significantly improves system performance and accuracy in detecting and preventing threats to institutional data. NetBASILISK comprises powerful load balancing threat prevention mechanisms, data filtering tools, and threat detection technology. NetBASILISK will create a secure network perimeter while facilitating science such as cryo-electron microscopy, Large Hadron Collider particle research, and non-distorted Internet measurement, as well as enabling innovative network enhancements such as technologies to circumvent web censorship.<br/><br/>NetBASILISK will be used to inform the design of advanced network security devices for universities that scale to accommodate the network traffic requirements of data intensive science. Lessons learned and technology enhancements discovered by the project will be shared with the university networking community, as well as commercial partners. The science community will be informed of lessons learned from new design patterns employed for border security. The science drivers of the project, including advances in cryo-electron microscopy, networking, and physics will provide broad, impactful benefits. Project funding will also be used to support faculty, graduate students, and a postdoctoral researcher.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835439","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/06/2020","Richard Alo","MS","Jackson State University","Standard Grant","Robert Beverly","10/31/2023","$40,000.00","Natarajan Meghanathan","richard.alo@famu.edu","1400 J R LYNCH ST.","Jackson","MS","392170002","6019792008","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835725","Frameworks: Software NSCI-Open OnDemand 2.0: Advancing Accessibility and Scalability for Computational Science through Leveraged Software Cyberinfrastructure","OAC","Data Cyberinfrastructure","11/01/2018","09/12/2018","David Hudak","OH","Ohio State University","Standard Grant","Amy Walton","10/31/2023","$3,345,802.00","Thomas Furlani, Brad Chalker, Robert Settlage","dhudak@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7726","026Z, 062Z, 077Z, 7925","$0.00","Reducing barriers that limit the adoption of high performance computing (HPC) addresses an important problem that broadly affects the science, engineering, and humanities communities.  This effort builds on existing capabilities with large and varied user communities, and on national scale cyberinfrastructure and high-performance computing resources.  The approach has several benefits:<br/> - It increases the use of HPC resources among communities that are not well represented on HPC yet, but have growing needs for HPC.   <br/> - It is also beneficial to HPC providers, by supporting advanced features for monitoring and visualization of the states of systems.<br/> - The resulting framework will be used for training and workforce development, expanding the future ability to use advanced cyberinfrastructure for science.<br/>This project builds on the strengths of existing efforts, and has the potential to benefit a broad user community. <br/><br/>The project develops Open OnDemand 2.0, an open-source software that enables access to high-performance computing, cloud, and remote computing resources via the web, and lower the barriers to access HPC systems. The project combines two widely used HPC resources: <br/> - Open OnDemand 1.0  - an existing open-source, web-based project for accessing HPC services; and <br/> - Open XDMoD - an open-source tool that facilitates the management of high performance computing resources.<br/>Project activities include enhancing an existing web portal-to-HPC system (OnDemand), integrating XDMoD, extending the portal to provide other methods of access for other science domains, and improving the scaling of the system.  The software employs a unique per-user web server architecture. This gives a user full system-level access to an HPC cluster through a web browser.  Job performance visibility is provided by XDMoD, which enables users to make more efficient usage of HPC resources. Innovation and discovery will be integrated through a study which investigates ways to leverage the system-level access provided by Open OnDemand with science gateways.  The integrated platform will enhance resource utilization visibility, extend to more resource types and institutions, and support a smooth and easy utilization of HPC resources with intuitive web interfaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835612","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Sharon Glotzer","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Bogdan Mihaila","09/30/2022","$450,000.00","","sglotzer@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931408","Elements: C++ as a service - rapid software development and dynamic  interoperability with Python and beyond","OAC","Software Institutes","10/01/2019","09/04/2019","David Lange","NJ","Princeton University","Standard Grant","Robert Beverly","09/30/2022","$599,608.00","","david.lange@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8004","026Z, 077Z, 7923","$0.00","A key enabler of innovation and discovery for many scientific researchers is the ability to explore data and express ideas quickly as software prototypes. Tools and techniques that reduce the ""time to insight"" are essential to the productivity of researchers. At the same time, massive increases in data volumes and computational needs require a continual focus on maximizing code performance to realize the potential science from novel scientific apparatus. Programming language usability and interoperability are omni-disciplinary issues affecting today's scientific research community. As a result, a common approach across many scientific fields research is for scientists to program in Python, while steering kernels written in C++. This C++ as a service (CaaS) project brings a novel interpretative technology to science researchers through a state-of-the-art C++ execution environment. CaaS will enable both beginners and experts in C++. It enables higher-productivity in development and extends the interactive education and training platform for programming languages. CaaS will enable existing technologies as well as truly new development and analysis approaches. CaaS will directly support grow cyber-capabilities that advance scientific research across a broad range of pursuits.<br/><br/>Performance-focused languages, such as C++, are a critical infrastructure component for many scientific fields that have either large computing challenges or the need for low latency for results. The productivity of data scientists can be dramatically increased by an easy to use dynamic programming and development environment, together with a fully featured interoperability layer. The CaaS project provides a dynamic C++ execution environment and enables runtime language interoperability between C++ and other languages, such as Python, through a native-like, dynamic environment. CaaS provides seamless offloading of work in a heterogeneous computing environment, including hardware accelerators, which is more and more often required by today's researchers. These advances will enable researchers to more easily develop in, and use, large C++ codebases that are critical infrastructure components in many scientific fields. CaaS also allows true interoperability with C++ in Jupyter notebooks, and a robust prototyping environment for C++ developments. It encourages analysis and code sharing and facilitates scientific provenance tracking. By reducing the technical burden of development, researchers can focus instead on their scientific productivity. More broadly, notebook-based training in C++, or in a mixed programming environment that includes C++, is a key functionality. Enhancements in technical training will enable national advancements in science, technology, engineering, and mathematics capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2154495","Better Scientific Software Fellowship Program","OAC","EDUCATION AND WORKFORCE","03/01/2022","11/30/2021","Shelly Olsan","IA","Krell Institute","Standard Grant","Alan Sussman","08/31/2024","$379,185.00","","shelly@krellinst.org","1609 Golden Aspen Drive","Ames","IA","500106215","5159563696","CSE","7361","","$0.00","Addressing the scientific software challenges facing the nation requires broad collaboration to foster practices, processes, and tools to improve software developer productivity and software sustainability.  Both of those are key aspects of ensuring the integrity of computational results and increasing scientific productivity. The Better Scientific Software (BSSw) Fellowship Program, launched in 2018 with support from the U.S. Department of Energy (DOE), provides recognition and funding for leaders and advocates of high-quality scientific software.  This project enables NSF to collaborate in sponsorship of the BSSw Fellowship Program and participate in managing the program.  The project was piloted for one year with NSF support of one BSSw Fellow and one Honorable Mention. This award expands NSF's participation in the program to support three BSSw Fellows and three Honorable Mentions each year for two years, the same level of support provided by DOE. Each BSSw fellow will receive a stipend that can be used for various activities that promote improved scientific software, such as organizing a workshop or creating a tutorial.  All BSSw fellows and honorable mentions receive travel support to work with scientific community members and form a cohort.<br/><br/>NSF and DOE are leaders in advanced computing, pushing the growth of computational and data-enabled science and engineering as an essential driver of scientific and technological progress. Moreover, NSF and DOE researchers have developed a wide range of high-impact scientific software for advanced modeling, simulation, discovery, and analysis. The country faces a software crisis, however, due to disruptive changes in computer architectures and increasing complexity in next-generation computational science.  NSF partnership in sponsoring the BSSw Fellowship Program will enable a more robust approach toward pioneering the future of advanced computing ecosystems in support of American leadership in science and engineering. The BSSw Fellowship Program enhances workforce development and pathways to the NSF and DOE software communities, though nurturing a network of people who advance software practices as a fundamental aspect of increasing overall scientific productivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835372","Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK","OAC","Data Cyberinfrastructure","10/01/2018","07/26/2018","Jennifer Haase","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Amy Walton","09/30/2022","$355,077.00","","jhaase@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","7726","062Z, 077Z, 7923","$0.00","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California).  This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes.  Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. <br/> <br/>This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities.  The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements.  Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities.  The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the Magnitude 5.8 September 2016 Pawnee earthquake.  The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835371","Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK","OAC","Data Cyberinfrastructure, EPSCoR Co-Funding","10/01/2018","07/26/2018","Priyank Jaiswal","OK","Oklahoma State University","Standard Grant","Amy Walton","09/30/2022","$244,704.00","Mohamed Soliman","priyank.jaiswal@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","7726, 9150","062Z, 077Z, 7923, 9150","$0.00","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California).  This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes.  Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. <br/> <br/>This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities.  The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements.  Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities.  The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake.  The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934637","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Rebecca Willett","IL","University of Chicago","Continuing Grant","Amy Walton","08/31/2022","$352,620.00","","willett@g.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","099Y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835874","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Peter Cummings","TN","Vanderbilt University","Standard Grant","Bogdan Mihaila","09/30/2022","$1,089,470.00","Akos Ledeczi, Clare McCabe, Christopher Iacovella","peter.cummings@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 8009, 9216, 9263","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839011","EAGER: Collaborative Research: Synchronization Across Terrestrial and Aquatic Ecosystems","OAC","NSF Public Access Initiative","09/01/2018","08/13/2018","Grace Wilkinson","IA","Iowa State University","Standard Grant","Martin Halbert","07/31/2021","$136,115.00","","gwilkinson@wisc.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7414","7916","$0.00","Aquatic ecosystems are closely connected to their surrounding watersheds through the flux of water, nutrients, and organic carbon.  Similarly, the flux of exogenous carbon from the landscape causes lakes to be substantial sources of greenhouse gases as well as sinks for organic carbon buried in the sediment.  There are also important fluxes from inland waters to the terrestrial ecosystems. Despite the recognition of the importance of terrestrial-aquatic coupling, synchronization (persistent relatedness) of dynamics between these ecosystems has not been broadly investigated.  Employing data reuse techniques to use data from Lake Multiscaled Geospatial and Temporal Database (LAGOS) and Global Lake Ecological Observatory Network (GLEON) databases, the PIs will apply new analysis tools to answer questions related to the synchrony of lakes and their surrounding watersheds.<br/> <br/>To quantify synchronization between terrestrial and aquatic habitats wavelet coherence will be used to measure the strength of synchronization between terrestrial and lake ecosystems, as well as phase relationships, describing time lags. Time lags are hypothesized to reflect mechanisms of synchrony and may be related to lake size, water residence time, trophic status, and watershed area. These factors are hypothesized to affect the degree of aquatic-terrestrial synchrony. Random forest regression will be applied to test this idea, leveraging the range of lake and watershed properties. Multiple regression for wavelet transforms will be used to determine the fraction of aquatic-terrestrial synchrony that can be explained by climate drivers and assess synchrony between terrestrial ecosystems and lake nutrient fluctuations for improved inference into non-climate mechanisms of synchrony.<br/> <br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939279","Conference and National Summits to Develop a Guide to Accelerating Public Access to Research Data at Academic Institutions","OAC","NSF Public Access Initiative","10/01/2019","08/15/2019","Tobin Smith","DC","Association of American Universities","Standard Grant","Martin Halbert","03/31/2022","$191,486.00","Robert Samors, Kacy Redd, Gregory Madden","Toby_Smith@aau.edu","1200 New York Avenue","Washington","DC","200053920","2024087500","CSE","7414","","$0.00","The Association of American Universities (AAU) and the Association of Public and Land-grant Universities (APLU) propose to broaden and deepen awareness and understanding of public access issues across the academic research community.  The conference will capitalize on the experiences of ""early adopter"" institutions and leading stakeholder groups in the open science community; the organizers will develop and broadly disseminate a Guide to assist institutions conducting research in creating and implementing public access policies and practices. <br/><br/>The steps to achieving the objectives are through  1) developing a framework and strategic plan to guide the long-term engagement necessary to accelerate public access to research data; 2) convening a conference of the prior NSF-funded public access workshop participants to review the plan and prepare for discussions on shared solutions; 3) convening a second conference that brings together early adopters with less engaged institutions and stakeholders (e.g., funders, scientific and professional societies, publishers) to increase understanding of public access and develop content for a Guide to accelerating public access to research data at academic institutions; 4) creating working groups of institutions and stakeholders to explore unresolved challenges; 5) holding a follow-up event to incorporate the working group recommendations into a final Guide; and 6) disseminating the Guide to the higher education research community through a strategic education campaign to reach key leadership communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835764","Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems","OAC","Software Institutes","11/01/2018","02/04/2021","Xian-He Sun","IL","Illinois Institute of Technology","Standard Grant","Seung-Jong Park","10/31/2022","$2,866,000.00","Gerd Heber, Ann Johnson, Elena Pourmal","sun@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","8004","026Z, 077Z, 7925, 8004, 9251","$0.00","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use  effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need  to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that will significantly accelerate I/O performance. <br/><br/>This project  proposes to extend HDF technologies with the Hermes design. Hermes is new,  and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adapt prefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756006","Efficient Utilization of Flexible  Transmission for Renewable Energy Integration","OAC","CAREER: FACULTY EARLY CAR DEV","03/01/2018","02/12/2018","Mostafa Sahraei Ardakani","UT","University of Utah","Standard Grant","Alan Sussman","02/28/2021","$167,240.00","","mostafa.ardakani@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","1045","026Z, 8228","$0.00","Energy systems are transitioning from fossil fuels to renewable energy resources, such as solar and wind.  Despite their obvious environmental advantages, large-scale integration of renewable energy faces a number of technical difficulties.  These challenges are linked to the fact that the availability of renewable energy, i.e., wind and sunlight, depends on nature rather than the controllable process of burning a fuel.  This project develops innovative software tools that enable the deployment of flexible transmission, which is a cost-effective solution to address these challenges.  As a result of these developments, system operators will be able to benefit from a new resource that was not available to them before.  Particularly, this project aims to employ flexible transmission in order to alleviate the uncertainty and intermittency of renewable energy resources, and facilitate higher levels of renewable energy production.  With the help of efficient mathematical modeling and high-performance computing, the models developed in this project are fast and appropriate for real-time operation.  As renewable energy is economical and emission-free, this project will have a significant positive impact on the national health, prosperity, and welfare.  Additionally, this project will integrate computational methods and algorithm development in power engineering education to fill a much-needed gap in power engineering curriculum.<br/><br/>The objective of this project is to enable frequent utilization of flexible transmission for one of the largest and most complex cyber-physical system that exists today: the North American power grid.  Co-optimization of flexible transmission and generation dispatch is not possible yet due to the computational burden of the underlying mathematical problem.  Specifically, transmission flexibility, in the form of controllable impedance, introduces non-convexities to power system operation that are challenging to handle within the limited available computational time.  This project substantially reduces such computational burden through a novel and fast optimization technique, which exploits the mathematical structure of power flows.  Consequently, utilization of flexible transmission can become possible which can help reduce the operation cost and improve the system reliability.  This project also aims to mitigate the intermittencies and uncertainties associated with renewable generation by utilizing transmission flexibility. It employs stochastic optimization as the mathematical framework to model renewable energy uncertainties, and optimizes flexible transmission and controllable generation as the decision variables.  Algorithm decomposition and high-performance computing are employed to reduce the solution time required for stochastic optimization in order to ensure fast and efficient computation.  This is an essential component of the project as the computational time available for real-time operation is less than five minutes. The education component of this project enriches the power engineering curriculum, by developing educational modules, on computational methods, algorithm design, and high-performance computing for power engineering students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827116","CC* Integration: Service Analysis and Network Diagnosis (SAND)","OAC","Campus Cyberinfrastructure","08/15/2018","08/13/2018","Brian Bockelman","NE","University of Nebraska-Lincoln","Standard Grant","Kevin Thompson","07/31/2021","$499,673.00","Robert Gardner, Shawn McKee","bbockelman@morgridge.org","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","8080","9150","$0.00","Increasingly, science has become a ""team sport"" with projects less likely to be conducted by a single investigator or university group but with multi-institution collaborations spanning many universities and national laboratories.  As these collaborations have become data-intensive, highly-performant network links connecting their distributed science platforms have become increasingly important.  While true across all science, some collaborations, like those at the Large Hadron Collider (LHC) at CERN, transfer millions of gigabytes across global networks today and anticipate orders of magnitude increases within the next decade. Without performant networks and efficient data transfer and access services, the time to science will be greatly compromised.  In this context, high-speed, long-distance networks see their performance degrade surprisingly quickly in the face of modest error rates.  Accordingly, network engineers and researchers use sophisticated tools to monitor the network and transfer services.  Without a means to aggregate and correlate network tests, performance measurements, and application response, they at best can only reveal a small piece of the overall problem.  This project focuses on techniques that better combine, visualize, and analyze disparate network monitoring and service logging data, providing a comprehensive picture critical to the engineers and scientists relying on the network.  This will allow problems to be located and fixed more quickly, reducing the time to science. <br/><br/>The ""CC* Integration: Service Analysis and Network Diagnosis (SAND)"" project brings together an experienced team that has been working for more than a decade on wide area data transfers for large scale science.  The project develops a network monitoring archive and analytics platform, SAND-NMA, which integrates widely used data analytics tools (such as ElasticSearch, Kibana and JupyterLab) with infrastructure components (perfSONAR, HTCondor) and application sensors. Data from disparate sources are published to a messaging bus providing low-latency metrics describing the performance of research platforms on a global scale. Exploratory work is performed to provide engineers with pragmatic tools to identify and locate problems and perform analytics to understand the long-term evolution of network and higher level service performance.  Programming interfaces allow external cyberinfrastructure (such as workflow management systems) to incorporate the network monitoring feeds into their decision making engines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925704","CC* Networking Infrastructure: A High-Performance Science DMZ and Dedicated Research Network for Duquesne University","OAC","Campus Cyberinfrastructure","07/01/2019","06/03/2019","Sheryl Reinhard","PA","Duquesne University","Standard Grant","Kevin Thompson","06/30/2022","$462,680.00","Rehana Leak, Brad Maloney, Donald Moskiewski, Demerit McCarthy","reinhard@duq.edu","Room 310 Administration Building","Pittsburgh","PA","152193016","4123961537","CSE","8080","","$0.00","Duquesne University is implementing a dedicated research network and Science DMZ for the facilitation of science-driven research, education and collaboration. A vibrant and growing Duquesne research community conducts leading edge research and scientific experimentation in a broad range of science domains. The campus network is unable to support the data-intensive workflows generated by the research community. While science drivers and research workflows vary, there is a common need for fast and unrestricted data movement to internal and external research computing resources. The research-focused Science DMZ cyberinfrastructure addresses the limitations of the general purpose network by establishing a dedicated, scalable and secure network that is optimized for data-intensive science workflows. <br/> <br/>The project objective is to establish a dedicated 120Gbps Science DMZ network to support the Duquesne research community. A critical element of the Science DMZ is a Data Transfer Node (DTN). The DTN will provide University researchers with a secure, highly-available, high-performance data exchange point to facilitate the secure and efficient sharing of research data with collaborating institutions and national computational research facilities. The Science DMZ will connect to a dedicated 10Gbps uplink to the Pennsylvania-wide education and research computing network (KINBER).<br/> <br/>The new friction-free cyberinfrastructure not only accelerates large scale data movement, it facilitates new and more efficient workflows resulting in the acceleration and advancement of scientific discovery. The research network improves opportunities to train the next generation of research scientists and to inspire underrepresented groups to participate in scientific research activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018758","CC* Compute: A high-performance computing cluster to accelerate research, education, and training at Rhodes College","OAC","Campus Cyberinfrastructure","07/01/2020","11/26/2021","Brian Larkins","TN","Rhodes College","Standard Grant","Kevin Thompson","06/30/2022","$389,662.00","José Rodriguez","larkinsb@rhodes.edu","2000 North Parkway","Memphis","TN","381121624","9018433966","CSE","8080","","$0.00","Rhodes College is building and deploying a high-performance computing cluster that serves as the cornerstone of a campus-wide research computing center. This project establishes a needed computing resource that enables new capabilities for researchers and students in computer science, biology, mathematics, chemistry, and economics. The cluster provides a new computing resource at a scale not previously available on campus and advances the institutions' ability to support research, including analyzing mitochondrial DNA, modeling inorganic compounds, and improving the performance and usability of parallel and distributed systems. As a research-intensive liberal arts college, this cluster system at Rhodes College dramatically advances the opportunities for students to be engaged in computation-based scientific research alongside researchers in a wide range of disciplines. This cluster system also functions as a platform for classroom experiences that help students understand how computational methods can be used to better understand the world. The system supports class activities in computer science, biology, and mathematics. Additionally, this project enables researchers to prepare their experiments to run on national compute resources through XSEDE and will also contribute unused computing resources to the Open Science Grid, a national, distributed computing partnership.<br/><br/>The cluster system supports both high-performance and high-throughput workloads with an overall system capacity of 1,584 cores, 9TB of memory, and 504 TB of storage. The cluster is comprised of a login server, storage server, 44 compute nodes, and a 100Gbit/s InfiniBand interconnect network.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019161","CC* Team: Oregon Big Data Research and Education Team","OAC","Campus Cyberinfrastructure","07/01/2020","11/26/2021","Brent Kronmiller","OR","Oregon State University","Continuing Grant","Kevin Thompson","06/30/2023","$1,400,000.00","Jason Podrabsky, Jacob Searcy","brent.kronmiller@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","8080","","$0.00","Today, more than ever before, big data pervade every area of the life, environmental, biomedical, earth, marine, computational, physical, urban, and social sciences, as well as numerous other domains. Increasingly powerful computing technologies have opened the pathway for researchers to address major global challenges through use of large and heterogeneous data sets and through complex models and simulations. This project provides domain scientists, including research students, with the expertise and training needed to collaborate effectively with specialists in these advanced computational and statistical methodologies. The project also provides training and research experiences for students and instructors from small regional colleges, including Hispanic-serving and Native-American-serving institutions. To widely share best practices, it supports a community of practice. <br/><br/>The project employs research and training staff (facilitators) with expertise in data integration, multi-modal data analytics and machine learning. These three related sets of methods enable the analysis of large complex data sets of different types or from different sources, which may or may not have been collected as part of a planned studies. Specifically, a four-person facilitation team is established across Oregon State University, the University of Oregon, and Portland State University. The interdisciplinary, cross institutional team will establish the tools and managements practices to serve the researchers in the state.  The research facilitated by this project will lead to better understanding of earthquakes, diverse ecosystems, and plant and animal form and function. It supports development of faster computing systems, more secure energy systems, and improved environmental health. The data challenges posed by these application areas also motivate new foundational research in advanced data analytics and machine learning. The project also prepares a new generation of students from diverse backgrounds to enter the knowledge economy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925766","CC* Compute: A Hybrid Cloud Environment for the Rocky Mountain Advanced Computing Consortium","OAC","Campus Cyberinfrastructure","10/01/2019","11/26/2021","Shelley Knuth","CO","University of Colorado at Boulder","Standard Grant","Kevin Thompson","09/30/2022","$399,532.00","Shelley Knuth, Brian Johnson, Jonathon Anderson, Thea Lindquist, Jason Armbruster","shelley.knuth@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8080","","$0.00","The Research Computing group at the University of Colorado Boulder provides a hybrid cloud infrastructure to support computing, data, and science gateway needs that are currently not met by the existing high-performance and high-throughput computing infrastructure. The system is also integrated into the Open Science Grid (OSG) to enable full utilization of the deployed on-premise hardware using otherwise idle compute capacity. The hybrid cloud system provides one integrated system view of the on-premise and public cloud to researchers so they can select the right resource to support their work. This new capability eases the burden of finding appropriate computational tools for their work, allowing researchers to focus on new discoveries and the advancement of their fields. Major emphasis areas of the research supported are in the geosciences, hydrological modeling, natural language processing, machine learning, and earth analytics. Members of the Rocky Mountain Advanced Computing Consortium (RMACC) have access to 20% of the provided cloud resources with a focus on smaller institutions.<br/><br/>This hybrid cloud provides: virtual machines either from a library or customized by the researcher; execution and orchestration of containers; serverless computing; and hosting for persistent science gateways and other science services. This project heavily leverages the NSF award #1659425 ""CC* Cyber Team: Creating a Community of Regional Data and Workflow Cyberinfrastructure Facilitators"" by reusing training materials on containerization and capitalizing on relationships established during focus groups and one-on-one consults.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1910428","OAC Core: Small: Collaborative Research: Scalable distributed algorithms for tree structured astronomical data","OAC","OAC-Advanced Cyberinfrast Core, OFFICE OF MULTIDISCIPLINARY AC, ","10/01/2019","07/09/2019","Laxmikant Kale","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alan Sussman","09/30/2022","$350,000.00","","kale@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","090Y, 1253, 1798","026Z, 1206, 7569, 9179","$0.00","Spatial astronomical data is often extremely large and it is highly non-uniformly distributed. Algorithms that deal with such data have to be parallelized over large distributed memory supercomputers to deal with its size. The non-uniformity in the spatial distribution can be extreme, with some regions of space having million times more particles than other similar size regions. This creates significant challenges for scalable and efficient performance, as well as for the productive programming of such algorithms. Yet, the field of computational astronomy increasingly needs such scalable algorithms in the coming era. The raw computing capability unleashed by modern PetaFLOP/s  and ExaFLOP/s computers, respectively, executing up to quadrillions  and quintillions of calculations per second, is making it potentially feasible to get answers via simulations to some fundamental questions in the field, including those of galaxy formation and the properties of dark matter and dark energy. As the Large Synoptic Survey Telescope maps out the entire visible sky every few nights, it is expected to generate more than 10 terabytes per day, and this data needs to be analyzed in a timely fashion to fulfill its scientific goals of discovering hazardous asteroids, new minor planets, and exploding stars. This project provides new techniques and tools for researchers to use for high-performance simulations of non-uniform data. This enables previously untenable computer simulations to be done by astrophysicists, unlocking new insights and answering questions about the nature of the cosmos. The results are also used as case studies and educational material in classes taught by the investigators. Additionally, the project aim to involve women and undergraduate students in performing this research, continuing their experience of having done so in the past. This project thus aligns with the NSF's mission: to promote the progress of science and to advance the national health, prosperity and welfare.<br/> <br/>This project aims at developing novel parallel algorithms, data structures, and application demonstrations for computational problems involving data organized into hierarchical trees. A canonical example of such a domain is astronomical data, where particles representing clustered mass (stars or galaxies) are spread over the space of a simulation box or survey field in a highly non-uniform manner. Organizing them into trees, with multiple alternative tree organizations possible, including k-d trees, octrees, space-filling-curve based trees, etc., allows the efficient computation of various quantities such as gravitational forces, densities (and therefore hydrodynamics), two-point or three-point correlations, etc. The optimum choice of tree structure and algorithm depends both on the problem and the parameters of the parallel machine. The research methods used will include complexity analysis and, more significantly, empirical comparisons over a range of possible application scenarios including particle distributions and classes of traversals and algorithms. This will include formulation of algorithms and their implementations on parallel machines. The main outcomes of this project will be research papers describing effective algorithms and comparison and evaluation of particle decomposition techniques and tree types. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Astronomical Sciences in the Directorate for Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931450","Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems","OAC","Software Institutes","11/01/2019","07/23/2019","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Seung-Jong Park","10/31/2022","$350,612.00","Mahidhar Tatineni","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","026Z, 075Z, 077Z, 7925, 8004","$0.00","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)<br/><br/>The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018982","CC* Planning: UCCS Computational Cluster for Science","OAC","Campus Cyberinfrastructure","12/01/2020","05/19/2021","Yanyan Zhuang","CO","University of Colorado at Colorado Springs","Standard Grant","Kevin Thompson","11/30/2022","$53,935.00","Terrance Boult, Jessi Smith","yzhuang@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","8080","","$0.00","Classified as a ?high research activity? university in 2019, the University of Colorado Colorado Springs (UCCS) is a school in transition. While UCCS has good Cyber Infrastructure for educational purposes, there is an urgent need and opportunity to develop a plan for computational support for scientific research. Maximizing a computational infrastructure?s impact depends on the scale and scope of usage. A major part of this project is the analysis of the current and planned scientific computational needs at UCCS and then mapping them to design, which requires careful research to optimize. The investigators are using a multi-pronged approach to raise awareness, gather qualitative data, and collect custom data grounded within established motivational theory. This approach yields baseline data and assesses change over time with future infrastructural changes.<br/><br/>One aim of this planning project is the design and testing of a Cyber Aspirations and Readiness Assessment (CARA) instrument. The instrument serves as a theoretically driven tool for longitudinal assessment of initial aspirations, perceptions of barriers, and opportunities for success. This planning process helps STEM faculty across campus better understand how computing could accelerate their research, and how they could utilize different computational resources. Such a model, processes, and experiences not only benefit UCCS, but could be replicated at other institutions for their risk assessment, benchmarking, and other activities during their computation infrastructure planning. This plan for a cyber infrastructure provides a much needed capability required by a broad range of science, engineering, and education teams at UCCS and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2151463","CRII: OAC: Data Collection Infrastructure for Panoramic Video Monitoring in Wildlife Science","OAC","CRII CISE Research Initiation","10/01/2021","09/16/2021","Zhisheng Yan","VA","George Mason University","Standard Grant","Alan Sussman","05/31/2022","$171,838.00","","zyan4@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","026Y","026Z, 8228","$0.00","Wildlife monitoring has significant scientific and societal impacts. By utilizing remote cameras, biologists and ecologists can monitor and manage wildlife in order to prevent the transmission of zoonotic disease from animals and the invasion of wildlife on crops and livestock. However, current cyberinfrastructure (CI) in wildlife monitoring is limited to normal angle videos with a limited field of view and has caused missing the recording of important events that occurred outside of the direction being filmed. Moreover, existing remote cameras only allow the recording of short videos for a few minutes and thus cannot document many hours of wildlife activity in the monitoring zone. This project proposes methods for panoramic video monitoring that capture 360 degree uninterrupted videos to document complete wildlife activities. The project will allow wildlife scientists to access high fidelity monitoring data in both the spatial and temporal domains. Panoramic videos will not only capture comprehensive details on and near the monitoring site, but also depict the monitoring context of the data collection. The abundant research data and metadata embedded in panoramic videos will enhance the productivity of biologists and ecologists. If successful, the proposed wildlife monitoring CI will accelerate the adoption of panoramic data collection in other field research such as agriculture and archeology. The research outcomes, including the datasets generated and the software developed, will provide an interdisciplinary opportunity for undergraduate research, course curriculum development, and high school outreach activities, especially for underrepresented groups.<br/><br/>This project investigates a video collection cyberinfrastructure to enable panoramic wildlife monitoring. The design objective is to archive days to weeks of high resolution video data for long lived monitoring under the limited storage and energy constraints of remote cameras. To this end, this project proposes a framework for collaborative local and networked storage. First, we propose camera computing strategies to understand the scientific value of monitoring content and maximally compress the video with negligible overhead. This would mitigate the overall need for storage. Second, we propose a networked storage scheme to address the intermittent nature of the network in the wild, where only partial video is transported while the remaining video is generated in the receiver. We then schedule compressed video tiles for local storage or networked storage by orchestrating the storage, network and battery resources. Finally, we will develop and deploy the panoramic video monitoring in real wildlife research. We will validate the CI on the Savannah River site and assist wildlife scientists to study the impacts of animal interaction on disease transmission.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940322","Collaborative Research: Biology-guided neural networks for discovering phenotypic traits","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","10/15/2020","Henry Bart","LA","Tulane University","Continuing Grant","Peter McCartney","09/30/2022","$298,454.00","","hbartjr@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","099Y, 7231","1165, 7231, 9150","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931584","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","Special Initiatives, Software Institutes","01/01/2020","09/04/2019","Steven Decaluwe","CO","Colorado School of Mines","Standard Grant","Seung-Jong Park","12/31/2022","$970,128.00","Robert Kee, Gregory Jackson","decaluwe@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1642, 8004","026Z, 077Z, 7925, 8004","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924185","Cybertraining: Pilot: Collaborative Research: Cybertraining for Earth Surface Processes Modelers","OAC","CyberTraining - Training-based","10/01/2019","06/26/2019","Nicole Gasparini","LA","Tulane University","Standard Grant","Alan Sussman","09/30/2023","$21,485.00","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","044Y","9102, 9150","$0.00","Living sustainably on a rapidly changing planet is one of the greatest modern scientific and societal challenges. One critical aspect of global change involves the earth's surface itself: the rearrangement of its landforms, soils, and sediments by processes such as landslides, debris flows, floods, and coastal erosion. The Community Surface Dynamics Modeling System, CSDMS, creates cyberinfrastructure to enable advanced numerical models of the earth's surface, its changes through time, and the influence of human activity. However, traditional earth science education does not usually equip students with skills to become effective cyberinfrastructure users and cyberinfrastructure contributors. In order to develop innovative models for analyzing and predicting how the earth's surface responds to environmental change and human influence, the earth surface processes (ESP) modeling community needs a platform to teach modern programming practices and High Performance Computing methods. This project implements a 10-day Cyberinfrastructure in Earth Surface Processes Institute (ESPIn) for graduate students, postdoctoral fellows and early career faculty at the CSDMS Integration Facility at the University of Colorado in Boulder in the summers of 2020-2021 trains the next generation to be innovators. ESPIn aims to transcend the traditional model of department-based graduate education through interdisciplinary, problem-based, ""Just in Time Teaching"" of model use and development. Over forty participants, selected from diverse disciplinary backgrounds with explicit slots reserved for underrepresented minorities, gain direct experience in converting their research codes into open-source distributed software. ESPIn hosts developed lesson material in online open access educational repositories. ESPIn helps to train a new generation of computationally savvy, integrative scientists, while accomplishing major community science priorities. This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national prosperity and welfare by building a capable geoscience workforce.<br/><br/>The Earth Surface Processes Institute (ESPIn) is a 10-day immersive experience for graduate students, postdoctoral fellows and early career faculty, allowing them to make advances on critical earth surface processes research questions with state-of-the-art modeling tools. This project targets learners who would benefit from critical knowledge, skills, and tools to become better cyberinfrastructure users and developers through a careful, inclusive selection procedure. This project aims to help make scientific advances in the study of Earth Surface Processes (ESP) that leverage the powerful and advanced capabilities of new cybertools, such as the Python Modeling Tool. To these ends, the primary objective is to expand the use of cyberinfrastructure among members of the ESP research community with training that (1) increases their competence and confidence with using cyberinfrastructure tools, methods, and resources and (2) moves the larger ESP community towards more widely adopting tools to advance the fundamental science of predicting surface change.  Experienced scientists, visiting faculty, and software engineers assist with training and mentoring of the participants. ESPIn offers hands-on training in best programming practices, numerical methods, open source software development, advanced use of version control systems, writing unit tests, HPC-based sensitivity testing and model uncertainty quantification techniques. Several days are dedicated to working collaboratively on research and coding projects. Participants work on developing their own codes, with the intent of making codes more robust and compliant with existing ESP CI frameworks. The Summer Institute is quantitatively evaluated for learning efficacy and evaluations are used to iterate on lesson material quality. ESPIn provides all developed lesson material as online learning and teaching modules and broadly advertises these resources to the geoscience community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835735","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","11/01/2018","06/16/2020","Chiara Daraio","CA","California Institute of Technology","Standard Grant","Amy Walton","10/31/2023","$666,834.00","","Daraio@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1712, 7726","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835648","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","11/01/2018","05/12/2020","Deborah McGuinness","NY","Rensselaer Polytechnic Institute","Standard Grant","Amy Walton","10/31/2023","$1,459,023.00","Deborah McGuinness","dlm@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1712, 7726","054Z, 062Z, 077Z, 7925, 9251","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836650","S2I2: Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, CYBERINFRASTRUCTURE, COMPUTATIONAL PHYSICS, PHYSICS AT THE INFO FRONTIER, Software Institutes","09/01/2018","10/05/2021","G.J. Peter Elmer","NJ","Princeton University","Cooperative Agreement","Bogdan Mihaila","08/31/2023","$22,620,000.00","Gordon Watts, Brian Bockelman, Brian Bockelman","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1253, 7231, 7244, 7553, 8004","026Z, 062Z, 7569, 8211","$0.00","The quest to understand the fundamental building blocks of nature and their interactions is one of the oldest and most ambitious of human scientific endeavors. In Elementary Particles Physics, the most successful theory to date is known as the ""Standard Model"" of particle physics. Facilities such as CERN's Large Hadron Collider (LHC) represent a huge step forward in this quest as evidenced by the discovery of the Higgs boson. The next phase of this global scientific project will be the High-Luminosity LHC (HL-LHC), which will collect data starting circa 2026 and continue into the 2030's. The primary science goal at the HL-LHC is to search for physics beyond the Standard Model. In the HL-LHC era, the ATLAS and CMS experiments will record 10 times as much data from 100 times as many collisions as were used to discover the Higgs boson. As such, significant R&D advances must be achieved in the software for acquiring, managing, processing and analyzing HL-LHC data to realize the scientific potential of the upgraded accelerator and detectors and the planned physics program. In this context, the Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) will play a leading role to meet the software and computing challenges of the HL-LHC. <br/><br/>The Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) addresses key elements of the international ""Roadmap for HEP Software and Computing R&D for the 2020s"" and implements the ""Strategic Plan for a Scientific Software Innovation Institute (S2I2) for High Energy Physics"" submitted to the NSF in December 2017. IRIS-HEP will advance R&D in three high-impact areas: (1) development of innovative algorithms for data reconstruction and triggering; (2) development of highly performant analysis systems that reduce `time-to-insight' and maximize the HL-LHC physics potential; and (3) development of data organization, management and access (DOMA) systems for the community's upcoming Exabyte era. IRIS-HEP will sustain investments in today's distributed high-throughput computing (DHTC) and build an integration path to deliver its R&D activities into the distributed production infrastructure. As an intellectual hub, IRIS-HEP will lead efforts to (1) build convergence research between HEP and the Cyberinfrastructure, Data Science and Computer Science communities for novel approaches to address the compelling software and computing challenges of HL-LHC era HEP experiments, (2) engage broadly with researchers and students from U.S. Universities and labs emphasizing professional development and training, and (3) sustain HEP software and underlying knowledge related to the algorithms and their implementations over the two decades required. In addition to enabling the best possible HL-LHC science, IRIS-HEP will bring together the larger Cyberinfrastructure and HEP communities to address the complex issues at the intersection of Exascale high-throughput computing and Exabyte-scale datasets in ways broadly relevant to many research domains with emerging data-intensive needs. The education and training provided by the Institute in the form of summer schools and a fellows program will contribute to a highly qualified STEM workforce as most students and even most post-docs move into the private sector taking their skills with them. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"", one of the 10 Big Ideas for Future NSF Investments. <br/><br/>This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2051643","MRI: Development of a Co-Located Multi-User Immersive Display Instrument","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","01/01/2020","09/18/2020","Dirk Reiners","FL","The University of Central Florida Board of Trustees","Standard Grant","Alejandro Suarez","08/31/2022","$601,907.00","","dirk.reiners@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","1189, 7231","1189, 9150","$0.00","The goal of this project is the development of a new kind of display system that allows multiple people to experience a virtual world together. The world will be perceived in a spatially consistent manner, combining a shared real space in which people can work, communicate and collaborate, while each maintaining their own personal 3D view. Such virtual reality environment will enable researchers to discuss and solve problems collaboratively without compromising their individual spatial understanding of the problem. This instrument will be used as a test and development platform to better understand effective means to collaboratively use Virtual Reality from a fundamental as well as application-oriented point of view. It will provide new ways to collaborate for applications in areas as diverse as material development, chemistry, molecular biology, archaeology, network analytics and others. The project will engage researchers and students in development of a new visualization instrument.<br/><br/>The system will combine existing 3D stereo display components with a new optical design to provide multiple users with their individual 3D view. It takes advantage of high-resolution displays and a custom microlens design that will be developed as part of the project. This includes developing an appropriate simulation system to define and specify the optimal lens sheet design as well as accounting for limitations in manufacturability of microlenses. The goal is to provide a group of 4 or more users with individually separate 3D views of the shared space inside a room-size environment that surrounds them with 3D screens. In addition to the optical design the project will develop the software infrastructure to drive the multi-user display. This includes calculating the correct images so that each user sees the correct views for their current position and orientation as well as fundamental interaction methodologies and methods to enable users from different disciplines to quickly and easily use the system productively to attack their problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906829","OAC Core: Small: Collaborative Research: Scalable distributed algorithms for tree structured astronomical data","OAC","OAC-Advanced Cyberinfrast Core, OFFICE OF MULTIDISCIPLINARY AC, ","10/01/2019","07/09/2019","Thomas Quinn","WA","University of Washington","Standard Grant","Alan Sussman","09/30/2022","$145,817.00","","TRQ@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","090Y, 1253, 1798","026Z, 1206, 7569, 9179","$0.00","Spatial astronomical data is often extremely large and it is highly non-uniformly distributed. Algorithms that deal with such data have to be parallelized over large distributed memory supercomputers to deal with its size. The non-uniformity in the spatial distribution can be extreme, with some regions of space having million times more particles than other similar size regions. This creates significant challenges for scalable and efficient performance, as well as for the productive programming of such algorithms. Yet, the field of computational astronomy increasingly needs such scalable algorithms in the coming era. The raw computing capability unleashed by modern PetaFLOP/s  and ExaFLOP/s computers, respectively, executing up to quadrillions  and quintillions of calculations per second, is making it potentially feasible to get answers via simulations to some fundamental questions in the field, including those of galaxy formation and the properties of dark matter and dark energy. As the Large Synoptic Survey Telescope maps out the entire visible sky every few nights, it is expected to generate more than 10 terabytes per day, and this data needs to be analyzed in a timely fashion to fulfill its scientific goals of discovering hazardous asteroids, new minor planets, and exploding stars. This project provides new techniques and tools for researchers to use for high-performance simulations of non-uniform data. This enables previously untenable computer simulations to be done by astrophysicists, unlocking new insights and answering questions about the nature of the cosmos. The results are also used as case studies and educational material in classes taught by the investigators. Additionally, the project aim to involve women and undergraduate students in performing this research, continuing their experience of having done so in the past. This project thus aligns with the NSF's mission: to promote the progress of science and to advance the national health, prosperity and welfare.<br/> <br/>This project aims at developing novel parallel algorithms, data structures, and application demonstrations for computational problems involving data organized into hierarchical trees. A canonical example of such a domain is astronomical data, where particles representing clustered mass (stars or galaxies) are spread over the space of a simulation box or survey field in a highly non-uniform manner. Organizing them into trees, with multiple alternative tree organizations possible, including k-d trees, octrees, space-filling-curve based trees, etc., allows the efficient computation of various quantities such as gravitational forces, densities (and therefore hydrodynamics), two-point or three-point correlations, etc. The optimum choice of tree structure and algorithm depends both on the problem and the parameters of the parallel machine. The research methods used will include complexity analysis and, more significantly, empirical comparisons over a range of possible application scenarios including particle distributions and classes of traversals and algorithms. This will include formulation of algorithms and their implementations on parallel machines. The main outcomes of this project will be research papers describing effective algorithms and comparison and evaluation of particle decomposition techniques and tree types. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Astronomical Sciences in the Directorate for Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004236","Collaborative Research: CDS&E: A framework for solution of coupled partial differential equations on heterogeneous parallel systems","OAC","CDS&E-MSS, CDS&E","09/01/2020","10/15/2020","Hari Sundar","UT","University of Utah","Standard Grant","Tevfik Kosar","08/31/2023","$367,000.00","Ponnuswamy Sadayappan","hari@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8069, 8084","026Z, 8084, 9263","$0.00","This project is aimed toward the development of a transformative software framework to more easily calculate complex mathematical models for science and engineering research. There is currently significant burden on scientists who need to execute their mathematical models on different supercomputers; code developed for one supercomputer must often be re-developing or significantly adjusted to run on other supercomputers. Streamlining this development process will make modern hardware including supercomputers more accessible and impactful for scientific computations. Beyond the benefits to the computational science research agenda of this project, the new software infrastructure is expected to similarly help other domain scientists who develop similar types of mathematical models. All developed software will be publicly released with an open-source license.<br/><br/>Our goal is to develop a framework for code generation and efficient parallel solution of a large coupled set of partial differential equations (PDEs) with minimal user intervention. The Finite Volume Method will be used for discretization of the PDEs on unstructured meshes.  From user-specified PDEs and their boundary and initial conditions in symbolic form, and characteristics of the target hardware platform, the framework will perform efficient code generation and enable load-balanced parallel execution. The generated parallel code can either be compiled and used as is, or can be used as a starting point for the insertion of additional physical models and features by advanced users. The software will be demonstrated for two problem types: (1) computation of reacting flows, in which multiple space and time dependent PDEs are non-linearly coupled, and (2) solution of the Boltzmann Transport Equation (BTE) for phonons, in which the discretization of the 7-dimensional BTE results in thousands of space and time dependent coupled PDEs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940152","Collaborative Research: Autonomous Computing Materials","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Rigoberto Hernandez","MD","Johns Hopkins University","Continuing Grant","Daryl Hess","09/30/2021","$330,000.00","","rhx@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","099Y, 1978","062Z, 9263","$0.00","The recent explosion in worldwide data together with the end of Moore's Law and the near-term limits of silicon-based data storage being reached are driving an urgent need for alternative forms of computing and data storage/retrieval platforms. In particular, exabyte-scale datasets are increasingly being generated by the biological sciences and engineering disciplines including genomics, transcriptomics, proteomics, metabolomics, and high-resolution imaging, as well as disparate other scientific fields including climate science, ecology, astronomy, oceanography, sociology, and meteorology, amongst others. In this data revolution, the continuously increasing size of these datasets requires a concomitant increase in available computational power to store, process, and harness them, which is driving a need for revolutionary new, alternative substrates for, and forms of, computing and data storage. Unlike traditional data storage and computing materials such as silicon, the human brain offers a remarkable ability to sense, store, retrieve, and compute information in a manner that is unrivaled by any human-made material. In this research project, analogous modes of information sensing, data storage, retrieval, and computation will be explored in non-traditional computing molecular systems and materials. The over-arching goal of the research is to discover revolutionary new modes of data storage/retrieval, sensing, and computation that rival conventional silicon-based technology, for deployment to benefit society broadly across all domains of data science. Graduate students and postdocs across five institutions will be trained and mentored in a highly interdisciplinary manner to attain this goal and prepare the next-generation of data scientists, chemists, physicists, and engineers to harness the ongoing data revolution. The research will be disseminated to a broad community through news outlets and integration of high school student internships in participating research laboratories. <br/><br/>Large-scale datasets from spatial-temporal calcium imaging of the mouse brain will be recorded into DNA-based, nanoparticle-based, and phononic 2D and 3D soft and hard materials. Continuous spatial-temporal data will first be transformed into discrete data for mapping onto DNA-conjugated fluorophore networks, dynamic barcoded nanoparticle networks, and phononic 2D and 3D materials. Sensing, computation, and data storage/retrieval will be demonstrated as proofs-of-principle in exploiting the chemical properties of molecular networks and materials to recover the encoded neuronal datasets and their sensing and computing processes. Success with any of these three prototypical materials would revolutionize the ability to encode arbitrarily complex, large-scale datasets into complex molecular systems, with the potential to scale across diverse data domains and materials frameworks. The investigators' Autonomous Computing Materials framework will thereby enable the encoding of arbitrary ""big data"" sets into diverse materials for data storage, sensing, and computing. This project maximizes opportunities for disruptive new computing and data science concepts to emerge from a multi-disciplinary, collaborative team spanning data science, neuroscience, materials science, chemistry, physics, and biological engineering. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931280","Elements:Collaborative Proposal: A task-based code for multiphysics problems in astrophysics at exascale","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2019","07/26/2019","Saul Teukolsky","NY","Cornell University","Standard Grant","Seung-Jong Park","09/30/2022","$291,894.00","Lawrence Kidder","saul@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","107y, 1253, 8004","026Z, 077Z, 7569, 7923, 8004","$0.00","Upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales.  The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments.  The new code will be made available as open-source community cyberinfrastructure.  The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Early-career researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.<br/><br/>The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multiphysics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astrophysics. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state.  The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.<br/><br/>This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931537","Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems","OAC","Software Institutes","11/01/2019","07/23/2019","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Seung-Jong Park","10/31/2022","$1,360,534.00","Karen Tomko, Hari Subramoni, Samuel Khuvis","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8004","026Z, 075Z, 077Z, 7925, 8004","$0.00","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)<br/><br/>The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118272","Collaborative Research: CyberTraining: Pilot: A Cybertraining Program to Advance Knowledge and Equity in the Geosciences","OAC","CyberTraining - Training-based","10/01/2021","09/09/2021","Nicole Gasparini","LA","Tulane University","Standard Grant","Alan Sussman","09/30/2023","$173,323.00","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","044Y","7231, 7301, 9102, 9150","$0.00","This project will build and evaluate sustainable pathways for participation in cybertraining within the Earth science fields. The project will enhance inclusivity by focusing on underrepresented geoscience students for training in cyberinfrastructure (CI), including computer programming, data analysis, and modeling, which are not typically parts of the geoscience curriculum. The project will enhance programming education for geoscience students and increase access and mentoring relationships in STEM fields, thus building sustainable CI for geoscience programing. Consequently, this project will lower barriers to entry for STEM students while developing networks and mentoring relationships in the geosciences.  <br/><br/>This project will provide geoscience graduate students with skills training that promote knowledge proliferation in software development and science communication. Students will learn to automate workflows, test code, manage code repositories, visualize data, and communicate research effectively, which will accelerate their research foundation and provide future professional opportunities. Given the size of the Spanish-speaking STEM student population in the United States, the current effort will focus on bi-lingual cybertraining. This CI project will provide opportunities to enhance coding skills and communication abilities, while expanding students? professional network and establishing mentors at multiple career stages. Cybertraining will occur in a series of virtual and in-person terms, where students will learn Python coding and data analysis, and will be exposed to a Hackathon to provide practical application of these skills. Course materials will be open source, which will create a sustainable cybertraining program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940260","Collaborative Research: Machine Learning methods for multi-disciplinary multi-scales problems","OAC","HDR-Harnessing the Data Revolu","01/01/2020","10/15/2020","Michael Lawler","NY","SUNY at Binghamton","Continuing Grant","Eva Zanzerkia","12/31/2022","$234,681.00","","mlawler@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","099Y","062Z, 9263","$0.00","This project addresses two of the most pressing challenges in modern scientific research: (a) modeling natural phenomena across a broad range of space and time scales, and (b) the application of data science to discover physically meaningful relationships from large datasets. It will leverage knowledge from related and disparate disciplines, connecting them through data science. Four specific problems will be studied: cloud formation and evolution, movement of particles through random media, frustrated magnetic systems, and the reconstruction of urban topography. These benchmark problems have been selected as they capture different disciplinary aspects of multi-scale challenges. State-of-the-art methods in machine learning (including Artificial Neural Networks) will be used to develop new mathematical representation for small-scale processes. If successful, this project will substantially increase the capability of scientific computing to address a wide variety of important problems from the natural and social sciences, and will be disseminated widely through a pair of workshops, multiple campus visits across the 5-institution consortium, high impact peer-reviewed publications and presentations and the training of a cadre of more than a dozen post-docs and students.<br/><br/>This project will develop, implement and evaluate a new constrained optimization framework to discover and test physical phenomena at different resolutions and scales, including new machine learning algorithms aimed at discovering the stochastic differential equations underlying noisy data. This will be used to train physical parameterizations that account for the effects of small-scale processes in coarse resolution models. Core to this will be the design of a new framework to constrain artificial neural networks to deliver solutions that are interpretable and meaningful in the domain sciences and that can be directly associated with differential operators.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103815","Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities","OAC","XC-Crosscutting Activities Pro","10/01/2021","09/13/2021","Nicole Gasparini","LA","Tulane University","Standard Grant","Tevfik Kosar","09/30/2026","$86,286.00","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","7222","077Z, 102Z, 7925, 8004, 9102, 9150","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.<br/><br/>As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934700","Collaborative Research: Advancing Science with Accelerated Machine Learning","OAC","CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Philip Harris","MA","Massachusetts Institute of Technology","Continuing Grant","Amy Walton","08/31/2022","$610,000.00","Erotokritos Katsavounidis, Song Han","pcharris@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","099y, 7231","062Z, 7231","$0.00","In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.<br/><br/>In this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2041897","EAGER: Computer Progress and Economic Prosperity","OAC","CYBERINFRASTRUCTURE","01/01/2021","10/14/2020","Neil Thompson","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","03/31/2022","$300,000.00","","neil_t@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7231","7231, 7916","$0.00","Besides contributing to U.S. productivity, information technology (IT) improvements have been an important source of competitive advantage for U.S. firms.  This research project explores alternative sources of information technology improvements that could drive U.S. economic prosperity in the future, and identifies research and policy interventions that would be needed to generate the improvements.  The work combines techniques and content from two fields ? computer science and empirical economics.  It applies new expertise, and incorporates novel disciplinary and interdisciplinary perspectives.  Many of these analyses will also be the first of these types to be done, fulfilling the EAGER goal of being exploratory.  <br/><br/>The project identifies opportunities for computer productivity improvement as the contributions from Moore?s Law ends, focusing in three specific areas: hardware, algorithms, and software.<br/> - Recent roadmaps developed by the International Roadmap for Devices and Systems (IRDS) and the Heterogeneous Integration Roadmap (HIR) propose domain specialization as the alternative driving force for hardware improvement. However, specialization also fractures the semiconductor market, undermining the economics of chip production. The hardware component of this project will examine how much specialization would be economical.<br/> - The President?s Council of Advisors on Science and Technology has claimed, based on case studies, that algorithms are more important than hardware for computer improvements.  The PI has recently created a first-of-its-kind census of algorithm progress, and shows that such rapid improvements only happened in a limited number of areas. The algorithm component of this project would map how algorithms are being used, to understand how much algorithm progress is benefiting users, who is benefiting, and how much this is changing over time.<br/> - Machine learning (ML) has been producing substantial benefits. However, much of this improvement has come from deep learning, an area where economics could impact future gains.  Continued progress will require either better software performance engineering or new, more-efficient machine learning techniques. The software/ML component of this project will explore the potential for these approaches.<br/>Overall, this EAGER project will fill an important gap in understanding of the technical, economic, and policy implications of various approaches to computing and IT productivity improvement, and provide a better understanding of the connections between scientific advances, industrial policies, and economic progress.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018472","CC* Integration-Small: Integrating Application Agnostic Learning with FABRIC for Enabling Realistic High-Fidelity Traffic Generation and Modeling","OAC","CISE Research Resources","10/01/2020","07/01/2020","Deniz Gurkan","TX","University of Houston","Standard Grant","Deepankar Medhi","09/30/2022","$299,956.00","","dgurkan@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","2890","9102","$0.00","Novel approaches to networking and application development require high-fidelity testing and evaluation supported by realistic network usage scenarios. Furthering the pursuit of these novel approaches, the FABRIC testbed (https://whatisfabric.net) can store and process information ""in the network"" in ways not possible in the current Internet, which will lead to completely new networking protocols, architectures and applications that address pressing problems with performance, security and adaptability in the Internet. This project will provide researchers the means to easily utilize the new capabilities of the FABRIC testbed through a suite of new tools - smoothing the transition of existing experiments to the testbed and enabling exciting new areas of research.<br/><br/>This project will produce three systems facilitating end to end traffic modeling and generation in the FABRIC environment. A model repository will be created for the storage and access of custom models by experimenters, and will be seeded with stock models of some popular applications for immediate use. The use of the models within FABRIC-hosted experiments will be advanced through a bespoke matching system that will align experiment resources with model requirements. Finally, for experiments developing novel applications, a tool will be provided for creating new models using data captured with FABRIC infrastructure components. <br/><br/>FABRIC users will gain direct low-friction access to the novel infrastructure capabilities of the testbed, freeing them to focus the bulk of their time and effort on their own research goals rather than dealing with the vagaries of resource availability, specialized driver setup, and complex data formats.  As a result, testbed resources can be more optimally shared between experiments, and individual research tasks will be completed more quickly. The project will also provide input to future researchers and testbed implementors on streamlining workflows of high level services in support of research objectives over advanced testbed technologies.<br/><br/>Documentation for project tools and code, as well as backing project data, will be located at http://docs.uh-netlab.org, and it will be publicly available for at least 5 years after the end of substantive project work.  In-development source code is available on an ongoing basis via public internet resources linked from the documentation site.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1943948","Campus Cyber-infrastructure Planning Grant","OAC","Campus Cyberinfrastructure","10/01/2020","01/07/2021","Isaac Sanders","TX","Texas Asssociation of Developing Colleges","Standard Grant","Kevin Thompson","03/31/2022","$94,738.00","","i.warrensanders@txadc.org","1140 Empire Central Drive","Dallas","TX","752474039","2146302511","CSE","8080","","$0.00","The Texas Association of Developing Colleges (TADC) is undertaking a data network planning grant in preparation for establishing a regional highspeed data network to support science, technology and engineering (STEM) research and education. The network will connect TADC and its five Historically Black Colleges and Universities (HBCU) members: Jarvis Christian College, Paul Quinn College, Texas College, Huston-Tillotson University, and Wiley College. It will be used to develop a deeper learning educational environment by bridging the gap between theory and practical utilization of technology. Access to gigabit research and education campus networking gives the African American students attending the member schools the opportunity to study in a next generation learning environment to acquire 21st century skills needed to advance professionally.<br/><br/>Currently, these under-resourced schools are not able to keep up with the rapid changes in campus cyberinfrastructure. They are lacking highspeed broadband for connecting to regional, national and international education and research needed to advance the knowledge of their students. Their campus information technology (IT) infrastructure is not capable of providing the capacity needed to meet the demands of their faculty, students, and administrators. Establishing the TADC regional network would set up a state-of-the-art Science DMZ between the member schools that will provide direct access to STEM education and research opportunities. Moreover, the faculty?s ability to compete for research and other grant opportunities would be greatly enhanced. The creation of the TADC regional network starts with this Planning Grant to collect information from each member school. The information will serve as the roadmap to guide future planning and proposals to establish gigabit  networking to five underserved HBCUs and TADC.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931541","Elements: Data: Sustaining Modern Infrastructure For Political And Social Event Data","OAC","Methodology, Measuremt & Stats, Software Institutes","10/01/2019","09/06/2019","Patrick Brandt","TX","University of Texas at Dallas","Standard Grant","Robert Beverly","09/30/2022","$588,032.00","Latifur Khan, Jennifer Holmes, Vito D'Orazio","pbrandt@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1333, 8004","026Z, 062Z, 075Z, 077Z, 7923","$0.00","This project extracts quantitative summaries of political and international conflict events among national and non-state actors across the world by combing news reports across the internet in multiple languages.  This generates event data, a machine-coded description of someone doing something to someone else as extracted from news reports. The project focuses on political and social events about conflict and cooperation between governments, individuals, non-governmental organizations, rebel groups, and others.  The main goal of this project is to integrate and expand the end-to-end cyberinfrastructure for the robust creation, validation, access, and analysis of political event data by national security, government, academic, and non-governmental actors.  A major component of this proposal is to continue to grow the project's engagement with the global event data community. <br/><br/>This project extends, produces, and integrates a dynamic, robust system for event data to study sub-national and international conflict processes at a global scale, with applications to the needs of the national security and intelligence communities. Using natural language processing software tools to code event data by annotating the kinds of political events that are of interest to political scientists, international relations scholars, sociologists, and the national security community, the project analyzes contemporaneous news reports in English and Spanish, automatically encodes relevant political events for data analysts, and serves the data along with other open event data via the project websites.  The technical challenges include: (1) additional extensions of the multilingual framework to more types of events; (2) smoother updates to political actor dictionaries; (3) robust data querying and linking mechanisms, and analytic tools for the broader research and user community; (4) improved methods for focus location extraction across languages and resolutions. This will improve event data quality and event detection through increased, multi-language comparisons.  The multi-lingual extensions of the event encoding software and interface will produce novel methods for detecting and analyzing rare and local events. The proposed database integrations and query optimizations will streamline access to the many open access event datasets that exist, enabling researchers across diverse communities to analyze and compare conflict and political processes. The refinements of the geolocation modules will allow detection of locations from biased training samples, which is an important advancement since some political events, such as human rights violations, tend to occur in locations with low news coverage. The robust and innovative geolocation approaches can be carried over to other domain applications. Scaling the related software and data infrastructure aids the political science, national security and big data research communities. We also will provide robust data linkages across a diverse set of event data from multiple and multilingual news reporting services.  The sustainable cyberinfrastructure not only includes the event data coding from news reports, but also analysis tools that include an R package and a thin-client browser-based analysis interface to the data. This sustains the cyberinfrastructure and creates a workforce that is able to work in both science, engineering, national security, and intelligence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940287","Collaborative Research: Machine Learning methods for multi-disciplinary multi-scales problems","OAC","HDR-Harnessing the Data Revolu","01/01/2020","08/12/2020","Dallas Trinkle","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Eva Zanzerkia","12/31/2022","$331,136.00","","dtrinkle@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","099Y","062Z, 9263","$0.00","This project addresses two of the most pressing challenges in modern scientific research: (a) modeling natural phenomena across a broad range of space and time scales, and (b) the application of data science to discover physically meaningful relationships from large datasets. It will leverage knowledge from related and disparate disciplines, connecting them through data science. Four specific problems will be studied: cloud formation and evolution, movement of particles through random media, frustrated magnetic systems, and the reconstruction of urban topography. These benchmark problems have been selected as they capture different disciplinary aspects of multi-scale challenges. State-of-the-art methods in machine learning (including Artificial Neural Networks) will be used to develop new mathematical representation for small-scale processes. If successful, this project will substantially increase the capability of scientific computing to address a wide variety of important problems from the natural and social sciences, and will be disseminated widely through a pair of workshops, multiple campus visits across the 5-institution consortium, high impact peer-reviewed publications and presentations and the training of a cadre of more than a dozen post-docs and students.<br/><br/>This project will develop, implement and evaluate a new constrained optimization framework to discover and test physical phenomena at different resolutions and scales, including new machine learning algorithms aimed at discovering the stochastic differential equations underlying noisy data. This will be used to train physical parameterizations that account for the effects of small-scale processes in coarse resolution models. Core to this will be the design of a new framework to constrain artificial neural networks to deliver solutions that are interpretable and meaningful in the domain sciences and that can be directly associated with differential operators.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939916","Collaborative Research: Machine Learning methods for multi-disciplinary multi-scales problems","OAC","HDR-Harnessing the Data Revolu","01/01/2020","10/15/2020","Singdhansu Chatterjee","MN","University of Minnesota-Twin Cities","Continuing Grant","Eva Zanzerkia","12/31/2022","$295,964.00","","chatt019@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","099Y","062Z, 9263","$0.00","This project addresses two of the most pressing challenges in modern scientific research: (a) modeling natural phenomena across a broad range of space and time scales, and (b) the application of data science to discover physically meaningful relationships from large datasets. It will leverage knowledge from related and disparate disciplines, connecting them through data science. Four specific problems will be studied: cloud formation and evolution, movement of particles through random media, frustrated magnetic systems, and the reconstruction of urban topography. These benchmark problems have been selected as they capture different disciplinary aspects of multi-scale challenges. State-of-the-art methods in machine learning (including Artificial Neural Networks) will be used to develop new mathematical representation for small-scale processes. If successful, this project will substantially increase the capability of scientific computing to address a wide variety of important problems from the natural and social sciences, and will be disseminated widely through a pair of workshops, multiple campus visits across the 5-institution consortium, high impact peer-reviewed publications and presentations and the training of a cadre of more than a dozen post-docs and students.<br/><br/>This project will develop, implement and evaluate a new constrained optimization framework to discover and test physical phenomena at different resolutions and scales, including new machine learning algorithms aimed at discovering the stochastic differential equations underlying noisy data. This will be used to train physical parameterizations that account for the effects of small-scale processes in coarse resolution models. Core to this will be the design of a new framework to constrain artificial neural networks to deliver solutions that are interpretable and meaningful in the domain sciences and that can be directly associated with differential operators.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940272","Collaborative Research: Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Qiang Zhu","NV","University of Nevada Las Vegas","Continuing Grant","Pui Ho","09/30/2022","$325,000.00","","qiang.zhu@unlv.edu","4505 MARYLAND PARKWAY","Las Vegas","NV","891549900","7028951357","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835608","Elements: Software: Distributed Workflows for Cyberexperimentation","OAC","Software Institutes","11/01/2018","08/31/2018","Jelena Mirkovic","CA","University of Southern California","Standard Grant","Seung-Jong Park","10/31/2022","$598,798.00","Genevieve Bartlett","mirkovic@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Industry and academic research in computing systems, networking and security  suffers from the inability to demonstrate and validate research outcomes, because the process of research consists of complex but largely experiments on distributed, large-scale, networked test-beds.  This project will address this issue by enabling rigorous and repeatable experimentation on test-beds by enabling experiments to be defined as executable workflows, that can then be repeatably run, as well as managed. This project will develop and deploy a software framework called Elie. Elie consists of an experiment representation capability, along with several supporting services that make experimentation workflows robust, fault tolerant, easily sharable and reusable.  <br/><br/>Specifically, Elie consists of: (1) A new experiment representation, called DEW, which encodes experiment behavior and allocation constraints instead of just experiment topology. (2) Capture tools, which capture a researcher's manual actions on the testbed and facilitate script-building, and DEW generation (3) Testbed-integrated version control, (4) An experiment orchestration mechanism, (5) Lightweight failure detection, (6) A GUI, which allows for experiment management throughout an experiment lifecycle. Elie's technologies can be used by researchers as a full environment or as individual tools.  Collectively these technologies provide basic testbed support throughout an experiment lifecycle including the design, running, analysis, monitoring, storage and sharing stages.  All of Elie's technologies are extensible, and open-sourced, to allow the research community to contribute to their development and customize them to their needs.  This project fundamentally differs from the prior work in its goal to co-exist with current testbed experimentation approaches (manual and scripted experimentation) and to be applicable to a broad range of testbeds (via its translation and generation tools). Outcomes of this project will significantly improve ease of testbed experimentation, by offloading tedious, repetitive and detail-sensitive tasks to testbeds.  This will shorten experiment duration and improve quality and reliability of results.  This work will further make experiments more robust, enable easy sharing and reuse of experiments with minimal user effort, and facilitate repeatability and reproducibility. Overall, such advances in testbed experimentation will enable vertical development (building upon the work of others) in fields, which use testbeds, such as distributed systems, networking and cybersecurity. Sharing and reuse will also improve quality of scientific research in these fields, by enabling creation of larger and more complex experiments built on the shared artifacts of other researchers. The project team will also conduct extensive user outreach and building collaborations with other researchers that work on improving rigor of testbed experimentation, with the goal of building a strong user and developer community for Elie.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2030508","Partnership to Advance Throughput Computing (PATh)","OAC","CYBERINFRASTRUCTURE","10/01/2020","11/15/2021","Miron Livny","WI","University of Wisconsin-Madison","Cooperative Agreement","Kevin Thompson","09/30/2025","$15,499,999.00","Todd Tannenbaum, Frank Wuerthwein, Brian Bockelman, Lauren Michael","miron@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7231","","$0.00","The Partnership to Advance Throughput Computing (PATh) project will expand Distributed High Throughput Computing (dHTC) technologies and methodologies through innovation, translational effort, and large-scale adoption to advance the Science & Engineering goals of the broader community. The project will offer improved ways for scientists and administrators to manage computing workloads and resources ? transforming raw capacity into effective capacity with High Throughput Computing (HTC) principles that maximize the sustained use of distributed computing resources toward computational problems. PATh?s distributed HTC approach will strive to increase the national return on investment of compute resources by enabling institutions to share computing capacity, simultaneously maximizing utilization while also giving smaller campuses easier access to this vital capacity. The techniques advanced by PATh serve the national interest by promoting the progress of science, from academic research groups, to large collaborations, institutions, and industry, with training and outreach components designed and executed to empower the communities that can benefit from these services. PATh is aligned with the NSF National CI Coordination Services blueprint?s goal of producing an ?an agile, integrated, robust, trustworthy and sustainable CI ecosystem that drives new thinking and transformative discoveries in all areas of science and engineering (S&E) research and education.?<br/> <br/>PATh brings together two entities with a strong history of supporting dHTC-enabled research: the Center for High Throughput Computing (CHTC) and the Open Science Grid (OSG) Consortium. The team that founded CHTC at University of Wisconsin?Madison pioneered, in the mid 1990s, the concept and principles of HTC and has advanced and sustained HTCSS ever since. Roughly in parallel and driven by the needs of physics researchers, a diverse and multidisciplinary collaboration laid the foundation of the OSG in the late 1990s to develop, deploy, operate, and sustain a shared and global dHTC ecosystem of sites spanning geographic and administrative boundaries. PATh enables these two partners to align their efforts and leverage each other?s respective strengths, coordinated under the roof of one project.<br/> <br/>PATh will develop a Fabric of Capacity Services (FoCaS) to build vertical dHTC capabilities and drive the next phase of innovation in the HTCondor Software suite (HTCSS). The HTCSS offers a rich collection of tools for scheduling, federating compute and storage resources, and provisioning these resources to support high-throughput workloads at local, institutional, national, and international scales. In PATh, the HTCSS will receive new capabilities for incorporating external resources (particularly, allocations on HPC machines) into scientific workloads. In addition to the central role of the HTCSS in FoCaS production services, PATh?s core commitment will continue and fortify these services by integrating external technologies and ideas into the dHTC ecosystem. Building on the success of the OSG Connect service, and on campus-oriented Research Computing Facilitation methodologies pioneered by the CHTC at UW-Madison, PATh will create local dHTC expertise and capabilities at a growing number of campuses, including those under the NSF CC* program. Enhanced training, outreach, and community-building activities in PATh will further diversify the reach of dHTC impacts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940168","Collaborative Research: Autonomous Computing Materials","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Raghu Machiraju","OH","Ohio State University","Continuing Grant","Daryl Hess","09/30/2022","$356,566.00","","machiraju.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","099Y, 1978","062Z, 9263","$0.00","The recent explosion in worldwide data together with the end of Moore's Law and the near-term limits of silicon-based data storage being reached are driving an urgent need for alternative forms of computing and data storage/retrieval platforms. In particular, exabyte-scale datasets are increasingly being generated by the biological sciences and engineering disciplines including genomics, transcriptomics, proteomics, metabolomics, and high-resolution imaging, as well as disparate other scientific fields including climate science, ecology, astronomy, oceanography, sociology, and meteorology, amongst others. In this data revolution, the continuously increasing size of these datasets requires a concomitant increase in available computational power to store, process, and harness them, which is driving a need for revolutionary new, alternative substrates for, and forms of, computing and data storage. Unlike traditional data storage and computing materials such as silicon, the human brain offers a remarkable ability to sense, store, retrieve, and compute information in a manner that is unrivaled by any human-made material. In this research project, analogous modes of information sensing, data storage, retrieval, and computation will be explored in non-traditional computing molecular systems and materials. The over-arching goal of the research is to discover revolutionary new modes of data storage/retrieval, sensing, and computation that rival conventional silicon-based technology, for deployment to benefit society broadly across all domains of data science. Graduate students and postdocs across five institutions will be trained and mentored in a highly interdisciplinary manner to attain this goal and prepare the next-generation of data scientists, chemists, physicists, and engineers to harness the ongoing data revolution. The research will be disseminated to a broad community through news outlets and integration of high school student internships in participating research laboratories. <br/><br/>Large-scale datasets from spatial-temporal calcium imaging of the mouse brain will be recorded into DNA-based, nanoparticle-based, and phononic 2D and 3D soft and hard materials. Continuous spatial-temporal data will first be transformed into discrete data for mapping onto DNA-conjugated fluorophore networks, dynamic barcoded nanoparticle networks, and phononic 2D and 3D materials. Sensing, computation, and data storage/retrieval will be demonstrated as proofs-of-principle in exploiting the chemical properties of molecular networks and materials to recover the encoded neuronal datasets and their sensing and computing processes. Success with any of these three prototypical materials would revolutionize the ability to encode arbitrarily complex, large-scale datasets into complex molecular systems, with the potential to scale across diverse data domains and materials frameworks. The investigators' Autonomous Computing Materials framework will thereby enable the encoding of arbitrary ""big data"" sets into diverse materials for data storage, sensing, and computing. This project maximizes opportunities for disruptive new computing and data science concepts to emerge from a multi-disciplinary, collaborative team spanning data science, neuroscience, materials science, chemistry, physics, and biological engineering. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924057","Collaborative Proposal: CyberTraining: Pilot: Aligning Learning Materials with Curriculum Standards to Integrate Parallel and Distributed Computing Topics in Early CS Education","OAC","CyberTraining - Training-based","08/01/2019","06/10/2019","Erik Saule","NC","University of North Carolina at Charlotte","Standard Grant","Alan Sussman","07/31/2022","$249,469.00","Kalpathi Subramanian","esaule@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","044Y","","$0.00","Driven by recent trends in computing hardware and architectures, the project investigators are working with the larger community of computer science educators to introduce parallel computing concepts into early courses in computer science.  A smart repository of available learning materials (e.g., lectures, exercises, assignments, assessments) is being developed that enables computer science educators to easily find and adopt learning materials developed by other educators. Educators are being trained to work with the investigators and the repository system.  The project serves the national interest as stated by NSF's mission: to promote the progress of science, by incorporating current and critical concepts in parallel computing at an early stage in a way that is available for all computer science students.<br/><br/>The project takes a scalable and robust approach to building a large collection of materials from a diverse group of instructors and institutions. This pilot study is taking the needed initial steps to reaching this goal: obtaining a sound understanding of the current materials and curricula used in early computer science and parallel computing courses and enable their reuse.  To achieve this goal, the project develops a system to classify learning materials against two national standards: the 2013 ACM Computer Science Curriculum Guidelines and the 2012 NSF/IEEE-TCPP Parallel and Distributed Computing curriculum. The system aims to support advanced search features for educational materials, help identify similarity and differences between large sets of materials, and reveal potential gaps in existing materials. The system is accessible through a web interface to maximize adoption by CS educators looking to introduce parallel computing concepts, especially those in early CS courses. The project engages early users of the system coming from the Computer Science Education community and the Parallel Computing community through a series of workshops held at various computer science education community events and parallel computing community events.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835560","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Jeremy Palmer","TX","University of Houston","Standard Grant","Bogdan Mihaila","09/30/2022","$147,746.00","","jcpalmer@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835067","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Joern Ilja Siepmann","MN","University of Minnesota-Twin Cities","Standard Grant","Bogdan Mihaila","09/30/2022","$238,958.00","","siepmann@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017760","CyberTraining: Implementation: Small: Enabling Dark Matter Discovery through Collaborative Cybertraining","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","10/01/2020","10/20/2020","Amy Roberts","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Alan Sussman","09/30/2023","$164,716.00","","amy.roberts@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","044Y, 7244","7569","$0.00","Detecting dark matter in the lab would be transformational for physics, and such a difficult measurement requires providing a foundation for early-career scientists in advanced data analytics.   The science question being pursued is generally acknowledged to be one of the most important questions in particle physics and astrophysics and is key to understanding what makes up the vast majority of the universe.  Effective training in good computing practices is required for major research advances in this field.  The project will consolidate and strengthen training efforts in scientific software development and data analysis within the field of experimental dark matter research.  Scientifically, the training will enable discovery that will come from a world-wide effort consisting of hundreds of junior scientists  searching for  extremely-rare events on petabytes of data - effectively looking for a needle in a haystack the size of Texas.  The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure, and will support STEM disciplines with critical software training that is much needed both in scientific fields and in industry.<br/><br/>The dark matter community consists of more than a thousand scientists at the frontier of ultra-rare event searches whose efforts support more than twenty different experiments.  Searching for dark matter in multiple ways has resulted in disparate and often inadequate computational training. This project addresses the training problem to maximize impact across the field.  Representing three leading dark matter experiments, the project investigators will develop educational material and training workshops for systematic data science education to ensure early career scientists can harness the data volumes being produced by modern experiments.  The project will host two training workshops per year, toward the goal of developing a community of instructors and also a set of training materials for free distribution and reuse. Beyond domain-specific training in rare-event searches, foundational computational knowledge will be developed when necessary by working with partners such as the Software and Data Carpentries.  The project includes specific goals to engage women and underrepresented minorities in the training activities and broaden their advancement within the field.  Additionally, the project will provide mentors for advanced students through hackathons. These trainings will directly contribute to broader STEM workforce development while training students such that they can pursue careers in data science and/or data-intensive research.  This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1948467","CRII: OAC: Data Collection Infrastructure for Panoramic Video Monitoring in Wildlife Science","OAC","CRII CISE Research Initiation","06/15/2020","04/24/2020","Zhisheng Yan","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Alan Sussman","10/31/2021","$175,000.00","","zyan4@gmu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","026Y","026Z, 8228","$0.00","Wildlife monitoring has significant scientific and societal impacts. By utilizing remote cameras, biologists and ecologists can monitor and manage wildlife in order to prevent the transmission of zoonotic disease from animals and the invasion of wildlife on crops and livestock. However, current cyberinfrastructure (CI) in wildlife monitoring is limited to normal angle videos with a limited field of view and has caused missing the recording of important events that occurred outside of the direction being filmed. Moreover, existing remote cameras only allow the recording of short videos for a few minutes and thus cannot document many hours of wildlife activity in the monitoring zone. This project proposes methods for panoramic video monitoring that capture 360 degree uninterrupted videos to document complete wildlife activities. The project will allow wildlife scientists to access high fidelity monitoring data in both the spatial and temporal domains. Panoramic videos will not only capture comprehensive details on and near the monitoring site, but also depict the monitoring context of the data collection. The abundant research data and metadata embedded in panoramic videos will enhance the productivity of biologists and ecologists. If successful, the proposed wildlife monitoring CI will accelerate the adoption of panoramic data collection in other field research such as agriculture and archeology. The research outcomes, including the datasets generated and the software developed, will provide an interdisciplinary opportunity for undergraduate research, course curriculum development, and high school outreach activities, especially for underrepresented groups.<br/><br/>This project investigates a video collection cyberinfrastructure to enable panoramic wildlife monitoring. The design objective is to archive days to weeks of high resolution video data for long lived monitoring under the limited storage and energy constraints of remote cameras. To this end, this project proposes a framework for collaborative local and networked storage. First, we propose camera computing strategies to understand the scientific value of monitoring content and maximally compress the video with negligible overhead. This would mitigate the overall need for storage. Second, we propose a networked storage scheme to address the intermittent nature of the network in the wild, where only partial video is transported while the remaining video is generated in the receiver. We then schedule compressed video tiles for local storage or networked storage by orchestrating the storage, network and battery resources. Finally, we will develop and deploy the panoramic video monitoring in real wildlife research. We will validate the CI on the Savannah River site and assist wildlife scientists to study the impacts of animal interaction on disease transmission.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835782","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","11/01/2018","08/23/2018","Wei Chen","IL","Northwestern University","Standard Grant","Amy Walton","10/31/2023","$599,778.00","","weichen@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","1712, 7726","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835677","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","11/01/2018","05/07/2021","L. Brinson","NC","Duke University","Standard Grant","Amy Walton","10/31/2023","$2,606,810.00","Cynthia Rudin","cate.brinson@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1712, 7726, 8004","026Z, 054Z, 062Z, 077Z, 7925, 9102, 9251","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2054506","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","XC-Crosscutting Activities Pro, Software Institutes","09/01/2020","10/29/2020","Reed Maxwell","NJ","Princeton University","Standard Grant","Seung-Jong Park","09/30/2022","$593,517.00","","reedmaxwell@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7222, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2025364","Accelerating Open and FAIR Data Practices Across the Earth, Space, and Environmental Sciences: A Pilot with the NSF to Support Public Access to Research Data","OAC","NSF Public Access Initiative, EarthCube","10/01/2020","10/13/2020","Royce Hanson","DC","American Geophysical Union","Standard Grant","Martin Halbert","09/30/2022","$623,694.00","","bhanson@agu.org","2000 FLORIDA AVE NW","Washington","DC","200099123","2024626900","CSE","7414, 8074","","$0.00","This project implements FAIR data practices across the Earth, space, and environmental sciences. By the end of the project, data citations for data funded by NSF EAR grants are captured in the NSF Public Access Repository (NSF PAR) and knowledge of leading practices and workflows around data citation are well known across the AGU community.  This project, funded as a pilot activity for the NSF PAR system, will have the outcome of strengthening the ecosystem for data in support of publications. <br/> <br/>AGU will work with its publishing partner Wiley to ensure that data citations are tracked and exposed in articles.  CHORUS will develop technical connections with Scholix to obtain data citation information to augment its publication information that it makes available to NSF.   AGU will join Dryad as a member and use the Dryad repository as a location for data deposit when a discipline specific repository is unavailable.  Outreach activities targeting awareness and adoption will be organized through AGU and ESIP?s Data FAIR workshops at scientific meetings, journal society meetings, and repository community meetings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031172","EAGER Research Organization Registry: An Open Science Approach to Linking the Products of Research through Open Affiliations","OAC","NSF Public Access Initiative","09/01/2020","08/05/2020","John Chodacki","CA","University of California, Office of the President, Oakland","Standard Grant","Martin Halbert","08/31/2022","$199,267.00","","john.chodacki@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","7414","7916","$0.00","Persistent identifiers (PIDs) are increasingly central to the research landscape. In light of emerging open science guidance such as by funding agencies, PIDs for institution affiliations is a relatively new development, following PIDs for research outputs (DOIs) and for researchers (ORCID).  Further research and analysis is needed to better understand and explain where open affiliation identifiers should be adopted, how they can connect to other PIDs for research products, and how these linkages can be leveraged to both achieve and enable open research practices across disciplines.<br/> <br/>California Digital Library (CDL) will work with partners in the Research Organization Registry (ROR) project to drive wide adoption of open affiliation identifiers in research infrastructure. CDL will research the current status of affiliation identifiers in existing infrastructure, explore the measurable benefits of adoption of these identifiers with a specific focus on how they enable open science, and develop tools and guidance to support adoption by specific stakeholder groups. This project will result in new tools and guidance for how open affiliation identifiers should be implemented and used to contribute to a more connected set of research products.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019164","CC* Integration-Large: Robust and Predictable Network Infrastructure for Wide-Area Hybrid Sensor Networks","OAC","CISE Research Resources","10/01/2020","06/30/2020","Engin Arslan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Deepankar Medhi","09/30/2022","$998,568.00","Shamik Sengupta, Scotty Strachan","earslan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","2890","9150","$0.00","Increased use of Internet-of-Things (IoT) sensor devices is revolutionizing science and engineering applications, such as smart cities and environmental hazard monitoring. These sensors are often deployed in remote and distributed environments and rely on complex data networks that use both wired and wireless communication to stream large volumes of data back for analysis and distribution. Design and management of these complex hybrid networks is a daunting task due to network capacity fluctuations and dynamic data flow characteristics. This project develops a new software-driven network infrastructure to help automate network management of these emerging hybrid sensor networks for science and public service.<br/><br/>This project develops and deploys an operational Software-Defined Networking (SDN) network management and monitoring infrastructure for hybrid wide-area research networks spanning hundreds of kilometers in Nevada for distributed applications in wildfire, climate, and traffic safety. Current practices of inflexible network setup with limited monitoring capability struggles to satisfy ever-increasing science needs, such as on-demand data pipeline creation and quality-of-service satisfaction. Moreover, the project enhances network transparency through deployment of high-precision (i.e., port-, flow-, and packet-level) network monitoring and performance measurement (i.e., PerfSonar) nodes. The project implements a deep-learning-based anomaly detection mechanism to protect sensitive data from cyber attacks. <br/><br/>Integrating SDN with high-precision monitoring into wide-area sensor networks has the potential to accelerate adoption of IoT devices in many science areas by addressing core hybrid WAN (wide-area network) challenges such as routing, troubleshooting, and anomaly detection. Developing these integrations now is critical, because hybrid WAN infrastructures (particularly in non-urban regions) will remain bandwidth-limited relative to data generation devices into the foreseeable future. This project allows University of Nevada, Reno (UNR) to continue leadership in wide-area research IoT systems, expand institutional platforms for hybrid-cloud operations, and scale up key products for communities as part of UNR's land-grant mission.<br/><br/>Any data produced in the context of this project will be made available to the public and maintained throughout the duration of the project and beyond. Developed source code will initially be maintained in a private GitHub repository, which will be released at ?https://github.com/UNR-HPN/SDNWideArea? periodically when the codebase becomes stable. The repository will be maintained as part of ongoing support operations by UNR cyberinfrastructure personnel assigned to the infrastructure at the close of the project. Performance monitoring in the project will be associated with regional dashboards as best practices dictate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835292","Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework","OAC","Software Institutes","11/01/2018","04/24/2020","Yuanfang Cai","PA","Drexel University","Standard Grant","Seung-Jong Park","10/31/2022","$315,679.00","","yfcai@cs.drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158956342","CSE","8004","026Z, 077Z, 7923, 8004, 9251","$0.00","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. <br/><br/>This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940145","Collaborative Research: Machine Learning methods for multi-disciplinary multi-scales problems","OAC","HDR-Harnessing the Data Revolu, Big Data Science &Engineering","01/01/2020","10/15/2020","Olivier Pauluis","NY","New York University","Continuing Grant","Eva Zanzerkia","12/31/2022","$872,008.00","Debra Laefer","pauluis@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","099Y, 8083","062Z, 9263","$0.00","This project addresses two of the most pressing challenges in modern scientific research: (a) modeling natural phenomena across a broad range of space and time scales, and (b) the application of data science to discover physically meaningful relationships from large datasets. It will leverage knowledge from related and disparate disciplines, connecting them through data science. Four specific problems will be studied: cloud formation and evolution, movement of particles through random media, frustrated magnetic systems, and the reconstruction of urban topography. These benchmark problems have been selected as they capture different disciplinary aspects of multi-scale challenges. State-of-the-art methods in machine learning (including Artificial Neural Networks) will be used to develop new mathematical representation for small-scale processes. If successful, this project will substantially increase the capability of scientific computing to address a wide variety of important problems from the natural and social sciences, and will be disseminated widely through a pair of workshops, multiple campus visits across the 5-institution consortium, high impact peer-reviewed publications and presentations and the training of a cadre of more than a dozen post-docs and students.<br/><br/>This project will develop, implement and evaluate a new constrained optimization framework to discover and test physical phenomena at different resolutions and scales, including new machine learning algorithms aimed at discovering the stochastic differential equations underlying noisy data. This will be used to train physical parameterizations that account for the effects of small-scale processes in coarse resolution models. Core to this will be the design of a new framework to constrain artificial neural networks to deliver solutions that are interpretable and meaningful in the domain sciences and that can be directly associated with differential operators.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939999","Collaborative Research: MEMONET: Understanding memory in neuronal networks through a brain-inspired spin-based artificial intelligence","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","11/08/2021","Claudia Mewes","AL","University of Alabama Tuscaloosa","Continuing Grant","Sylvia Spengler","09/30/2022","$375,224.00","","cmewes@mint.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","099Y, 1640","062Z, 9102, 9150","$0.00","The brain is arguably the most sophisticated and the most efficient computational machine in the universe. The human brain, for example, comprises about 100 billion neurons that form an interconnected circuit with well over 100 trillion connections. Understanding how a multitude of brain functions emerge from the underlying neuronal circuit will give insights into the operating principles of the brain. In this award, a multidisciplinary team of systems biologist, computational biologist, material scientist, neuroscientist, and machine learning expert will work synergistically to leverage the data revolution in neuroscience to answer a fundamental question: How does the brain learn, store, and process information?  The team will develop and apply advanced data analysis algorithms to harness the great volume of neuronal data generated by the latest imaging and molecular profiling technologies, for elucidating the neuronal circuits driving brain functions. Computer simulations of a spin-electronic (spintronic) device will further serve as a platform to validate and emulate important operational characteristics of such neuronal circuits. The award sets the groundwork for an interdisciplinary data science research and educational program that will bring a new and powerful paradigm for studying brain functions as well as for designing transformative brain-inspired devices for information processing, data storage, computing, and decision making.<br/><br/>The project has a specific focus on an essential function of the brain: motor-skill learning. This function emerges from the underlying circuitry of neurons that governs the activities of molecular signal transmission and neuronal firing. Importantly, the neuronal circuit in a mammalian brain is highly plastic and dynamic, features that endow animals with the ability to respond to myriad external stimulations through learning. By harnessing the latest data revolution in neuronal imaging, single neuron molecular profiling, spintronic device simulation, network inference, and machine learning, a team of multidisciplinary investigators will be supported by this award to investigate the fundamental principle of neuronal circuit rewiring that drives brain?s learning function. More specifically, the team sets out to achieve the following specific tasks: (A) Infer learning-induced rewiring of large-scale neuronal networks from two-photon calcium imaging data through the development of novel and powerful network inference algorithms; (B) Build biochemical-based models of neuronal circuits by integrating molecular profiling with neuron firing and connectome dynamics; and (C) Develop a spintronic material network model that emulates learning and memory formation by exploiting the spin dynamics in spintronic materials. The project seeks to lay the foundation for the creation of an interdisciplinary data-intensive brain-to-materials initiative that will be applied to understand and emulate the operational principles of brain neuronal circuits underlying learning, cognition, memory formation, and other behaviors. The outcomes of the initiative will have a paramount impact on the society, not only in our understanding of the brain and its functions, but also in overcoming current bottlenecks of existing computing architectures.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1944784","Collaborative Research: Examining Impact and Fostering Academic Support for Open Science Products","OAC","NSF Public Access Initiative","10/01/2019","11/05/2021","Suzanne Ortega","DC","Council of Graduate Schools","Standard Grant","Martin Halbert","09/30/2022","$25,390.00","Hironao Okahana","sortega@cgs.nche.edu","1 Dupont Circle NW","Washington","DC","200361146","2022233791","CSE","7414","","$0.00","Open Science has the potential to significantly advance STEM knowledge beyond individual academic research silos. This project will support a conference entitled, ""Examining Impact and Fostering Academic Support for Open Science Products."" The meeting is jointly organized by the American Educational Research Association (AERA) and the Council of Graduate Schools (CGS).  The objective of the conference is to examine the products of open science practices and explore what counts as productivity and quality in contributions that are non-traditional research outcomes. Non-research outcomes have attendant activities that include data sharing; the development of multi-user data sets, harvesting and archiving big data, replication studies, registered reports, data management plans, and use as well as citation to persistent identifiers (DOIs).<br/><br/>The conference attendees are higher education leaders, education researchers, and related scientists, together with collective expertise in scientific productivity, science professions, higher education institutions.  The conference will engage attendees in a policy conversation and development of performance metrics and assessments on non-traditional research outcomes beyond publication citations in highly ranked journals. This several-day conference, scheduled for Summer 2020, will be held at the AERA Conference Center in Washington DC and will involve about 28 participants. This conference has the potential to support change and foster experimentation in the very institutions where the next generation of researchers are being trained, where science is organized, and where open science products are produced.<br/><br/>This award is co-funded by the NSF Research Traineeship (NRT) Program. The NRT Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1941481","EAGER: An Exaflop-hour simulation in AWS to advance Multi-messenger Astrophysics with IceCube","OAC","CYBERINFRASTRUCTURE, COMPUTATIONAL PHYSICS","10/01/2019","09/07/2019","Frank Wuerthwein","CA","University of California-San Diego","Standard Grant","Kevin Thompson","09/30/2021","$295,000.00","","fkw@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7231, 7244","026Z, 069Z, 7569, 7916","$0.00","Exascale High Throughput Computing (HTC) demonstrates burst capability to advance multi-messenger astrophysics (MMA) with the IceCube detector. At peak, the equivalent of 1.2 Exaflops of 32 bit floating-point compute power is used. This is equivalent to approximately 3 times the scale of the #1 in the Top500 Supercomputer listing as of June 2019. In one hour, roughly 125 terabytes of input data is used to produce 250 terabytes of simulated data that is stored at the University of Wisconsin, Madison to be used to advance IceCube science. This data amounts to about 5% of the annual simulation data produced by the IceCube collaboration in 2018.<br/><br/>This demonstration tests and evaluates the ability of HTC-focused applications to effectively utilize availability bursts of Exascale-class resources to produce scientifically valuable output and explores the first 32-bit floating point Exaflop science application.  The application concerns IceCube simulations of photon propagation through the ice at its South Pole detector. IceCube is the pre-eminent neutrino experiment for the detection of cosmic neutrinos, and thus an essential part of the MMA program listed among the NSF's 10 Big Ideas.<br/><br/>The simulation capacity of the IceCube collaboration is significantly enhanced by efficiently harnessing the power of  short-notice Exascale processing capacity at leadership class High Performance Computing  systems and commercial clouds. Investigating these capabilities is important to facilitate time-critical follow-up studies in MMA, as well as increasing the overall annual capacity in aggregate by exploiting opportunities for short bursts.<br/><br/>The demonstration is powered primarily by Amazon Web Services (AWS) and takes place in the Fall of 2019 during the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC19) in Denver, Colorado. It is a collaboration between the IceCube Maintenance & Operations program and a diverse set of Cyberinfrastructure projects, including the Pacific Research Platform, the Open Science Grid, and HTCondor. By further collaborating with Internet2 and AWS, the experimental project also explores more generally, large high-bandwidth data flows in and out of AWS. The outcomes of this project will thus have broad applicability across a wide range of domains sciences, and scales, ranging from small colleges to national and international scale facilities.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925550","CC* Integration: Archipelago: Linking Researchers On-Campuses and in the Cloud through SDN-Enabled Microsegmentation","OAC","CISE Research Resources","11/01/2019","07/27/2019","Tracy Futhey","NC","Duke University","Standard Grant","Deepankar Medhi","10/31/2022","$999,631.00","Maria Gorlatova, Richard Biever, William Brockelsby","futhey@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","2890","9102","$0.00","Researchers often generate very large amounts of data in their work, so much so that their data stresses the underlying computer networks.  Some academic institutions build a separate network for research data, but this is very expensive.  Archipelago is a project that seeks to improve the performance and security of computer networks to make it easier and less costly for scientists to move, store, and analyze their data.  It uses relatively small numbers of very fast, flexible network devices which use a technique known as Software Defined Networking (SDN) to steer research data around bottlenecks giving the appearance of a separate science network without having to spend all the money needed to build a true second network.<br/><br/>Archipelago addresses shortcomings that have limited adoption of SDN within campus networks, beginning with evaluation of next-generation SDN forwarding elements, SmartNIC (Smart Network Interface Card) devices and purpose built SDN data planes to create its hybrid architecture.  This architecture connects islands of SDN by enforcing traffic policies at nearby distribution points linked to commodity servers with SmartNICs or next-generation hardware-driven SDN forwarding elements (depending on scale) that provide control plane functionality.  This shift from software to SmartNIC forwarding accelerates data plane performance and efficient use of distributed CPU resources for the control plane frees server resources for other uses. This robust policy-driven architecture enables fine-grained traffic forwarding policies for microsegmentation, enhanced cybersecurity, efficient data transfer, and other applications.<br/><br/>Utilizing agile software development techniques that are informed by the deployment experience at Duke and within partner environments, this project will deliver a powerful, vendor-agnostic research network design that is easy-to-adopt for institutions that have been reluctant to venture into SDN architectures due to personnel or technical resource constraints.  Since the Archipelago architecture augments existing networks incrementally (and therefore cost effectively) and because its software components are deployed via open-source licensing, it will provide new and powerful capability to institutions that already have established SDN networks, as well as for those that have been reluctant to venture into SDN architectures due to financial constraints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835630","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Edward Maginn","IN","University of Notre Dame","Standard Grant","Bogdan Mihaila","09/30/2022","$372,486.00","","ed@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835593","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Eric Jankowski","ID","Boise State University","Standard Grant","Bogdan Mihaila","09/30/2022","$235,000.00","","ericjankowski@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2140378","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","09/01/2021","08/31/2021","Chris Myers","CO","University of Colorado at Boulder","Standard Grant","Peter McCartney","11/30/2022","$111,471.00","","chris.myers@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","099Y, 7231","1165, 7231","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1904444","Exploring Clouds for Acceleration of Science (E-CAS)","OAC","CYBERINFRASTRUCTURE","11/15/2018","10/07/2020","Howard Pfeffer","DC","INTERNET2","Cooperative Agreement","Kevin Thompson","12/31/2021","$3,030,955.00","James Bottum, Ana Hunsinger","hpfeffer@internet2.edu","1150 18th St NW","Washington","DC","200363825","7349134264","CSE","7231","","$0.00","Internet2 leads the ""Exploring Clouds for Acceleration of Science (E-CAS)"" project in partnership with representative commercial cloud providers to accelerate scientific discoveries. The effort seek to demonstrate the effectiveness of commercial cloud platforms and services in supporting applications that are critical to growing academic and research computing and computational science communities, and seeks to illustrate the viability of these services as an option for leading-edge research across a broad scope of science. The project helps researchers understand the potential benefit of larger-scale commercial platforms for scientific application workflows such as those currently using NSF's High-Performance Computing (HPC). It also explores how scientific workflows can innovatively leverage advancements provided by commercial cloud providers.  The project aims to accelerate scientific discovery through integration and optimization of commercial cloud service advancements; identify gaps between cloud provider capabilities and their potential for enhancing academic research; and provide initial steps in documenting emerging tools and leading deployment practices to share with the community.<br/><br/>Cloud computing has revolutionized enterprise computing over the past decade and it has the potential to provide similar impact for campus-based scientific workloads. The E-CAS project explores this potential by providing two phases of funded campus-based projects addressing acceleration of science. Each phase is followed by a community-led workshop to assess lessons learned and to define leading practices. Projects are selected from two categories; time-to-science (to achieve the best time-to-solution for scientific application/workflows that may be time or situation sensitive) and innovation (to explore innovative use of heterogeneous hardware resources, serverless applications and/or machine learning to support and extend application workflows). The project is guided by an external advisory board including leading academic experts in computational science and other fields, commercial cloud representatives, NSF program officers, and others. It leverages prior and concurrent NSF investments while creating a new model of scalable cloud service partnerships to enhance science in a broad spectrum of disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827211","CC* Integration: End-to-End Software-Defined Cyberinfrastruture for Smart Agriculture and Transportation","OAC","CISE Research Resources","10/01/2018","07/23/2018","Hongwei Zhang","IA","Iowa State University","Standard Grant","Deepankar Medhi","09/30/2022","$999,919.00","Patrick Schnable, Arun Somani, Ahmed Kamal, Anuj Sharma","hongwei@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","2890","9102","$0.00","Imaging and other sensor-based understanding of plant behavior is becoming key<br/>to new discoveries in plant genotypes leading to more productive and environment-friendly farming.<br/>Similarly, distributed sensing is seen as a key component of a safe, efficient, and sustainable autonomous transportation systems.<br/>Existing research and education in agriculture and transportation systems are constrained by the lack of connectivity between field-deployed testbed equipment and computing infrastructure. To realize that connectivity, this project proposes to deploy CyNet wireless networks to<br/>connect experimental science testbeds to high-performance cloud computing infrastructures.<br/><br/>The CyNet project will:<br/>1) deploy Predictable, Reliable, Real-time, and high-Throughput (PRRT) wireless networking solutions using the standards-compliant, open-source Open Air Interface software framework and commodity Universal Software Radio Peripheral (USRP) hardware;<br/>2) integrate these wireless networks with software defined networks to seamlessly integrate outdoor cameras, sensors, and autonomous vehicles, and connect these components to high performance cloud computing systems;<br/>3) implement an infrastructure virtualization system that partitions CyNet into programmable, isolated experiments; and<br/>4) create an infrastructure management system that performs admission and access control and establishes specified resource allocation policies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940263","Collaborative Research:  Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Peter Crozier","AZ","Arizona State University","Continuing Grant","Pui Ho","09/30/2022","$325,000.00","","crozier@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940179","Collaborative Research: Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Roberto Rivera","PR","University of Puerto Rico Mayaguez","Continuing Grant","Pui Ho","09/30/2022","$325,000.00","","roberto.rivera30@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940074","Collaborative Research: Science-Aware Computational Methods for Accelerating Data-Intensive Discovery: Astroparticle Physics as a Test Case","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Waheed Bajwa","NJ","Rutgers University New Brunswick","Continuing Grant","Vyacheslav (Slava) Lukin","09/30/2022","$320,701.00","","waheed.bajwa@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","099Y","062Z","$0.00","The rapid technological advances of the last two decades have ushered in an era of data-rich science for several disciplines.  One such discipline is astroparticle physics, where researchers aim to discover what our Universe is made of by trying to directly detect Dark Matter. This discovery can be hastened if data science tools are used to extract significant domain-specific information from data, and to reliably test scientific hypotheses at scale. The overarching goal of this two-year project is to lay the groundwork for incorporating scientific knowledge into machine learning and data science methods in the context of scientific disciplines in which discovery requires effective, efficient analysis of lots of noisy data gathered by multiple imperfect sensors. In doing so, it not only advances the state-of-the-art in data science, machine learning, and astrophysics, but it also has the potential to accelerate data-driven discoveries in other scientific disciplines where data shares similar characteristics.<br/><br/>This project will develop innovative domain-enhanced data science methods that will be based on probabilistic graphical models and graph-regularized inverse problems. Using the leading astroparticle experiment XENON as a test bed, the investigators will explore and demonstrate approaches for incorporating domain knowledge into machine learning and data science methods. In doing so, the investigators will address major data-analysis challenges in the context of dark matter identification. Additionally, the investigators will invest significant effort reaching out to other data-intensive science communities, such as materials science, oceanography, and meteorology, that can benefit from the new methods and ideas. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835713","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","08/24/2021","Jeffrey Potoff","MI","Wayne State University","Standard Grant","Bogdan Mihaila","09/30/2022","$240,637.00","","jpotoff@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216, 9251","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827050","CC* Network Design: Transforming Arcadia's Networking Capability, Enhancing for Innovation to Grow Research Leaders in a Technology-driven World","OAC","Campus Cyberinfrastructure","07/01/2018","11/18/2019","Rashmi Radhakrishnan","PA","Arcadia University","Standard Grant","Kevin Thompson","06/30/2021","$352,500.00","Vitaly Ford","radhakrishnanr@arcadia.edu","450 S. Easton Road","Glenside","PA","190383215","2155722887","CSE","8080","","$0.00","As a small private university, Arcadia's existing computing infrastructure constrains the productivity of faculty in Bioinformatics, Computer Science, Chemistry, and Physics who are conducting data-intensive research. Specifically, the current infrastructure impedes researchers' ability to efficiently and securely access, share, or analyze large-data sets with collaborators at other institutions. To address these research and education needs, a collaborative team representing key university faculty and technologists at Arcadia is creating a Science DMZ with a data transmission network capable of 10Gbps connectivity (more than 10 times faster than current speeds) to the Keystone Initiative for Network Based Education and Research's (KINBER) PennREN network.<br/><br/>Project's objectives are to: (1) provide high performance, secure Science DMZ network capabilities for sharing of large datasets and cloud-based education; (2) eliminate technical barriers for faculty engaged in data-intensive projects through a dedicated, friction-free path to Internet2, PennREN, and other high performance computing and data resources; (3) leverage authentication and authorization mechanisms to support our faculty through the InCommon Federation; and (4) enable future scientific possibilities and unleash innovation for students and faculty researchers. <br/><br/>Arcadia is currently considering to incorporate a data analytics requirement into its general curriculum and leverage the newly developed cyberinfrastructure to enable cloud-based opportunities, distance learning and researching on a global scale. This opportunity is supporting greater faculty and student analytical scholarship by forming a frictionless environment built to innovate and thrive in our technology-drive world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827151","CC* Network Design and Implementation for Small Institutions: The Tommie Science Network- A dedicated research network for the University of St. Thomas","OAC","Campus Cyberinfrastructure","07/01/2018","02/28/2020","Edmund Clark","MN","University of St. Thomas","Standard Grant","Kevin Thompson","06/30/2021","$390,671.00","Charles Nguyen, Kemal Badur, Eric Tornoe, Chih Lai, William Bear","edmund.clark@stthomas.edu","2115 Summit Avenue","St. Paul","MN","551051096","6519626038","CSE","8080","","$0.00","The University of St. Thomas, in conjunction with the University of Minnesota's Gopher Science Network Team, is building the Tommie Science Network, a dedicated research network that transforms the campus research environment by providing a reliable and secure high-speed research network capable of achieving sustained transmission rates of up to 100 Gigabits per second (Gbps) between campus research locations and Internet2 (I2). This project creates efficiencies in end-to-end workflows between existing instrumentation facilities and research centers, centralized computing and data storage facilities, and partner institutions. It significantly increases bandwidth available to researchers, allowing the St. Thomas community to fully participate in collaborative, global research activity, and provides a world-class working environment for professors and students to practice distributed research and collaboration techniques to learn skills they will use in their future employment.<br/><br/>The Tommie Science Network connects Owens Science Hall, O'Shaughnessy Science Hall and the St. Thomas E-learning and Research center (STELAR) via high-speed access layer switches linked to the network core at 80Gbps. Traffic on the Tommie Science Network then bypasses the firewall to connect directly to I2 at 100Gbps via the Northern Lights Gigapop. Laboratories and research locations in the science halls and STELAR are connected at 10Gbps to the access layer switches. Globus File Transfer Protocol is used to allow secure, reliable high-speed transfers between the science buildings, the DMZ, and I2. PerfSonar is used to monitor performance between the science buildings and the and Science Demilitarized Zone (Science DMZ) as well as from the DMZ to Internet2. The St. Thomas networking staff is responsible for ongoing operations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747507","Collaborative Research: Building the Community for the Open Storage Network","OAC","BD Spokes -Big Data Regional I","06/15/2018","06/07/2018","John Goodhue","MA","The Massachusetts Green High Performance Computing Center, Inc.","Standard Grant","Alejandro Suarez","05/31/2021","$428,797.00","Derek Simmel","jtgoodhue@mghpcc.org","100 Bigelow St.","Holyoke","MA","010405984","4135524902","CSE","024Y","062Z","$0.00","The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?<br/><br/>Many universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841471","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Kyle Cranmer","NY","New York University","Standard Grant","Bogdan Mihaila","09/30/2021","$486,879.00","Heiko Mueller","kyle.cranmer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925553","CC* Regional:  Accelerating Research and Education at Small Colleges in Texas via an Advanced Networking Ecosystem Using a Virtual LEARN Science DMZ","OAC","Campus Cyberinfrastructure","10/01/2019","01/25/2021","Akbar Kara","TX","LEARN: Lonestar Education and Research Network","Standard Grant","Kevin Thompson","09/30/2022","$957,535.00","Amy Schultz, Akbar Kara, Curtis White, James Bradley, Ryan Fitzgerald","akbar.kara@tx-learn.net","3726 20th Street","Lubbock","TX","794101208","8067437878","CSE","8080","","$0.00","LEARN - Lonestar Education And Research Network, a consortium of 41 organizations throughout Texas, is expanding opportunities for students and faculty at smaller college campuses to participate in research that relies on increasingly sophisticated network technology. LEARN has identified an initial set of smaller colleges to take part in the project and is providing these campuses with advanced network and performance monitoring technologies, the expertise to manage the infrastructure, training and assistance in increasing utilization of the underlying technologies, and a community of similarly engaged scholars and administrators across Texas.<br/><br/>The specific objectives of this project are to<br/>1. Establish a small college collaborative environment within the LEARN community<br/>2. Improve network connectivity/services at each college campus for research and education<br/>3. Establish a network performance monitoring infrastructure<br/>4. Establish a means to facilitate the transfer of large data sets<br/>5. Deliver technical training to personnel at each campus<br/>6. Develop and implement an outreach program for informing/educating faculty, staff, and students at each college, and develop and disseminate project results.<br/><br/>LEARN is partnering with national organizations in the implementation of this project. Projected impacts include increased opportunities for students to learn about and gain experience in advanced aspects of science, technology, engineering and mathematics (STEM) for which they might not otherwise have had an opportunity. This project can also extend to students and faculty at other campuses in Texas and can be replicated at other regional networks and smaller colleges throughout the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934675","Collaborative Research: High-Dimensional Spatio-Temporal Data Science for a Resilient Power Grid: Towards Real-Time Integration of Synchrophasor Data","OAC","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2019","10/15/2020","Le Xie","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Donald Wunsch","08/31/2022","$185,959.00","","le.xie@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","099y, 7607","062Z, 1653, 7607","$0.00","The project will establish an Institute at Arizona State University (ASU) with Texas A&M (TAMU) that considers the electric power grid and examines critical real-time decision-making by developing core data-driven science methods and applications.  This is motivated by the modern electric power system which is experiencing heightened unpredictability from increasing demand for renewable energy, efficiency, and resilience. To address this, industry stakeholders are deploying GPS-synchronized phasor measurement units (PMUs), or synchrophasors, that provide direct measurements of voltage and current phasors with high temporal granularity. However, the potential real-time situational awareness enabled by these measurements has been impeded by the massive scale of the time-series PMU data and have limited its use to passive, post-event forensics. The Institute meets this need for PMU-based real-time decision-making by examining five critical problems: (i) ensure data quality against bad, missing, or stale data; (ii) exploit the fine granularity of PMU data to track real-time changes in network parameters; (iii) detect, identify, localize, and visualize oscillation and failure events; (iv) assess and visualize cybersecurity threats and countermeasures specific to PMUs; and (v) create synthetic PMU datasets for testing and validation. The Institute leverages the PIs' synergistic multidisciplinary background in information sciences and statistics, machine learning, data visualization, cybersecurity, and power systems. The team will apply state-of-the-art techniques including hidden Markov models, LSTM neural networks, graphical models, errors-in-variables models, graph signal processing, adversarial examples, low-dimensional feature extraction, and constrained GANs. Another key research focus is the development of visual analytics for high-granularity spatio-temporal PMU data to enable improved operator review and decision-making. These innovations will be fueled by massive PMU datasets accessible to the PIs.<br/><br/>This Phase I institute has the potential to tip PMUs from a promising-but-mostly-underused resource into an essential part of power system best practices. The data science outcomes will impact application domains such as transportation networks, smart buildings, and manufacturing, each of which increasingly faces high-dimensional streaming data challenges. The PIs will disseminate their research to both academic and industry stakeholders and will continue their outreach on teaching AI and machine learning (ML) modules to underrepresented high school students. Finally, the multi-disciplinary strength of this institute lends itself naturally to a larger, integrated, and comprehensive Phase II institute focused on data-intensive research for critical infrastructure networks.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.  This effort is co-funded by the Division of Electrical, Communications and Cyber Systems within the Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931577","Elements: Transformation-Based High-Performance Computing in Dynamic Languages","OAC","Software Institutes","10/01/2019","09/06/2019","Andreas Kloeckner","IL","University of Illinois at Urbana-Champaign","Standard Grant","Robert Beverly","09/30/2022","$599,688.00","","andreask@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","026Z, 077Z, 7923","$0.00","A key capability in technical computing is the processing of large, regularly-shaped arrays of numbers by a wide variety of different processes. This facility is foundational in, for example, weather prediction, artificial intelligence, and image processing.  Correspondingly, modern computing hardware has evolved advanced capabilities for carrying out such computations with high efficiency. Unfortunately, the process of adapting a desired process to a given piece of hardware thus far is costly, laborious, and error-prone. Differences of a factor of 50 in performance between a naive realization and a careful one is the rule, rather than the exception. Loopy, the subject of this project, attacks this problem by using human-guided, automated program rewriting. Loopy has demonstrated application impact in a number of applications ranging from the simulation of natural and engineering phenomena to neuroscience, where it has helped its users achieve higher performance with less effort. The present proposal concerns several important improvements that will contribute to making Loopy more effective and easier to apply, through enlarging the class of programs that Loopy can transform, improving the means by which Loopy represents on-chip communication, and permitting it to realize important basic operations that routinely pose difficulty in efficient implementation. An important component of the effort is making Loopy itself easy to use for its user community, through the realization of an interactive user interface, so that program transformations can be applied with the click of a mouse, rather than by writing computer code. The proposed advances will be demonstrated through a sample workload that is emblematic of many of the computational and software challenges faced in technical computing today.<br/><br/>Multidimensional arrays (sometimes called 'tensors') are a foundational data structure for much of scientific computing, with applications ranging from weather prediction to deep learning, to image processing and computational neuroscience.  Even the efficient execution of one of the simplest operations on arrays, matrix-matrix multiplication, poses considerable technical challenges on modern computers. Through a polyhedrally-based program transformation tool, the proposed software will provide separation between mathematical intent and the technical challenges of program optimization, allowing each task to be performed by a domain expert. In the proposed project, the PI will develop means for more efficient on-chip communication, code generation for prefix sums, reuse and abstraction in program transformation, increasing the ease of use in transformation discovery and performance analysis, and for expressing array computations in user programs. The PI will validate the proposed techniques through a challenging application with broad applicability. The intellectual merit of the proposed research lies in (1) mapping out and extending the landscape of transformation-based programming from one-off scripts to reusable transform components, (2) the development of a unifying, loop/array-axis-based approach to expressing on-chip communication while reducing redundancy in Loopy?s program representation and transformation, (3) exploring the design space of high-performance languages that establish a close link between execution placement and data placement, (4) the development of an interactive program transform and performance analysis tool, along with the discovery of potential implications for workforce training in high-performance computing, (5) a demonstration that all the developed components can be applied together in a practical and coherent manner. Through graduate and undergraduate teaching as well as mentoring of the students and postdocs supported by this project, the PI contributes to enlarging the talent pool.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827153","CC* Networking Infrastructure: High Performance Research Data Infrastructure at the American Museum of Natural History","OAC","Campus Cyberinfrastructure","07/01/2018","06/18/2018","Juan Montes","NY","American Museum Natural History","Standard Grant","Kevin Thompson","06/30/2021","$499,722.00","Rebecca Oppenheimer, Michael Benedetto","jmontes@amnh.org","Central Park West at 79th St","New York","NY","100240000","2127695975","CSE","8080","","$0.00","Through the National Science Foundation?s Campus Cyberinfrastructure (CC*) program, this project provides a major data network upgrade for the American Museum of Natural History (AMNH) that makes scientific data flows a priority. AMNH conducts scientific research and education activities spanning astrophysics, geosciences, and genomics. Scientific collaborations require increasing network capacity among scientific instruments, collaborators and researchers. These data networking improvements to AMNH directly support these research activities. AMNH's ability to move large data sets quickly between its campus and other sites across the nation and throughout the world is critical to the success of the Museum's research program.<br/><br/>In the project, AMNH implements a high-speed Science DMZ, a network specifically tuned for large data transfers. Through the Science DMZ, AMNH connects to the NYSERNet Research and Education Network, which provides pathways to the higher education community in New York State, Internet2, and beyond. Using purpose-built Data Transfer Nodes (DTNs) based on the FIONA concept, AMNH scientists can move diverse, large-scale data sets between research facilities, supercomputing centers, and collaborators via a dedicated 10Gb/s connection. The efficiencies gained enable greater collaboration, more intelligent and timely coordination of research, and increased research throughput and quality. To further facilitate collaboration, the project leverages the GLOBUS file transfer system to easily orchestrate data transfers and for disseminating AMNH's research to the broader research community. Additionally, AMNH integrates with the InCommon Federation, utilizing a common framework to provide researchers secure access to online resources. Using perfSONAR, the Science DMZ is continually monitored to ensure that throughput, latency, and performance targets are met, and data can flow unimpeded. This allows Museum researchers to focus on science rather than the logistical details of moving data. Additionally, the work of AMNH scientists informs the Museum's educational and curatorial programs, directly benefitting AMNH students and the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916573","BD Hubs: Collaborative Proposal: West: Accelerating the Big Data Innovation Ecosystem","OAC","BD Spokes -Big Data Regional I, , ","06/01/2019","11/03/2021","David Culler","CA","University of California-Berkeley","Cooperative Agreement","Martin Halbert","09/30/2023","$2,255,547.00","Meredith Lee","culler@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","024Y, S315, S342","026Z, 062Z, 8083, 9102, 9150, 9251","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The West Big Data Innovation Hub builds and strengthens strategic partnerships -- harnessing the data revolution to address scientific and societal challenges. Whether working towards the future of data-informed healthcare or tackling projects in disaster recovery, the Hub envisions a diverse community empowered to contribute to areas of national priority. The Hubs focus on data science activities and initiatives that inspire cross-sector collaboration and exemplify the need for multi-disciplinary approaches.<br/><br/>With this award, the West Big Data Innovation Hub will: (1) Develop and enable translational data science (TDS) pilot projects in our thematic areas to highlight the value of cross-sector collaboration, enhance fluency with real-world use cases, and emphasize a pragmatic and holistic view of the data and analytics lifecycles. We envision our signature TDS initiatives for 2019-2023 to include: Fire and Water: Data Collaborative for the Future of Natural Resource Management; Stress-Testing Access for Road Video; and Housing Instability: Trusted Data Collaborative for Responsible Data Management. (2) Facilitate team formation across different stakeholder groups through our activities, capturing inspirational stories and encouraging teams to reflect and share their insights about cross-sector collaboration. (3) Raise awareness of regional opportunities and inspire work in priority areas including Natural Resources & Hazards, Metro Data Science, Health & Medicine, Data-Enabled Discovery & Learning, Data Sharing, Cloud Computing, and Responsible Data Science. (4) Support data science education and workforce development. Recognizing that a diverse, multi-faceted workforce is key to addressing current scientific and societal challenges, we will continue to expand our portfolio of education and workforce development efforts, including a focus on Train-the-Trainer sessions, Pedagogy and Practice, Data Science for Social Good and the Data Science Corps, Findable Accessible Interoperable and Reusable (FAIR) data, and institutional change -- providing a platform for broadening participation in data science. Core to our progress in Programmatic Activities, Socio-Technical Shared Resources and Services, and Education and Workforce Development Activities will be a coordinated evaluation, opportunities for scaling regional successes to the national network of Big Data Hubs, and strategic efforts for Hub sustainability including the development of external funding streams. These efforts will be designed to enable community input and to strengthen channels for ongoing dialogue.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017233","Collaborative Research: CyberTraining: Implementation: Medium: Broadening Adoption of Parallel and Distributed Computing in Undergraduate Computer Science and Engineering Curricula","OAC","CyberTraining - Training-based","10/15/2020","07/22/2020","Ramachandran Vaidyanathan","LA","Louisiana State University","Standard Grant","Almadena Chtchelkanova","09/30/2023","$72,231.00","","vaidy@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","044Y","026Z, 9150","$0.00","This collaborative project represents a multi-faceted effort to shift computer science and engineering education toward ensuring that students can use 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly designed around a single processor, executing a sequence of operations. But this century is characterized by widespread deployment of multi-core, graphics, and AI tensor processors, as well as a shift to cloud servers, and the internet of things, all of which depend on the much different PDC approach to problem solving and programming. Financial, technical, scientific, engineering and medical companies, government labs, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the old model. Yet most students continue to learn the old approach due to significant inertia in academia. To turn the tide toward infusing PDC into the early stages of computer science and engineering education, this project will guide curricula and accreditation standards, prepare teachers, and foster a strong PDC education community. It will thus strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) is preparing the 2020 update of their 2013 curriculum guidelines for introducing parallel and distributed computing (PDC) into early undergraduate courses. This project will engage in four areas of activity to foster adoption of the curriculum, and extend it, with the goal of modernizing computer science and engineering workforce development.<br/>One major thrust is running summer training workshops for teachers, to learn both PDC concepts and experimental course evaluation methodology. The discipline is still in a phase of discovery with respect to PDC education approaches, and must encourage a diverse set of well-designed experiments to test and evaluate a broad range of pedagogical hypotheses. The workshop participants will be drawn from a diverse pool of educators, and given curriculum development grants in support of experimental course offerings and evaluation, leading to conference or journal publications, as well as contributions of exemplars to the CDER online course materials repository. <br/>A second effort is to help ABET/CSAB to formulate core PDC requirements and to inform/train ABET/CSAB evaluators (CSAB is the lead society within ABET for accreditation of degree programs in computer science). <br/>A third effort is expanding the curriculum guidelines to explicitly address adding PDC to computer engineering programs, which present novel curricular opportunities. <br/>Lastly, it will continue CDER's successes in organizing PDC education workshops in conjunction with major conferences, publishing PDC education books and journal special issues, maintaining and curating an online repository of PDC education resources, and providing free access to a publicly available PDC education cluster system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939885","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Mai Nguyen","CA","University of California-San Diego","Standard Grant","Peter McCartney","09/30/2022","$271,519.00","","mhnguyen@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","099Y, 7231","1165, 7231","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762010","Spokes: Medium: Western: Collaborative: Big Data to Promote Community Learning and Impact","OAC","BD Spokes -Big Data Regional I, IUSE","09/01/2018","11/05/2021","Rene Vellanoweth","CA","California State L A University Auxiliary Services Inc.","Standard Grant","Earnestine Psalmonds","08/31/2022","$948,683.00","Hengchun Ye, Rene Vellanoweth, Michael Willard","rvellan@calstatela.edu","5151 State University Drive","Los Angeles","CA","900324221","3233433648","CSE","024Y, 1998","8083, 9102","$0.00","The project will create a partnership among California State University Los Angeles, City of Los Angeles (LA) GEOHub, Community Partners, and the Social Equity Engagement geo-Data Scholars (SEEDS) Program to  provide access to and greater utilization of the city's big data collection. The partnership will introduce GeoHub data as decision making tools to highly motivated, ethnically diverse, and civic-minded students and increase participation of citizens in using GeoHub data. The project activities will promote widespread access to and understanding of LA's open data portal among a broad cross-section of the population, especially groups that are traditionally disadvantaged and less likely to be digitally connected. It will provide training that will allow citizens and non-profits to become proficient in the use of big data, empower citizens so that they can see data-driven government in action and effectively participate in civic decision-making, and make local cities more livable and equitable for all. With citizen involvement, open data portals can become sustainable and independent of political changes in the local, state, or national governments.  <br/><br/>The City of Los Angeles will provide California State University Los Angeles, community partners, and non-profit organizations GeoHub accounts to access software and apps free of charge and train California State faculty members who will, in turn, train their students in GeoHub's data and visualization applications. The students will help Community Partners to train non-profit organizations to use GeoHib for all of their service needs. This approach will effectively reach large numbers of users and contributors in diversified communities. The project will produce course modules with GeoHub hands-on service learning activities in disciplines across humanities, social sciences, geosciences, political sciences, public health, and civil engineering. It will also generate data access and visualization manuals and support community-based research projects. The project plan aligns with the Western Big Data Hub big data education and literacy efforts and contributes to the Hub's cross-sector activities.<br/><br/>This award is co-funded by the Improving Undergraduate STEM Education: Education and Human Resources (IUSE): EHR Program (NSF 17-590). IUSE supports projects that are designed to improve student learning through development of new curricular materials and methods of instruction and development of new assessment tools to measure student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841530","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Frank Wuerthwein","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","09/30/2021","$696,085.00","","fkw@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841479","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","David Schultz","WI","University of Wisconsin-Madison","Standard Grant","William Miller","11/30/2020","$197,221.00","","david.schultz@icecube.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835607","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","Special Initiatives, Software Institutes","01/01/2019","09/06/2018","Kristen Fichthorn","PA","Pennsylvania State Univ University Park","Standard Grant","Bogdan Mihaila","12/31/2022","$365,621.00","","fichthorn@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1938743","Creating FAIR Data in Lake Observatories of the Future","OAC","NSF Public Access Initiative","12/01/2019","08/06/2019","Kathleen Weathers","NY","Cary Institute of Ecosystem Studies, Inc.","Standard Grant","Martin Halbert","11/30/2022","$49,958.00","Renato Figueiredo, Cayelan Carey, Paul Hanson","weathersk@caryinstitute.org","2801 SHARON TPKE","Millbrook","NY","125450129","8456777600","CSE","7414","","$0.00","Lake observatories produce data critical to the development of insights into the response of ecosystems to the global demands on freshwater resources. To help scientists study these issues, and to inspire educators and the public to learn about and further appreciate the importance of lake ecosystems to human well being, lake observatories need to accelerate the rate at which their observational data can be discovered and used by others. Through organizing a conference around the FAIR principles (data that are Findable, Accessible, Interoperable, and Reusable), the PIs propose to engage members of both the global lake ecology and the data and information science communities to identify and develop a roadmap by which lake observatories of the future can deliver FAIR data to researchers, managers, and the public. <br/><br/>A one-and-a-half day workshop is planned for Spring 2020.  Attendees will represent key stakeholders from the Global Lake Ecological Observatory Network (GLEON), the data and informatics community emerging around FAIR, and affiliated observatory and international members.   The agenda will include reviewing lessons learned from current FAIR initiatives; gap identification and opportunities for lake observatories of the future; framework for discussing data valuation and longevity of data products from lake observatories; and use cases for edge computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103662","Framework: An A+ Framework for Multimessenger Astrophysics Discoveries through Real-Time Gravitational Wave Detection","OAC","PHYSICS AT THE INFO FRONTIER, Software Institutes","06/01/2021","05/11/2021","Chad Hanna","PA","Pennsylvania State Univ University Park","Standard Grant","Tevfik Kosar","05/31/2026","$3,397,540.00","Madeline Wade, Aaron Viets","crh184@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","107y, 1798, 7553, 8004","069Z, 075Z, 077Z, 1207, 7569, 7925, 8004","$0.00","The first direct detection of ripples in space, known as gravitational waves, by the NSF-funded LIGO (Laser Interferometer Gravitational-wave Observatory) project in 2015 opened a new window on the universe and provided an unprecedented ability to study distant astronomical phenomena that could otherwise not be seen with conventional telescopes. The subsequent 2017 detection of merging neutron stars, through gravitational waves with LIGO combined with the light detected by conventional telescopes, opened a new era whereby scientists hope to routinely study the universe using information analogous to both sight and sound. This project will directly enable future detections of gravitational waves through the development of robust signal processing software and an ecosystem of cyberinfrastructure services designed to analyze LIGO data in real time. The program will involve a diverse group of undergraduate students, graduate students, postdoctoral researchers, computational scientists, and faculty in transformative science. This work contributes to the national cyberinfrastructure as a core data-producing component for astronomy and will be relied upon by thousands of scientists globally as they progress the state of knowledge through the study of black holes, neutron stars, fundamental physics, and the evolution of the Universe.<br/><br/>With the goal of making new coincident gravitational-wave and electromagnetic observations commonplace, this project targets the development of a software framework for the real-time discovery of gravitational waves with the world-wide network of gravitational-wave detectors including LIGO, Virgo, and KAGRA. With this project, the investigators intend to provide a sustainable community-driven framework supporting current gravitational-wave detectors while developing new infrastructure for the LIGO A+ upgrade in about 2025.  The team will develop a real-time gravitational wave processing framework around the following themes: 1) accelerating the pace of discovery and dissemination of results, 2) advancing the use of machine learning and artificial intelligence in production gravitational-wave astronomy, 3) improving scientific robustness and reproducibility, and 4) increasing adoption of the developed software and services. The framework will be used to create libraries, applications and services for real-time calibrated strain data, real-time data quality information and a quick-response gravitational-wave search for merging neutron stars and black holes, all of which will culminate in daily gravitational-wave discoveries released publicly. This framework will contribute gravitational-wave discovery services operating in a high-availability mode with the goal of greater than 99% uptime. A host of scientific metrics will be developed into a real-time test infrastructure to ensure that gravitational-wave alerts are accurate and robust throughout software development and release cycles.  This research will have a far-reaching impact on several scientific disciplines with new gravitational-wave and multi-messenger astrophysics discoveries and it will impact society through a gradual change in the shared knowledge about the universe. Beyond these general societal impacts, the project personnel intend to directly weave training and participation broadening activities into their research through 1) training and broadening participation in the research community via quarterly workshops, 2) providing professional development opportunities for the project personnel through training seminars, and 3) engaging and educating the next generation of scientists in the geographic community with a summer school for high school students.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe NSF Big Idea program, the Physics at the Information Frontier (PIF) program in the Division of Physics (PHY), and the Division of Astronomical Sciences (AST).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032713","EAGER: Expanding public access to restricted research data","OAC","NSF Public Access Initiative","09/01/2020","07/17/2020","Rick Gilmore","PA","Pennsylvania State Univ University Park","Standard Grant","Martin Halbert","08/31/2022","$199,977.00","Cynthia Vitale","rogilmore@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","7414","7916","$0.00","Researchers in disciplines across the behavioral, social, educational, computational, and life sciences collect data whose sharing can pose significant practical and ethical challenges due to privacy concerns, political risk, cultural sensitivities, legal or policy restrictions, and intellectual property constraints. How should researchers protect data while making access as open as possible, in alignment with the principles of open science? And how should data repositories clearly communicate who has what sort of access in ways that encourage suitable uses?   This project will create generalizable models and infrastructure that strike a balance between providing open access to research data and materials while protecting research data. The results will bolster transparent, reproducible, integrative, interdisciplinary and insight-generating research across scientific fields by enhancing an existing research data infrastructure that specializes in sharing sensitive research data with restricted scientific audiences.<br/><br/>The proposed infrastructure builds on Databrary.org, a restricted access data library specializing in storing and sharing video data and documentation. Databrary was co-developed by the PI with support from NSF. The project has two aims: (1) Implement interface and metadata enhancements to the NSF-supported Databrary digital library to make sensitive, restricted-access data maximally discoverable to the widest range of audiences, including users of the NSF Public Access Repository (NSF-PAR), and (2) Ensure that Databrary embodies best practices and shares its innovations widely. The activities associated with these aims will expand access to Databrary?s existing holdings, improve the findability, accessibility, interoperability, and reusability of data shared there, and provide insights and knowledge that should benefit other scholarly communities facing similar challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018299","CC* Team: Research Innovation with Scientists and Engineers (RISE)","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","Chad Hanna","PA","Pennsylvania State Univ University Park","Continuing Grant","Kevin Thompson","06/30/2023","$786,387.00","Jenni Evans, Edward O'Brien","crh184@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","7231, 8080","","$0.00","The pace of scientific discovery and the dissemination of scientific knowledge are increasingly being driven by computation through modeling, data science, and digital communication platforms. Not all researchers are equally positioned to leverage this computational revolution due to having inadequate expertise in their groups or insufficient funding to hire computational experts into full time positions.  Penn State is working to ensure that researchers and educators across the 24-campus Penn State system have access to the cutting-edge cyberinfrastructure and computational expertise that they need to conduct the highest quality research and education.  Penn State's approach is to build a team of cyberinfrastructure facilitators who are shared across investigators and who consult on projects at various scales to bring shared knowledge and the best-practices of modern computational techniques and tools to the broadest possible Penn State community.  These facilitators, known as the ""Research Innovation with Scientists and Engineers"" (RISE) team, are experts in databases, visualization, code optimization, application development, web services, and cloud computing. They have broad knowledge of research cyberinfrastructure and are able to architect, design, and develop new cyberinfrastructure.  They will also have deep knowledge of various scientific domains and will enable computational discovery. Investing in such a team will pay substantial dividends through increased productivity of faculty, more efficient use of research and education funding, and ultimately new discoveries across a broad swath of scientific domains including Physics, Astronomy, Bioinformatics, Chemistry, Energy, and Climate Modeling.<br/><br/>This project builds a cyber-team for Research Innovation with Scientists and Engineers (RISE) who will partner with campus-level CI experts, domain scientists, research groups, and educators to drive new approaches that support scientific discovery across the state-wide Pennsylvania State University system including 24 campuses serving more than 100,000 students.  The RISE team will directly facilitate the usage and creation of research cyberinfrastructure across domains including Astronomy, Biology, Chemistry, Meteorology, Physics and more through consulting and providing direct services to faculty. The RISE team will partner with the Open Science Grid to establish Penn State as an OSG site, develop replica-exchange molecular dynamics software, apply machine learning to molecular biophysics, build digital signal processing software for radio astronomy,  collaborate on feature development and testing with HTCondor, onboard new climate modeling tools and software in a sustainable and maintainable ecosystem, develop a gene sequencing management platform, deploy and maintain infrastructure to support real-time gravitational wave analysis with LIGO, and build a science gateway for stellar astrophysics simulations. Through the RISE team's shared knowledge, they will elevate the productivity of researchers who use and develop cyberinfrastructure allowing them to accomplish far more than they could in isolation.  In order to broaden participation, the investigators will develop a seed grant program whereby faculty can apply to receive extended engagement with the RISE team where members would be embedded into research groups.  RISE members will also regularly conduct training workshops and seminars in response to the needs of faculty across all Penn State campuses.  Participation broadening and coordination activities will involve regular travel among the branch campuses by RISE team members and project investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940076","Collaborative Research: Precision Learning: Data-Driven Experimentation of Learning Theories using Internet-of-Videos","OAC","HDR-Harnessing the Data Revolu, IUSE, Cyberlearn & Future Learn Tech","10/01/2019","02/28/2020","Dongwon Lee","PA","Pennsylvania State Univ University Park","Standard Grant","Finbarr Sloane","09/30/2022","$416,000.00","","dongwon@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","099Y, 1998, 8020","062Z, 7645, 9178, 9251","$0.00","This is a project to study what works to help students learn more effectively in the context of the ASSISTments system. ASSISTments is an online system that provides both assistance to students and real time assessment data to teachers. ASSISTments now supports 100,000 students who have completed more than 12 million mathematics problems. The system uses teacher input and artificial intelligence to provide assistance to students who are attempting to solve mathematics problems. This project will increase the assistance provided by the teacher and machine learning by incorporating video suggestions, such as those produced by the Kahn academy, targeted to the needs of the student. The experimentation will take content from three Open Educational Resource textbooks that are openly licensed and free to schools.<br/><br/>More specifically, the researchers will identify a large collection of videos that address mathematics skills in the textbooks and will extract features of these videos including language complexity, speaking rate, and other features. These videos and features will be checked by both teachers and through a Mechanical Turk process for usability before they are presented to students. Additionally, the project will develop a suite of novel technologies for precision learning including fine grained video feature extraction, student feature learning from heterogeneous raw data, causal modeling, and fairness aware and causal relationship enhanced optimized personalized recommendation. The research will advance theoretical understanding of fundamental issues related to personalized learning and will enable data-driven experimentation of learning theories. Causal modeling will enable the researchers to learn the features of video that are correlated with learning effectiveness. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity and is co-funded by the Division of Undergraduate Education and the Division of Research on Learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931531","Frameworks: Re-Engineering Galaxy for Performance, Scalability and Energy Efficiency","OAC","Software Institutes","10/01/2019","09/16/2019","Mahmut Kandemir","PA","Pennsylvania State Univ University Park","Standard Grant","Robert Beverly","09/30/2023","$3,500,000.00","Chitaranjan Das, Anton Nekrutenko, Paul Medvedev","mtk2@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","085y, 8004","026Z, 077Z, 1165, 7925","$0.00","Biomedical research is an important branch of science that deals with the problem of studying biological processes and identifying, preventing and curing diseases. This research forms the pathway to the discovery of new medicines as well as new therapies. As such, biomedical research is crucial to advance the national health and prosperity. Given the geographically distributed research groups and biomedical labs, collaborative science plays a very important role in biomedical research. Galaxy is an open source, web-based framework that is extensively used by more than 20,000 researchers world-wide for conducting research in many application domains, the most prominent of which is biomedical research.  It provides a web-based environment using which scientists perform various computational analyses on their data, exchange results from these analyses, explore new research concepts, facilitate student training, and preserve their results for future use. Galaxy currently runs on a large variety of high-performance computing (HPC) platforms including local clusters, supercomputers in national labs, public datacenters and Cloud. Unfortunately, while most of these systems supplement conventional CPUs with significant accelerator capabilities (in the form of Graphical Processing Units (GPUs) and/or Field-Programmable Gate Arrays (FPGAs)), the current Galaxy implementation does not take advantage of these powerful accelerators. This project enhances the Galaxy framework so that it can take full advantage of the tremendous computational capabilities offered by GPUs and FPGAs. By doing so, the important applications running under Galaxy experiences significant speedups, thereby accelerating scientific discoveries.  <br/><br/>This project consists of four complementary tasks, which follow a logistic progression as follows: Task-I focuses on redesigning existing Galaxy tools with GPU/FPGA support and integrate them to Galaxy tool-chains; Task-II provides containerization support for the tools and accelerator-aware orchestration for running Galaxy on cloud platforms; Task-III implements specific policy driven scheduling schemes for Task-I and Task-II; and finally, Task-IV redesigns Galaxy storage to speed up execution and reduce bottlenecks related to data transfer. The proposed enhancements to Galaxy enables the integration of innovation with discovery by providing a state-of-the art experimental platform to a larger community of researchers across several disciplines. On the broader impact and outreach/educational front, this project impacts the performance and energy efficiency of Galaxy tools and applications and improves the productivity of a typical Galaxy user tremendously; that is, the main beneficiaries of this project are thousands of members of existing Galaxy Community. However, this project also (i) helps existing GPU and FPGA based (non-Galaxy) applications start using Galaxy, thereby taking full advantage of all existing toolsets within the framework, (ii) enables Galaxy tools to take better advantage of emerging cluster scheduling capabilities, and (iii) creates a synergy with concurrent Galaxy related efforts and existing infrastructure efforts the PIs are involved with, to further expedite scientific discoveries. As such, this proposed system support will have a broad societal impact via the enhanced Galaxy system support.  On the education side, the project involves under-represented groups in computer science as well as in bio-informatics, outreach to undergraduates, various K-12 related activities (Science-U, CSATS, VIEW), and engagement with researchers in other disciplines (e.g., natural language processing, image processing, drug discovery and cosmology) via a workshop open to the Galaxy community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934548","Collaborative Research: Knowledge Guided Machine Learning: A Framework for Accelerating Scientific Discovery","OAC","HDR-Harnessing the Data Revolu","09/01/2019","08/12/2020","Christopher Duffy","PA","Pennsylvania State Univ University Park","Continuing Grant","Eva Zanzerkia","08/31/2022","$152,000.00","","cxd11@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","099Y","062Z","$0.00","The success of machine learning (ML) in many applications where large-scale data is available has led to a growing anticipation of similar accomplishments in scientific disciplines. The use of data science is particularly promising in scientific problems involving processes that are not completely understood. However, a purely data-driven approach to modeling a physical process can be problematic. For example, it can create a complex model that is neither generalizable beyond the data on which it was trained nor physically interpretable. This problem becomes worse when there is not enough training data, which is quite common in science and engineering domains.  A machine learning model that is grounded by explainable theories stands a better chance at safeguarding against learning spurious patterns from the data that lead to non-generalizable performance. This is especially important when dealing with problems that are critical and associated with high risks (e.g., extreme weather or collapse of an ecosystem).  Hence, neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for knowledge discovery in complex scientific and engineering applications. This project is developing novel techniques to explore the continuum between knowledge-based and ML models, where both scientific knowledge and data are integrated synergistically. Such integrated methods have the potential for accelerating discovery in a range of scientific and engineering disciplines. This project will train interdisciplinary scientists who are well versed in such methods and will disseminate results of the project via peer-reviewed publications, open-source software, and a series of workshops to engage the broader scientific community.<br/><br/>This project aims to develop a framework that uses the unique capability of data science models to automatically learn patterns and models from data, without ignoring the treasure of accumulated scientific knowledge. Specifically, the project builds the foundations of knowledge-guided machine learning (KGML) by exploring several ways of bringing scientific knowledge and machine learning models together using pilot applications from four domains: aquatic ecodynamics, climate and weather, hydrology, and translational biology. These pilot applications were selected because they are at tipping points where knowledge-guided machine learning can have a transformative effect.  KGML has the potential for providing scientists and engineers with new insights into their domains of interest and will require the development of innovative new machine learning approaches and architectures that can incorporate scientific principles. Scientific knowledge, KGML methods, and software developed in this project could potentially be extended to a wide range of scientific applications where mechanistic (also known as process-based) models are used.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854312","CIF21 DIBBs: PD: Cyberinfrastructure Tools for Precision Agriculture in the 21st Century","OAC","Hydrologic Sciences, Data Cyberinfrastructure","06/01/2018","05/22/2020","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","Amy Walton","06/30/2021","$513,105.00","","taufer@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1579, 7726","062Z, 077Z, 7433, 8048, 9150","$0.00","This interdisciplinary project applies computer science approaches and computational resources to large multidimensional environmental datasets, and synthesizes this information into finer resolution, spatially explicit products that can be systematically analyzed with other variables.  The main emphasis is ecoinformatics, a branch of informatics that analyzes ecological and environmental science variables such as information on landscapes, soils, climate, organisms, and ecosystems.  The project focuses on synthesis/computational approaches for producing high-resolution soil moisture datasets, and the pilot application is precision agriculture. The effort combines analytical geospatial approaches, machine learning methods, and high performance computing (HPC) techniques to build cyberinfrastructure tools that can transform how ecoinformatics data is analyzed.<br/><br/>The investigators build upon publicly available data collections (soil moisture datasets, soil properties datasets, and topography datasets) to develop: (1) tools based on machine-learning techniques to downscale coarse-grained data to fine-grained datasets of soil moisture information; (2) tools based on HPC techniques to estimate the degree of confidence and the probabilities associated with the temporal intervals within which soil-moisture-base changes, trends, and patterns occur; and (3) data- and user- interfaces integrating data preprocessing to deal with data heterogeneity and inaccuracy, containerized environments to assure portability, and modeling techniques to represent temporal and spatial patterns of soil moisture dynamics. The tools will inform precision agriculture through the generation and use of unique information on soil moisture for the coterminous United States.  Accessibility for field practitioners (e.g., local soil moisture information) is made possible through lightweight virtualization, mobile devices, and web applications.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Earth Sciences within the NSF Directorate for Geosciences."
"2115134","CICI: UCSS: ACSP4HR: Assuring Cyber Security and Privacy for Human Resilience Research: Requirements, Framework, Architecture, Mechanisms and Prototype","OAC","Cybersecurity Innovation","07/01/2021","06/11/2021","Shouhuai Xu","CO","University of Colorado at Colorado Springs","Standard Grant","Robert Beverly","06/30/2024","$499,094.00","Charles Benight, Yanyan Zhuang","sxu@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","8027","7923, 8027, 9102","$0.00","Citizens live in a world of prevalent adversities, ranging from childhood trauma, to domestic violence, war, flooding, hurricanes, wildfires, terrorist attacks, acute physical injuries, motor vehicle accidents, and pandemics. These adversities affect many lives; for example, during the past decade alone, unprecedented flooding, catastrophic hurricanes, and devastating wildfires destroyed thousands of homes, vital infrastructure, and communities. Citizens often struggle to cope with these adversities due to the trauma experienced during these events as well as the extensive recovery stress. The Human Resilience research community conducts scientific studies to (i) understand how citizens suffer from these adversities and (ii) seek effective solutions to help citizens recover or bounce back from the stress and trauma caused by these adversities. The importance of Human Resilience research can hardly be overstated because of the large population of citizens affected. Despite this clear importance, the scientific discovery process of Human Resilience research is hindered by the lack of cyberinfrastructure support for secure, privacy-preserving, and policy-complying data sharing that would foster collaborative research. This calls for solutions to modernizing and accelerating the scientific discovery process of Human Resilience research.<br/><br/>This project investigates a competent technical solution to modernizing and accelerating the scientific discovery process of Human Resilience research, by tackling a range of technical challenges including: (i) specifying the requirements of a competent cyberinfrastructure; (ii) defining a solution framework to adequately address these requirements; (iii) designing a comprehensive system architecture to fulfill the framework; (iv) investigating novel mechanisms and supporting techniques; and (v) developing a prototype system and demonstrating its usefulness. The project bolsters the scientific collaboration in the Human Resilience research community by encouraging the adoption of security and privacy into its unique scientific workflows and by pioneering a holistic security cyberinfrastructure environment spanning the entire Human Resilience research data-sharing ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931258","Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/07/2019","Edgar Solomonik","IL","University of Illinois at Urbana-Champaign","Standard Grant","Robert Beverly","09/30/2022","$954,820.00","Lucas Wagner","solomon2@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7925, 8004, 8009, 9216, 9263","$0.00","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies.  However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties.  This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.<br/><br/>This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201028","CICI: UCSS: Building a Community of Practice for Supporting Regulated Research","OAC","Cybersecurity Innovation","12/01/2021","11/01/2021","Carolyn Ellis","CA","University of California-San Diego","Standard Grant","Robert Beverly","12/31/2024","$499,781.00","","carolynEllis@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8027","7923, 8027, 9102","$0.00","The daily news clearly shows the increasing threat to safety and privacy of data, personal as well as intellectual property. The Department of Defense modified the DFARS clause to mandate that NIST 800-171 be followed for data classified and marked as CUI in 2017. The next evolution of this program is called Cybersecurity Maturity Model Certification CMMC. Other agencies, for example, Department of Education, have indicated that they are considering following a similar path to safeguard data. While these requirements improve the consistency of data handling between agencies and contractors and grantees, it leaves academic institutions to figure out how to meet such requirements in a cost-effective way that fits the research and education mission of the institution. Most institutions, agencies, and companies act in isolation with one-off contract language to address data security and safeguarding concerns. The Regulated Research Community of Practice (RRCoP) builds a network of people able to help each other in implementing an affordable but effective cybersecurity and compliance program at academic institutions. Even though cybersecurity has a clear and uniform goal of protecting data, a onesize solution does not fit all academic institutions.<br/><br/>Through this project, RRCoP accomplishes 1) Developing cybersecurity training resources that share validated and diverse best-practices. 2) Establishing a leadership training and development program accelerating availability of distributed university resources 3) Developing representation through strategic partnerships with industry and government entities. By supporting this community with development of a community strategic roadmap, regular discussions and workshops, and a repository of generalized and specific resources for handling regulated research programs RRCoP lowers the barrier to entry for institutions handling new regulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004826","Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science","OAC","ANT Glaciology, Polar Cyberinfrastructure, Software Institutes, EarthCube","10/01/2020","08/03/2020","Jason Briner","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","09/30/2025","$3,522,878.00","Beata Csatho, Anton Schenk, Jeanette Sperhac, Kristin Poinar","jbriner@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","5116, 5407, 8004, 8074","026Z, 077Z, 7925, 8004","$0.00","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.<br/><br/>The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018627","MRI: RADiCAL: Reconfigurable Major Research Cyberinfrastructure for Advanced Computational Data Analytics and Machine Learning","OAC","Major Research Instrumentation","10/01/2020","07/17/2020","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Alejandro Suarez","09/30/2023","$770,000.00","Raghu Machiraju, Srinivasan Parthasarathy, Rajiv Ramnath, anil parwani","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","1189","075Z, 1189","$0.00","The analysis of high-resolution images in both two and three-dimensions is becoming important for many scientific areas, such as in medicine, astronomy and engineering. Discoveries in these disciplines often require analyzing millions of images. The analysis of these images is complex and requires many steps on powerful computers. Some of these steps require looking through lots of images while some of these steps require deep analysis of each image. In many cases, these analyses have to be completed quickly, i.e. in ""real-time"", so that information and insights can be provided to humans as they do their work. These kinds of operations require powerful computers consisting of many different, heterogeneous but simple computing components. These components need to be configured and reconfigured so that they can efficiently work together to do these large-scale analyses. In addition, the software that controls these computers also has to be intelligently designed so that these analyses can be run on the right types of configurations. This project aims to acquire the necessary computing components and assemble such a powerful computer (named RADiCAL). Research done using RADiCAL will result in important scientific discoveries that will make us more prosperous, improve our health, and enable us to better understand the world and universe around us. Doing this research will also educate many students, including those from under-represented groups, who will become part of a highly-trained workforce capable of addressing our nation's needs long into the future.<br/><br/>The intellectual merit of RADiCAL is in the design a novel, high-performance, next-generation, heterogeneous, reconfigurable hardware and software stack to provide real-time interaction, analytics, machine/deep learning (ML/DL) and computing support for disciplines that involve massive observational and/or simulation data. RADiCAL will be built from commodity hardware, and designed for reconfiguration and observability. RADiCAL will enable a comprehensive research agenda on software that will facilitate rapid and flexible construction of analytics workflows and their scalable execution. Specific software research include: 1) a library with support for storage and retrieval of multi-resolution, multi-dimensional datasets, 2) scalable learning and inference modules, 3) data analytics middleware systems, and 4) context-sensitive human-in-the-loop ML models and libraries that encode domain expertise, coupling tightly with both lower level layers and the hardware components to facilitate scalable analysis and explainability. With the proposed hardware acquisition and software research, the transformative goal will be to facilitate decision-making and discovery in Computational Fluid Dynamics (CFD) and medicine (pathology). With respect to broader impacts, RADiCAL will provide a unique research, testing, and training infrastructure that will catalyze research in multiple disciplines as well as facilitate convergent research across disciplines. The advanced imaging applications and techniques for expert-assisted image analysis will be broadly applicable to other human-in-the-loop systems and have the potential to advance medicine and health. Projects that use RADiCAL will also provide unique test-beds for valuable empirical research on human-computer interaction and software engineering best practices.  Well-established initiatives at The Ohio State University will facilitate the recruitment of graduate and undergraduate students from underrepresented groups for involvement in using the cyberinfrastructure. The heterogeneous and reconfigurable research instrument will be utilized to create sophisticated educational modules on how to co-design computational science experiments from the science goals to the underlying cyberinfrastructure. Tutorials and workshops will be organized at PEARC, Supercomputing and other conferences to share the research results and experience with the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2001385","Elements: The ThYme database and identifying representative amino acid sequences that originate thioester-active enzyme families","OAC","Cellular & Biochem Engineering, Special Initiatives, Software Institutes","07/01/2020","04/21/2020","David Cantu","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Seung-Jong Park","06/30/2023","$599,999.00","Tin Nguyen","dcantu@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","1491, 1642, 8004","077Z, 1491, 7923, 8004, 9150","$0.00","The ThYme database includes most enzymes involved in the formation of fatty acids and polyketides. These are ultimately converted into valuable products. Such products include cosmetics, detergents, insecticides, fungicides, antibiotics, and other medicinal compounds. The updated ThYme database will provide vital support to research related to these products. Metabolic engineers, plant biologists, natural products and medicinal chemists will all benefit from improved access to enzyme structure and function. High school students from underrepresented populations will be recruited and trained in various aspects of coding. They will then have the opportunity to work with graduate students and postdocs to contribute to the improvement of the platform. This should ultimately strengthen the STEM workforce in Nevada, and nationally.<br/><br/>The ThYme database contains most known sequences and structures of enzymes that act on thioesters, classified by sequence similarity into families. The advantage of classifying enzymes by sequence similarity is that one can infer that all enzymes in a family will have very similar structures and nearly identical catalytic residues and mechanisms. The goal of this project is to launch a new and updated ThYme database by identifying the current families of thioester active enzymes, developing a new approach to identify representative sequences, improving the database management scheme, and modernizing the online user interface. We will develop an efficient method, using submodular functions, to select the representative sequence(s) of an enzyme family among sequences experimentally verified to have a particular enzymatic activity. The database will be disseminated by a website, where every enzyme family will have its own webpage including relevant knowledge and an open query field where users will be able to search by organism, sequence accession code, function, name, sequence, or crystal structure. With input from the user community, the new website will have features to make the content more interactive and allow automated data querying. The user community will be engaged and supported with a forum page to pose questions and begin discussions to which both developers and users can contribute, as well as with tutorials of useful website functionalities.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the ivision of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003725","Elements: FLARE infrastructure for reproducible active learning of Bayesian force fields for ex-machina exascale molecular dynamics","OAC","Special Initiatives, DMR SHORT TERM SUPPORT, Software Institutes","07/01/2020","04/20/2020","Boris Kozinsky","MA","Harvard University","Standard Grant","Seung-Jong Park","06/30/2022","$379,050.00","","bkoz@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","1642, 1712, 8004","054Z, 075Z, 077Z, 079Z, 7923, 8004, 8396, 8399, 8604, 9216","$0.00","Much needed progress in technologies for energy storage and conversion relies on our ability to design and understand next-generation key functional materials at the core of these systems. The fundamental physical effects that govern the functions of batteries, catalysts and fuel cells originate at the atomic level. Molecular dynamics simulations are indispensable tools with broad applicability for materials research due to their ability to probe microscopic details of atomic motion and predict thermodynamics, reaction kinetics and ionic diffusivities of many materials. Machine learning approaches are transforming how simulations of complex materials are performed; hence software tools are needed to make this transition faster and smoother. Our main goal is to advance machine learning methods and create software for constructing accurate fast simulation models that contain principled uncertainty of their predictions, which is a highly desirable target in many data science areas beyond atomistic modeling. Principled uncertainty quantification is especially critical for prediction of non-equilibrium dynamics, where rare important events, such as the breaking of bonds or atomic migration, determine the material?s performance but involve atomic configurations that are unlikely to be in a naive unbiased training set. Tools that we aim to develop will accelerate and automate computational research efforts in the fields of catalysis, batteries, thermal coatings, soft structural and functional materials, and actuators, to name a few.<br/><br/>Currently available tools for machine learning based force fields are based on non-parametric methods that only provide estimates without uncertainties, require large amounts of training data, and are slow to evaluate for large numbers of atoms of different species. Our goal is to develop community software infrastructure to enable a new paradigm of simulating non-equilibrium dynamics of complex materials, where ML models are automatically trained and dramatically accelerate ab-initio simulations on-the-fly, preserving exact physical symmetries with minimal accuracy loss. We will create and freely disseminate FLARE (Fast Learning of Atomistic Rare Events), a parallelized database-driven automation framework tightly coupling ML model training with high-fidelity DFT computations, using rigorous model uncertainty to guide data acquisition via closed-loop active learning. Specially designed many-body multi-species kernels and tools for systematic hyperparameter optimization will allow the models to be mapped to fast tabulated Bayesian force fields, implemented in the widely used MD software aimed at exascale computing performance. The result will be the ability to perform MD simulations of materials systems of millions of atoms at near-DFT accuracy and with predictive uncertainty. The unique advantages of the proposed infrastructure are (1) the automated training requiring minimal amounts of DFT data, (2) predictions containing principled Bayesian uncertainty, (3) scalable performance of at least 5 orders of magnitude faster than ab-initio molecular dynamics, and (4) ability to record full provenance and reproducibility information of training and prediction workflows.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940093","Collaborative Research: Precision Learning: Data-Driven Experimentation of Learning Theories using Internet-of-Videos","OAC","HDR-Harnessing the Data Revolu, IUSE","10/01/2019","09/17/2019","Xintao Wu","AR","University of Arkansas","Standard Grant","Finbarr Sloane","09/30/2022","$400,000.00","","xintaowu@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","099Y, 1998","062Z, 7645, 9178","$0.00","This is a project to study what works to help students learn more effectively in the context of the ASSISTments system. ASSISTments is an online system that provides both assistance to students and real time assessment data to teachers. ASSISTments now supports 100,000 students who have completed more than 12 million mathematics problems. The system uses teacher input and artificial intelligence to provide assistance to students who are attempting to solve mathematics problems. This project will increase the assistance provided by the teacher and machine learning by incorporating video suggestions, such as those produced by the Kahn academy, targeted to the needs of the student. The experimentation will take content from three Open Educational Resource textbooks that are openly licensed and free to schools.<br/><br/>More specifically, the researchers will identify a large collection of videos that address mathematics skills in the textbooks and will extract features of these videos including language complexity, speaking rate, and other features. These videos and features will be checked by both teachers and through a Mechanical Turk process for usability before they are presented to students. Additionally, the project will develop a suite of novel technologies for precision learning including fine grained video feature extraction, student feature learning from heterogeneous raw data, causal modeling, and fairness aware and causal relationship enhanced optimized personalized recommendation. The research will advance theoretical understanding of fundamental issues related to personalized learning and will enable data-driven experimentation of learning theories. Causal modeling will enable the researchers to learn the features of video that are correlated with learning effectiveness. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity and is co-funded by the Division of Undergraduate Education and the Division of Research on Learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934790","Collaborative Research: Near Term Forecasts of Global Plant Distribution, Community Structure, and Ecosystem Function","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Brian Enquist","AZ","University of Arizona","Continuing Grant","Peter McCartney","08/31/2022","$966,186.00","Nirav Merchant","benquist@u.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","099Y, 7231","062Z, 7231","$0.00","This project is the first to explore how plant species distributions across the entire globe may respond to global change. The project brings together ecologists, environmental engineers, data scientists, and conservation stakeholders to determine optimal ways to integrate these data sources to make near term forecasts for all plants globally by addressing changes in (1) species' abundance and geographic distribution, (2) community structure, and (3) ecosystem function. This three-pronged approach is designed to span a range of approaches to understand the spectrum of possible futures consistent with current knowledge while integrating knowledge across scales of biological organization. These forecasts will be used along with input from conservation stakeholders to assess how differing conservation decisions can minimize the impacts of global change responses. An ultimate goal of the project is to automate a pipeline to ingest new incoming data, update forecasts, and serve these to end-users to enable a near-real time forecasting workflow to provide best-available predictions at any given time to inform conservation decisions. <br/><br/>A key aspect of these forecasts is their reliance on novel environmental information that better characterize the conditions that influence plant performance, including soil moisture and extreme weather events based on NASA satellite observations. These species-level predictions will be linked to community demography models that integrate a variety of relatively untapped data sources for understanding global change, including plant trait data, community plot data across the globe, highly detailed plot data from National Ecological Observatory Network (NEON) and Long Term Ecological Research (LTER) sites, and global biomass data from NASA's Global Ecosystem Dynamics Investigation (GEDI) mission. By integrating this wide variety of data sources, the mechanistic understanding needed to make robust near term forecasts can be made, to understand ecosystem properties like Net Primary productivity, Carbon stock, and resilience. Based on workshops with conservation stakeholders, researchers will determine how best to use this unique suite of forecasts to best inform different conservation questions in different regions of the world. The project will also result in an open, cleaned and curated database on global plant distributions. This will aid others in exploring data and predictions by delivering and visualizing complex future scenarios in an easy to use portal. All results of the project can be found at the website for the Biodiversity Informatics and Forecasting Institute or BIFI, at https://enquistlab.github.io/BIFI .<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931587","Collaborative Research: Frameworks: Cyberloop for Accelerated Bionanomaterials Design","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/09/2019","Hendrik Heinz","CO","University of Colorado at Boulder","Standard Grant","Amy Walton","09/30/2023","$620,000.00","","hendrik.heinz@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7237, 7295, 7925, 8004, 8614, 9216, 9263","$0.00","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.<br/><br/>Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837915","PEPIG: Empowering Academic Partnership with Federal Safety and Security Priorities","OAC","","10/01/2018","10/15/2020","Renata Rawlings-Goss","GA","Georgia Tech Research Corporation","Standard Grant","Martin Halbert","09/30/2022","$120,650.00","","rrawlingsgoss3@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","R275, U284","7484, 8237","$0.00","There is a growing shortage of people with data science expertise working in federal agencies.  This project will help overcome this problem, by providing fellowship opportunities to faculty, postdocs, research scientist, graduate student, and senior undergraduates to carry out research in data science priority areas within the Department of Homeland Security (DHS).  Participant will be co-located with DHS mentors and staff. The program broadly benefits the data ecosystem by serving as a backbone for research collaboration on pressing United states safety and security concerns.<br/><br/>The goals of this program are to support academics in developing industry and government relationships for research collaboration and career success; prepare students for the workforce landscape in data science, computing, and Big Data research; and create a diverse Big Data pipeline from university research through to real-world impact in government service.  The proposed program is modeled after the innovative and successful Big Data Regional Innovation South Hub's Program to Empower Partnerships with Industry and Government (PEPI-G) program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931366","Elements: Libra: The Modular Software for Nonadiabatic and Quantum Dynamics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","01/01/2020","09/03/2019","Alexey Akimov","NY","SUNY at Buffalo","Standard Grant","Seung-Jong Park","12/31/2022","$449,546.00","","alexeyak@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1253, 8004","026Z, 077Z, 7923, 8004, 8005","$0.00","This project endeavors to develop a modular open-source software library (Libra) of reusable nonadiabatic and quantum dynamics (NA/QD) algorithms and methods (elements). Sustained progress in scientific endeavors in solar energy, functional, and nanoscale material sciences requires advanced methods and software components that can be used to model the complex dynamics of excited states, including charge and energy transfer. Providing researchers with advanced expert-developed methods for modeling these processes via modular software components can enable new breakthroughs in theoretical and computational chemistry, computationally-enabled and data-driven material sciences, and can foster further exciting innovations in the solar energy materials domain and beyond. The Libra software will enable accurate, reliable, and efficient modeling of excited states dynamics in atomistic systems and should be suitable for the rapid and systematic development of new, improved modeling approaches. The project will contribute to a broader specialized scientific training and will support education and diversity.<br/><br/>Over the course of this project the PI will enhance and extend the Libra code with modern nonadiabatic and quantum dynamics methodologies, thereby enabling accurate and reliable modeling of electron and energy transfer dynamics in solar energy materials. The interface of the Libra code with the DFTB+ package will enable modeling excited states dynamics in molecular and periodic systems with 1000+ atoms, thus providing access to new classes of materials that can be studied computationally. The resulting software will enable modeling new types of processes which were computationally-prohibitive to study, such as photoinduced reorganization or exciton trapping in nanoscale systems. The software will enable accounting for the many-body effects in nonadiabatic dynamics, improving the reliability of computational predictions in solar energy materials studies. The software and tools resulting from this project will contribute to fundamental research and rational discovery of novel photovoltaic materials. The large research community of NA/QD users will benefit from the new, enhanced software and online educational materials created in this project. Multiple research groups that develop in-house solutions that did not gain much attention will directly benefit from the Libra library, which will disseminate the expert groups' solutions in a modular, easy-to-use way, and will facilitate their adoption and re-use by others. The project will foster collaborations and help integrate the present-day research efforts and the best software development practices into the computational and materials research communities. Scholars of various levels will be educated during workshops on excited state dynamics.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931380","Elements: Development of Assumption-Free Parallel Data Curing Service for Robust Machine Learning and Statistical Predictions","OAC","Software Institutes, EPSCoR Co-Funding","09/01/2019","07/23/2019","In Ho Cho","IA","Iowa State University","Standard Grant","Amy Walton","08/31/2022","$592,386.00","Jae-Kwang Kim","icho@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8004, 9150","075Z, 077Z, 7923, 9150","$0.00","Large, incomplete datasets create major challenges for statistical prediction in research.  This project will develop a data curing service that is able to manage large, incomplete, and diverse datasets, and would provide uncertainty measures for the cured data.  The project identifies and collaborates with several communities where this data service is central to scientific research, including civil engineering, building science, urban energy, and social science. <br/><br/>The effort creates a parallel data curing service, provides uncertainty measures for the cured data, and develops supplementary imputing algorithms.  The team develops a data curing platform with imputation for incomplete, heterogeneous data; robust machine learning (ML) and statistical predictions would be established by developing an easy-to-use, general-purpose, large data-friendly imputation program.  The focus is on a novel combination of three established imputation methods: two-level finite mixture model-based imputation (FMMI), fractional hot deck imputation (FHDI), and Gaussian mixture model-based imputation (GMMI), for which parallel implementations in R would also be provided. <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly funded by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1927880","Category II : Ookami: A high-productivity path to frontiers of scientific discovery enabled by exascale system technologies","OAC","Innovative HPC","10/01/2019","08/18/2021","Robert Harrison","NY","SUNY at Stony Brook","Cooperative Agreement","Robert Chadduck","09/30/2024","$5,560,745.00","Barbara Chapman, Matthew Jones, Alan Calder","robert.harrison@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7619","","$0.00","The State University of New York proposes to procure and operate for at least four years the first computer outside of Japan with the A64fx processor developed by Fujitsu for the Japanese path to exascale computing (i.e., computers capable of 10^18 operations per second). The ARM-based, multi-core, 512-bit SIMD-vector processor with ultrahigh-bandwidth memory promises to retain familiar and successful programming models while achieving very high performance for a wide range of applications including simulation and big data.  The testbed significantly extends current NSF-sponsored HPC technologies and will enable the community to evaluate and demonstrate the potential of this technology for deployment in multiple settings. Through integration with NSF's Extreme Science and Engineering Discovery Environment (XSEDE), the system will be widely accessible and fully leverages existing cyber infrastructure including the XDMoD monitoring system.<br/><br/>What does this mean for science? Compared with the best CPUs anticipated during the deployment period, A64fx offers 2-4x better performance on memory-intensive applications such as sparse-matrix solvers found in many engineering and physics codes. For nearly all other applications, performance is also better or competitive. This transformational performance should be available nearly out of the box, with additional performance possible from tuning. To the scientist or engineer this means faster time to solution with significantly less programmer effort. The target class of applications to be enabled are memory-bandwidth intensive with 32GB/node memory, with significant gains anticipated for many other applications. Analysis of XSEDE workload shows 86% of all jobs (85% cycles) will fit within the available memory per node and that the majority of jobs are memory-bandwidth intensive. Finally, we have concrete plans to substantially broaden participation in science and engineering research by partnering with external organizations at the institutional, regional, and national levels.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923791","Collaborative Proposal: CyberTraining: Pilot: Aligning Learning Materials with Curriculum Standards to Integrate Parallel and Distributed Computing Topics in Early CS Education","OAC","CyberTraining - Training-based","08/01/2019","06/10/2019","Jamie Payton","PA","Temple University","Standard Grant","Alan Sussman","07/31/2022","$49,930.00","","jamie.payton@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","044Y","026Z, 9102","$0.00","Driven by recent trends in computing hardware and architectures, the project investigators are working with the larger community of computer science educators to introduce parallel computing concepts into early courses in computer science.  A smart repository of available learning materials (e.g., lectures, exercises, assignments, assessments) is being developed that enables computer science educators to easily find and adopt learning materials developed by other educators. Educators are being trained to work with the investigators and the repository system.  The project serves the national interest as stated by NSF's mission: to promote the progress of science, by incorporating current and critical concepts in parallel computing at an early stage in a way that is available for all computer science students.<br/><br/>The project takes a scalable and robust approach to building a large collection of materials from a diverse group of instructors and institutions. This pilot study is taking the needed initial steps to reaching this goal: obtaining a sound understanding of the current materials and curricula used in early computer science and parallel computing courses and enable their reuse.  To achieve this goal, the project develops a system to classify learning materials against two national standards: the 2013 ACM Computer Science Curriculum Guidelines and the 2012 NSF/IEEE-TCPP Parallel and Distributed Computing curriculum. The system aims to support advanced search features for educational materials, help identify similarity and differences between large sets of materials, and reveal potential gaps in existing materials. The system is accessible through a web interface to maximize adoption by CS educators looking to introduce parallel computing concepts, especially those in early CS courses. The project engages early users of the system coming from the Computer Science Education community and the Parallel Computing community through a series of workshops held at various computer science education community events and parallel computing community events.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925960","CAREER: Towards Fast and Scalable Algorithms for Big Proteogenomics Data Analytics","OAC","CAREER: FACULTY EARLY CAR DEV, Software & Hardware Foundation, Computational Biology","09/01/2018","03/28/2019","Fahad Saeed","FL","Florida International University","Standard Grant","Alan Sussman","03/31/2022","$415,950.00","","FSAEED@FIU.EDU","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","1045, 7798, 7931","1045, 7931, 7942, 9102","$0.00","Proteogenomics studies require combination and integration of mass spectrometry data (MS) for proteomics and next generation sequencing (NGS) data for genomics.  This integration drastically increases the size of the data sets that need to be analyzed to make biological conclusions.  However, existing tools yield low accuracy and exhibit poor scalability for big proteogenomics data.  This CAREER grant is expected to lay a foundation for fast algorithmic and high performance computing solutions suitable for analyzing big proteogenomics data sets.  Design of accurate computational algorithms suitable for peta-scale data sets will be pursued and the software implementation will run on massively parallel supercomputers and graphical processing units.  The direction in this CAREER proposal is towards designing and building infrastructure, which would be useful for the broadest biological and ecological community.  A comprehensive interdisciplinary education will be executed for K12, undergraduate and graduate students to ensure that US retains its global leadership position in STEM fields.  This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The goal of the proposed CAREER grant is to design and develop algorithmic and high performance computing (HPC) foundations for practical sublinear and parallel algorithms for big proteogenomics data - especially for non-model organisms with previously unsequenced or partially sequenced genomes.  Integration of MS and NGS data sets required for proteogenomics studies exhibit enormous volume and velocity of data: NGS technologies such as Chip-Seq can generate tera-bytes of DNA/RNA data and mass spectrometers can generate millions of spectra (with thousand of peak per spectra).  The current systems for analyzing MS data are mainly driven by heuristic practices and do not scale well.  This CAREER proposal will explore a new class of reductive algorithms for analysis of MS data that can allow peptide deductions in sublinear time, compression algorithms that operate in sub-linear space, and denovo algorithms that operate on lossy reduced-form of the MS data.  Novel low-complexity sampling and reductive algorithms that can exploit the sparsity of MS data such as non-uniform FFT based convolution kernels can lead to superior similarity metrics not prone to spurious correlations.  The bottleneck in large system-biology studies is the low-scalability of coarse-grained parallel algorithms that do not exploit MS-specific data characteristics and lead to unbalanced loads due to non-uniform compute time required for peptide deductions.  This project aims to explore design and implementation of scalable algorithms for both NGS and MS data on multicore and GPU platforms using domain decomposition techniques based on spectral clustering, MS-specific hybrid load-balancing based on work-load estimate, and HPC dimensionality reduction strategies and novel out-of-core sketching & streaming fine-grained parallel algorithms.  These HPC solutions can enable previously impractical proteogenomics projects and allow biologists to perform computational experiments without needing expensive hardware.  All of the implemented algorithms will be made available as open-source code interfaced with Galaxy framework to ensure maximum impact in systems biology labs.  These designed techniques will then be integrated so that matching of spectra to RNA-Seq data can be accomplished without a reconstructed transcriptome.  The proposed tools aim to reveal new biological insight such as novel genes, proteins and PTM's and are crucial steps towards understanding the genomic, proteomic and evolutionary aspects of species in the tree of life."
"1835674","Collaborative Research: Elements:Software:NSCI: Chrono - An Open-Source Simulation Platform for Computational Dynamics Problems","OAC","Dynamics, Control and System D, Software Institutes","08/01/2019","07/17/2019","Dan Negrut","WI","University of Wisconsin-Madison","Standard Grant","Robert Beverly","07/31/2022","$529,484.00","Radu Serban","negrut@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7569, 8004","026Z, 034E, 077Z, 7923, 8004","$0.00","This project seeks to augment modeling and solution methods employed by Chrono, an open-source computer simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. Chrono will be able to capture dynamics at various size and time scales spanning from millisecond (impact phenomena) to decades (geophysics). These performance levels open up new directions of research in several fields. Chrono is widely used and further developed by other users and has an active forum with more than 250 registered users currently. This project will enhance the richness of Chrono's modeling features, sound numerical solution foundation, and leverage of emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform. Chrono has been used by universities, national labs, and industry. Over the past two years, various groups have used Chrono in extraterrestrial applications, machine learning in robotics, image processing, pattern recognition and computer vision, mechanical watch design, architectural studies, autonomous vehicles, fluid-solid interaction applications, wind turbine dynamics, next generation space suit design, oil extraction and accident mitigation, hardware-in-the-loop simulation, etc. Finally, this project will engage high-school students from under-represented groups in a six-day residential camp run (now at its 12th edition) and will train a group of undergraduate students from California State University at University of Wisconsin-Madison through a new residential program that will introduce them to the use of Chrono in simulation-based robotics design.<br/><br/>This project seeks to augment modeling and solution methods employed by Chrono, a BSD3 open-source simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. The software infrastructure enhancements in this project aim at sustaining teraflops-grade simulation of MBD and FSI systems with more than ten billion degrees of freedom; i.e., two to three orders of magnitude beyond conventional simulations today. In order to increase adoption and impact, the performance levels aimed at will be reached on budget/affordable hardware that leverages GPU computing. Chrono will be able to capture micro-, meso- and macro-scale dynamics on time scales spanning from millisecond (impact phenomena) to decades (geophysics). The intellectual merit of this project stems from the following key ideas: (i) with an eye towards the sunsetting of Moore's law, the software design solution embraces a scalable multi-GPU hardware layout poised to solve effectively large multi-physics problems; (ii) a hardware-aware software design paradigm, which aggressively reduces data storage and movement, will allow budget-conscious hardware systems to run billion-degree-of-freedom models, or, for models of similar size, accomplish a two orders of magnitude speedup when compared to the state of the art; (iii) a unified Lagrangian formulation for both solid and fluid dynamics is implemented in one software platform that can simulate complex multi-physics (coupled) problems; and (iv) Chrono promotes an alternative approach for handling friction and contact that revolves around the concept of differential variational inequality and thus avoids the small integration time step and numerical instability issues that hinder most of the existing many-body dynamics simulators. In relation to its educational and outreach initiatives, this project: (a) will be instrumental in establishing a new University of Wisconsin-Madison undergraduate course that introduces students to computing concepts subsequently refined in a graduate advanced computing class; (b) will promote the discipline of Computational Science and Computational Dynamics at high-school and undergraduate levels via two yearly residential summer programs for under-represented students; (c) will expand an advanced computing forum that facilitates technology transfer to industry and promotes Chrono adoption; and, (d) will strengthen ongoing collaborations that critically depend on Chrono in robotics, geomechanics, and soft-matter physics. Chrono is presently cloned on average 10 times every day, has been forked from its public repository by more than 150 parties, and has an active forum with more than 250 registered users. This project will enhance the richness of Chrono modeling features, improve its numerical solution foundation, and leverage emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841448","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Michael Hildreth","IN","University of Notre Dame","Standard Grant","Bogdan Mihaila","09/30/2022","$422,981.00","","hildreth.2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835673","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","09/26/2018","Wolfgang Bangerth","CO","Colorado State University","Standard Grant","Seung-Jong Park","09/30/2023","$1,000,000.00","","bangerth@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812675","Collaborative Research: CESER: EAGER: ""FabWave"" - A Pilot Manufacturing Cyberinfrastructure for Shareable Access to Information Rich Product Manufacturing Data","OAC","CM - Cybermanufacturing System, CESER-Cyberinfrastructure for","04/01/2018","03/26/2018","Yong Chen","CA","University of Southern California","Standard Grant","William Miller","03/31/2020","$108,625.00","","yongchen@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","018Y, 7684","016Z, 067E, 152E, 7916, MANU","$0.00","Advanced manufacturing research is dependent on access to large datasets of product models to enable product designers to learn from past errors, and to discover and develop new solutions. However, such datasets are typically archived in inaccessible repositories and may be poorly described and difficult to use by others. Current manufacturing research must shift from siloed repositories of product manufacturing data to a federated, decentralized, open and inter-operable approach. This transformation can be achieved by embedding cyber-capability in every physical end-point, be it on a desktop used by a product designer or within the control systems of a manufacturing machine. The goal of this project is to develop a pilot manufacturing-focused cyberinfrastructure called FabWave. FabWave will gather together digital data on product and manufacturing processes from diverse sources, in particular, three-dimensional (3D) computer aided design (CAD) models generated by academia and public community users, into a new rich dataset that, in turn, will be fully accessible to manufacturing researchers and others. FabWave's system design will be driven by the needs of the manufacturing research community, and the project will encourage its adoption by academic users, government labs and the public. FabWave will aim to lower the barriers for users to upload, share and download 3D product model data; it will also enable manufacturing science researchers to gain access to an information rich dataset for testing and evaluation of algorithms to advance manufacturing research and offer the capability for the community to build custom apps that provide services to the community. This infrastructure development project aligns with the NSF Harnessing Data for 21st Century Science and Engineering Big Ideas vision and can serve to improve US competitiveness in advanced manufacturing.<br/><br/>The de novo cyberinfrastructure tools to be built in the FabWave project include: 1) Direct plugins within design software to allow easier capture of metadata and upload of 3D product models to the FabWave repository with limited human interaction; 2) a data-store with application programming interface (API) tools to allow users to write scripts to search through the repository for content specific to their research questions. Innovative use of the Inter-Planetary File System (IPFS) protocols along with next generation unstructured databases will serve to index and enable search services for Fabwave's users. the project will demonstrate use of FabWave for development of community facing third-party apps and libraries. Ultimately, the research community will have access to a rich source of product datasets created within research laboratories, within classes and through other open sourced projects. This data network, if expanded across universities, would accelerate advancements in cybermanufacturing, engineering systems design, and cyber-physical systems in manufacturing. In addition, FabWave will enable new areas of research in manufacturing information integration and informatics combined with reducing the barriers to exploring the science of manufacturing machine intelligence. This award is supported by the Division of Civil, Mechanical and Manufacturing Innovation and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761780","Spokes: SMALL: NORTHEAST: Collaborative: Building the Community to Address Data Integration of the Ecological Long Tail","OAC","BD Spokes -Big Data Regional I","09/15/2018","09/06/2018","Holly Ewing","ME","Bates College","Standard Grant","Martin Halbert","08/31/2022","$16,723.00","","hewing@bates.edu","2 Andrews Road","Lewiston","ME","042406028","2077868375","CSE","024Y","028Z, 8083","$0.00","Frequently research on data integration carried out by computer scientists and resulting tools must be modified to fit the needs of domain practitioners (ecologists in this case). This challenge is a socio-technical, collective action problem that can be addressed through a combination of tools and incentives. The project proposes to holding a series of workshops along with proofs-of-concept implementations. These workshops will result in approaches to decentralize the sharing of data in the long tail, through socio-technical approaches that appropriately incentivize and facilitate data integration by smaller labs. Such an interdisciplinary community will provide crucial real-world input to computer science researchers, which will give their research into tools the potential for larger impact in ecological practice and will yield better tools for ecologists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839022","Encouraging Data Sharing and Reuse in the Field of Collective Behavior through Hackathon-Style Collaborative Workshops","OAC","NSF Public Access Initiative","10/01/2018","11/05/2021","James Curley","TX","University of Texas at Austin","Standard Grant","Martin Halbert","09/30/2022","$25,000.00","","curley@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7414","7556","$0.00","The investigators will bring together diverse researchers who work in the field of collective and emergent behavior. Collective and emergent behavior is the study of complex biological and social systems, ranging from bacterial colonies to human groups.  The hackathon-style workshop draws researchers around identifying best practice mechanisms for sharing data, communicating methods of data analysis, and reusing publicly available data.   The investigators propose a series of two workshops where teams of 2-5 participants work on a specific project during the duration of the 3-day event. An objective of the workshop is to foster novel collaborations between researchers in biology, data science, mathematics, computer science and physics, all of whom have an interest in collective behavior.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841598","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Derek Fox","PA","Pennsylvania State Univ University Park","Standard Grant","William Miller","09/30/2020","$33,712.00","Chad Hanna","dfox@astro.psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807740","Collaborative Research: CDS&E: ReaxFF2: Efficient and Scalable Methods for Long-time Reactive Molecular Dynamics Simulations","OAC","DMR SHORT TERM SUPPORT, Chem Thry, Mdls & Cmptnl Mthds, CDS&E","09/01/2018","08/31/2018","Adri van Duin","PA","Pennsylvania State Univ University Park","Standard Grant","Tevfik Kosar","08/31/2022","$200,000.00","","acv13@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1712, 6881, 8084","026Z, 054Z, 7926, 8084, 9216, 9263","$0.00","This project aims to enable long-time simulations of reactive molecular systems through efficient<br/>and scalable techniques. Long-time reactive simulations are critical for several scientific problems<br/>such as catalysis, battery interfaces, biological simulations involving water, and emerging<br/>areas like surface oxidation and chemical vapor deposition (CVD) growth. However, progress on<br/>these fronts is limited because long-time simulations of large-scale systems are very difficult, if<br/>not impossible, to perform using existing methods. The Reactive Force Field (ReaxFF) method is in principle  <br/>ideally suited for this purpose. However, the short time steps required in current ReaxFF simulations <br/>and the computationally expensive force field formulation limit ReaxFF's temporal capabilities to <br/>narrow simulation time ranges. This project aims to overcome such limitations by creating ReaxFF2,<br/>which will extend time scales by one to two orders of magnitude - thus making large-scale, <br/>long-time RMD simulations accessible to a wide community. Codes developed will be made publicly <br/>available and results from this project will be highlighted on a dedicated website, and they will also be <br/>incorporated into workshops by the PIs.<br/><br/>In creating ReaxFF2, the PIs will enhance the Reax force field formulation significantly, and develop <br/>innovative algorithms and software implementations for scalable simulations. More specifically, <br/>alternative ReaxFF interactions will be formulated to eliminate sharp derivatives in energy terms <br/>and enhance ReaxFF time step lengths by at least a factor of four. To accelerate the dynamic charge <br/>distribution models needed in RMD, scalable parallel preconditioning techniques for the iterative <br/>solvers will be developed. A task parallel approach to compute interactions, hierarchical problem <br/>decomposition, vectorization of the key kernels, and use of mixed precision arithmetics constitute<br/>the main techniques that will be utilized to fully leverage the performance capabilities of large<br/>computer clusters. Finally, capabilities of accelerated RMD concepts in the proposed ReaxFF2 <br/>formulation will be evaluated and inlined trajectory analysis tools for RMD will be developed <br/>to facilitate the study of long-time RMD simulations. This project will significantly enhance the PIs' <br/>software development, community building,  and sustenance efforts for the RMD community. Codes, <br/>functional forms, and parameter sets developed will be made publicly available, enabling fast and <br/>accurate modeling of diverse reactive systems beyond the scope of this project. For community outreach, <br/>results from this project will be highlighted on a dedicated website, and they will also be incorporated <br/>into workshops by the PIs.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research <br/>and the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018556","CC* CRIA: Planning a Regional Cyber-Infrastructure-Research Consortium for Middle Tennessee","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","Anthony Skjellum","TN","University of Tennessee Chattanooga","Continuing Grant","Kevin Thompson","12/31/2022","$249,713.00","Ryan Otter, Gerald Gannod, Farah Kandah, Sheikh Ghafoor","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","7231, 8080","","$0.00","This CC* planning and research alignment project (CRIA) project is led by researchers from the University of Tennessee at Chattanooga (UTC), Tennessee Technological University (TTU), and Middle Tennessee State University (MTSU). The mission is to build a regional Cyberinfrastructure consortium to advance collaborative computing research in Middle Tennessee, building on the team's expertise in computer and cyber systems. Middle Tennessee is an underserved but growing region of the nation that combines rural, suburban, and urban areas and sits at a nexus of major transportation routes and commerce in the Southeastern United States. The project is also improving regional workforce development, devising and implementing an innovative plan to broaden participation in computing in the surrounding region, and building interdisciplinary proposal teams to seek external funding to support educational and research activities of consortium members. <br/> <br/>UTC, TTU, and MTSU have complementary strengths further strengthened by consortium partners. MTSU, for example, has a strong agriculture component to their research endeavors UTC is the strongest of the three teaming institutions in computational science. Sharing information, cyber resources, expertise, and forming collaborative groups will enhance personnel at each member campus.  A major goal of this CRIA project is to intensively study approaches to broadening participation in computer/computational science in order to develop and implement a regional plan for recruitment, engagement, and retention of students from underrepresented groups. The team is also committed to improving regional workforce development, which will have a broader impact on students within the teaming institutions and the region.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925558","CC* Compute: Triton Stratus","OAC","Campus Cyberinfrastructure","07/15/2019","07/15/2019","Ronald Hawkins","CA","University of California-San Diego","Standard Grant","Kevin Thompson","09/30/2021","$399,514.00","Mary Thomas, Robert Sinkovits, Subhashini Sivagnanam","rbhawkins@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8080","","$0.00","The University of California, San Diego, deploys Triton Stratus, an addition to its existing high performance computing system that allows campus researchers in many schools and departments to apply computational methods to their scientific research.  Triton Stratus helps researchers advance their scientific aims by providing improved facilities for accessing emerging computing tools and scaling them to commercial cloud computing resources.  Researchers, especially data scientists, are increasingly using tools such as Jupyter notebooks and RStudio to implement computational and data analysis functions and workflows. Jupyter notebooks provide a web-browser-based environment that permits the assembly of text, graphics, and computing functions into a living laboratory notebook, promoting research documentation and reproducibility. RStudio is another tool growing in popularity which provides a graphical environment for the R statistical language, widely used for data analysis.  These tools are part of a general trend in research computing towards web-based and graphical interfaces, especially for attracting newer generations of researchers and data scientists. Triton Stratus runs JupyterHub server and RStudio server, addressing the interactive computing needs of scientists from various domain sciences.<br/><br/>The project delivers a productive research computing capability serving investigators in different scientific domains.  Triton Stratus permits exploration of the emerging hybrid model of on-premise cluster computing resources coupled with commercial cloud computing services and will help answer questions such as the right balance of on-premise and cloud resources, best modes for scaling or bursting to cloud resources, and investigating new models of interactive research computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2100003","CI CoE: Demo Pilot: Advancing Research Computing and Data: Strategic Tools, Practices, and Professional Development","OAC","CYBERINFRASTRUCTURE","05/15/2021","05/19/2021","Dana Brunson","DC","INTERNET2","Standard Grant","Alan Sussman","04/30/2023","$1,497,243.00","Patrick Schmitz, Claire Mizumoto, Scott Yockel, Thomas Cheatham","dbrunson@internet2.edu","1150 18th St NW","Washington","DC","200363825","7349134264","CSE","7231","9102","$0.00","Research is increasingly dependent upon Cyberinfrastructure (CI), from instruments and sensors to Research Computing and Data (RCD) infrastructure and services. High Performance Computing (HPC) is an important element, however RCD has expanded well beyond HPC into secure enclaves for data compliance; big data management, analytics, and movement; AI/machine learning; and more recently into heterogeneous compute models, edge computing, and cloud-based computing. Researchers are struggling to keep pace with the explosion of data and the rapid evolution of computing resources, and often lack the skills to make full use of emerging tools and techniques. Developing and maintaining the needed expertise takes valuable time from research and is often beyond the capacity of researchers; they must depend upon partners (RCD professionals) who combine technology expertise and an understanding of research workflows and researcher needs. Institutions face challenges recruiting and retaining RCD professional staff, and building effective RCD support services. This CI CoE Demonstration Pilot (RCD CoE) will develop an RCD Resource and Career Center to advance and enable the nation's capacity for computational and data-intensive research by providing institutions and individuals the products, tools, services, and community to build and sustain successful RCD operations.  The project will also provide expertise in the development of new RCD professionals, and provide a voice and a connector for the broad community and the profession at large. <br/><br/>The RCD Resource and Career Center will build upon work that the NSF-supported Campus Research Computing Consortium (CaRCC) has done and continues to do to develop products and tools for effective RCD support. The Center will develop a robust and sustainable implementation of the RCD Capabilities Model that provides an assessment framework, support for benchmarking and strategic planning, and a community data set to understand the landscape of RCD support. The Center will expand and disseminate resources for RCD Professionalization, including support for adoption of an RCD Job Family Matrix to properly classify RCD professional roles and conducting a national census of RCD Professionals to characterize the current state of RCD staff and programs. The Center will create and disseminate a model of Career Arcs for RCD Professionals to explain career options and help existing RCD professionals explore professional development and advancement. Work in the project will aggregate resources for staff training and workforce development, including leading practices for recruitment, onboarding, advancing diversity, equity and inclusion (DEI), and professional development, as well as proven models for student internship and training programs. The pilot period and the community of communities will be used to facilitate the creation of a coordinating organization to connect all the relevant communities and to develop a shared voice for RCD professionals as the project plans for a full Center of Excellence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2008286","Collaborative Research: OAC Core: Small: Efficient and Policy-driven Burst Buffer Sharing","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","05/20/2020","Daniel Katz","IL","University of Illinois at Urbana-Champaign","Standard Grant","Robert Beverly","09/30/2022","$206,778.00","","dskatz@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","090Y","026Z, 7923","$0.00","Modern scientific research heavily relies on supercomputers. Supercomputing applications, such as traditional numerical simulations (HPC), data intensive applications (Big Data), and most recently, deep learning (DL) applications, are increasingly run on supercomputers to obtain timely results and explore new research methods that combine multiple application types. However, a bottleneck in their design reduces the potential performance of modern supercomputers. This project, bbThemis, addresses this problem by enabling efficient and policy-driven sharing of an intermediate storage layer known as a ""burst buffer"", so that more scientists and applications can leverage state-of-the-art storage techniques to significantly reduce their runtime and enhance the productivity of their research. This project will deliver substantial gains to almost every research area that uses HPC resources, leading to improved science and engineering methods and products in all fields. This research will have an immediate and significant impact on existing scientific applications and on deriving guidelines for next-generation HPC system design, deployment, and utilization. The project will also contribute to educational outcomes. In addition to students working directly on project goals, results developed in the project will be used in tutorial and training sessions at Texas Advanced Computing Center?s summer institute in deep learning and other major conferences, and in University of Illinois Urbana-Champaign student projects. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC.<br/><br/>This project, bbThemis (https://github.com/bbThemis), leverages a suite of technologies, such as disassociation of I/O processing from control logic, time-sliced intra I/O node sharing, function interception for low overhead POSIX I/O, and metadata and data placement for optimal individual application performance. It is investigating how to best apply these technologies, by: 1) Identifying optimal burst buffer configurations for a suite of representative supercomputing applications; 2) Proposing, prototyping, and verifying different design options to address intra-node and inter-node I/O performance sharing; and 3) Designing and evaluating a set of sharing policies, such as fair sharing and priority sharing, with real applications and I/O traces. This project will dramatically increase the sharing capacity of existing burst buffers and enhance domain scientists? productivity at a large scale.  It explores various sharing policies that permit efficient sharing of I/O resources and that meet the requirements of computing centers. The results will enable the provisioning of I/O resources, where users can request specific IOPS or bandwidth for a period of time.  The prototype burst buffer sharing framework will immediately increase the capacity of existing supercomputers with enhanced I/O performance. The lessons learned will guide next-generation I/O system design for large scale systems. The general improvement of HPC, Big Data, and DL applications will also increase the coherence of the hardware and software used for data analytics computing and modeling and simulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017309","Collaborative Research: CyberTraining: Implementation: Medium: Broadening Adoption of Parallel and Distributed Computing in Undergraduate Computer Science and Engineering Curricula","OAC","CyberTraining - Training-based","10/15/2020","07/22/2020","Sheikh Ghafoor","TN","Tennessee Technological University","Standard Grant","Almadena Chtchelkanova","09/30/2023","$64,220.00","","sghafoor@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","044Y","026Z","$0.00","This collaborative project represents a multi-faceted effort to shift computer science and engineering education toward ensuring that students can use 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly designed around a single processor, executing a sequence of operations. But this century is characterized by widespread deployment of multi-core, graphics, and AI tensor processors, as well as a shift to cloud servers, and the internet of things, all of which depend on the much different PDC approach to problem solving and programming. Financial, technical, scientific, engineering and medical companies, government labs, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the old model. Yet most students continue to learn the old approach due to significant inertia in academia. To turn the tide toward infusing PDC into the early stages of computer science and engineering education, this project will guide curricula and accreditation standards, prepare teachers, and foster a strong PDC education community. It will thus strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) is preparing the 2020 update of their 2013 curriculum guidelines for introducing parallel and distributed computing (PDC) into early undergraduate courses. This project will engage in four areas of activity to foster adoption of the curriculum, and extend it, with the goal of modernizing computer science and engineering workforce development.<br/>One major thrust is running summer training workshops for teachers, to learn both PDC concepts and experimental course evaluation methodology. The discipline is still in a phase of discovery with respect to PDC education approaches, and must encourage a diverse set of well-designed experiments to test and evaluate a broad range of pedagogical hypotheses. The workshop participants will be drawn from a diverse pool of educators, and given curriculum development grants in support of experimental course offerings and evaluation, leading to conference or journal publications, as well as contributions of exemplars to the CDER online course materials repository. <br/>A second effort is to help ABET/CSAB to formulate core PDC requirements and to inform/train ABET/CSAB evaluators (CSAB is the lead society within ABET for accreditation of degree programs in computer science). <br/>A third effort is expanding the curriculum guidelines to explicitly address adding PDC to computer engineering programs, which present novel curricular opportunities. <br/>Lastly, it will continue CDER's successes in organizing PDC education workshops in conjunction with major conferences, publishing PDC education books and journal special issues, maintaining and curating an online repository of PDC education resources, and providing free access to a publicly available PDC education cluster system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004311","Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","Software Institutes","07/01/2020","04/01/2020","Zachariah Etienne","WV","West Virginia University Research Corporation","Standard Grant","Amy Walton","06/30/2024","$335,902.00","","zbetienne@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","107y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940097","Collaborative Research: Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Carlos Fernandez Granda","NY","New York University","Continuing Grant","Pui Ho","09/30/2022","$320,017.00","","cfg3@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940233","Biology-guided Neural Networks for Discovering Phenotypic Traits","OAC","CYBERINFRASTRUCTURE","10/01/2019","10/15/2020","Jane Greenberg","PA","Drexel University","Continuing Grant","Peter McCartney","09/30/2022","$245,818.00","","janeg@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158956342","CSE","099y, 7231","1165, 7231","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934766","Collaborative Research: High-Dimensional Spatio-Temporal Data Science for a Resilient Power Grid: Towards Real-Time Integration of Synchrophasor Data","OAC","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2019","10/15/2020","Lalitha Sankar","AZ","Arizona State University","Continuing Grant","Donald Wunsch","08/31/2022","$1,330,040.00","Oliver Kosut, Anamitra Pal, Gautam Dasarathy, Christopher Bryan","lalithasankar@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","099y, 7607","062Z, 1653, 7607, 9102, 9251","$0.00","The project will establish an Institute at Arizona State University (ASU) with Texas A&M (TAMU) that considers the electric power grid and examines critical real-time decision-making by developing core data-driven science methods and applications.  This is motivated by the modern electric power system which is experiencing heightened unpredictability from increasing demand for renewable energy, efficiency, and resilience. To address this, industry stakeholders are deploying GPS-synchronized phasor measurement units (PMUs), or synchrophasors, that provide direct measurements of voltage and current phasors with high temporal granularity. However, the potential real-time situational awareness enabled by these measurements has been impeded by the massive scale of the time-series PMU data and have limited its use to passive, post-event forensics. The Institute meets this need for PMU-based real-time decision-making by examining five critical problems: (i) ensure data quality against bad, missing, or stale data; (ii) exploit the fine granularity of PMU data to track real-time changes in network parameters; (iii) detect, identify, localize, and visualize oscillation and failure events; (iv) assess and visualize cybersecurity threats and countermeasures specific to PMUs; and (v) create synthetic PMU datasets for testing and validation. The Institute leverages the PIs' synergistic multidisciplinary background in information sciences and statistics, machine learning, data visualization, cybersecurity, and power systems. The team will apply state-of-the-art techniques including hidden Markov models, LSTM neural networks, graphical models, errors-in-variables models, graph signal processing, adversarial examples, low-dimensional feature extraction, and constrained GANs. Another key research focus is the development of visual analytics for high-granularity spatio-temporal PMU data to enable improved operator review and decision-making. These innovations will be fueled by massive PMU datasets accessible to the PIs.<br/><br/>This Phase I institute has the potential to tip PMUs from a promising-but-mostly-underused resource into an essential part of power system best practices. The data science outcomes will impact application domains such as transportation networks, smart buildings, and manufacturing, each of which increasingly faces high-dimensional streaming data challenges. The PIs will disseminate their research to both academic and industry stakeholders and will continue their outreach on teaching AI and machine learning (ML) modules to underrepresented high school students. Finally, the multi-disciplinary strength of this institute lends itself naturally to a larger, integrated, and comprehensive Phase II institute focused on data-intensive research for critical infrastructure networks.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.  This effort is co-funded by the Division of Electrical, Communications and Cyber Systems within the Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940099","Collaborative Research: Integrating Physics and Generative Machine Learning Models for Inverse Materials Design","OAC","HDR-Harnessing the Data Revolu","10/01/2019","08/11/2020","Jianjun Hu","SC","University of South Carolina at Columbia","Continuing Grant","Daryl Hess","09/30/2022","$408,736.00","","jianjunh@cse.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","099Y","054Z, 062Z, 8396, 8399, 9150","$0.00","This project is aimed to address a grand challenge in data-intensive materials science and engineering to find better materials with desired properties, often with the goal to enhance performance in specific applications. This project addresses this grand challenge with a specific focus on finding metal organic framework (MOF) materials that are used to separate gas mixtures and finding better battery materials for energy storage. The PIs will combine theoretical methods from statistical mechanics and condensed-matter physics, and physics-based models, to generate information-rich materials data which is integrated with generative machine learning (ML) algorithms to search a complex chemical design space efficiently and to train deep learning models for fast screening of materials properties. This project will be carried out by a multidisciplinary collaboration involving researchers from physics, materials science and engineering, computer science, and mathematics. The resulting multidisciplinary environment fosters training the next generation data savvy scientists who will engage in collaborative multidisciplinary research.  <br/><br/>Existing approaches for computational design of metal organic frameworks (MOF) and solid-state electrolyte materials are largely based on screening of known materials or enumerative search of hypothetical materials. This project develops a new approach that integrates first principles calculations, experimental data and abundant data generated by physics-based models to train generalized antagonistic network (GAN) models for efficient search of the materials design space, and to train deep convolutional neural network (DCNN) models for fast and accurate screening of properties of the GAN-generated candidate materials. Additionally, graph-based GAN models will be used for MOF topology exploration and can be applied to other nanomaterials designs. More specifically, the investigators will: 1) develop and exploit physics-based models for fast calculation of properties such as diffusivity, ion conductivity, and mechanical stability; 2) develop generative adversarial network (GAN) models with built-in physics rules for efficient exploration of the chemical design space for both MOF materials and solid electrolytes; 3) use persistence homology and Bravais lattice sequence representations of MOF materials and solid electrolytes, respectively, to build Deep Convolutional Neural Network (DCNN) models for fast and accurate prediction of the physical properties of generated materials; 4)  apply high-level quantum-mechanical calculations for verification of discovered materials. Accomplishments from this project will lead to accelerated discovery of novel nanostructured materials for gas separation and energy storage, materials for lithium-ion batteries, novel data-driven scheme for materials design, and theoretical methods enabling implementation of advanced data science techniques. The highly interdisciplinary collaboration will offer students unique opportunities to interact with a variety of disciplines, and training the next-generation scientists with the mindset for multidiscipline collaborations. Educational and outreach activities will be developed and undertaken in conjunction with the proposed research activities.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931436","Elements: Enabling Accurate Thermal Transport Calculations in LAMMPS","OAC","TTP-Thermal Transport Process, Special Initiatives, Software Institutes","10/01/2019","06/02/2020","Christopher Wilmer","PA","University of Pittsburgh","Standard Grant","Alan Sussman","09/30/2022","$312,867.00","Hasan Babaei","wilmer@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","1406, 1642, 8004","026Z, 077Z, 7923, 8004","$0.00","Computational thermal transport research is critical to the development of new materials that can address challenging energy and environmental problems. Molecular dynamics (MD) simulations are used extensively to study thermal transport in materials. One of the most widely used MD software packages is the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS). It is the primary aim of this project to create and carefully implement improved thermal transport calculation methods in LAMMPS. This problem is made challenging by the fact that this software has hundreds of thousands of users and the solution must be merged into the core LAMMPS code, as opposed to offered as a modular ""plug-in"". The three objectives of the project are: (1) to implement a corrected heat flux computation for all supported many-body potentials in LAMMPS, (2) to identify the types of molecular systems most affected by the changed heat flux computations, and (3) educate the LAMMPS community on how to implement heat flux in new potentials correctly as well as train new scientists to contribute professional-quality code to the LAMMPS code base. This research will, among other broad impacts, enable large-scale computational screening of materials to accurately predict their thermal properties, which fulfills one of the key goals of the Materials Genome Initiative (MGI). Furthermore, it is an innovative, scalable, reusable software component that supports training for the broad LAMMPS user community as well as general workforce development via training to undergraduates, and ensures the new software capacities are widely available via the open-source LAMMPS package. Additionally, the project provides professional software engineering training to graduate and undergraduate students via a highly-trained resident software developer and resources from the Center for Research Computing at the University of Pittsburgh. Due to the large user base and open source nature of LAMMPS, the research is expected to have a broad impact; these software innovations will be widely available across both industry and academia.<br/><br/>The most common MD technique to compute thermal conductivity, the Green-Kubo method, yields incorrect results in LAMMPS for the majority of molecular simulations. Although the ramifications of the error in the heat flux for the thousands of papers already published using LAMMPS has yet to be fully determined, preliminary data indicates that for liquid hydrocarbons, the heat flux through a many-body potential can be underreported by 95%, leading to a total error of the heat flux of 22%. Unless correct thermal transport calculations can be achieved and implemented for this widely used and highly optimized software package, research and development of materials with novel thermal properties will be significantly hindered. There is no widely available MD code that is able to correctly compute heat flux. The one exception is for molecular systems where only pair-wise interactions exist (which excludes all molecules with greater than two atoms), in which case the current LAMMPS implementation gives correct results. The heat flux computation problem in LAMMPS was identified more than four years ago, but the lack of a correct implementation in any widely used MD software package speaks to the challenge of finding and dedicating software engineers to do this important work. This project addresses the theory, implementation pathway, and validation strategy for an expansive re-implementation of heat flux computations in LAMMPS for many-body potentials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835536","Collaborative Research: Elements: Software: NCSI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Colin Bischoff","OH","University of Cincinnati Main Campus","Standard Grant","Bogdan Mihaila","08/31/2022","$39,005.00","","colin.bischoff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811287","Collaborative Research: Photons from Binary Black Hole Inspiral","OAC","Leadership-Class Computing","08/01/2018","08/20/2021","Julian Krolik","MD","Johns Hopkins University","Standard Grant","Edward Walker","07/31/2022","$8,846.00","","jhk@pha.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7781","","$0.00","Every large galaxy contains a supermassive black hole in its center.   Galaxies merge from time to time, therefore it is expected, but not yet demonstrated, that recently merged galaxies hold two large black holes, which eventually form a bound pair and later merge. It is generally expected that supermassive black hole binaries can often accumulate sizable quantities of gas, and therefore be bright for a significant time before and during the merge, as well as the relaxation period there after.  However, because it is estimated that there are only very few such mergers per year in the entire Universe, understanding the specific features to search for is required to discover them. A team of researchers and graduate students from the Rochester Institute of Technology (RIT), Johns Hopkins University (JHU), and the University of Tulsa (TU) propose to use the Blue Waters supercomputer to perform the first realistic simulations of gas surrounding supermassive binary black holes on route to merger.  These simulations will provide predictions of light and timing signatures emitted by supermassive black hole mergers in order to allow for their observational discovery.<br/><br/>The focus of this project is to define the features that identify the process of supermassive black hole mergers, using large-scale numerical simulations that assume astrophysically relevant initial conditions and include all the principal relevant physics: magnetohydrodynamics, general relativity, and radiation transfer.  These simulations are extremely challenging computationally because of the complexity of the physics involved. However, the project introduces two significant computation innovations: a time-dependent high-order post-Newtonian spacetime to substitute for explicit solution of the Einstein Field Equations and a multipatch scheme to coordinate separate treatment of regions with contrasting grid requirements.  In addition to the simulation activities, the project is expected to create opportunities to integrate many existing research programs into a larger community of scientists, students, and the general public. It will foster cross-disciplinary collaboration among undergraduate students, graduate students, postdoctoral researchers and faculty at RIT, JHU and TU. Through this network, previous highly successful outreach programs will be able to reach an even larger and more diverse audience, and will provide many more opportunities for students to interact and collaborate with researchers outside their home institutions. It will also result in educating a number of graduate students and post-docs in the techniques of very large-scale computation, which will be important skills for their future careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761946","Spokes: SMALL: NORTHEAST: Collaborative: Building the Community to Address Data Integration of the Ecological Long Tail","OAC","BD Spokes -Big Data Regional I","09/15/2018","09/06/2018","Kathleen Weathers","NY","Cary Institute of Ecosystem Studies, Inc.","Standard Grant","Martin Halbert","08/31/2022","$155,944.00","","weathersk@caryinstitute.org","2801 SHARON TPKE","Millbrook","NY","125450129","8456777600","CSE","024Y","028Z, 8083","$0.00","Frequently research on data integration carried out by computer scientists and resulting tools must be modified to fit the needs of domain practitioners (ecologists in this case). This challenge is a socio-technical, collective action problem that can be addressed through a combination of tools and incentives. The project proposes to holding a series of workshops along with proofs-of-concept implementations. These workshops will result in approaches to decentralize the sharing of data in the long tail, through socio-technical approaches that appropriately incentivize and facilitate data integration by smaller labs. Such an interdisciplinary community will provide crucial real-world input to computer science researchers, which will give their research into tools the potential for larger impact in ecological practice and will yield better tools for ecologists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762034","Spokes: MEDIUM: MIDWEST: Smart Big Data Pipeline for Aging Rural Bridge Transportation Infrastructure (SMARTI)","OAC","BD Spokes -Big Data Regional I, S&CC: Smart & Connected Commun","09/01/2018","07/25/2018","Robin Gandhi","NE","University of Nebraska at Omaha","Standard Grant","Martin Halbert","08/31/2022","$999,988.00","Daniel Linzell, Deepak Khazanchi, Chungwook Sim, Brian Ricks","rgandhi@unomaha.edu","6001 Dodge Street","Omaha","NE","681820210","4025542286","CSE","024Y, 033Y","042Z, 8083, 9150","$0.00","America's bridges received a C+ from the American Society of Civil Engineers (ASCE) in 2017. Additionally, the US ranks only 11th world-wide in terms of infrastructure competitiveness. America's infrastructure, particularly its 50-100+ year-old bridges, is in poor health, representing a hidden crisis. Rural areas, due to their lower population density and distance from urban centers, are acutely affected by this crisis, particularly in terms of public safety and economic growth. Limited budgets for planning and maintenance only serve to exacerbate the crisis. To better inform future research, the research team held collaborative conferences and workshops, and conducted stakeholder surveys to identify issues impacting rural bridge health. Outcomes from these activities underscored the value of big data technologies to address the crisis and informed the proposed work. The mission of this multi-institution and multi-sector project is to produce a big data pipeline for rural bridge health management that improves transportation network performance and enhances safety.<br/><br/>The research team will combine existing and new datasets to address challenges of relevance to bridge owners using scalable and replicable big data pipeline components. Activities will inform bridge owner decision-making by integrating existing datasets and data collected using next-generation health monitoring technologies (e.g., contact and non-contact sensors, unmanned aerial vehicles) with innovative data management techniques. Socio-technical impacts associated with potential decisions will also be assessed. Aging, rural bridge testbeds will be selected in consultation with public and private owners to produce data products that facilitate decision-making and ultimately, provide economical and reliable solutions that improve bridge health. Results from the research will be shared with engineers, owners, and builders at workshops hosted by the research team. Project findings will be disseminated through publications, conferences, meetings, and forums. The research team will engage with the Big Data Hubs and Spokes network to make data, methods, and results available across regions. By extension, project findings will also benefit activities used to monitor and manage other important infrastructure assets, including highways, buildings, power grids, offshore oil platforms, water networks, and other civil infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017427","Collaborative Research:CyberTraining: Implementation: Medium:Broadening Adoption of Parallel and Distributed Computing in Undergraduate Computer Science and Engineering Curricula","OAC","CyberTraining - Training-based","10/15/2020","07/22/2020","Charles Weems","MA","University of Massachusetts Amherst","Standard Grant","Almadena Chtchelkanova","09/30/2023","$160,006.00","Neena Thota","weems@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","044Y","026Z","$0.00","This collaborative project represents a multi-faceted effort to shift computer science and engineering education toward ensuring that students can use 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly designed around a single processor, executing a sequence of operations. But this century is characterized by widespread deployment of multi-core, graphics, and AI tensor processors, as well as a shift to cloud servers, and the internet of things, all of which depend on the much different PDC approach to problem solving and programming. Financial, technical, scientific, engineering and medical companies, government labs, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the old model. Yet most students continue to learn the old approach due to significant inertia in academia. To turn the tide toward infusing PDC into the early stages of computer science and engineering education, this project will guide curricula and accreditation standards, prepare teachers, and foster a strong PDC education community. It will thus strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) is preparing the 2020 update of their 2013 curriculum guidelines for introducing parallel and distributed computing (PDC) into early undergraduate courses. This project will engage in four areas of activity to foster adoption of the curriculum, and extend it, with the goal of modernizing computer science and engineering workforce development.<br/>One major thrust is running summer training workshops for teachers, to learn both PDC concepts and experimental course evaluation methodology. The discipline is still in a phase of discovery with respect to PDC education approaches, and must encourage a diverse set of well-designed experiments to test and evaluate a broad range of pedagogical hypotheses. The workshop participants will be drawn from a diverse pool of educators, and given curriculum development grants in support of experimental course offerings and evaluation, leading to conference or journal publications, as well as contributions of exemplars to the CDER online course materials repository. <br/>A second effort is to help ABET/CSAB to formulate core PDC requirements and to inform/train ABET/CSAB evaluators (CSAB is the lead society within ABET for accreditation of degree programs in computer science). <br/>A third effort is expanding the curriculum guidelines to explicitly address adding PDC to computer engineering programs, which present novel curricular opportunities. <br/>Lastly, it will continue CDER's successes in organizing PDC education workshops in conjunction with major conferences, publishing PDC education books and journal special issues, maintaining and curating an online repository of PDC education resources, and providing free access to a publicly available PDC education cluster system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2034850","OAC Core: SHF: SMALL: ICURE -- In-situ Analytics with Compressed or Summary Representations for Extreme-Scale Architectures","OAC","OAC-Advanced Cyberinfrast Core","07/01/2020","06/02/2020","Gagan Agrawal","GA","AUGUSTA UNIVERSITY RESEARCH INSTITUTE, INC.","Standard Grant","Robert Beverly","06/30/2023","$500,000.00","","gagrawal@augusta.edu","1120 Fifteenth Street","Augusta","GA","309120004","7067212592","CSE","090Y","7923","$0.00","Systems for High Performance Computing (HPC) have been providing rapidly increasing computing power. However, this growth has also led to systems where the memory and data movement bandwidth is relatively lower.  This makes analyzing the data from scientific simulations very challenging.  A paradigm called in-situ analytics has emerged in response.  This project is further improving this paradigm, by using what can be referred to as homomorphic compressions.  The idea of homomorphic compression is to compress the data in a way that queries can be directly executed on the compressed data (without need for decompression).  This project is developing such compression methods, developing techniques to perform such compression efficiently on Graphic Processing Units (GPUs), techniques for query processing using such compressed representations, and finally, an overall system that will simplify development of in-situ analytics implementations.   Overall, this project will be making analysis of data from simulations more effective on the upcoming systems for HPC.  This project will seek to broaden participation in computing through direct participation in the project development teams by undergraduate and graduate students from under-represented groups.  <br/><br/>Systems for High Performance Computing (HPC) have been providing rapidly increasing computing power. However, this growth has also led to systems where the memory and data movement bandwidth is relatively lower.  This makes analyzing the data from scientific simulations very challenging.  A paradigm called in-situ analytics has emerged in response.  This project is further improving this paradigm, by using what can be referred to as homomorphic compressions.  The idea of homomorphic compression is to compress the data in a way that queries can be directly executed on the compressed data (without need for decompression).  The resulting framework, ICURE, can facilitate in situ analytics on accelerators themselves, reduce overall memory requirements for the analytics, reduce total data movements costs, and even reduce the time cost of performing the analytics.  Achieving the goals of ICURE involves many open challenges. The first is the choice of summarization structure and its constructions. This project experiments with two different summary or concise representations:  bitmap indices and an integrated value index. The second issue is analyses methods using summary and compressed representations, where the focus is on the use of these representations for a variety of analyses tasks: computing aggregations, correlations, value-based joins, time-step selection, and interesting subregions analysis.   The third issue is automating placement and quality. Driven by the consideration of providing the lowest   interference between the simulation and analytics, this project automates decisions on placement of specific analytics operations and data within the node of HPC system. Similarly, automatic selection of sampling level driven by desired accuracy and overheads of the analyses is performed. This project will seek to broaden participation in computing through direct participation in the project development teams by undergraduate and graduate students from under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940855","CICI: SSC: SciTrust: Enhancing Security for Modern Software Programming Cyberinfrastructure","OAC","Cybersecurity Innovation","06/05/2019","09/23/2019","Yanfang Ye","OH","Case Western Reserve University","Standard Grant","Robert Beverly","09/30/2022","$633,744.00","","yye7@nd.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","8027","","$0.00","Software plays a vital role supporting scientific communities. Modern software programming cyberinfrastructure (CI), consisting of online discussion platforms (such as Stack Overflow) and social coding repositories (such as Github), offers an open-source and collaborative environment for distributed scientific communities to expedite the process of software development. Within the ecosystem, researchers and developers can reuse code snippets and libraries, or adapt existing ready-to-use software to solve their own problems. Despite the apparent benefits of this new social coding paradigm, its potential security-related risks have been largely overlooked; insecure or malicious codes could be easily embedded and distributed, which could severely damage the scientific credibility of CI. Therefore, there is an urgent need for developing scalable techniques and tools to automatically detect these open-source insecure or malicious codes. To address this issue, this proposed project seeks to explore innovative links between Artificial Intelligence (AI) and cybersecurity to enhance the security of modern software programming CI. <br/><br/>The key components of the proposed research are three-fold: (1) a novel AI-based solution (iTrustSO) utilizing social coding properties is developed to automatically identify suspicious insecure code snippets on Stack Overflow; (2)  a cross-platform model is constructed to represent the complex interplay between GitHub and Stack Overflow; deep learning techniques are then utilized to build a predictive model (iTrustGH) for automatic detection of malicious codes on GitHub; and (3) a user-friendly tool (SciTrust) is developed to enhance code security for software development. The broader impacts of this work include benefits to scientific communities and the whole society by promoting the efficiency of cyber-enabled software development without sacrificing the security. The establishment of a Cybersecurity Lab through this project enhances the education and workforce training in cybersecurity. The project integrates research with education through curriculum development and student mentoring activities for the newly-established cybersecurity degree program. It is also expected to increase the participation of underrepresented groups including minority and women.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939795","HDR: DIRSE-IL: COLLABORATIVE RESEARCH: Harnessing data advances in systems biology to design a biological 3D printer: The synthetic coral","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Hollie Putnam","RI","University of Rhode Island","Continuing Grant","Sylvia Spengler","09/30/2022","$340,584.00","","hputnam@uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","099Y","062Z, 9102, 9150","$0.00","Corals are important natural resources that are key to the ocean's vast biodiversity and provide economic, cultural, and scientific benefits. As a result of human activities, locally and globally, coral reefs are declining rapidly. The complexity of corals makes conserving and restoring reefs very challenging. Corals are made up of thousands of different organisms, including the animal host and the algae, bacteria, viruses, and fungi that coexist as a so-called holobiont. Thus, corals are more like cities than individual animals, as they provide factories, housing, restaurants, nurseries, and more for an entire ecosystem. This project brings together experts in computer science, materials science, and biology to harness the data revolution in biology with machine learning to study how corals grow and function, when viewed as if they were manufacturing sites in the ocean. The study will focus on three key coral capabilities: (1) they create calcium carbonate skeletons that provide 3D structures for diverse sea life to live in, (2) they can heal damage to their tissues, and, (3) they live with the other organisms in a process referred to as symbiosis. Through these remarkable abilities, corals can 'print' resources for themselves and hundreds of thousands of other species, just like a 3D printer. The goal of this project is to understand these processes well enough to control them in the lab. This project may allow finding new ways to help coral survival, by deciphering the reasons why certain conditions damage them and find ways of repairing them. Furthermore, by synthetically growing corals, new types of materials may be identified for manufacturing. This project offers an opportunity to educate a diverse scientific workforce and the public by creating and disseminating the outcomes of a convergent research environment and will train postdoctoral researchers, graduate, and undergraduate students. Results of this research will be made available to the broader scientific community through web interfaces, peer-reviewed publications and workshops/conferences and shared with the public through outreach activities online, at schools, and public aquariums.<br/>    <br/>Through convergence of three disciplines, computer science, material science and biology, this project will provide a data-driven framework and toolset to learn from, control, engineer, and manufacture a combined form of living material, the 'synthetic coral', thereby opening new avenues for material synthesis and manufacturing. The research methodology will offer new analytical approaches to identify and quantify the parameters that govern coral growth and foster innovative new tools for controlling their growth. To understand the key functions of coral biology of biomineralization, wound healing, and symbiosis, this research will : (1) harness and analyze large amounts of coral '-omics' data to decipher critical molecules and their interactions for the aforementioned key functions, (2) experimentally validate the resulting predictions in coral individuals and cell lines, (3) manipulate the material properties of the calcium carbonate structures of the coral individuals and cell lines, and (4) test the biological and physical interactions in a network model of the 'synthetic coral'. This project develops and integrates fundamental building blocks that are essential for  an integrated computational and experimental validation system. Specifically, using machine learning, diverse data will be harnessed to identify physical conditions (e.g., surface characteristics), environmental conditions (e.g., temperature, pH), and key biological constituents (e.g., small molecule ligands and proteins encoded in the DNA) that are correlated to key structural and functional properties of the coral holobiont. These predicted conditions and molecules will be verified experimentally by perturbing individual coral nodes in a network of a 3D printed array of intact corals or their constituent cells and measuring their effects on the network of interactions and resulting structures. The results from this prediction-validation cycle will then be transferred back as input to manufacture novel adaptive materials fully embracing the organic/inorganic interface. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931324","Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research","OAC","EnvS-Environmtl Sustainability, Software Institutes","10/01/2019","08/17/2019","Mikhail Chester","AZ","Arizona State University","Standard Grant","Seung-Jong Park","09/30/2024","$410,000.00","","Mikhail.Chester@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7643, 8004","026Z, 077Z, 7925, 8004","$0.00","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. <br/><br/>SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919691","MRI: Proteus++: Enabling Data-Intensive Computing at Drexel University","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","09/01/2019","07/13/2020","Gail Rosen","PA","Drexel University","Standard Grant","Alejandro Suarez","08/31/2022","$566,740.00","Brigita Urbanc, Antonios Kontsos, Hasan Ayaz","gailr@ece.drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158956342","CSE","1189, 7231","075Z, 096Z, 1189, 9251","$0.00","This grant will acquire, install, and maintain Proteus++, which is composed of Graphical Processing Unit (GPU) nodes and a high memory node. This project brings data-intensive computing (computing requiring large memory and high-throughput) hardware to Drexel University.  The computing will strengthen research requiring large datasets in precision medicine (genomics, mapping the brain, and simulating molecules), advancing manufacturing, environmental modeling, and many other applications.  The computing will also help train students in data-intensive computing through classes and co-operative work experiences and broaden participation in computing by engaging students in computing research as well as undergraduate institutions (including Historically Black Colleges and Universities and women's colleges) throughout the region to use the resources and enhance computing education. The research and training that results will not only advance fundamental research but enable innovation that will benefit the tech, health, and biology-enabled industries in the Philadelphia region.<br/><br/>Proteus++ will enable computational discovery that is data-intensive (needing large memory and high-throughput) and impact over 200 users by offering hybrid architectures that provide new scientific capabilities and enable faster computations.   The intellectual merit of this project derives from a large collection of research topics that will be greatly enhanced by the sheer computational power and large-data processing that Proteus++ provides.  Thanks to hybrid and large-memory computing, high-throughput genomics can finally enter the regime of searching simultaneously tens of thousands of microbial genomes from a large volume and diversity of queries, not least which involves discovery of currently unknown microorganisms in a variety of environments. Enhanced-sampling molecular simulations and hybrid molecular-dynamics/docking methods will significantly increase in speed under the hybrid architecture of Proteus++, enabling precise views of rare biomolecular event processes and investigation of an unprecedentedly large number of complex protein targets.   Also, the prediction and simulation of material behavior and impurity benefits from co-processor architectures improving materials design for textiles, medicine, and energy.  Finally, understanding brain activity is now within reach through GPU-accelerated Monte Carlo simulations, which will improve technologies that exploit light tissue interaction in the brain. Besides its direct impact on Drexel, the enhancement of URCF capabilities enables the creation of the Philadelphia High Performance Computing Consortium (PHPCC), a partnership among Drexel and a number of local/regional undergraduate-only institutions, for the purposes of exposing larger numbers of Consortium members, including Drexel, faculty, postdocs and students to the power and possibilities of computational discovery techniques.  Capacity-building training in the form of workshops and courses will help new and existing PHPCC users learn not only about local University Research Computing Facility resources but also about advanced computational resources available at other institutions within the US.  Furthermore, up to 2 million core-hours will be made available for use by non-Drexel PHPCC faculty and students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925645","CC* Compute: CAML - Accelerating Machine Learning via Campus and Grid","OAC","Campus Cyberinfrastructure","07/01/2019","06/10/2019","Kevin Lannon","IN","University of Notre Dame","Standard Grant","Kevin Thompson","06/30/2022","$400,000.00","Olaf Wiest, Paul Brenner, Jun Li, Geoffrey Siwo","klannon@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8080","","$0.00","Machine learning, a subset of the field of artificial intelligence, has enabled impressive progress on a range of problems from identifying objects in images to translating French into English.  At the University of Notre Dame, the Cyberinfrastructure to Accelerate Machine Learning (CAML) resource allows faculty across the university to leverage machine learning to address problems within their disciplines.  CAML uses graphical processing units (GPUs) to provide a significant boost in the speed of training machine learning algorithms, enabling researching to solve problems faster and to train more complex models capable of addressing more difficult problems.  CAML benefits a wide range of research activities, from searching for new particles at the Large Hadron Collider to exploring new chemicals leading to medical breakthroughs.  CAML also benefits the broader community as part of the Open Science Grid, serving researchers from universities and labs across the US.  Furthermore, CAML is used for education and outreach involving students ranging from high school to graduate school, helping to train the next generation to tackle data science challenges in  the public and private sector.<br/><br/>CAML provides GPU resources for accelerating machine learning to the research community both locally at the University of Notre Dame and nationally through the Open Science Grid (OSG).  CAML physically hosts GPU resources suitable for accelerating the training of models from standard Deep Learning libraries, but also enables on-demand cloud access to more experimental architectures like FPGA resources.  Configured for both interactive and batch access, CAML supports both small-scale explorations to large-scale discovery science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925603","CC* Compute: A Cost-Effective, 2,048 Core InfiniBand Cluster at UTC for Campus Research and Education","OAC","Campus Cyberinfrastructure","07/01/2019","07/22/2020","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Kevin Thompson","06/30/2022","$408,235.00","Kidambi Sreenivas, Craig Tanis, Farah Kandah, Eleni Panagiotou, Ethan Hereth","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","8080","9251","$0.00","A team of researchers at the University of Tennessee at Chattanooga (UTC) will make a significant upgrade to the campus cyberinfrastructure that will provide state-of-the-art, cost-effective high-performance computing not previously possible.  This project will significantly improve university researchers' and students' ability to perform, enhance, and expand their current computationally-intensive research, prototyping, and development activities and will complement other investments already made, in-progress, or on the plan-of-record of UTC, including access to commercial cloud computing services. In addition to computer science and engineering, the UTC team anticipates significant research projects in mathematics, hydrology and computational fluid dynamics which will engage four regional partner universities.  Two teaching projects address HPC education and use of HPC for mechanical engineering undergraduate research/design. In addition to these funded projects, merited additional research projects are enabled over time as the PIs, Central IT, and the cluster's Advisory Board attract and onboard additional researchers and students requiring HPC. Among other users are the more than 20 computational science Ph.D. students, plus several postdocs. Furthermore, SimCenter---UTC's research computing hub---supports undergraduate research through self-funding and REU in HPC, providing additional users for the proposed cluster.<br/> <br/>This award allows the University of Tennessee at Chattanooga (UTC) to procure an innovative, 2,048-compute core, 16-server AMD EPYC2 cluster networked with 100Gbit/s InfiniBand plus 8TB of main memory and 77 Tflop/s of double-precision floating point arithmetic. EPYC2 Rome 7nm processors will be newly available at or near the start of the period of performance, so this project includes state-of-the-art, cost-effective, high-performance computing not previously possible using Intel or AMD processors. The university has invested in a ""commodity"" cluster as recently as three years ago, and it is heavily utilized by the existing user base. This system will be nearly four years old by the beginning of this proposed grant. By way of complement, upgrades to storage (1.1PB), internal networking, data center infrastructure, and private cloud virtualization (coming online by mid-2019) have prepared UTC to support a new campus-wide cluster with a growing number of users in addition to those named here. The proposed new campus cluster will enable core scales and total cluster memory not previously available on campus and thus help researchers prepare their scalable problem scenarios for greater scales on national resources such as XSEDE. Projects enabled immediately are 14 science driver projects (12 research, two teaching). Seven projects involve four regional partner universities.  At least ten NSF grants at UTK, UTC, UAB, Tennessee Tech, and Ole Miss are enhanced. Project areas highlighted include fault-tolerant parallel computing, performance monitoring of HPC, next-generation parallel programming with MPI, special-purpose linear algebra, hydrology, and computational fluid dynamics research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916518","BD Hubs: Collaborative Proposal: Midwest: Midwest Big Data Hub: Building Communities to Harness the Data Revolution","OAC","BD Spokes -Big Data Regional I","06/01/2019","07/21/2021","Valentin Pentchev","IN","Indiana University","Cooperative Agreement","Martin Halbert","05/31/2023","$254,997.00","Inna Kouper, Franco Pestilli, Valentin Pentchev","vpentche@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","024Y","062Z, 8083","$0.00","This project builds on a prior Midwest Big Data Hub effort. In 2015 stakeholders in the Midwest region of the United States formed a consortium of partners and working groups called the Midwest Big Data Hub (MBDH).  MBDH aimed to help member organizations working in Big Data coordinate current activities and launch new collaborative projects.  The project included stakeholders in the twelve states of the Midwest Census region (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin) and six leading universities that support hundreds of researchers, technologists, and students.  This hub provides a basis for collaboration and outreach that increases the potential for benefitting society. <br/><br/>The current award is a collaboration among five academic sites (Indiana University, Iowa State University, UIUC/NCSA, the University of Michigan, the University of North Dakota, and the University of Minnesota - Twin Cities).  The project focuses on priority areas that are important to the region and can also be influential on the national stage. <br/>  -  The five thematic areas of focus, and the institutional partner leading that thematic area, are: Digital Agriculture (led by Iowa State); Smart, Connected, and Resilient Communities (Indiana University); Water Quality (University of Minnesota); Advanced Materials and Manufacturing (UIUC); and Health and Biomedicine (University of Michigan). <br/>  -  Three cross-cutting areas that are emphasized across the project are: data science education and workforce development; cyberinfrastructure, data access and use; and communication and community development.<br/>The priority areas have regional relevance and also have the prospect for integration into societal contexts at the national level. The overall goal is to enable the use of existing and emerging cyberinfrastructure and best practices to improve access to and use of data.  The project plans to reach out to the Midwest community at large and to connect people, resources, and organizations. Ties to Big Data Hubs in three other regions provide a means to advance knowledge across these fields at the national level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808582","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Panagiotis Markopoulos","NY","Rochester Institute of Tech","Standard Grant","Tevfik Kosar","08/31/2022","$323,973.00","Andreas Savakis","pxmeee@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029322","RAPID: MolSSI COVID-19 Biomolecular Simulation Data and Algorithm Consortium","OAC","COVID-19 Research","05/01/2020","04/23/2020","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Seung-Jong Park","04/30/2021","$200,000.00","Teresa Head-Gordon, Cecilia Clementi","crawdad@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","158Y","075Z, 077Z, 096Z, 7914, 8004","$0.00","In response to the growing COVID-19 pandemic, the Molecular Sciences Software Institute (MolSSI) will leverage its position as a neutral commodity resource to help the global computational molecular sciences community quickly provide their scientific data and expertise to address the COVID-19 crisis. The MolSSI is jointly supported by the Office of Advanced Cyberinfrastructure and the Divisions of Chemistry and Materials Research. The centerpieces of this engagement will be (1) a centralized repository for simulation-related data targeting the virus and host proteins and potential pharmaceuticals, and (2) a select set of MolSSI Software Seed Fellowships for Ph.D. students and postdocs targeting COVID-19 related software tools that operate on the data developed in the repository.  These two components will enable the biomolecular simulation community to share and utilize key data and other resources to help identify the structural and dynamic characteristics of the host-virus complex to generate potential leads for therapeutics.  Although this project is intended to address the acute COVID-19 crisis, in the near term, it also will impact research communities and the next generation of computational molecular scientists in the confrontation and proactive resolution of future world problems.<br/><br/>The MolSSI will create and curate a large-scale repository containing: simulation input files (structures, configurations, scripts, Jupyter notebooks) in an organized structure; MD trajectories, analysis tools, and ready models for drug discovery; pointers to preprint servers such as arXiv, bioRxiv, and ChemRxiv on biomolecular simulation research in regards SARS-CoV-2; and DOI services that create citable data.  In addition, it will engage the molecular sciences community through a set of Software Fellowships for graduate student and postdocs to carry out software development, such as large-scale MD simulations, design of drug discovery tools such as docking, machine learning for small molecule toxicity predictions, and methods for determining whether new drugs are bioavailable or can be synthesized.  Collectively, these resources will speed the identification and development of leads for antiviral drugs, analyzing structural effects of genetic variation in the SARS-CoV-2 virus, and inhibitors that can disrupt protein-protein interactions to viral entry into cells and adherence to surfaces that cause disease spread.<br/><br/>This award is being funded by the CARES Act supplemental funds allocated to CISE and MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940169","HDR: DIRSE-IL: Collaborative Research: Harnessing data advances in systems biology to design a biological 3D printer: the synthetic coral","OAC","HDR-Harnessing the Data Revolu, Info Integration & Informatics","10/01/2019","10/15/2020","Judith Klein","CO","Colorado School of Mines","Continuing Grant","Sylvia Spengler","09/30/2022","$494,014.00","","jkleinse@asu.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","099Y, 7364","062Z, 9102, 9251","$0.00","Corals are important natural resources that are key to the ocean's vast biodiversity and provide economic, cultural, and scientific benefits. As a result of human activities, locally and globally, coral reefs are declining rapidly. The complexity of corals makes conserving and restoring reefs very challenging. Corals are made up of thousands of different organisms, including the animal host and the algae, bacteria, viruses, and fungi that coexist as a so-called holobiont. Thus, corals are more like cities than individual animals, as they provide factories, housing, restaurants, nurseries, and more for an entire ecosystem. This project brings together experts in computer science, materials science, and biology to harness the data revolution in biology with machine learning to study how corals grow and function, when viewed as if they were manufacturing sites in the ocean. The study will focus on three key coral capabilities: (1) they create calcium carbonate skeletons that provide 3D structures for diverse sea life to live in, (2) they can heal damage to their tissues, and, (3) they live with the other organisms in a process referred to as symbiosis. Through these remarkable abilities, corals can 'print' resources for themselves and hundreds of thousands of other species, just like a 3D printer. The goal of this project is to understand these processes well enough to control them in the lab. This project may allow finding new ways to help coral survival, by deciphering the reasons why certain conditions damage them and find ways of repairing them. Furthermore, by synthetically growing corals, new types of materials may be identified for manufacturing. This project offers an opportunity to educate a diverse scientific workforce and the public by creating and disseminating the outcomes of a convergent research environment and will train postdoctoral researchers, graduate, and undergraduate students. Results of this research will be made available to the broader scientific community through web interfaces, peer-reviewed publications and workshops/conferences and shared with the public through outreach activities online, at schools, and public aquariums.<br/>    <br/>Through convergence of three disciplines, computer science, material science and biology, this project will provide a data-driven framework and toolset to learn from, control, engineer, and manufacture a combined form of living material, the 'synthetic coral', thereby opening new avenues for material synthesis and manufacturing. The research methodology will offer new analytical approaches to identify and quantify the parameters that govern coral growth and foster innovative new tools for controlling their growth. To understand the key functions of coral biology of biomineralization, wound healing, and symbiosis, this research will : (1) harness and analyze large amounts of coral '-omics' data to decipher critical molecules and their interactions for the aforementioned key functions, (2) experimentally validate the resulting predictions in coral individuals and cell lines, (3) manipulate the material properties of the calcium carbonate structures of the coral individuals and cell lines, and (4) test the biological and physical interactions in a network model of the 'synthetic coral'. This project develops and integrates fundamental building blocks that are essential for  an integrated computational and experimental validation system. Specifically, using machine learning, diverse data will be harnessed to identify physical conditions (e.g., surface characteristics), environmental conditions (e.g., temperature, pH), and key biological constituents (e.g., small molecule ligands and proteins encoded in the DNA) that are correlated to key structural and functional properties of the coral holobiont. These predicted conditions and molecules will be verified experimentally by perturbing individual coral nodes in a network of a 3D printed array of intact corals or their constituent cells and measuring their effects on the network of interactions and resulting structures. The results from this prediction-validation cycle will then be transferred back as input to manufacture novel adaptive materials fully embracing the organic/inorganic interface. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939699","HDR: DIRSE-IL: Collaborative Research: Harnessing data advances in systems biology to design a biological 3D printer: the synthetic coral","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Nastassja Lewinski","VA","Virginia Commonwealth University","Continuing Grant","Sylvia Spengler","09/30/2022","$333,352.00","","nalewinski@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","099Y","062Z, 9102","$0.00","Corals are important natural resources that are key to the ocean's vast biodiversity and provide economic, cultural, and scientific benefits. As a result of human activities, locally and globally, coral reefs are declining rapidly. The complexity of corals makes conserving and restoring reefs very challenging. Corals are made up of thousands of different organisms, including the animal host and the algae, bacteria, viruses, and fungi that coexist as a so-called holobiont. Thus, corals are more like cities than individual animals, as they provide factories, housing, restaurants, nurseries, and more for an entire ecosystem. This project brings together experts in computer science, materials science, and biology to harness the data revolution in biology with machine learning to study how corals grow and function, when viewed as if they were manufacturing sites in the ocean. The study will focus on three key coral capabilities: (1) they create calcium carbonate skeletons that provide 3D structures for diverse sea life to live in, (2) they can heal damage to their tissues, and, (3) they live with the other organisms in a process referred to as symbiosis. Through these remarkable abilities, corals can 'print' resources for themselves and hundreds of thousands of other species, just like a 3D printer. The goal of this project is to understand these processes well enough to control them in the lab. This project may allow finding new ways to help coral survival, by deciphering the reasons why certain conditions damage them and find ways of repairing them. Furthermore, by synthetically growing corals, new types of materials may be identified for manufacturing. This project offers an opportunity to educate a diverse scientific workforce and the public by creating and disseminating the outcomes of a convergent research environment and will train postdoctoral researchers, graduate, and undergraduate students. Results of this research will be made available to the broader scientific community through web interfaces, peer-reviewed publications and workshops/conferences and shared with the public through outreach activities online, at schools, and public aquariums.<br/>    <br/>Through convergence of three disciplines, computer science, material science and biology, this project will provide a data-driven framework and toolset to learn from, control, engineer, and manufacture a combined form of living material, the 'synthetic coral', thereby opening new avenues for material synthesis and manufacturing. The research methodology will offer new analytical approaches to identify and quantify the parameters that govern coral growth and foster innovative new tools for controlling their growth. To understand the key functions of coral biology of biomineralization, wound healing, and symbiosis, this research will : (1) harness and analyze large amounts of coral '-omics' data to decipher critical molecules and their interactions for the aforementioned key functions, (2) experimentally validate the resulting predictions in coral individuals and cell lines, (3) manipulate the material properties of the calcium carbonate structures of the coral individuals and cell lines, and (4) test the biological and physical interactions in a network model of the 'synthetic coral'. This project develops and integrates fundamental building blocks that are essential for  an integrated computational and experimental validation system. Specifically, using machine learning, diverse data will be harnessed to identify physical conditions (e.g., surface characteristics), environmental conditions (e.g., temperature, pH), and key biological constituents (e.g., small molecule ligands and proteins encoded in the DNA) that are correlated to key structural and functional properties of the coral holobiont. These predicted conditions and molecules will be verified experimentally by perturbing individual coral nodes in a network of a 3D printed array of intact corals or their constituent cells and measuring their effects on the network of interactions and resulting structures. The results from this prediction-validation cycle will then be transferred back as input to manufacture novel adaptive materials fully embracing the organic/inorganic interface. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939860","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Eric Young","MA","Worcester Polytechnic Institute","Standard Grant","Peter McCartney","09/30/2022","$204,148.00","","emyoung@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","099Y, 7231","1165, 7231","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934721","Collaborative Research: Knowledge Guided Machine Learning: A Framework for Accelerating Scientific Discovery","OAC","HDR-Harnessing the Data Revolu","09/01/2019","07/10/2020","Vipin Kumar","MN","University of Minnesota-Twin Cities","Continuing Grant","Eva Zanzerkia","08/31/2022","$663,777.00","John Nieber, Michael Steinbach","kumar@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","099Y","062Z","$0.00","The success of machine learning (ML) in many applications where large-scale data is available has led to a growing anticipation of similar accomplishments in scientific disciplines. The use of data science is particularly promising in scientific problems involving processes that are not completely understood. However, a purely data-driven approach to modeling a physical process can be problematic. For example, it can create a complex model that is neither generalizable beyond the data on which it was trained nor physically interpretable. This problem becomes worse when there is not enough training data, which is quite common in science and engineering domains.  A machine learning model that is grounded by explainable theories stands a better chance at safeguarding against learning spurious patterns from the data that lead to non-generalizable performance. This is especially important when dealing with problems that are critical and associated with high risks (e.g., extreme weather or collapse of an ecosystem).  Hence, neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for knowledge discovery in complex scientific and engineering applications. This project is developing novel techniques to explore the continuum between knowledge-based and ML models, where both scientific knowledge and data are integrated synergistically. Such integrated methods have the potential for accelerating discovery in a range of scientific and engineering disciplines. This project will train interdisciplinary scientists who are well versed in such methods and will disseminate results of the project via peer-reviewed publications, open-source software, and a series of workshops to engage the broader scientific community.<br/><br/>This project aims to develop a framework that uses the unique capability of data science models to automatically learn patterns and models from data, without ignoring the treasure of accumulated scientific knowledge. Specifically, the project builds the foundations of knowledge-guided machine learning (KGML) by exploring several ways of bringing scientific knowledge and machine learning models together using pilot applications from four domains: aquatic ecodynamics, climate and weather, hydrology, and translational biology. These pilot applications were selected because they are at tipping points where knowledge-guided machine learning can have a transformative effect.  KGML has the potential for providing scientists and engineers with new insights into their domains of interest and will require the development of innovative new machine learning approaches and architectures that can incorporate scientific principles. Scientific knowledge, KGML methods, and software developed in this project could potentially be extended to a wide range of scientific applications where mechanistic (also known as process-based) models are used.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940118","Collaborative Research: Integrating Physics and Generative Machine Learning Models for Inverse Materials Design","OAC","HDR-Harnessing the Data Revolu","10/01/2019","07/28/2020","Jianzhong Wu","CA","University of California-Riverside","Continuing Grant","Daryl Hess","09/30/2022","$424,386.00","","jwu@engr.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","099Y","054Z, 062Z, 8396, 8399","$0.00","This project is aimed to address a grand challenge in data-intensive materials science and engineering to find better materials with desired properties, often with the goal to enhance performance in specific applications. This project addresses this grand challenge with a specific focus on finding metal organic framework (MOF) materials that are used to separate gas mixtures and finding better battery materials for energy storage. The PIs will combine theoretical methods from statistical mechanics and condensed-matter physics, and physics-based models, to generate information-rich materials data which is integrated with generative machine learning (ML) algorithms to search a complex chemical design space efficiently and to train deep learning models for fast screening of materials properties. This project will be carried out by a multidisciplinary collaboration involving researchers from physics, materials science and engineering, computer science, and mathematics. The resulting multidisciplinary environment fosters training the next generation data savvy scientists who will engage in collaborative multidisciplinary research.  <br/><br/>Existing approaches for computational design of metal organic frameworks (MOF) and solid-state electrolyte materials are largely based on screening of known materials or enumerative search of hypothetical materials. This project develops a new approach that integrates first principles calculations, experimental data and abundant data generated by physics-based models to train generalized antagonistic network (GAN) models for efficient search of the materials design space, and to train deep convolutional neural network (DCNN) models for fast and accurate screening of properties of the GAN-generated candidate materials. Additionally, graph-based GAN models will be used for MOF topology exploration and can be applied to other nanomaterials designs. More specifically, the investigators will: 1) develop and exploit physics-based models for fast calculation of properties such as diffusivity, ion conductivity, and mechanical stability; 2) develop generative adversarial network (GAN) models with built-in physics rules for efficient exploration of the chemical design space for both MOF materials and solid electrolytes; 3) use persistence homology and Bravais lattice sequence representations of MOF materials and solid electrolytes, respectively, to build Deep Convolutional Neural Network (DCNN) models for fast and accurate prediction of the physical properties of generated materials; 4)  apply high-level quantum-mechanical calculations for verification of discovered materials. Accomplishments from this project will lead to accelerated discovery of novel nanostructured materials for gas separation and energy storage, materials for lithium-ion batteries, novel data-driven scheme for materials design, and theoretical methods enabling implementation of advanced data science techniques. The highly interdisciplinary collaboration will offer students unique opportunities to interact with a variety of disciplines, and training the next-generation scientists with the mindset for multidiscipline collaborations. Educational and outreach activities will be developed and undertaken in conjunction with the proposed research activities.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931304","Collaborative Research: Framework: Cyberloop for Accelerated Bionanomaterials Design","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/09/2019","Ellad Tadmor","MN","University of Minnesota-Twin Cities","Standard Grant","Amy Walton","09/30/2023","$589,997.00","","tadmor@aem.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7237, 7925, 8004, 8009, 8614, 9216, 9263","$0.00","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.<br/><br/>Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931555","Elements: Comprehensive Time Series Data?Analytics for the Prediction of Solar Flares and Eruptions","OAC","Software Institutes, EarthCube, Space Weather Research","10/01/2019","09/04/2019","Rafal Angryk","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Robert Beverly","09/30/2022","$599,787.00","Emmanouil Georgoulis","angryk@cs.gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","8004, 8074, 8089","026Z, 075Z, 077Z, 4444, 7923","$0.00","Solar flares are some of the largest explosive events in our solar system. Together with the accompanying eruptions, solar flares have the potential to disrupt the technology we rely on, such as GPS, radars, high-frequency radio communications between aircraft and air traffic control, communication technology that relies on satellites (such as cell phones and Internet), and electricity grid distribution networks. This project aims to, first, improve scientists' understanding of the time-dependent physical and statistical behavior of solar active regions to the point that we can exploit their observations to predict whether and when they will flare, or erupt, in general, and, second, to enable scientists worldwide to perform comparative, reproducible, and data-driven studies on the prediction of solar explosive events. This project, together with its advanced solar flare prediction software infrastructure, will strengthen our nation's efforts to mitigate the potentially catastrophic impact of solar eruptions. Moreover, by helping achieve reliable forecasts of the timing, location, and magnitude of solar flares (already considered as natural disasters), this project will also contribute to mitigating the impact on other types of infrastructure, such as satellites (GPS, Internet, satellite communications), that are critical to not only our national defense but also a broad range of sectors, such as enterprise operations (e.g., oil-drilling) and communications. This award will also invest in the development of a highly educated, diverse, globally competitive STEM workforce trained in an interdisciplinary environment through the educational and research efforts at Georgia State University. <br/><br/><br/>The goal of this project is to improve the scientists' understanding of eruptive solar events by re-shaping the state-of-the-art on data mining techniques. This project will build cyberinfrastructure for data-driven scientific research on complex multivariate time-series data sets. This includes public releases of comprehensive benchmark data sets ideal for data mining research on multivariate time series classification, regression, and clustering, as well as open-source software for these applications (i.e., the public release of pre-trained models). While the domain area is focused on solar physics, this software and data sets can benefit other domains that involve event tracking and mining of spatiotemporal trajectories (e.g., security and healthcare applications of monitoring movement, traffic and weather data analyses, and business predictions). The project will also advance research in Space Weather forecasting through the delivery of reproducible data-driven solar flare predictions, inspired by new research directions (most notably, time series analysis at an unprecedented level) and spearheaded by the data-driven, physically interpretable machine learning models. In this direction, the projects will produce publicly available flare forecasting big data benchmark data sets aligning with recommendations by the National Science and Technology Council and being useful to both Data Science and Space Weather communities. Finally, this project will advance Computer Science research in data mining and information retrieval areas through the above cutting-edge work on time series data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931479","Collaborative Research: Elements: Flexible & Open-Source Models for Materials and Devices","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","11/01/2019","08/22/2019","Oliviero Andreussi","TX","University of North Texas","Standard Grant","Seung-Jong Park","10/31/2022","$253,516.00","","oliviero.andreussi@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7923, 8004, 8005, 9216","$0.00","The project will develop first principles materials modeling software that can approach multiple length and time scales (multiscale). This software will be capable of modeling systems as complex as entire devices and materials of mesoscopic sizes. Over the course of the project the principal investigators plan to develop an open-source python-based software aimed at standardizing and generalizing multiscale simulations methods.  This will enable the use of computer modeling in the design of new compounds, materials and devices. The goals are to render multiscale simulations reproducible and accessible by the broader community. In that context, the project will address the notion of ""lab 2.0"", by which computer simulations replace laboratory experiments in tasks such as materials design and costly combinatorial searches for viable chemical processes. The software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will be promoted by direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.<br/><br/>An approach that leverages the long-range multiscale capabilities of continuum models with accurate short-range atomistic descriptions of specific interactions, and that exploits the ideal scalability of quantum-embedding techniques, will be investigated. The main driver of the proposed implementation will be a Python codebase which will carry out the part of current software that is not computationally heavy, but instead is code heavy where many lines of code are needed in typically non-object-oriented languages. This is key to obtain the desired cluster-topology-agnostic workflows. Longstanding problems related to computational scalability and code stiffness will addressed in a three-pronged approach aimed at developing (1) modular tools implementing modules with highly object-oriented codes (e.g., quantum, classical atomistic, and continuum solvers), (2) hybrid tools implementing combinations of modular tools in a way that best exploits high-performance computing architectures, and (3) hyper tools implementing a high-level data-enabled optimization strategy that generates optimal workflows combining several hybrid tools, thereby making the software of broad applicability and accessible to nonexperts.  These goals will render multiscale simulations reproducible and accessible by the broader community. The project will address the ""lab 2.0"" paradigm, by which computer simulations replace laboratory experiments in tasks such as materials design and combinatorial searches for viable chemical processes. The resultant software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will include the direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919571","MRI: Acquisition of a High Performance Computing Cluster for Undergraduate Chemistry Research and Teaching by the Midwest Undergraduate Computational Chemistry Consortium (MU3C)","OAC","Major Research Instrumentation","08/01/2019","07/19/2019","Brent Krueger","MI","Hope College","Standard Grant","Alejandro Suarez","07/31/2022","$400,400.00","Keith Kuwata, Daniela Kohen, Jason Gillmore, Erin Speetzen","kruegerb@hope.edu","141 E. 12th Street","Holland","MI","494229000","6163957316","CSE","1189","1189","$0.00","The acquisition of a high-performance computing system by the Midwest Undergraduate Computational Chemistry Consortium (MU3C) will enable undergraduate students and their faculty mentors at a diverse set of 17 colleges and universities to use computational chemistry to advance our understanding of multiple basic and applied phenomena in terms of the properties of individual atoms and molecules. Fundamental research topics will advance prediction of environmental chemistry, explanation of reaction mechanisms, design of new materials and new catalysts, understanding of spectroscopy and excited state dynamics, and the development of new molecular dynamics methods. During the three years of the grant, MU3C faculty will train over 140 undergraduates to conduct computational chemistry research and to communicate their results at professional scientific conferences. Over 5000 undergraduates will use the computing system as part of their study of general, organic, inorganic, physical, and biological chemistry. MU3C includes institutions serving large numbers of first-generation college students, students eligible for Pell grants, and underrepresented minority students.<br/><br/>The project will provide 40 new CPU-based compute nodes and four new GPU-based compute nodes (32 GPUs) combined with 40 existing CPU-based compute nodes to yield a computer cluster well-suited to the broad range of research undertaken by MU3C's collaborative teams of faculty and undergraduate students. The new CPU-based compute nodes will have 256 GB of RAM, compared to 24 GB in the existing CPU-based nodes, enabling a variety of demanding quantum chemical (QC) calculations that cannot be executed on the current MU3C cluster. The new GPU-based compute nodes will replace existing GPU-based nodes that, because of their age, will soon be unable to run current versions of computational software. The existing CPU-based nodes are still useful for a variety of modestly-demanding QC calculations. The combined power of all three types of compute nodes will provide enough capacity to support the growing MU3C consortium, which has doubled in the number of research groups since its last MRI award. The research projects undertaken by these groups will contribute to a number of fields as described above. For example, calculations using DLPNO-CCSD(T) and CASSCF/NEVPT theories will aid the design of new catalysts in which expensive 2nd and 3rd row transition metals are replaced by inexpensive 1st row transition metals; calculations using EOM-CCSD(dT) theories will model complex reaction mechanisms to aid prediction of the evolution of atmospheric peroxides; calculations using time-dependent density functional theory will help explain the novel enhanced luminescence of ?magic-sized? quantum dots; and calculations using Monte Carlo methods will aid the design of new zeolite materials for the selective capture and sequestration of atmospheric CO2. Over two dozen different projects will advance our understanding of chemistry while preparing a broad set of undergraduate students for careers in STEM fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1908021","OAC Core: Small:  Collaborative Research: Data Provenance Infrastructure towards Robust andReliable Data Sharing and Analytics","OAC","OAC-Advanced Cyberinfrast Core","07/15/2019","07/15/2019","Yonghwi Kwon","VA","University of Virginia Main Campus","Standard Grant","Robert Beverly","06/30/2022","$250,000.00","","yk2bb@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","090Y","026Z, 9179","$0.00","The rapidly increasing number of interconnected devices and systems brings unprecedented collaborative opportunities between researchers, business partners, and healthcare organizations that can extend discoveries beyond those derivable from any single study. For instance, data sharing between various medical organizations can enhance understanding of the results from an individual clinical by polling of results from various trials from multiple organizations, and thus it enables extending the analysis of treatment options and accelerating biomedical research. While a vast amount of data collected from various sources brings us benefits, it imposes, at the same time, an important challenge of ensuring trustworthiness and quality of data due to the integration of disparate data from various sources. Furthermore, faulty, improperly configured, or broken sensors, as well as buggy or compromised data processing units, can severely affect the quality of data and the analyzed results. This project proposes to develop an infrastructure that provides robust, fine-grain, and end-to-end provenance for collaborative data sharing and analytics. The outcome of this research will directly serve as the foundation of trustworthy data sharing and analytic infrastructures by providing robust and attack/fault-resilient fine-grain provenance of the shared data by creating fine-grain end-to-end data provenance framework for diverse communication infrastructures.<br/><br/>In this project, the PIs will develop an end-to-end data provenance framework that provides robust and fine-grain data lineage for trustworthy data sharing and analytic infrastructures. First, they will develop a scalable and reliable infrastructure for collecting data provenance for distributed interconnected devices that can derive a concise provenance data in various environments of distributed devices (e.g., devices using diverse hardware and software platforms). Next, they will design and implement a framework to enable proper derivation and propagation of fine-grain provenance records for data sharing, processing, and analytics. It will provide services for provenance tracking as well as provenance record processing when the data are aggregated, analyzed, and processed (e.g., merge, split, duplicate, delete, extract, or statistical analytics). The PIs also plan to develop a system that can analyze and visualize the complete lineage of data to measure the quality of data and conduct various root cause analyses to identify the fundamental reasons behind data quality issues. The system will be capable of handling a large amount of data generated over a long period of time across multiple devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924059","Collaborative Research: CyberTraining: Pilot: Semi-Automatic Assessment of Parallel Programs in Training of Students and Faculty","OAC","CyberTraining - Training-based, IUSE","09/01/2019","07/10/2019","Prasun Dewan","NC","University of North Carolina at Chapel Hill","Standard Grant","Bogdan Mihaila","08/31/2022","$218,000.00","","dewan@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","044Y, 1998","8209, 8244, 9150, 9178","$0.00","Modern computers allow a computer program to be decomposed into multiple activities or threads that can execute concurrently.  Emerging big-data scientific, health, social, and engineering applications require such concurrency to give results in a timely manner. The key to matching the computation needs of these applications to the available computing resources is training of a workforce to develop, maintain, and configure concurrent programs. Ongoing work on developing toolkits for teaching concurrency is challenging because instruction is particularly labor-intensive, and thus, these toolkits, on their own, cannot help instructors meet the demands for such instruction. Specifically, concurrent programs are notoriously difficult to write, and substantial instructor effort is required to evaluate the performance and correctness of these programs, and identify potential problems. This project will extend an existing instructional toolkit with a new software framework to automate assessment of concurrent programs, and using instructional workshops and university courses to validate the extended toolkit. Successful execution of the project will improve the workforce development and promote the progress of science.<br/><br/>The main research question we are exploring is: What should be the nature of a rule-based software framework for assessing concurrent programs written in multiple programming languages that improves the productivity and learning, respectively, of trainers and trainees? The key novel steps we are taking to explore this question are (a) development of a semi-automatic assessment model in which manual evaluation, integrated with automatic rules, reduces false positives and negatives of the automated checks; (b) identification of  new protocols and associated architectures that leverage the capabilities of several existing powerful tools that have not been used before to address our question, (c) creation of new techniques based on the insight that solutions to a concurrent programming assignment often have a prescribed code-structure and algorithm, (d) support for layered techniques that allow rule-writers to tradeoff assessment quality for low rule-writing effort, (e) development of a meta-assessment framework to train the trainers to write rules, (f) use of the meta-assessment and assessment framework in instructional workshops and university course offerings, respectively, and (g) evaluation of the usability, programmability, effectiveness and learning gain of the frameworks through diverse mechanisms including pre-post surveys, course exit interviews, and focus groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1850353","CRII: OAC: Online Optimization of End-to-End Data Transfers in High Performance Networks","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","03/15/2019","02/18/2020","Engin Arslan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Alan Sussman","02/28/2022","$190,299.00","","earslan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","026Y, 9150","026Z, 8228, 9150, 9251","$0.00","With the advancement of computing and sensing technology, the amount of data generated by scientific applications is growing rapidly. To accommodate this growth, high speed networks with up to 400 Gbps capacities have been established. Despite the increasing availability of high-speed wide-area networks and the use of modern data transfer protocols designed for high performance, file transfers in practice attain only a fraction of theoretical maximum throughput, leaving networks underutilized and users unsatisfied. This project aims to develop a real-time transfer tuning algorithm to optimize file transfer throughput in high speed networks. Improved data transfer performance does not only enable efficient execution of distributed scientific applications but also fosters collaboration between scientists at geographically separated institutions by reducing time it takes to share data. This project complements the efforts to build next generation networking infrastructure by offering a novel solution to maximize utilization. The project also facilitates the development of a graduate level high-performance networking course at University of Nevada, Reno, and contribute to the education of undergraduate, female, and under-representative students. Therefore, this research aligns with the NSF's mission to promote the progress of science and to advance national prosperity, and welfare. <br/><br/>It is critical to fully utilize available network bandwidth to meet stringent end-to-end performance requirements of distributed scientific workflows. Yet, existing data transfer applications (e.g., scp, bbcp, and ftp) fail to saturate the available network bandwidth due to several factors, such as end system limitations, ill-designed transfer protocols, and poor storage performances. Application-layer transfer tuning offers a comprehensive solution to enhance transfer throughput significantly and can be applied with only client-side modifications. However, finding optimal configuration for application-layer parameters is challenging due to large search space and complex dynamics of network and storage subsystems. This project applies state-of-the-art online convex optimization to application-layer parameter tuning problem as it offers performance and convergence guarantees even under complete uncertainty. In addition to being fast and optimal, online learning algorithms can guarantee the fair distribution of resources among users when combined with game-theory inspired utility functions. The project aims to improve the performance of large streaming applications under dynamic network conditions through anomaly detection and mitigation. It has three unique and innovative aspects: (i) It uses state-of-the art online learning algorithm to fine tune application-layer parameters in real-time. (ii) It improves accuracy and efficiency of sample transfers to minimize the overhead of real-time tuning. (iii) It offers quality of service for delay-sensitive transfers (e.g., high-speed streaming applications) through continuous performance monitoring and adaptive tuning.<br/><br/>This project is jointly funded by Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829644","CyberTraining: CIC: Widening the CI Workforce On-ramp by Exposing Undergraduates to Heterogeneous Computing","OAC","CyberTraining - Training-based, Special Projects - CCF","09/01/2018","06/02/2020","Apan Qasem","TX","Texas State University - San Marcos","Standard Grant","Alan Sussman","08/31/2022","$241,791.00","","apan@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","CSE","044Y, 2878","026Z, 062Z, 7361, 9102, 9251","$0.00","In keeping with NSF's mission of promoting scientific progress, this project strengthens the workforce of future cyberinfrastructure researchers and professionals by preparing undergraduate students to program and employ heterogeneous computing systems. The need for increased performance per watt and demands of processing diverse workloads have triggered a major industry shift towards systems containing different kinds of specialized components. These heterogeneous architectures are quickly becoming the dominant platform in high-performance computing (HPC), cloud computing, and Internet of Things (IoT) networks. As such, it is imperative that the scientific workforce in advanced cyberinfrastructure have a deep understanding of heterogeneity in computing systems. Current undergraduate computer science curricula lack sufficient coverage of heterogeneous computing concepts and there is an imminent risk that tomorrow's scientific workforce will be ill-equipped to program the complex heterogeneous systems of the future.  This project addresses this by developing and disseminating modules covering heterogeneous computing concepts. Module development is complemented with out-of-class training camps that will serve as a springboard for undergraduates to pursue further training and career opportunities.  <br/><br/>The project takes an early-and-often, module-driven approach to curricular integration. A collection of modules is being designed to cover a range of heterogeneous computing concepts including heterogeneous architectures, hybrid algorithms, and heterogeneous programming models such as CUDA and OpenCL. For easy adoption, modules are designed to be self-contained with all requisite teaching material including in-class interactive exercises, problem sets and solutions, and pedagogical notes.  Each module is designed to exploit heterogeneous context within the curriculum and introduce topics at an appropriate level of abstraction. The summer training camps are designed to be immersive experiences for undergraduates that (i) reinforce and extend the material covered in the modules, (ii) present the role of cyberinfrastructure in advancing scientific research, and (iii) expose students to opportunities in the fields of computational and data science.  A key component of this project is forging alliances with industry partners, who are engaged throughout the design process.  A detailed evaluation plan is in place to evaluate student learning outcomes and engagement.  Both internal and external evaluators are involved in carrying out the evaluation plan.  The modules and training activities are expected to impact over 1,100 computer science undergraduates enrolled at Texas State, Knox College, and Concordia University Texas.  Extensive dissemination efforts through faculty training workshops are expected to further broaden the impact of this work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812687","Collaborative Research: CESER: EAGER: ""FabWave"" - A Pilot Manufacturing Cyberinfrastructure for Shareable Access to Information Rich Product Manufacturing Data","OAC","CM - Cybermanufacturing System, CESER-Cyberinfrastructure for","04/01/2018","03/26/2018","Binil Starly","NC","North Carolina State University","Standard Grant","William Miller","03/31/2020","$191,223.00","","bstarly@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","018Y, 7684","016Z, 067E, 152E, 7916, MANU","$0.00","Advanced manufacturing research is dependent on access to large datasets of product models to enable product designers to learn from past errors, and to discover and develop new solutions. However, such datasets are typically archived in inaccessible repositories and may be poorly described and difficult to use by others. Current manufacturing research must shift from siloed repositories of product manufacturing data to a federated, decentralized, open and inter-operable approach. This transformation can be achieved by embedding cyber-capability in every physical end-point, be it on a desktop used by a product designer or within the control systems of a manufacturing machine. The goal of this project is to develop a pilot manufacturing-focused cyberinfrastructure called FabWave. FabWave will gather together digital data on product and manufacturing processes from diverse sources, in particular, three-dimensional (3D) computer aided design (CAD) models generated by academia and public community users, into a new rich dataset that, in turn, will be fully accessible to manufacturing researchers and others. FabWave's system design will be driven by the needs of the manufacturing research community, and the project will encourage its adoption by academic users, government labs and the public. FabWave will aim to lower the barriers for users to upload, share and download 3D product model data; it will also enable manufacturing science researchers to gain access to an information rich dataset for testing and evaluation of algorithms to advance manufacturing research and offer the capability for the community to build custom apps that provide services to the community. This infrastructure development project aligns with the NSF Harnessing Data for 21st Century Science and Engineering Big Ideas vision and can serve to improve US competitiveness in advanced manufacturing.<br/><br/>The de novo cyberinfrastructure tools to be built in the FabWave project include: 1) Direct plugins within design software to allow easier capture of metadata and upload of 3D product models to the FabWave repository with limited human interaction; 2) a data-store with application programming interface (API) tools to allow users to write scripts to search through the repository for content specific to their research questions. Innovative use of the Inter-Planetary File System (IPFS) protocols along with next generation unstructured databases will serve to index and enable search services for Fabwave's users. the project will demonstrate use of FabWave for development of community facing third-party apps and libraries. Ultimately, the research community will have access to a rich source of product datasets created within research laboratories, within classes and through other open sourced projects. This data network, if expanded across universities, would accelerate advancements in cybermanufacturing, engineering systems design, and cyber-physical systems in manufacturing. In addition, FabWave will enable new areas of research in manufacturing information integration and informatics combined with reducing the barriers to exploring the science of manufacturing machine intelligence. This award is supported by the Division of Civil, Mechanical and Manufacturing Innovation and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931469","Collaborative Research: Frameworks: Machine learning and FPGA computing for real-time applications in big-data physics experiments","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","10/01/2019","09/17/2019","Erotokritos Katsavounidis","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","09/30/2022","$904,179.00","Philip Harris","kats@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253, 7244, 8004","026Z, 077Z, 7569, 7925, 8004","$0.00","The cyberinfrastructure needs for gravitational wave astrophysics, high energy physics, and large-scale electromagnetic surveys have rapidly evolved in recent years. The construction and upgrade of the facilities used to enable scientific discovery in these disparate fields of research have led to a common pair of computational grand challenges: (i) datasets with ever-increasing complexity and volume; and (ii) data mining analyses that must be performed in real-time with oversubscribed computational resources. Furthermore, the convergence of gravitational wave astrophysics with electromagnetic and astroparticle surveys, the very birth of Multi-Messenger Astrophysics, has already provided a glimpse of the transformational discoveries that it will enable in years to come. Given the unique potential for scientific discovery with the Large Hadron Collider (LHC) and the combination of the Laser Interferometer Gravitational-wave Observatory (LIGO) and the Large Synoptic Survey Telescope (LSST) for Multi-Messenger Astrophysics, the community needs to accelerate the development and exploitation of deep learning algorithms that will outperform existing approaches. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. It will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. Because these methods are also applicable to many other parts of our national and global economy and society, this work will positively impact many fields. The students and junior scientists to be mentored and trained in this research will interact closely with our industry partners, creating new career opportunities, and strengthening synergies between academia and industry. The team will share the algorithms with the community through open source software repositories, and through our tutorials and workshops the team will train the community regarding software credit and software citation.<br/><br/>In this project, the PIs will build upon our recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets, as open source software. This work combines scalable deep learning algorithms, trained with TB-size datasets within minutes using thousands of GPUs/CPUs, with state-of-the-art approaches to endow the predictions of deterministic deep learning models with complete posterior distributions. The team will also investigate the use of Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms to minimize the demands of future computing, which is a central goal for Multi-Messenger Astrophysics and particle physics. The open source tools to be developed as part of these activities will be readily shared with and adopted by LIGO, LHC, and LSST as core data analytics algorithms that will significantly increase the speed and depth of existing algorithms, enabling new physics while requiring minimal computational resources for real-time inferences analyses. The team will organize deep learning workshops and bootcamps to train students and researchers on how to use and contribute to our framework, creating a wide network of contributors and developers across key science missions. The team will leverage existing open source and interactive model repositories, such as the Data and Learning Hub for Science (DLHub) at Argonne, to reach out to a large cross-section of communities that analyze open datasets from LIGO, LHC, and LSST, and that will benefit from the use of these technologies that require minimal computational resources for inference tasks.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919667","MRI: Development of ACCORD, a Community Cyberinstrument for Broadening Access to Research on Sensitive Data","OAC","Special Projects - CNS, CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","10/01/2019","06/15/2020","Ronald Hutchins","VA","University of Virginia Main Campus","Standard Grant","Kevin Thompson","09/30/2022","$3,770,656.00","Scott Midkiff, Masha Sosonkina, Thomas Cheatham, Deborah Crawford","rrh8z@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1714, 7231, 8080","1189, 9251","$0.00","This 11 public university collaboration in Virginia develops ACCORD, an innovative research computing cyberinstrument to support the storage, access, and usage of sensitive data for researchers at participating institutions across the State. Beyond simply securing research data, the ACCORD cyberinstrument provides customizable secured research environments that can be integrated into each institution's workflows and security models to assure compliance with relevant laws and regulations applicable to specific data types. Through ACCORD, researchers across the State of Virginia have access to new research computing capabilities not previously possible (or not easily accessible), enabling them to undertake projects that require complex protection of sensitive data. In addition to enabling new research, ACCORD advances important dimensions of research such as reproducibility and data accountability. ACCORD is also a new model for collaboration, providing a common platform for researchers at multiple institutions and in different disciplines to securely share data and analytics.<br/><br/>In addition to driving exciting scientific discoveries with strong societal impact, the Virginia ACCORD program prioritizes research training and education in data security and privacy for students and researchers across the State. The Virginia ACCORD Consortium comprises universities of different sizes and missions, including minority-serving and non-PhD granting institutions, and is an effective platform for outreach and broadening participation to underrepresented and underserved communities. Lessons learned from the ACCORD's new model of collaboration and sharing compute resources are also being disseminated to other research computing consortia and communities across the U.S.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126303","CC* Compute: The Arizona Federated Open Research Computing Enclave (AFORCE), an Advanced Computing Platform for Science, Engineering, and Health","OAC","Campus Cyberinfrastructure","10/01/2021","09/01/2021","Douglas Jennewein","AZ","Arizona State University","Standard Grant","Kevin Thompson","09/30/2023","$399,997.00","Gil Speyer, Susanne Pfeifer, Suren Jayasuriya, Marisa Brazil","douglas.jennewein@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Drawing upon its mission to enable access to discovery and scholarship in science, engineering, and health, Arizona State University (ASU) is deploying the Arizona Federated Open Research Computing Enclave (AFORCE). AFORCE provides cutting-edge technology to support research and education while advancing the knowledge and understanding of deploying 21st-century cyberinfrastructure in a large public research university. Specifically, this state-of-the-art system is supporting multidisciplinary research and education in science, technology, engineering, and mathematics domains including computational genomics, molecular dynamics, computational materials science, robotics, and imaging.<br/><br/>To increase computational capacity, AFORCE comprises a pool of multiple graphical processing unit (GPU) accelerated computing nodes accessible to extramural researchers through federated authentication provided via InCommon. Moreover, the AFORCE system itself is part of the global Open Science Grid computing pool.  ASU also promotes and enables the use of Open Science Grid by incorporating its capabilities into regular training sessions and faculty engagement events. Finally, AFORCE is configured to also provide cloud burst capabilities allowing compute jobs to be scheduled on commercial clouds. Early career faculty will be specifically targeted for workshops and tutorials, helping encourage their participation in the AFORCE system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2125646","CC* Compute: A campus-wide computing resource for research and teaching at the University of Washington Bothell","OAC","Campus Cyberinfrastructure","09/01/2021","08/31/2021","Eric Salathe","WA","University of Washington","Standard Grant","Kevin Thompson","08/31/2023","$394,473.00","Kristina Hillesland, Joey Key","salathe@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>UW Bothell has no general access to research computing resources - this project transforms the capacity for computational science research and education on campus. The University of Washington maintains an integrated, scalable, super-computing infrastructure called Hyak to support research computing across all units and campuses. Academic and research units at the university purchase nodes on Hyak as an alternative to deploying and operating their own high-performance systems. This award allows the small University of Washington Bothell campus to become a participant in the Hyak system creating a dedicated research computing capacity for our faculty and students. The team of researchers participating in this project form the core users of the new system and span a wide range of computational research fields including astrophysics, climate modeling, biochemistry, genomics, machine learning, operations research, and mathematics. The new capacity allows the development of a larger cross-campus initiative for computational science education supporting our undergraduate teaching mission.<br/><br/>This project supports the acquisition and operation of compute nodes and associated hardware in the Hyak system. UW Bothell is a sponsor of the Hyak facility and participates in its governance and decision-making processes. HPC nodes procured include 60 Intel Xeon Scalable Gold 6230 20-core 2.1GHz CPUs for 1,200 total cores and 8 NVIDIA Quadro RTX6000 24GB GPUs. Each node includes 192 GB RAM and a 100 Gbps cluster interface. Data storage is provided by 33 TB high-speed scratch storage and 33 TB of archival tape storage.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126227","Advanced Cyberinfrastructure for Teaching and Research at Rowan University and the Southern New Jersey Region","OAC","Campus Cyberinfrastructure","09/01/2021","08/30/2021","Tabbetha Dobbins","NJ","Rowan University","Standard Grant","Kevin Thompson","08/31/2022","$99,964.00","David Klassen, Nidhal Bouaynaya, Forough Ghahramani, Mira Lalovic-Hand","dobbins@rowan.edu","Office of Sponsored Programs","Glassboro","NJ","080281701","8562564057","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Over the last several years, Rowan University has seen an increase in computational research, teaching, and innovation activities. Rowan faces a challenge in supporting the diverse and growing computational needs of its faculty, while also developing a plan for optimal investment in institutional infrastructure.  To this end, Rowan University is planning for a cyberinfrastructure that can sustainably support this increased activity.  The work generates institutional knowledge that will allow Rowan, its community college affiliates & partners, and other regional universities to reach the full potential of its faculty and students in conducting larger scale transformative work and increase their competitiveness in data-driven computational research.<br/><br/>The CC* Planning Grant addresses the challenge of building cyberinfrastructure (CI).  Early analysis on the gap between the needs of the university (and Southern New Jersey region) and the barriers for accessing state of the art cyberinfrastructure revealed the following needs: assessment and self-study to understand researchers needs, planning at the institutional level for financing CI needs, and student engagement and training related to computational research & experimentation.  The project focuses on strategic planning in order to develop a coordinated approach that informs the communities in Southern New Jersey of the latest CI developments, assesses the stakeholders? needs and Rowan?s capacity to support those needs, builds partnerships with other institutions for sustainable regional based computing, and develops a robust plan that will enable Rowan to build shared and accessible CI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126291","CC* Integration-Large:  (BLUE) Software-Defined CyberInfrastructure to enable data-driven smart campus applications","OAC","Campus Cyberinfrastructure","09/01/2021","07/09/2021","Dijiang Huang","AZ","Arizona State University","Standard Grant","Kevin Thompson","08/31/2023","$500,000.00","Baoxin Li, Ming Zhao, Jessica Shoop, Zach Jetson","dijiang@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8080","102Z","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>A new campus infrastructure called BLUE is developed to enable efficient and secure data-driven research and application development based on distributed IoT devices. BLUE addresses three main challenges for supporting innovative smart campus applications based on distributed IoT devices: (a) establishing a programable campus infrastructure to support distributed and ad hoc IoT services, (b) providing strong security and privacy protection of IoT data, and (c) constructing an edge-cloud infrastructure to provide computing, networking, and storage resources to support smart-campus applications. <br/><br/>BLUE is a new software-defined infrastructure to support IoT-based data processing, analysis, and distribution over distributed IoT data sources. BLUE also supports a set of tangible metrics, such as network QoS metrics, location, resource consumption, etc., to effectively enable researchers to validate their research models. Moreover, BLUE takes privacy and security protection as a fundamental enabling technique by pushing the computation towards the edge computing and networking infrastructure. Research applications built on this project share a common requirement for low-latency transfer of ever-larger data sets with collaborators across multiple geographic sites.  This project will contribute to a national paradigm of campus-level dynamic network services that enables leading-edge network and domain-specific research.<br/><br/>BLUE can benefit the full range of campus scholarly activities, including research activities funded by NSF and other federal agencies.  The outcomes of this project will be shared with the public based on an open-source license agreement. In addition, undergraduate and graduate student researchers will receive diverse STEM skills training, including networking research, big data analysis, and domain-specific research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939965","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Brandon Sepulvado","IL","National Opinion Research Center","Continuing Grant","Sylvia Spengler","12/31/2021","$82,734.00","","sepulvado-brandon@norc.org","1155 E. 60th Street","Chicago","IL","606372745","7732566000","CSE","099Y","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835855","Collaborative Research: Framework: Software: NSCI : Computational and Data Innovation Implementing a National Community Hydrologic Modeling Framework for Scientific Discovery","OAC","Software Institutes","10/01/2018","09/12/2018","Ilkay Altintas","CA","University of California-San Diego","Standard Grant","Seung-Jong Park","09/30/2022","$397,684.00","","altintas@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019080","CC* Networking Infrastructure: Research on the Hill","OAC","Campus Cyberinfrastructure","07/15/2020","10/12/2021","Raziq Yaqub","AL","Alabama A&M University","Standard Grant","Kevin Thompson","06/30/2022","$483,522.00","Ernst Cebert, Andrew Scott, Jian Fu, Raziq Yaqub","raziq.yaqub@aamu.edu","Office of Research and Sponsored","Normal","AL","357627500","2563728186","CSE","8080","9150","$0.00","Alabama Agricultural & Mechanical University?s Research on the Hill Network (RotH Net) is a dedicated and frictionless network fabric to serve as a Science DMZ for multidisciplinary research programs. The RotH Net provides a secure digital workspace where researchers can compute, store, and share research data on a 10G connection to Internet2. This network fabric facilitates transfers of large datasets from projects serving multiple research labs. Data collected from Unmanned Aerial Systems (UAS) for scientists and farmers to evaluate crops at scale while simultaneously creating crop maps and large consistent datasets that were previously imaginable. Normalized Difference Vegetation Index images will prescribe fertilizer applications, estimate yields, and identify weeds. Other facilities dedicated to data-intensive science are enabled by the project, including research into Ion Accelerator Technology and Ion beam modification of materials.<br/><br/>The RotH Net includes a full set of science DMZ components serving to re-architect the campus border to support high performance science data flows across campus and external to the campus network. One additional component is the Open Storage Network (OSN) pod, a set of deployable units of distributed storage with minimal administrative overhead and petabyte-sized storage capable of high-throughput, and high-speed large volume data transfers. The OSN provides a cyberinfrastructure (CI) service to address specific data storage, transfer, sharing, and access, challenges. The project also supports a fiber extension connecting campus research facilities and paving the way for flexible growth in high performance research and education network connectivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835449","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","Special Initiatives, Software Institutes","01/01/2019","09/06/2018","Matteo Turilli","NJ","Rutgers University New Brunswick","Standard Grant","Bogdan Mihaila","12/31/2022","$611,994.00","","matteo.turilli@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003683","CDS&E: Collaborative Research: Hierarchical Kernel Matrices for Scientific and Data Applications","OAC","CDS&E-MSS, CDS&E","10/01/2020","10/15/2020","Edmond Chow","GA","Georgia Tech Research Corporation","Standard Grant","Tevfik Kosar","09/30/2023","$315,981.00","","echow@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8069, 8084","026Z, 8084, 9263","$0.00","Kernel matrices in machine learning and scientific computing describe the relationships between collections of points which may represent various types of information.  The increasing size of data sets in various disciplines and the increasing computational capability of computer hardware make it essential that our algorithms and software for kernel matrices are scalable, and that the time it takes for their execution grows linearly or close to linearly, with the problem size. Otherwise, such large-scale data problems may not be tractable.  This project addresses the scaling bottlenecks associated with handling the kernel matrix by exploiting a hierarchical structure that is often found in these matrices.  By accelerating computations with kernel matrices, this research enables large-scale data analysis and scientific simulation in diverse areas such as uncertainty quantification, integral equation problems, particle simulations, and geostatistics.  High-performance software implementing the newly developed methods will be developed in an open-source environment.<br/><br/>This project specifically addresses high-dimensional problems, the use of specialized kernel functions in machine learning, and the high initial computational cost of constructing a hierarchical representation for a kernel matrix.  New methods developed will be applied to large-scale cases in a scientific application and a machine learning application: Brownian dynamics and Gaussian process regression.  In machine learning, the new methods will complement existing large-scale approaches for Gaussian processes.  High-performance software will address specific scaling challenges in constructing hierarchical matrices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018453","CC* Regional: Small Institution Multiple Organization Regional OneOklahoma Friction Free Network (SI-MORe-OFFN)","OAC","Campus Cyberinfrastructure","08/01/2020","09/21/2021","Vonley Royal","OK","OKLAHOMA STATE REGENTS FOR HIGHER EDUCATION","Standard Grant","Kevin Thompson","07/31/2022","$232,275.00","Dany Doughan, Jon Fields, Brian Burkhart, Sky Pettett, April Goode","von@onenet.net","655 RESEARCH PKWY STE 200","OKLAHOMA CITY","OK","731046217","4056309620","CSE","8080","9150","$0.00","The Small Institution Multiple Organization Regional OneOklahoma Friction Free Network (SI-MORe-OFFN) project makes advanced cyberinfrastructure tools and services available to five smaller campuses in Oklahoma, covering scientific disciplines from chemistry and agriculture to health and nursing by connecting these campuses to the OneOklahoma Friction Free Network (OFFN). The five targeted campuses are Redlands Community College, University of Science and Arts of Oklahoma, Oklahoma State University Institute of Technology, Oklahoma State University in Oklahoma City and Oklahoma Christian University. The project provides new and diverse research collaboration opportunities for faculty across Oklahoma and enables the smaller, target institutions to expand their research and education activities. The project also has a significant educational impact for undergraduate and community college students at these metropolitan and rural campuses by providing them with additional STEM and cyber-infrastructure educational opportunities as well as exposure to leading researchers and cyber-infrastructure practitioners.<br/><br/>The SI-MORe-OFFN project extends the OneOklahoma Friction Free Network to five smaller institutions by implementing Science DMZs, Data Transfer Nodes and network performance monitoring capabilities at identified locations and integrating these with other similar capabilities across Oklahoma. In addition, the project enhances the skills and abilities of technical personnel at each campus and facilitates cross-campus collaborations, including shared uses of computing tools, scientific equipment, and educational materials by engaging faculty and students in the larger OneOklahoma CyberInfrastructure Initiative (OneOCII). OFFN is managed by OneNet, Oklahoma?s research and education network, and scientific leadership is provided by the University of Oklahoma.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940059","Collaborative Research: Converging Genomics, Phenomics, and Environments Using Interpretable Machine Learning Models","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Arun Ross","MI","Michigan State University","Continuing Grant","Peter McCartney","09/30/2022","$400,000.00","","rossarun@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","099Y","062Z, 1165","$0.00","Mitigating the effects of climate change on public health and conservation calls for a better understanding of the dynamic interplay between biological processes and environmental effects. The state-of-the-art, which has led to many important discoveries, utilizes numerical or statistical models for making predictions or performing in silico experimentation, but these techniques struggle to capture the nonlinear response of natural systems. Machine learning (ML) methods are better able to cope with nonlinearity and have been used successfully in biological applications, but several barriers still exist, including the opaque nature of the algorithm output and the absence of ML-ready data. This project seeks to significantly advance technologies in ML and create a new interdisciplinary field, computational ecogenomics. This will be accomplished by designing ML techniques for encoding heterogeneous genomic and environmental data and mapping them to multi-level phenotypic traits, reducing the amount of necessary training data, and then developing interactive visualizations to better interpret ML models and their outputs.  These advances will responsibly and transparently inform policy to maximize resources during this crucial window for planetary health, while revealing underlying biological mechanisms of response to stress and evolutionary pressure.<br/><br/>The long-term vision for this project is to develop predictive analytics for organismal response to environmental perturbations using innovative data science approaches and change the way scientists think about gene expression and the environment. The goal for this two-year award is to develop a proof-of-concept for an institute focused on predicting emergent properties of complex systems; an institute that would itself foster the development of many new sub-disciplines.  The core of this activity is developing a machine learning framework capable of predicting phenotypes based on multi-scale data about genes and environments.  Available data, ranging from simple vectors to complex images to sequences, will be ingested into this framework by applying proven semantic data integration tools and algorithmic data transformation methods.  The central hypothesis of this research is that deep learning algorithms and biological knowledge graphs will predict phenotypes more accurately across more taxa and more ecosystems than do current numerical and traditional statistical modeling methods.  The rationale for this project is that a timely investment in data science will push through a bottleneck in life science, accelerating discovery of gene-phenotype-environment relationships, and catalyzing a new computational discipline to uncover the complex ""rules of life.""<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018754","CC* Integration-Large: Q-Factor: A Framework to Enable Ultra High-Speed Data Transfer Optimization based on Real-Time Network State Information provided by Programmable Data Planes","OAC","CISE Research Resources","10/01/2020","08/10/2020","Jeronimo Bezerra","FL","Florida International University","Standard Grant","Deepankar Medhi","09/30/2022","$760,000.00","Julio Ibarra, Richard Cziva","jbezerra@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","2890","9102","$0.00","Communication networks are critical components of today?s scientific workflows. Researchers require long-distance, ultra high-speed networks to transfer huge data from acquisition sites (such as Vera C. Rubin Observatory, also knowns as Large Synoptic Survey Telescope in Chile) to processing sites, and to share measurements with scientists worldwide. However, while network bandwidth is continuously increasing, the majority of data transfers are unable to efficiently utilize the added capacity due to inherent limitations of parameter settings of the network transport protocols and the lack of network state information at the end hosts. To address these challenges, Q-Factor plans to use temporal network state data to dynamically configure current transport protocol parameters to reach higher network utilization and, as a result, to improve scientific workflows.<br/><br/>Q-Factor leverages programmable network devices with the In-band Network Telemetry (INT) application and delivers a software solution to process in-band measurements at the end hosts. Using Q-Factor on Data Transfer Nodes (DTN)s, TCP/IP parameters will be configured according to temporal network characteristics, such as round-trip time, network utilization, and network congestion. This tuning is expected to result in increased network utilization, shorter flow completion times, and significantly fewer packet drops caused by network buffers overflow. Additionally, Q-Factor is geared to save host memory by tailoring kernel parameters and buffers to optimal sizes.<br/><br/>Q-Factor targets a timely issue in communication networks: underutilization of ultra high-speed networks for science workflows. In order to keep scientific progress unconstrained, future science workflows need to support emerging data-intensive science experiments (e.g., the Vera Rubin Observatory, High Luminosity Large Hadron Collider) where data generation grows significantly, reaching exabytes of traffic each year. Results of this project will also allow better understanding of optimal buffer sizes of network devices for huge flows and the interaction of various congestion control algorithms.<br/><br/>Experimental measurement data, network state information, network topology, software code, TCP tuning guidelines, and results will be available on the Q-Factor website https://q-factor.io, which will be maintained and indexed for at least three years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761969","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","09/01/2018","08/27/2018","Raghu Machiraju","OH","Ohio State University","Standard Grant","Wendy Nilsen","08/31/2022","$651,000.00","Anish Arora, Ayaz Hyder, Courtney Lynch, Pamela Salsberry","machiraju.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","024Y","028Z","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities.  (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934360","Collaborative Research: Advancing Science with Accelerated Machine Learning","OAC","CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Shih-Chieh Hsu","WA","University of Washington","Continuing Grant","Amy Walton","08/31/2022","$600,001.00","Scott Hauck","schsu@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","099y, 7231","062Z, 7231","$0.00","In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.<br/><br/>In this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126248","CC* Regional: LEARN Extending & Accelerating Participation in Science (Texas LEAPS)","OAC","Campus Cyberinfrastructure","10/01/2021","09/16/2021","Akbar Kara","TX","LEARN: Lonestar Education and Research Network","Standard Grant","Kevin Thompson","09/30/2023","$890,486.00","Amy Schultz, Lonie Packer, catherine howard","akbar.kara@tx-learn.net","3726 20th Street","Lubbock","TX","794101208","8067437878","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Higher education institutions at all levels rely on Cyberinfrastructure (CI) to provide environments for teaching and learning. For smaller institutions, having access to advanced CI can enrich the opportunities for teaching and learning, while presenting new opportunities for programs that rely on external information and service resources. The counterpoint is that sophisticated technology and the expertise to manage that technology are challenges for small colleges; however, these challenges in part can be met by state, regional and national organizations specializing in these technologies that can assist small colleges to access state-of-the-art technology and expertise.   <br/><br/>Lonestar Education and Research Network (LEARN) is further developing and expanding its model program (LEARN Smaller Institution CI Program) for regional network connectivity for small campuses in Texas.  A set of three campuses ? Texarkana College, Trinity Valley Community College, and Texas Lutheran University -  are taking part in this project to receive:  <br/><br/> ? Advanced network services and gigabit connectivity,  <br/> ? Expertise to manage these technologies, <br/> ? Training and assistance to effectively adopt and use the technologies, and <br/> ? A community of similarly engaged scholars and administrators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126108","CC* CIRA: Shared Arkansas Research Plan for Community Cyber Infrastructure (SHARP CCI)","OAC","Campus Cyberinfrastructure","09/01/2021","08/31/2021","Donald DuRousseau","AR","University of Arkansas","Standard Grant","Kevin Thompson","08/31/2022","$199,592.00","Jackson Cothren, Brian Berry, Mansour Mortazavi, Lawrence Tarbox","drdurous@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","8080","102Z, 9102, 9150","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>SHARP CCI creates a statewide research cyber infrastructure (RCI) Plan for Arkansas that is focused on the eight degree granting institutions performing science and engineering research. Participating institutions include: Arkansas State University (ASU); Southern Arkansas University (SAU); University of Arkansas (UA) Fayetteville (UAF); UA Division of Agriculture (UADA); UA Little Rock (UALR); UA Medical Sciences (UAMS); UA Pine Bluff (UAPB); and University of Central Arkansas (UCA). Each school has a growing demand for federated access to high-speed resources, managed services and technical training; however, a coordinated plan for providing these capabilities does not currently exist. Most schools in Arkansas lack the connectivity, budget and staffing to fully utilize these statewide resources, and each school is unique in technical capability and expertise. SHARP CCI assembles the senior administrators and research leaders at each school to document the environments, capabilities, technology needs and resource gaps and create an equitable RCI Plan to inform decision makers in the state. This plan will also provide a necessary resource for each school to apply for additional NSF support through solicitations that expand school infrastructure and training capabilities.<br/><br/>A statewide RCI Plan is requisite for organizing and expanding research collaborations and it fosters an economy-of-scale for building compliant facilities and systems and establishing a managed service environment for schools with limited resources. SHARP CCI includes establishing a comprehensive data science training program, Arkansas Cyber Team (ACT), to provide engineers, educators, researchers and students access to technical expertise in a broad range of scientific domains. The project team includes Arkansas Research and Education Optical Network, Great Plains Network, Open Science Grid and NSF?s Engagement and Performance Operations Center to provide engineering and training expertise, managed service experience and help in implementing a statewide RCI Plan for Arkansas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2127188","CC* Compute: A HPC Cluster for Science Research and Education at Tennessee Tech University","OAC","Campus Cyberinfrastructure","09/01/2021","07/12/2021","Michael Rogers","TN","Tennessee Technological University","Standard Grant","Kevin Thompson","08/31/2023","$399,983.00","Sheikh Ghafoor, Alfred Kalyanapu, Syed Rafay Hasan, Michael Renfro","mrogers@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","8080","102Z","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Researchers at Tennessee Tech University (TN Tech) are making significant upgrades to the campus computing infrastructure that will significantly improve the university researchers? and students? ability to perform, enhance, and expand their systems-oriented, algorithms-oriented, or applications-oriented research activities. This enhanced new computing infrastructure complements other investments already made, in-progress, or in-planning at TN Tech. This modern campus cluster will enable TN Tech to grow and sustain the HPC culture by expanding on its current NSF-funded CyberTraining activities to include hundreds of faculty and their undergraduate students from resource-limited institutions from across the southeast.  This new cluster will help TN Tech to build a regional resource for computational capacity and workforce development expanding opportunities to underrepresented groups in the region.<br/><br/>This award allows TN Tech to procure a state-of-the-art, cost-effective 10 node GPU cluster supporting 20 NVIDIA A100 GPUs, 1280 AMD Epyc2 CPU cores, and 5 TiB of main memory connected with 100 Gbit/s Infiniband.  The cluster provides capabilities not previously available on campus. This project team anticipates significant research projects in computational fluid dynamics, biomechanics, and geospatial analysis, which will engage four partner universities. Seventeen ongoing research projects including 3 teaching projects will directly benefit from the improved infrastructure. Additional research projects will be enabled over time as the PIs and the planned internal advisory committee attract additional researchers and students requiring heterogeneous HPC. The project team expects about 100 of College of Engineering Ph.D. students will use the HPC infrastructure for their research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031647","Connecting Researchers in Sharing and Re-Use of Research Data and Software","OAC","CI REUSE, NSF Public Access Initiative","12/01/2020","07/17/2020","Guenter Waibel","CA","University of California, Office of the President, Oakland","Standard Grant","Martin Halbert","11/30/2022","$46,793.00","","guenter.waibel@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","6892, 7414","077Z, 8004","$0.00","Open science practices have gained widespread adoption, globally, with the help of federal funding and<br/>publisher policies, as well as the increasing visibility and growing awareness of the value of sharing<br/>work. This has been largely evident in light of the current COVID19 pandemic, with data sharing driving<br/>many areas of research, and open software resources must evolve to meet the needs of researchers.  To meet the emerging demands and growing requirements of the research community who need support for both data and software sharing, Dryad and California Digital Library partnered in 2018 and Dryad and Zenodo partnered in 2019. These partnerships have allowed for the three organizations to re-think the data and publishing processes, explore ways for data curation, software preservation, and for output re-use to be tied together more seamlessly. <br/><br/>This project is a one-day, invitational workshop bringing together researchers and adjacent community members with diverse backgrounds to discuss needs, challenges, and priorities for re-using research data and software.  The goal of the meeting is to develop pathways for consistent engagement with individuals and groups across the diverse scientific disciplines in order to be connected with and responsive to researchers' needs and goals.  Meeting topics include dataset re-use, deposition guidance, curation standards and requirements, integrations and relationships between data and code, and advocacy and adoption. The anticipated outputs are a set of requirements and needs to better enable data and software sharing and re-use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018308","CC* Integration-Small: Science Traffic as a Service (STAAS)","OAC","Special Projects - CNS, CISE Research Resources, Campus Cyberinfrastructure","10/01/2020","06/15/2020","John Brassil","NJ","Princeton University","Standard Grant","Deepankar Medhi","09/30/2022","$514,208.00","Jennifer Rexford","jbrassil@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1714, 2890, 8080","9251","$0.00","Future advances in scientific research will require computing on massive datasets and high bandwidth streaming scientific instrument data. New experimental research infrastructures will be required to advance the understanding of the networks capable of supporting these increasingly demanding science data flows. Testing advances in networking technologies and protocols with actual high-speed science data traffic is vital to networking experimenters, scientific instrument users, and data scientists. To address this need, this project will develop a prototype of a decentralized computing and networking system to create, collect and distribute a diverse collection of real and synthetic science traffic flows to the experimental research infrastructure user community. The proposed work will first develop and deploy the Science Traffic as a Service (STAAS) prototype on the Network Programming Initiative testbed connecting two US universities, and then prepare STAAS for later nationwide deployment on the FABRIC midscale networking research infrastructure now under development. The students exposed to research on networking testbeds with demanding science traffic workloads will learn skills to help strengthen a workforce prepared to advance the global-scale computing cloud application service platforms that are increasingly central to the US economy. All documents, software, presentations, and other artifacts created under this project will be made publicly available at http://www.cs.princeton.edu/~jbrassil/public/projects/staas/<br/><br/>The key project insight is that many science flows are already in transit at any moment on or between campuses. Using new campus cyberinfrstucture including passive optical Test Access Points, Network Packet Brokers, and data-plane programmable ethernet switches, STAAS will safely tap and forward copies of these flows onto the experimental testbed, while preserving both the timing integrity of the flows and the data privacy of their payloads. Large scale, high bandwidth experiments will be achieved by enlisting participation of many or all STAAS edge nodes on multiple campuses. By introducing a service-based model, STAAS can help advance the networking research community's transport of emerging science data, and help the operators of scientific instruments increase the amount and quality of data collected by their instruments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920129","MRI: Acquisition of High Performance Computing Facilities for Research, Instruction and Outreach","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2019","09/12/2019","Henry Castejon","PA","Wilkes University","Standard Grant","Alejandro Suarez","09/30/2022","$486,615.00","Del Lucent, Bobak Karimi, Caroline Fortunato, Sofya Chepushtanova","henry.castejon@wilkes.edu","84 W. South St.","Wilkes-Barre","PA","187660997","5704085534","CSE","1189, 7231","1189","$0.00","This project aims to build and deploy a high-performance computing cluster to facilitate inter-disciplinary and collaborative research at Wilkes University. The cluster will support a broad range of computationally demanding research efforts including development of algorithms for computational chemistry and materials science, modeling protein folding and misfolding, predicting catastrophic geological events, large-scale analysis of bacterial genomes, and analysis of microwave tomography data for medical imaging, among others.  Each of these projects will yield positive societal impacts including environmental hazard prevention, understanding and preservation of ecosystems, design of new nanomaterials, and detecting and treating diseases.  These projects share the common theme of pattern recognition within large, complex, and noisy datasets and simulations.  Thus, this instrument will enable a collaborative and multi-disciplinary approach for the execution of these projects while enhancing pedagogical activities within the college.  The instrument will be administered so as to provide hands-on training for undergraduate students in modern computational techniques across multiple disciplines. The instrument will be the key piece of a new facility that will ultimately support the scholarship activities of a diverse group of investigators working mainly with first-generation college students as well as outreach activities aimed to inspire and encourage underrepresented groups and girls into scientific careers.<br/><br/>The hybrid architecture of the instrument will simultaneously support data- and memory-intensive projects as well as real-time modeling and visualization.  The primary computational engine, consisting of 8-nodes each possessing 24 computing cores (spanned across 2 processors), 4 commercial grade graphics cards, and 96 GB of RAM, will double as a visualization engine with the aid of virtualization software. This will allow the instrument to perform a variety of computational tasks such as molecular dynamics simulations, medium scale finite element analyses, training of smaller neural networks, performing medium-scale bioinformatics analyses, and MCMC analyses/ Bayesian modelling while enabling collaborative work among faculty and students as well as outreach activities through virtualization and visualization.  Additionally, the cluster will include 4 ultra-high-performance nodes each containing 40 CPU cores, 1.5 TB of RAM, and 2 state-of-the-art GPUs.  These nodes will facilitate extraordinarily demanding calculations such as the training of very large neural networks, simulating molecular systems with hundreds of thousands of atoms, and simultaneous alignment of multiple genomes.  By using a combination of a standard batch queuing system as well as more modern container-based resource allocation schemes, this instrument will serve its ultimate goals of enabling computationally intensive, multi-disciplinary collaborative research at Wilkes University and providing hands-on experience and training for undergraduate students in key skills required for our nation's growing scientific work force.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919771","MRI: Acquisition of a High-Performance Computing Cluster for Occidental College","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2019","09/11/2019","Justin Ning Hui Li","CA","Occidental College","Standard Grant","Alejandro Suarez","09/30/2022","$493,878.00","Janet Scheel, Amanda Zellmer, Jeffrey Cannon, Diana Ngo","justinnhli@oxy.edu","1600 Campus Road","Los Angeles","CA","900413314","3232591414","CSE","1189, 7231","1189","$0.00","This award is made to Occidental College to purchase a high-performance computer cluster to support faculty research and teaching across the scientific disciplines. Computation has played an increasingly important role in almost all academic disciplines, and Occidental recognizes its centrality to modern research. This cluster will be used by faculty and students to advance research across biology, chemistry, computer science, physics, and economics. The cluster will support on-going research projects that include improving how computers use big data, studying chemical reactivity, and understanding heat transfer in fluids. Other faculty in math, cognitive science, sociology, and the media arts will also use the cluster for interdisciplinary research in numerical simulations and data analysis and visualization, as well as in emerging areas in the digital humanities and interactive media. Additionally, the cluster will prepare undergraduates in 20+ courses for future careers in the sciences. With Occidental's track record of attracting a gifted and diverse student body, the cluster provides an opportunity to engage a broader population in cutting-edge scientific research.<br/><br/>This award will provide infrastructure to support the rapidly growing use and need for computational resources across the Occidental College campus. To accommodate the broad range of scientific applications of the co-PIs and the collaborators, the proposed 456-core cluster will consist of a master node, a storage node, and 17 compute nodes. The emphasis on a high number of simultaneous jobs supports the research of faculty while leaving available cores for undergraduate training and classroom use. One of the compute nodes is further equipped with two GPUs, for efficient matrix and vector operations required by many research applications. An uneven distribution of RAM across the compute nodes, ranging from 64GB to 512GB per node, allows differential use of the nodes between memory-intensive and computation-intensive users. The cluster will also include 297TB of storage, 272TB of which are located on dedicated storage nodes, to host the large datasets produced in biology, chemistry, cognitive science, computer science, physics, and economics. In addition to the physical equipment, this proposal requests funding for research software licenses as well as an administrator who will deploy, maintain, and manage the instrument. The administrator will provide technical support and create maintenance documentation and training materials, accelerating the use of the cluster by faculty and students in all areas of the natural and social sciences and the arts and humanities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919839","MRI: Acquisition of a Big Data and High Performance Computing System to Catalyze Delaware Research and Education","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE, EPSCoR Co-Funding","10/01/2019","09/11/2019","Rudolf Eigenmann","DE","University of Delaware","Standard Grant","Alejandro Suarez","09/30/2022","$1,399,992.00","Cathy Wu, Arthi Jayaraman, Benjamin Bagozzi, William Totten","eigenman@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1189, 1640, 7231, 9150","1189, 9150","$0.00","Cyberinfrastructure (CI) drives progress in science, engineering, and business in a major way, as stated prominently in the 2015 presidential National Strategic Computing Initiative.  CI - consisting of computer, data, networking and related resources - enables new ideas and products to be explored ""virtually,"" without building expensive prototypes and physical experiments. CI also powers almost all business processes today.  This project implements a major computational and data resource at the University of Delaware (UD), enabling and accelerating progress in all sciences and addressing grand challenges facing our society. Through partnerships with regional universities, colleges, health institutions and the private sector, the new resource will also boost research, development, and education in the greater Delaware area.<br/><br/>The acquired compute and storage system will serve as a critical and transformative upgrade to Delaware's cyberinfrastructure, enabling research and educational activities for a large number of faculty across all UD colleges as well as for users from academic and industrial partners within the broader Delaware region. The requested instrumentation is designed to enable research broadly across disciplines with diverse software and hardware needs including, but not limited to, problems that scale to large numbers of processors and data sets, involve large data transfers, use advanced graphics accelerators, and require new operating modes. It will also serve to train students and researchers on computational and data-intensive methods and enhance these skills in the greater Delaware region. The acquisition of the instrumentation, called DARWIN (Delaware Advanced Research Workforce and Innovation Network), will complement ongoing UD initiatives focused on improving and enhancing networking, storage, and compute infrastructure. This addition will also be timely, providing great synergy with the new UD Data Science Institute and the faculty network in high-performance computing led by the project investigators.<br/><br/>This project is jointly funded by the Office of Advanced Cyberinfrastructure (OAC), the Office of Integrative Activities, and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017917","MRI: Acquisition of a High Performance Computing Cluster for Next-Generation Computational Science in Southern Colorado","OAC","Major Research Instrumentation","10/01/2020","10/20/2020","Brandon Runnels","CO","University of Colorado at Colorado Springs","Standard Grant","Alejandro Suarez","09/30/2023","$435,228.00","Qing Yi, Byeong Lee, Amanda Morgenstern, John Quinlan","brunnels@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","1189","075Z, 1189","$0.00","This project will enable the acquisition, deployment, and maintenance of a high performance computing (HPC) cluster (to be called INCLINE). This instrument will provide much-needed computational resources to the UCCS campus and the Southern Colorado scientific and academic communities. The size and power of the instrument will bridge the growing gap between workstation-level machines and Top 500 supercomputers, allowing researchers to test and leverage code scalability on an HPC platform, to expedite result processing, and to gain expertise on a local HPC environment. The work to be performed will yield insight into biomedical applications, such as microbubble drug delivery and bone fracture, military applications, such as additively manufactured energetics, and civil applications, including improved structural materials.  In addition to research applications, INCLINE will be used as an educational platform for teaching the fundamentals of an HPC to undergraduate and graduate students and allow students to investigate classroom problems more deeply with the computational power provided by an HPC. It will also be used to supplement existing outreach programs to spark enthusiasm and interest in HPC in the Southern Colorado community, which is diverse in both population and the range of applications. <br/><br/>The instrument?s state-of-the-art hardware is designed to support a broad range of high-performance scientific applications, ranging from compiler design to computational fluid dynamics. The instrument will contain both CPU compute (including standard and high memory), and GPU nodes. The compute nodes will enable standard massively parallel computations such as computational solid mechanics and fluid dynamics. The GPU nodes will allow acceleration on computational physics and machine learning projects and will be used in conjunction with CPU nodes to test optimal load balancing on heterogeneous architectures. All nodes will be connected using InfiniBand high speed interconnects to minimize latency for communication-intense applications, including CFD and computational solid mechanics. A high speed SCRATCH storage file system will minimize I/O latency for applications with large output file sizes and I/O requirements. The estimated peak performance of the instrument is approximately 90 TFLOPS. The Slurm queueing system will be used to manage accounts and allocations across the diverse user base. The robust design of this instrument will allow it to fill the growing need for a local HPC research facility at UCCS and in Southern Colorado and will facilitate the training of the next generation of computational scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931429","Collaborative Research: Frameworks: An open source software ecosystem for plasma physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes, Space Weather Research","10/01/2019","09/07/2019","Stephen Vincena","CA","University of California-Los Angeles","Standard Grant","Seung-Jong Park","09/30/2024","$539,583.00","","vincena@physics.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","1253, 7244, 8004, 8089","026Z, 077Z, 4444, 7569, 7925, 8004","$0.00","Software is crucial to all areas of modern plasma physics research.  Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas.  Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions.  Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education.  PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy.  PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality.  The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.<br/><br/>The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics.  The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities  include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data.  They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126285","CC* Regional: Extended Small Institution - Multiple Organization Regional - OneOklahoma Friction Free Network (ESI MORe OFFN)","OAC","Campus Cyberinfrastructure","09/15/2021","09/23/2021","Vonley Royal","OK","OKLAHOMA STATE REGENTS FOR HIGHER EDUCATION","Standard Grant","Kevin Thompson","08/31/2023","$414,595.00","Kelly McClure, Brian Burkhart, Sky Pettett, Jeffrey Price, April Goode","von@onenet.net","655 RESEARCH PKWY STE 200","OKLAHOMA CITY","OK","731046217","4056309620","CSE","8080","9102, 9150","$0.00","The Extended Small Institution Multiple Organization Regional OneOklahoma Friction Free Network (ESI-MORe-OFFN) connects two under-resourced campuses, Oklahoma City University and Fires Innovation Science and Technology Accelerator campus of Cameron University, to the OneOklahoma Friction Free Network (OFFN) in support of research and education drivers in the areas of engineering and economics. The project extends existing and creates new and diverse research collaboration opportunities for faculty and students through fostering broader and deeper collaborations and expanding research and education activities. The project provides cyberinfrastructure resources to these two under-resourced campuses, reaching a diverse student population, including first-generation students. The project has significant educational impact for both undergraduate and graduate students at the campuses, providing them with additional STEM and cyberinfrastructure opportunities as well as exposure to leading researchers and CI practitioners.<br/><br/>The ESI-MORe-OFFN project extends the OneOklahoma Friction Free Network to two under-resourced campuses by implementing Science DMZs, Data Transfer Nodes and network performance monitoring capabilities in support of science application drivers at the two campuses and integrating these capabilities with other similar capabilities across Oklahoma.?In addition, the project enhances the skills and abilities of technical personnel at each campus and facilitates cross-campus collaborations, including shared uses of computing tools, scientific equipment, and educational materials by engaging faculty and students in the larger OneOklahoma CyberInfrastructure Initiative (OneOCII). OFFN is managed by OneNet, Oklahoma's research and education network, and scientific leadership is provided by the University of Oklahoma.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940124","Collaborative Research: Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","David Matteson","NY","Cornell University","Continuing Grant","Pui Ho","09/30/2022","$331,001.00","","dm484@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835727","Collaborative Research: Elements: Software: NSCI: Chrono-An open-source simulation platform for computational dynamics problems","OAC","Dynamics, Control and System D, Software Institutes","08/01/2019","10/19/2021","Arman Pazouki","CA","California State L A University Auxiliary Services Inc.","Standard Grant","Robert Beverly","07/31/2022","$70,516.00","","apazouk@calstatela.edu","5151 State University Drive","Los Angeles","CA","900324221","3233433648","CSE","7569, 8004","026Z, 034E, 077Z, 7923, 8004","$0.00","This project seeks to augment modeling and solution methods employed by Chrono, an open-source computer simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. Chrono will be able to capture dynamics at various size and time scales spanning from millisecond (impact phenomena) to decades (geophysics). These performance levels open up new directions of research in several fields. Chrono is widely used and further developed by other users and has an active forum with more than 250 registered users currently. This project will enhance the richness of Chrono's modeling features, sound numerical solution foundation, and leverage of emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform. Chrono has been used by universities, national labs, and industry. Over the past two years, various groups have used Chrono in extraterrestrial applications, machine learning in robotics, image processing, pattern recognition and computer vision, mechanical watch design, architectural studies, autonomous vehicles, fluid-solid interaction applications, wind turbine dynamics, next generation space suit design, oil extraction and accident mitigation, hardware-in-the-loop simulation, etc. Finally, this project will engage high-school students from under-represented groups in a six-day residential camp run (now at its 12th edition) and will train a group of undergraduate students from California State University at University of Wisconsin-Madison through a new residential program that will introduce them to the use of Chrono in simulation-based robotics design.<br/><br/>This project seeks to augment modeling and solution methods employed by Chrono, a BSD3 open-source simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. The software infrastructure enhancements in this project aim at sustaining teraflops-grade simulation of MBD and FSI systems with more than ten billion degrees of freedom; i.e., two to three orders of magnitude beyond conventional simulations today. In order to increase adoption and impact, the performance levels aimed at will be reached on budget/affordable hardware that leverages GPU computing. Chrono will be able to capture micro-, meso- and macro-scale dynamics on time scales spanning from millisecond (impact phenomena) to decades (geophysics). The intellectual merit of this project stems from the following key ideas: (i) with an eye towards the sunsetting of Moore's law, the software design solution embraces a scalable multi-GPU hardware layout poised to solve effectively large multi-physics problems; (ii) a hardware-aware software design paradigm, which aggressively reduces data storage and movement, will allow budget-conscious hardware systems to run billion-degree-of-freedom models, or, for models of similar size, accomplish a two orders of magnitude speedup when compared to the state of the art; (iii) a unified Lagrangian formulation for both solid and fluid dynamics is implemented in one software platform that can simulate complex multi-physics (coupled) problems; and (iv) Chrono promotes an alternative approach for handling friction and contact that revolves around the concept of differential variational inequality and thus avoids the small integration time step and numerical instability issues that hinder most of the existing many-body dynamics simulators. In relation to its educational and outreach initiatives, this project: (a) will be instrumental in establishing a new University of Wisconsin-Madison undergraduate course that introduces students to computing concepts subsequently refined in a graduate advanced computing class; (b) will promote the discipline of Computational Science and Computational Dynamics at high-school and undergraduate levels via two yearly residential summer programs for under-represented students; (c) will expand an advanced computing forum that facilitates technology transfer to industry and promotes Chrono adoption; and, (d) will strengthen ongoing collaborations that critically depend on Chrono in robotics, geomechanics, and soft-matter physics. Chrono is presently cloned on average 10 times every day, has been forked from its public repository by more than 150 parties, and has an active forum with more than 250 registered users. This project will enhance the richness of Chrono modeling features, improve its numerical solution foundation, and leverage emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027541","RAPID: COVID-19 Response Support: Building Synthetic Multi-scale Networks","OAC","COVID-19 Research","07/01/2020","06/18/2020","Madhav Marathe","VA","University of Virginia Main Campus","Standard Grant","Seung-Jong Park","12/31/2021","$173,640.00","Henning Mortveit, Srinivasan Venkatramanan","mvm7hz@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","158Y","096Z, 7914, 7923","$0.00","The Novel 2019 Coronavirus (COVID-19) has already caused unprecedented global social, economic, and health impact. This project will develop synthetic global multi-scale social contact networks. The synthetic but realistic social contact networks can capture human interactions either at an individual or community level. The networks can be used in conjunction with agent-based models to simulate the ongoing COVID-19 pandemic. The simulations can in-turn be used to design and assess various interventions that balance health benefits with social and economic costs. Data will be made available to the scientific community. The PIs will also work with other research groups and continue their partnership with other federal and state agencies to support their response efforts.  <br/><br/>Developing synthetic social contact networks is a statistically and algorithmically challenging problem. This project will synthesize ensembles of two classes of synthetic social contact networks -- patch-based meta-population networks and individualized synthetic social contact populations and networks using a combination of machine learning and data driven modeling techniques. The need for such data driven mechanistic modeling methods has become abundantly clear in regimes when the available data is sparse and noisy. The project will undertake a detailed statistical analysis of the algorithms and the synthetic networks they produce. This includes methods to conduct global sensitivity analysis and methods to quantify the uncertainty in the outcomes as a function of the network structure. One of the many uses of this resource, is to support individual-based as well as meta-population-based simulation models for epidemic spread in general, and COVID-19 in particular.  Beyond supporting ongoing COVID-19 outbreaks, these synthetic social contact networks will be useful in responding to other epidemics. The PIs plan to make this data available to the global research community so that researchers around the world can immediately use it to assess the pandemic and the response efforts in their respective regions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931343","Collaborative Research: Frameworks: Cyberloop for Accelerated Bionanomaterials Design","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2019","09/09/2019","Wonpil Im","PA","Lehigh University","Standard Grant","Amy Walton","09/30/2023","$589,999.00","","woi216@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","1253, 8004","026Z, 054Z, 077Z, 7237, 7925, 8004, 8009, 8614, 9216, 9263","$0.00","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.<br/><br/>Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. <br/><br/>This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939951","Collaborative Proposal: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","01/24/2020","Bridget McInnes","VA","Virginia Commonwealth University","Standard Grant","Peter McCartney","09/30/2022","$155,766.00","","btmcinnes@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","099Y, 7231","1165","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939929","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","John Downie","IL","University of Illinois at Urbana-Champaign","Standard Grant","Peter McCartney","09/30/2022","$211,699.00","","jdownie@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","099Y, 7231","1165, 7231","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761945","Spokes: MEDIUM: SOUTH: Collaborative: Integrating Biological Big Data Research into Student Training and Education","OAC","BD Spokes -Big Data Regional I","10/01/2018","09/14/2018","Mentewab Ayalew","GA","Spelman College","Standard Grant","Earnestine Psalmonds","09/30/2022","$150,000.00","Mark Lee, Jerry Volcy","mayalew@spelman.edu","350 Spelman Lane SW","Atlanta","GA","303144399","4042705897","CSE","024Y","028Z, 8083, 9102","$0.00","The project is a collaborative effort among the University of Tennessee Chattanooga, Tuskegee University, Spelman College, and West Virginia University to integrate and automate biological big data into student training and education. Leveraging the team's expertise in computer science and ecology, the project will offer training workshops on using network models to integrate heterogeneous genomic big data and heterogeneous ecological big data to address life sciences questions. The team will engage faculty and students in developing a protocol to automate field data collection. The team also will prototype automated methods to enhance plant digitization, leveraging the collection of digitized plant images and meta-information at the Southeast Regional Network of Expertise and Collections, as well as the ecological datasets in collaboration with the Encyclopedia of Life.<br/><br/>The project objectives are to (1) enhance faculty expertise in big biological data through summer workshops; (2) catalyze interdisciplinary collaboration on big biological data research and education through hackathons, working groups, and community-building via a Video Education Faculty Network; and (3) develop hands-on, constructively peer-evaluated learning modules incorporating high-quality video tutorials. The proposed activities will address challenges surrounding the integration and automation of big biological data into education and training at predominantly undergraduate institutions and Historically Black Colleges and Universities. The project will help bridge the gaps between big biological data and the fields of systems biology, ecology and evolution, and environmental sciences. Overall, the project will catalyze collaborations among diverse institutions and disciplines while increasing diversity in big data. <br/><br/>This award is co-funded by the Improving Undergraduate STEM Education: Education and Human Resources (IUSE): EHR Program (NSF 17-590). IUSE supports projects that are designed to improve student learning through development of new curricular materials and methods of instruction and development of new assessment tools to measure student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920011","MRI: Acquisition of FlashTAIL - An All-NVMe Flash Storage Instrument for the Talon Artificial Intelligence & Machine Learning Cloud","OAC","Major Research Instrumentation, Data Cyberinfrastructure","10/01/2019","09/11/2019","Aaron Bergstrom","ND","University of North Dakota Main Campus","Standard Grant","Alejandro Suarez","09/30/2022","$229,390.00","Jeremiah Neubert, Prakash Ranganathan, Susan Ellis-Felege","aaron.bergstrom@email.und.edu","264 Centennial Dr Stop 7306","Grand Forks","ND","582027306","7017774151","CSE","1189, 7726","062Z, 075Z, 1189, 9150","$0.00","This Major Research Instrumentation (MRI) award supports the acquisition of a data storage instrument named FlashTAIL that will be managed by University of North Dakota (UND). This instrument will enable high speed data transfers for high-performance computing environments that use Graphical Processing Unit (GPU) computational accelerators and specialize in advanced Artificial Intelligence (AI) and Machine Learning (ML) applications. UND has committed to significant investments over the next five years to hire new faculty and form a cluster of computational researchers with specializations in areas relevant to Big Data, AI, and ML. FlashTAIL will allow these researchers and other faculty at UND to grow Data Science, AI, and ML research capabilities across a wide swath of university departments. Courses offered by the participating researchers will attract students from departments to which cluster faculty have collaborative relationships. Course training will incorporate the GPU computing resources enhanced by the FlashTAIL instrument, providing students with the opportunity to work within a cutting-edge AI computing ecosystem. Through these avenues, FlashTAIL will improve the competitiveness of North Dakota research, contribute to the emergence of a diverse tech-enabled regional workforce, and assist university faculty in their efforts to address the Grand Challenge objectives regarding unmanned aircraft systems (UAS), big data and AI, rural health, and energy sustainability.<br/><br/>ML workflows often require the use of large datasets to train predictive algorithms that serve as the basis for robust AI. If an efficient data pipeline is not implemented, the algorithm training process can turn from a computationally restricted (compute bound) problem into a slower input/output (I/O bound) problem. This state of performance is known as ""data starvation"" and occurs when processors must wait for the data pipeline to deliver more data before the ML training can continue. The slower the training process, the less overall computational work can be completed, making advanced AI solutions more difficult to implement. Acquisition of FlashTAIL will serve to mitigate the impact of ""data starvation"" on ML workflows by equipping UND's OpenStack cloud system, Talon, with the DataDirect Networks AI400 storage instrument capable of streaming large datasets at a concurrent high input/output operations per second (IOPS) data rate of 40GB/s - 20GB/s to each of Talon's two HPE Apollo 6500 NVLink GPU-enabled compute nodes (8 x Nvidia Tesla smx2 V100 GPU cards per node). This will allow UND to support faculty research that requires large datasets to properly train predictive ML algorithms. This will improve the ability of UND research to develop computer vision AI applications for large-scale, large-dataset UAS-collected data for projects such as wildlife surveys and building and infrastructure analysis, and as well as small-scale, large-dataset microscopy-collected data for evaluating 3D biointerfaces for biological tissue development<br/><br/>This award is managed by and jointly funded through the MRI and Data programs within the NSF Office of Advanced Cyberinfrastructure (OAC) and the Electronics, Photonics and Magnetic Devices (EPMD) program within the Division of Electrical, Communications and Cyber Systems (ECCS).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906052","Collaborative Research: SS2-SSI: The Agave Platform: An Open Science-As-A-Service Cloud Platform for Reproducible Science","OAC","Software Institutes","10/01/2018","02/05/2019","Rion Dooley","CA","Chapman University","Standard Grant","Bogdan Mihaila","07/31/2021","$1,240,246.00","","deardooley@gmail.com","One University Drive","Orange","CA","928661099","7146287383","CSE","8004","7433, 8004, 8009","$0.00","In today's data-driven research environment, the ability to easily and reliably access compute, storage, and derived data sources is as much a necessity as the algorithms used to make the actual discoveries. The earth is not shrinking, it is digitizing, and the ability for US researchers to stay competitive in the global research community will increasingly be determined by their ability to reduce the time from theory to discovery.  Over the last 5 years, the open source commercial sector has greatly outpaced the academic research world in its growth and adoption of programming languages, infrastructure design, and interface development. Problems that were primarily academic in nature several years ago are now common in the commercial world. Terms like big data, business intelligence, remote visualization, and streaming event processing, have moved from the classroom to the board room.  However, academic projects are largely unable to take advantage of many today's most popular and widely used open source technologies within the context of their campus and shared research infrastructure. The recently completed, NSF funded, Science Gateway Institute planning project revealed just how far behind many communities are. In a survey of over 26,000 NSF-funded PIs, science gateway developers, and leaders in higher education (i.e., CIOs, CTOs, and others), over 85% of respondents said they needed help adapting existing technologies to realize the needs of their gateway. Another 80% said they needed help simply understanding what technologies were available to them. The research community doesn't just see the gap, they live it. This project seeks to quickly close the capability gap between academic and commercial infrastructure by extending and making robust the Agave Platform, an open, Science-as-a-Service cloud platform for reproducible science. Essentially, this project will allow scientists to focus their energies on their science rather than so much on the computing technologies they use. <br/><br/><br/>This Agave Platform will build upon the success of the existing Agave Developer APIs which currently serve over 20,000 users in the plant biology community. This project includes three well-defined efforts which will synergistically evolve the current technology into a sustainable Science-as-a-Service platform for the national research community. First,it will extend the Agave Developer APIs with additional services and management interfaces to create a cohesive, self-provisioning Agave Platform which will enable Science-as-a-Service to the developer community. Second, the project team will partner with commercial and academic institutions to create a community driven Application Exchange (AX) based on Docker container technology to facilitate application transparency, portability, attribution, and reproducibility. Third, the project will consolidate existing open source contributions from projects already with the Agave ecosystem into Agave ToGo, a collection of reference science gateways in multiple languages and web frameworks. The Agave Platform will democratize access to software and infrastructure across all areas of science and engineering by modernizing the mechanisms with which the research community can utilize and access academic research infrastructure. This will bridge the gap between industrial and academic research infrastructure and allow researchers to use a new generation of open source software and technologies. The AX will enable greater interoperability and accountability in the way computational science results are published and reviewed. Through the matching investment of industrial partners, reproducibility, best practices, and rigorous scientific review will be brought to the mainstream and promoted as a fundamental aspect of the scientific process in an open, sustainable way. Agave ToGo will make custom gateways readily available to end users and developers alike. For end users, it will empower them to focus on domain science rather than computer science. For developers, it will stimulate innovation and increase the opportunity for discovery. When combined with the Agave Platform and Application Exchange, Agave ToGo will enable novice users to create scalable, reproducible, digital labs that span their office, commercial cloud, and national data centers in a matter of minutes."
"1747490","Collaborative Research: Building the Community for the Open Storage Network","OAC","CYBERINFRASTRUCTURE, EarthCube","06/15/2018","10/15/2020","Michael Norman","CA","University of California-San Diego","Standard Grant","Alejandro Suarez","05/31/2021","$494,918.00","Christine Kirkpatrick","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7231, 8074","062Z","$0.00","The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?<br/><br/>Many universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2200907","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","12/01/2021","10/27/2021","Sapna Sarupria","MN","University of Minnesota-Twin Cities","Standard Grant","Alan Sussman","01/31/2026","$105,000.00","","sarupria@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","044Y","7231, 7301, 9102, 9150","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829701","Collaborative Research: CyberTraining: CIU: Towards Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Blake Joyce","AZ","University of Arizona","Standard Grant","Alan Sussman","08/31/2022","$60,631.00","","bjoyce@uab.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940080","Collaborative Research: Science-Aware Computational Methods for Accelerating Data-Intensive Discovery: Astroparticle Physics as a Test Case","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Hagit Shatkay","DE","University of Delaware","Continuing Grant","Vyacheslav (Slava) Lukin","09/30/2022","$333,338.00","","shatkay@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","099Y","062Z, 9150","$0.00","The rapid technological advances of the last two decades have ushered in an era of data-rich science for several disciplines.  One such discipline is astroparticle physics, where researchers aim to discover what our Universe is made of by trying to directly detect Dark Matter. This discovery can be hastened if data science tools are used to extract significant domain-specific information from data, and to reliably test scientific hypotheses at scale. The overarching goal of this two-year project is to lay the groundwork for incorporating scientific knowledge into machine learning and data science methods in the context of scientific disciplines in which discovery requires effective, efficient analysis of lots of noisy data gathered by multiple imperfect sensors. In doing so, it not only advances the state-of-the-art in data science, machine learning, and astrophysics, but it also has the potential to accelerate data-driven discoveries in other scientific disciplines where data shares similar characteristics.<br/><br/>This project will develop innovative domain-enhanced data science methods that will be based on probabilistic graphical models and graph-regularized inverse problems. Using the leading astroparticle experiment XENON as a test bed, the investigators will explore and demonstrate approaches for incorporating domain knowledge into machine learning and data science methods. In doing so, the investigators will address major data-analysis challenges in the context of dark matter identification. Additionally, the investigators will invest significant effort reaching out to other data-intensive science communities, such as materials science, oceanography, and meteorology, that can benefit from the new methods and ideas. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1909856","OAC Core: Small: Collaborative Research: Data Provenance Infrastructure towards Robust and Reliable Data Sharing and Analytics","OAC","OAC-Advanced Cyberinfrast Core","07/15/2019","07/15/2019","Kyu Hyung Lee","GA","University of Georgia Research Foundation Inc","Standard Grant","Robert Beverly","06/30/2022","$249,880.00","","kyuhlee@uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","CSE","090Y","026Z, 9179","$0.00","The rapidly increasing number of interconnected devices and systems brings unprecedented collaborative opportunities between researchers, business partners, and healthcare organizations that can extend discoveries beyond those derivable from any single study. For instance, data sharing between various medical organizations can enhance understanding of the results from an individual clinical by polling of results from various trials from multiple organizations, and thus it enables extending the analysis of treatment options and accelerating biomedical research. While a vast amount of data collected from various sources brings us benefits, it imposes, at the same time, an important challenge of ensuring trustworthiness and quality of data due to the integration of disparate data from various sources. Furthermore, faulty, improperly configured, or broken sensors, as well as buggy or compromised data processing units, can severely affect the quality of data and the analyzed results. This project proposes to develop an infrastructure that provides robust, fine-grain, and end-to-end provenance for collaborative data sharing and analytics. The outcome of this research will directly serve as the foundation of trustworthy data sharing and analytic infrastructures by providing robust and attack/fault-resilient fine-grain provenance of the shared data by creating fine-grain end-to-end data provenance framework for diverse communication infrastructures.<br/><br/>In this project, the PIs will develop an end-to-end data provenance framework that provides robust and fine-grain data lineage for trustworthy data sharing and analytic infrastructures. First, they will develop a scalable and reliable infrastructure for collecting data provenance for distributed interconnected devices that can derive a concise provenance data in various environments of distributed devices (e.g., devices using diverse hardware and software platforms). Next, they will design and implement a framework to enable proper derivation and propagation of fine-grain provenance records for data sharing, processing, and analytics. It will provide services for provenance tracking as well as provenance record processing when the data are aggregated, analyzed, and processed (e.g., merge, split, duplicate, delete, extract, or statistical analytics). The PIs also plan to develop a system that can analyze and visualize the complete lineage of data to measure the quality of data and conduct various root cause analyses to identify the fundamental reasons behind data quality issues. The system will be capable of handling a large amount of data generated over a long period of time across multiple devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934649","Research Data Alliance Ideate Incubator (RDA-I2) : Conceptualizing an Interdisciplinary Research Framework for Strengthening Community Impact and Advancing an Innovation Platform","OAC","CYBERINFRASTRUCTURE","02/01/2020","08/26/2020","Rebecca Koskela","NJ","Ronin Institute for Independent Scholarship Incorporated","Standard Grant","Amy Walton","01/31/2023","$2,202,049.00","Lynn Yarmey","rebecca.koskela@rda-foundation.org","127 Haddon Place","Montclair","NJ","070432314","9737072485","CSE","7231","7231","$0.00","The Research Data Alliance (RDA) is a global organization providing a community infrastructure of researchers, data scientists, librarians, practitioners who work within and across disciplines to identify and solve grand challenges of data sharing and interoperability.  This project augments the RDA infrastructure with an incubator framework that can be used to accelerate data sharing and data-driven innovation, by providing targeted community support and strategic dissemination of community outputs. The goals are to establish mechanisms to integrate and link work across different RDA working groups and improve the discoverability and usability of RDA outputs.<br/><br/>The project includes a pilot facilitation training program for RDA group chairs, to share strategies and tactics for building better connections across efforts. In a quickly-scaling organization, the chairs are uniquely placed as a consistent, visible leadership presence for all group participants. By equipping chairs with skills to lead more inclusive session discussions that give space to the knowledge of the many participants, RDA can better leverage the Plenary meetings to connect groups. A second goal of the project is to increase the intelligibility and discoverability of RDA outputs, to prepare for broader adoption of the RDA work. RDA's current process of documenting and tracking approved and supported RDA outputs makes it difficult to capture the full array of work associated with RDA, including publications, posters, lectures, webinars, white-papers, and adoption stories. Developing a strategy to better track output development and adoption would enhance the capability to support all group work and to monitor, measure, and disseminate the impact of RDA group outputs within the wider scientific community. In addition, enhancing the documentation of organizational outputs would improve the ability of group collaboration and cross-fertilization of ideas, as outputs are reused and adapted to additional purposes, methods, and audiences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007976","OAC Core: SMALL: DeepJIMU: Model-Parallelism Infrastructure for Large-scale Deep Learning by Gradient-Free Optimization","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","06/17/2020","Liang Zhao","VA","George Mason University","Standard Grant","Alan Sussman","12/31/2020","$498,609.00","Yue Cheng","liang.zhao@emory.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","090Y","075Z, 079Z, 7923","$0.00","In recent years, the use of deep neural networks (DNNs) has been increasing to obtain useful insights for scientific explorations, business management, security, and healthcare. The constant improvement of DNN model performance has been accompanied by an increase in their complexity and size, which indicate a clear trend toward larger and deeper models. Such a trend is especially the case for numerous important application domains, such as remote sensing where super-high-resolution geospatial image processing is required. Such applications lead to a huge challenge for the training of very large models to fit on a single computing device (e.g., a graphics processing unit, GPU), and hence raises urgent demands for partitioning such models across multiple computing devices and parallelizing the training process (i.e., model parallelism). However, until now model parallelism for DNNs has been poorly explored and is very difficult due to the inherent bottleneck from the backpropagation algorithm, where the training of one layer closely depends on input from all the previous layers. To overcome these challenges, this project aims a radically new pathway toward model parallelism infrastructure for large-scale DNNs based on optimization methods that do not rely on backpropagation for training. This project plans to address the challenges of training very large and very deep neural network models that require huge amounts of high-dimensional data. The project will develop new optimization techniques and distributed DNN training software infrastructure to enable wider applications and deployment of model parallel deep learning training. The project includes educational and engagement activities that will greatly increase the community's understanding of distributed machine learning algorithms and systems. Those activities include teaching and training students and peers, providing graduate and undergraduate students with new courses, and research and internship opportunities, as well as broadening participation of underrepresented groups and students at local high schools.<br/><br/>This project brings together researchers in machine learning algorithms, distributed computing systems, remote sensing, and spatial data science, to boost the performance and scalability of deep learning applications enhanced by model parallelism. Specifically, this project focuses on proposing and developing a suite of new model parallelism optimization algorithms and system infrastructure for training large-scale DNNs, especially for image processing of massive datasets for geospatial scientific research. To enable model parallelism in the training, new gradient-free optimization methods are proposed to break down the whole problem of DNN optimization into subproblems, which can then be solved separately in parallel (by many workers) with high efficiency. The products of this project include new theories and algorithms for model parallelism, along with an efficient gradient-free DNN training framework with new scheduling and work balancing techniques. Specifically, this project has the following research thrusts: 1) Develop new gradient-free methods for training various types of DNNs; 2) Designing an algorithmic and theoretical framework of model parallelization based on gradient-free optimization; and 3) Building a scalable and efficient distributed training framework for a broad range of model parallel DNN training applications, such as deep learning for large graphs and very deep convolutional neural networks for image processing. This project also involves both theoretical and experimental comparison between the new techniques and current state-of-the-art methods, including those using gradient-based optimizations and pipeline parallelism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118302","CyberTraining: Implementation: Medium: Collaborative Research: Computational and Data-Centric Ecology Training","OAC","CyberTraining - Training-based","01/01/2022","09/08/2021","Benjamin Galluzzo","NY","Clarkson University","Standard Grant","Alan Sussman","12/31/2024","$920,482.00","Eric Simoneau","bgalluzz@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","044Y","7231, 7301","$0.00","While the scientific community has ever cheaper and more rapid access to large amounts of data with respect to ecology and other issues, the training necessary to take full advantage of these large data streams has not kept up. This project will create an online learning and community platform known as Data4Ecology to support the integration of computing, statistics, and data science into undergraduate ecology curriculum and courses. To do so, this proposal will build a freely accessible website enabling students and others in ecology and allied STEM disciplines to develop the skills in computational and statistical reasoning necessary to address problems related to ecology and data science. <br/><br/>This project, Data4Ecology, will develop and deploy a website to curate, assemble and embed open educational and learning resources that will serve as training for a multitude of undergraduate students in ecology and environmental science at various college and universities. In turn, this project will integrate computing, statistics, and data science into an ecology curriculum. The project will build and make accessible collaboration and community resources while providing professional development and disseminating project products. The project will serve the larger scientific community by providing researchers and educators with a dedicated repository of data-centric ecology data, resources, and curriculum. Further, the learning resources from this project will be ported to STATS4STEM, an NSF-funded project that collects educational resources on data, computing, and statistics for use by educators and students at many levels, thus exposing users beyond the project to an expanded pool of ecology-based data-centric learning resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1928147","Category I: Bridges-2: Scalable Converged Computing, Data, and Analytics for Rapidly Evolving Science and Engineering Research","OAC","Innovative HPC","10/01/2019","08/16/2021","Shawn Brown","PA","Carnegie-Mellon University","Cooperative Agreement","Robert Chadduck","09/30/2022","$22,499,999.00","Sergiu Sanielevici, Paola Buitrago, Robin Scibek, Jason Sommerfield","stbrown@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7619","","$0.00","Unlocking the power of data will accelerate discovery to advance science, improve our quality of life, and enhance national competitiveness. The increasing importance of data is driving a rapid evolution of research, creating opportunities for breakthroughs by combining the strengths of artificial intelligence (AI) and high-performance computing (HPC) simulation. To address this exciting opportunity for the nation's research community, the Pittsburgh Supercomputing Center (PSC), a joint research center of Carnegie Mellon University and the University of Pittsburgh, in partnership with Hewlett Packard Enterprise (HPE), will deploy Bridges-2, a resource that will provide exceptional capacity and transformative capability for rapidly evolving, data- and computation-intensive research. Building on PSC's experience with its very successful Bridges system, Bridges-2 will take the next step in pioneering converged, scalable HPC, AI, and data; prioritize researcher productivity and ease of use; and provide an extensible architecture for interoperation with complementary data-intensive projects, campus resources, and clouds. Bridges-2 will deploy new high-performance technologies to deliver results faster by combining AI with simulation and through exceptional data-handling capabilities and will be integrated with NSF-funded cyberinfrastructure (CI) services for allocation, user-support, monitoring, etc., such as those currently provided by the Extreme Science and Engineering Discovery Environment (XSEDE) and XDMoD.<br/><br/>Bridges-2 will accelerate discovery to benefit science, society, and the nation. Its unique architecture will catalyze breakthroughs in critically important areas such as understanding the brain, developing new materials for sustainable energy production and quantum computing, assembling genomes of crop species to improve agricultural efficiency, exploring the universe via multimessenger astrophysics, and enabling technologies for smart cities. A great amount of valuable research is in areas that did not traditionally use HPC until Bridges. Bridges-2 will therefore serve nontraditional communities and applications, which now have workloads requiring great capacity, as well as traditional ones. Its community data collections and user-friendly interfaces will democratize participation in science and engineering and foster collaboration and convergence research. The Bridges-2 project includes areas of focus on developing STEM talent, engaging industry, reaching beyond borders, and broadly engaging stakeholders to develop the nation's workforce, maximize learning through collaboration, and bring the benefits of advanced computing to the general public.<br/><br/>PSC is integrating cutting-edge technologies to deploy a uniquely powerful national resource for turning data into knowledge. Bridges-2 will consist of Hewlett Packard Enterprise Apollo 2000 and ProLiant DL560 central processing unit (CPU) servers of three different memory capacities (256GB, 512GB, and 4TB) for extremely broad workloads; Apollo 6500 servers each with two CPUs and eight graphics processing units (GPUs)  for AI (particularly deep learning) and to accelerate simulations; and ProLiant DL360/380 servers for gateways, databases, and utility and management functions. A hierarchical data management system using HPE's Data Management Framework (DMF) will provide seamless access with exceptional ease of use to active and archival data, spanning a Lustre parallel filesystem and StoreEver MSL6480 Tape Library. An HPE TierZero high-performance flash array will provide even higher performance for data-intensive applications. A Mellanox HDR InfiniBand interconnect will provide unified, high-performance data transport across compute and storage components. The Bridges-2 user environment will be extremely robust and flexible, supporting HPC applications, AI frameworks, high-productivity programming languages, containers, interactive and batch execution, workflows, databases, and science gateways. Bridges-2's uniquely flexible software environment and heterogeneous hardware resources will serve the needs of rapidly evolving research, opening the door to discovery and the path to prosperity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2001329","Workshop on Clusters, Clouds, and Data Analytics for Scientific Computing","OAC","EDUCATION AND WORKFORCE","09/01/2020","10/15/2020","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Alan Sussman","08/31/2023","$20,000.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7361","026Z, 7556","$0.00","This workshop will be held in La Maison Des Contes, Dareize, France, and will bring together international experts from the US, France, and other countries to discuss advances in cyberinfrastructure (CI). The specific focus will be on commonalities that exist between cluster, cloud, and high-end data analytics. CI has become of growing importance to nearly all NSF research, which makes increasing use of computational simulations and large-data analytics methods. The results will be published in an open report as well as in multiple journal papers. The results will contribute to and leverage new ideas in cluster, cloud, and data analytics computing for the benefit of the research community.<br/><br/>The architectural similarities between the above issues, and the fact that high performance clusters typically make up the major compute nodes of computational grids and clouds, means that deployment, operational, and usage issues surrounding computational clouds form a superset of the issues that revolve around clusters. The workshop will focus on five tasks: (1) survey and analyze the key deployment, operational, and usage issues for clusters, clouds, and data analytics-- focusing especially on discontinuities produced by multicore and hybrid architectures, data-intensive science, and the increasing need for wide-area/local-area interaction; (2) document the current state-of-the-art in each of these areas, identifying interesting questions and limitations; (3) discuss experiences with clusters, clouds, and data relative to the research communities and science domains benefiting from the technology; (4) explore interoperability among disparate clouds and between various clouds and grids and their impact on domain sciences; and (5) explore directions for future research and development against the background of disruptive trends and technologies and the recognized gaps in the current state-of-the-art.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004510","Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science","OAC","ANT Glaciology, Software Institutes","10/01/2020","08/03/2020","William Lipscomb","CO","University Corporation For Atmospheric Res","Standard Grant","Alan Sussman","09/30/2025","$162,478.00","","lipscomb@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","5116, 8004","026Z, 077Z, 7925, 8004","$0.00","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.<br/><br/>The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934389","Collaborative Research: Near Term Forecasts of Global Plant Distribution, Community Structure, and Ecosystem Function","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Laura Duncanson","MD","University of Maryland, College Park","Continuing Grant","Peter McCartney","08/31/2022","$296,104.00","","lduncans@umd.edu","3112 LEE BLDG 7809 Regents Drive","College Park","MD","207425141","3014056269","CSE","099Y","062Z","$0.00","This project is the first to explore how plant species distributions across the entire globe may respond to global change. The project brings together ecologists, environmental engineers, data scientists, and conservation stakeholders to determine optimal ways to integrate these data sources to make near term forecasts for all plants globally by addressing changes in (1) species' abundance and geographic distribution, (2) community structure, and (3) ecosystem function. This three-pronged approach is designed to span a range of approaches to understand the spectrum of possible futures consistent with current knowledge while integrating knowledge across scales of biological organization. These forecasts will be used along with input from conservation stakeholders to assess how differing conservation decisions can minimize the impacts of global change responses. An ultimate goal of the project is to automate a pipeline to ingest new incoming data, update forecasts, and serve these to end-users to enable a near-real time forecasting workflow to provide best-available predictions at any given time to inform conservation decisions. <br/><br/>A key aspect of these forecasts is their reliance on novel environmental information that better characterize the conditions that influence plant performance, including soil moisture and extreme weather events based on NASA satellite observations. These species-level predictions will be linked to community demography models that integrate a variety of relatively untapped data sources for understanding global change, including plant trait data, community plot data across the globe, highly detailed plot data from National Ecological Observatory Network (NEON) and Long Term Ecological Research (LTER) sites, and global biomass data from NASA's Global Ecosystem Dynamics Investigation (GEDI) mission. By integrating this wide variety of data sources, the mechanistic understanding needed to make robust near term forecasts can be made, to understand ecosystem properties like Net Primary productivity, Carbon stock, and resilience. Based on workshops with conservation stakeholders, researchers will determine how best to use this unique suite of forecasts to best inform different conservation questions in different regions of the world. The project will also result in an open, cleaned and curated database on global plant distributions. This will aid others in exploring data and predictions by delivering and visualizing complex future scenarios in an easy to use portal. All results of the project can be found at the website for the Biodiversity Informatics and Forecasting Institute or BIFI, at https://enquistlab.github.io/BIFI .<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914715","NSF Student Travel Grant for 2019 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (PADS)","OAC","EDUCATION AND WORKFORCE","04/01/2019","03/21/2019","Dong Jin","IL","Illinois Institute of Technology","Standard Grant","Alan Sussman","03/31/2020","$5,000.00","Philippe Giabbanelli","dongjin@uark.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7361","026Z, 7556, 9179","$0.00","Discrete simulations are widely employed across a variety of settings, from the large-scale models that are commonplace in national laboratories (e.g., to simulate particles or populations) to industrial applications (e.g., in healthcare or transportation). The growing interest in simulations is also fueled by dynamic research in big data, which is both used and generated by simulations. In line with ongoing national efforts to train and diversify the workforce in science, this award supports students in the United States to attend the annual Conference on Principles of Advanced Discrete Simulation held in Chicago in June, 2019, with special emphasis on members of under-represented groups such as female and minorities. This serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Selected students have access to traditional opportunities to support their career success and gain tools that support the advancement of science in theoretical as well as applied research on discrete simulations. Such traditional opportunities include presenting their work at the event (through oral presentations as well as posters), developing their scientific networks, and acquiring new state-of-the-art methods. Although the conference builds on a rich history dating back to 1985, the innovative format of the 2019 edition goes beyond traditional opportunities offered to students. In particular, the conference actively supports workforce development and the advancement of science by organizing (i) panels on careers beyond a doctoral program, with contributors from national laboratories, liberal arts colleges, and research universities; and (ii) round tables with editors of several prominent journals in the field. The funding provided by NSF thus has a significant impact on the careers of the future generation of researchers in discrete simulation, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827199","Nilch Bee Naa Alkaa Go Ohooa Doo Eidii Tii  (Using Air (Technology) to Learn and Understand New Things)","OAC","Campus Cyberinfrastructure","09/01/2018","08/18/2018","Jason Arviso","NM","Navajo Technical University","Standard Grant","Kevin Thompson","08/31/2022","$667,909.00","Marla Meehl, Jared Ribble, John Hernandez","jarviso@navajotech.edu","LOWER POINT ROAD","Crownpoint","NM","873130849","5057864112","CSE","8080","","$0.00","Navajo Technical University (NTU) is one of the nation's largest tribal colleges and a leader in delivering academic and research programs for Native Americans.  NTU students have access to a plethora of academic programs including strong programs in Science, Technology, Engineering, and Math (STEM).  However, NTU and the residents of the Navajo Nation are not well connected to the Internet and to the larger research and education community.  Connectivity limitations, especially at Navajo community centers and at their homes, restrict NTU's ability to collaborate and contribute in the ever-growing integrated global research and education environment.  There is a fundamental lack of Internet connectivity with sufficient bandwidth to successfully participate in the ever-increasing distance or online learning courses/programs. <br/><br/>This proposal will increase Wide Area Network connectivity by connecting NTU to the Front Range GigaPoP (FRGP) regional network at much higher network speeds with dedicated bandwidth for NTU research and academic projects.  The proposal addresses distance education challenges by implementing an advanced wireless test bed to deliver NTU distance education courses to Chapter Houses, tribal libraries, and other community anchor locations.  This proposal engages the country's largest tribal university and is a collaboration with New Mexico and Arizona Tribal Colleges and Universities.  It leverages a strong existing regional relationship with the FRGP, and it provides an organizational model for other tribal colleges to adopt a similar technology and associated collaborations.  The proposal emphasizes needs and requirements-gathering meetings, followed by design and training workshops, which will benefit regional Native American community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818661","US-Serbia and West Balkan Data Science Workshop","OAC","BD Spokes -Big Data Regional I, GVF - Global Venture Fund, Archaeology","06/01/2018","05/24/2018","Zoran Obradovic","PA","Temple University","Standard Grant","Beth Plale","05/31/2019","$99,000.00","","zoran.obradovic@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","024Y, 054Y, 1391","5980, 8083","$0.00","Data science research advances society globally through new knowledge and methods.  Research collaborations that cross international boarders allows the best ideas to emerge from wherever they exist.  The US-Serbia and West Balkan Data Science Workshop, co-funded by the National Science Foundation and partners in West Balkans region, brings together the best minds in data science.  The workshop focuses on four timely areas of research:  mathematical foundations, big data processing including security and privacy, data science in biomedical informatics, and data science in computational archaeology.<br/><br/>The US-Serbia and West Balkan Data Science Workshop is structured in such a way as to have a lasting impact on data science collaboration between the US and the West Balkans region through facilitated interaction and follow through on high impact research topics in data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940276","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE, Big Data Science &Engineering","10/01/2019","09/17/2019","David Matteson","NY","Cornell University","Standard Grant","Amy Walton","09/30/2022","$734,697.00","Remi Cousin","dm484@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","099Y, 7231, 8083","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2009007","OAC: Small: Data Locality Optimization for Sparse Matrix/Tensor Computations","OAC","OAC-Advanced Cyberinfrast Core","07/01/2020","06/11/2020","Ponnuswamy Sadayappan","UT","University of Utah","Standard Grant","Alan Sussman","06/30/2023","$499,391.00","","saday@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","090Y","079Z, 7923","$0.00","The cost of data movement vastly exceeds the cost of execution of arithmetic operations on current computers and the imbalance is only expected to get worse. Hence the minimization of data movement in the implementation of algorithms is critical. Tiling is a well known technique for data-locality optimization and is widely used in compilers as well as high-performance numerical libraries for dense matrix/tensor computations. However, data-locality optimization for sparse computations is a significant challenge, in large part because the data access patterns are not known a priori. This project proposes a plan of research to systematically explore a number of issues pertaining to data-locality optimization for sparse matrix/tensor computations. The project identifies an important subclass of sparse computations used in machine learning and data analytics, and proposes tools and techniques to enable high-performance parallel implementations on multicore CPUs and GPUs. The broader impact of the project will be the enhancement of programmer productivity and the enabling of software portability and high performance for applications in data analytics and machine learning.<br/><br/>The challenge of data-locality optimization for the data-dependent and irregular access patterns that occur with sparse matrix/tensor computations will be addressed through research along multiple directions: 1) Compact signatures for sparse matrices: the strong relationship between the data access patterns for key sparse matrix primitives of use in machine learning and data analytics drives the development of one-dimensional signature vectors that capture the essential characteristics of the two-dimensional sparsity pattern as it pertains to needed data movement in a memory hierarchy; 2) Sparse tiling: Sparse matrix signature vectors will serve as a basis for dynamic decisions based on target platform characteristics, for tile size selection and scheduling of tiles for load-balanced execution; 3) Matrix renumbering/reordering: The impact of row/column reordering on the performance of sparse matrix primitives will be investigated, and new reordering schemes will be devised to enhance data-locality for key sparse matrix/tensor primitives; 4) Sparse microkernels: Microkernels will be developed and optimized for CPUs/GPUs, and used as the lowest-level building blocks that execute the innermost tiles in the tiled execution of sparse matrix/tensor computations; 5) Architecture-aware performance prediction: Models will be developed that combine analysis of predicted data-movement volume in combination with machine learning using algorithmic and architectural features.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808530","CDS&E: Collaborative Research: A Computational Framework for Reconstructing and Visualizing Myocardial Active Stresses","OAC","CDS&E-MSS, CDS&E","10/01/2018","09/06/2018","Cristian Linte","NY","Rochester Institute of Tech","Standard Grant","Tevfik Kosar","09/30/2022","$523,295.00","Niels Otani","clinte@mail.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8069, 8084","026Z, 028E, 8084, 9263","$0.00","The normal heart functions by contracting and pushing the blood from the left ventricle into the rest of the body. Due to various diseases, the contraction capabilities of the heart become diminished in certain regions of the heart chamber wall, compromising the overall function of the heart. In order to identify and select optimal treatment, it is critical to identify the regions of the heart wall that exhibit reduced contractions. Unfortunately, contractions cannot be easily measured. This project will estimate the stress (contraction power) developed within the heart muscle by combining medical imaging and mechanical modeling of the heart. These stresses will serve as a quantitative measure of the contractile function of the heart and help detect and localize disease. Therefore, this research has the potential to evolve into a future tool to diagnose cardiac function. This project will also feature a synergistically integrated education and outreach program. We will foster research opportunities for graduate and undergraduate students in computer science, biomedical engineering, mathematics, and imaging science at Rochester Institute of Technology and the University of Kansas. The PIs will develop innovative hands-on workshops to inspire and educate K-12 students from underrepresented groups on biomedical computing and medicine.<br/> <br/>This project proposes to develop a method that enables non-invasive appraisal and visualization of the active stresses developed in the myocardium to serve as a direct means to assess the bio-mechanical function of the heart. The PIs will develop open-source cyberinfrastructure and integrate it into a novel computational framework for cardiac biomechanics that will reconstruct the active stresses from cardiac deformations. The PIs will accomplish this goal by developing and integrating techniques for medical image computing, high-order meshing, and inverse-problem bio-mechanical modeling. This research will address a currently unexplored niche in the cardiac modeling field, specifically the reconstruction and visualization of myocardial active stresses, to enable direct appraisal of cardiac function. This research will contribute to medical image computing through the development of algorithms for medical image processing and visualization. The PIs will develop and implement novel high-order meshing techniques and integrate them with the fiber architecture to enable accurate and efficient scientific modeling and computing. The project will contribute new knowledge in mathematical modeling and simulation by implementing efficient nonlinear least-squares solutions for inverse cardiac biomechanics. Lastly, the PIs will release the resulting cyberinfrastructure to the scientific computing community for research and education use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104013","Frameworks: Collaborative Research: ChronoLog: A High-Performance Storage Infrastructure for Activity and Log Workloads","OAC","Software & Hardware Foundation, Software Institutes","06/01/2021","05/11/2021","Xian-He Sun","IL","Illinois Institute of Technology","Standard Grant","Tevfik Kosar","05/31/2025","$2,676,450.00","Antonios Kougkas","sun@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7798, 8004","077Z, 7925, 7942, 8004","$0.00","Modern computing applications generate massive amounts of data at unprecedented rates. Beyond simply storing data, one increasingly common requirement is to store activity data, also known as log data, which describe things that happen rather than things that are. Activity data are generated by computing systems, scientific instruments, electrical devices, etc. as well as by humans. The fast growing of activity data stresses current data management systems beyond their capability and becomes a known killer performance bottleneck of high-performance computing systems. This project develops ChronoLog, a novel system for organizing and storing activity data effectively and efficiently. ChronoLog leverages modern storage hardware and provides user-focused plugins and easy-to-use interface for productivity. It will benefit a diverse range of communities in various ways, such as enabling better fraud detection in financial transactions, faster and more accurate weather predictions and simulations, reduced time-to-insight for medical and bioengineering data, autonomous computing (e.g., driving), and more secure web and mobile services. <br/><br/>ChronoLog uses physical time to provide a synchronization-free data distribution and the total ordering on a log. It first leverages multiple storage tiers, such as storage-class memories (e.g., 3D XPoint) and new flash storage (e.g., NVMe SSDs), to transparently scale the log via log auto-tiering. It then adopts a tunable parallel access model, which offers multiple-writers-multiple-readers (MWMR) semantics and highly concurrent I/O, to fully utilize the multi-tiered storage environment. ChronoLog's innovative design supports high-performance data access via I/O isolation between tails and historical operations, efficient resource utilization with newly developed elastic storage capabilities, and scalability using a novel 3D log distribution. It facilitates data processing pipelining by acting as an authoritative source of strong consistency and with the help of fast append and commit semantics. It can be used as an arbitrator offering a plethora of features such as transactional isolation and atomicity, a consensus engine for consistent replication and indexing services, and a scalable data integration and warehousing solution. ChronoLog and its plugins establish a robust, flexible, and high-performance storage ecosystem that promotes the development of scalable applications and services for high performance computing systems. The project includes a diverse group of collaborators who share a common need for a fundamentally new approach to distributed logging to address their use cases. These close partnerships will strengthen the bonds between academic and applied science, ultimately leading to new applications and driving discovery in domains as diverse as geoscience, cosmology, and astrophysics. Forming these collaborations and integrating students and junior IT professionals will create a well-trained workforce in cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029200","Collaborative Research: IRNC: Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","09/01/2020","08/12/2020","Anita Nikolich","IL","Illinois Institute of Technology","Continuing Grant","Kevin Thompson","03/31/2021","$19,707.00","","anitan@illinois.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7369","","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013750","NSF Student Travel Grant for 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (PADS)","OAC","EDUCATION AND WORKFORCE","04/01/2020","01/07/2021","Yuan Hong","IL","Illinois Institute of Technology","Standard Grant","Alan Sussman","03/31/2022","$5,000.00","Philippe Giabbanelli","yuan.hong@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7361","026Z, 7556, 9179","$0.00","Discrete simulations are widely employed across a variety of settings, from the large-scale models that are commonplace in national laboratories (e.g., to simulate particles or populations) to industrial applications (e.g., in healthcare or transportation). The growing interest in simulations is also fueled by dynamic research in big data, which is both used and generated by simulations. In line with ongoing national efforts to train and diversify the workforce in science, this award supports students in the United States to attend the annual Conference on Principles of Advanced Discrete Simulation held in Miami in June, 2020, with special emphasis on members of under-represented groups such as female, minorities, and students with disabilities. This serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Selected students have access to traditional opportunities to support their career success and gain tools that support the advancement of science in theoretical as well as applied research on discrete simulations. Such traditional opportunities include presenting their work at the event (through oral presentations as well as posters), developing their scientific networks, and acquiring new state-of-the-art methods. Although the conference builds on a rich history dating back to 1985, the innovative format of the 2020 edition goes beyond traditional opportunities offered to students. In particular, the conference actively supports workforce development and the advancement of science by organizing (i) panels on careers beyond a doctoral program, with contributors from national laboratories, liberal arts colleges, and research universities; and (ii) a PhD colloquium where students create and present a poster, among other activities. The funding provided by NSF thus has a significant impact on the careers of the future generation of researchers in discrete simulation, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835135","NSCI Elements: Software - PFSTRASE - A Parallel FileSystem TRacing and Analysis SErvice to Enhance Cyberinfrastructure Performance and Reliability","OAC","Software Institutes","10/01/2018","08/13/2018","Richard Evans","TX","University of Texas at Austin","Standard Grant","Robert Beverly","09/30/2022","$385,893.00","","rtevans@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","This project will develop an open-source software service, the Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE), that improves the reliability and performance of data storage systems for the nation?s largest supercomputers.  As simulations and computations represent reality more faithfully they grow commensurately in scale along with the size of the data they consume and generate.  To handle the storage and movement of this data, supercomputing systems are built on the backbone of massively parallel data storage systems.  Due to their parallel nature these storage systems are capable of moving data at hundreds of times the speed of conventional storage systems, enabling otherwise impractical computations.  The performance capabilities these storage systems provide is accompanied by a complexity that results in them often functioning significantly less than optimally and even in some instances failing.  This results in wasted computational time and ultimately lost scientific progress.  The state of development of tools that could cast light on these problems and improve storage system reliability and performance is inadequate for current and future computing systems.  PFSTRASE will fill this gap by continually and automatically monitoring storage system health and performance, providing insights through an easy to use interface that will improve the reliability and performance of storage and supercomputer systems. <br/><br/>Parallel filesystems (PFSs) are the most critical high-availability components of High Performance Computing (HPC) architectures, providing input/output (I/O) services to running computations, the environment that users and system services operate in, and storage for applications and data. Because of this central role, failure or performance degradation events in the PFS impact every user of an HPC resource. PFS events must be dealt with quickly and effectively by system administrators; however, there is typically insufficient information to establish precise causal relationships between PFS activity and events, impeding the implementation of timely and targeted remedies. To fill this information gap, an open-source Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE) that traces and analyzes the requisite data to establish causal relationships between PFS activity and both realized and imminent events will be developed. This project will implement the service for the open-source Lustre filesystem, which is the most commonly used PFS at large-scale HPC sites. Loads for specific PFS directory and file operations will be measured and incorporated into the service to construct authentic server load contributions from every job, process, and user. The service?s infrastructure will continuously monitor the entire PFS and generate a real-time, seamless representation that connects contributions of jobs, processes, and users to storage server loads, network bandwidth, and storage capacities. The infrastructure will provide an easily navigable web interface that presents this data, both real-time and historical, in a visual format.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2137123","CI CoE: Demo Pilot: Minority Serving Cyberinfrastructure Consortium","OAC","Campus Cyberinfrastructure","10/01/2021","09/17/2021","Ana Hunsinger","DC","INTERNET2","Standard Grant","Kevin Thompson","09/30/2023","$2,999,998.00","Algirdas Kuslikis, Damian Clarke, Richard Alo, Deborah Dent","ana@internet2.edu","1150 18th St NW","Washington","DC","200363825","7349134264","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The Minority Serving - Cyberinfrastructure Consortium (MS-CC) is a collaborative initiative to improve cyberinfrastructure at Historically Black Colleges and Universities (HBCUs), Tribal Colleges and Universities (TCUs), Hispanic Serving Institutions (HSIs) and other Minority Serving Institutions (MSIs). In collaboration with Internet2 and the American Indian Higher Education Consortium, MS-CC is developing mechanisms to increase access to cyberinfrastructure resources, funding, and professional development opportunities for faculty, staff, and students at HBCUs, TCUs, HSIs, and other MSIs. MS-CC is taking an agile and adaptive approach to operating as a consortium, with a long-term commitment to learning and adjusting based on successes and lessons learned, matched to the diverse size and missions of these colleges and universities.<br/><br/>A key outcome of this grant is the formalization of a vibrant community of practice across MS-CC campuses that involves collaboration on cyberinfrastructure, education, and research applications.  This grant enables HBCUs, TCUs, HSIs and other MSIs to accomplish together what they cannot do separately. The MS-CC is broadening participation in science, technology, engineering, and mathematics (STEM) by historically underrepresented groups in the United States? research enterprise, enabling new perspectives to emerge and expand capabilities for the nation.  MS-CC is advancing our nation?s economic growth, national security and global prosperity in ways that reflect the unique expertise and talent from HBCUs, TCUs, HSIs and other MSIs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2134436","2021 NSF Campus Cyberinfrastructure PI Workshop","OAC","Information Technology Researc, Campus Cyberinfrastructure","10/01/2021","09/13/2021","Jennifer Leasure","WA","The Quilt","Standard Grant","Kevin Thompson","09/30/2022","$11,326.00","Jennifer Griffin","jen@thequilt.net","2442 NW Market Street","Seattle","WA","981074137","2067821091","CSE","1640, 8080","102Z, 7556, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The 2021 Campus Cyberinfrastructure Workshop builds upon the success of the previous Campus Cyberinfrastructure Workshops providing an opportunity for recipients of all active NSF Campus Cyberinfrastructure (CC*) awards to meet in-person, exchange project findings, interact with national cyberinfrastructure experts and collaborate across project areas and project regions. Due to the on-going pandemic, the 2021 PI workshop will be conducted virtually.  The advantage of conducting a virtual workshop is the flexibility of the virtual environment for broader participation of additional CC* award project collaborators such as co-PIs, researchers, and other campus representatives to participate in workshop conversations without the additional time demands and travel expenses for participation.   The workshop program will encourage interactions among the PIs with the broader R&E community through invited participation of cyberinfrastructure experts and representatives from regional and national networks.  As demonstrated before, the broader scope encourages relationships between campus cyberinfrastructure, science driven applications, and regional and national cyberinfrastructure resources.  <br/><br/>The intellectual merit of this project is the interaction between national cyberinfrastructure experts, exchange of project findings and collaboration across project areas and regions, all of which will lead to the new ideas, relationships, and collaborations associated with a focused workshop. The workshop breakout sessions will provide the opportunity for program PIs and co-PIs to gain exposure to other cyberinfrastructure technologies or resources. The workshop furthers the development of cross-discipline, regional, and national collaborations on cyberinfrastructure to support science research and education applications. By including awardees from all areas of the CC* programs, the workshop facilitates interaction and collaboration between research intensive and under-resourced colleges and universities, as well as different cyberinfrastructure program areas, in ways that would not otherwise occur.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104055","Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities","OAC","XC-Crosscutting Activities Pro","10/01/2021","09/13/2021","Allison Pfeiffer","WA","Western Washington University","Standard Grant","Tevfik Kosar","09/30/2026","$71,937.00","","allison.pfeiffer@wwu.edu","516 High Street","Bellingham","WA","982259038","3606502884","CSE","7222","077Z, 102Z, 7925, 8004, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.<br/><br/>As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126280","CC* Planning: Undertaking a Process that will Create a Comprehensive Blueprint for Improving Cyber-Infrastructure at John Jay College, CUNY","OAC","Campus Cyberinfrastructure","07/15/2021","07/09/2021","Anthony Carpi","NY","CUNY John Jay College of Criminal Justice","Standard Grant","Kevin Thompson","06/30/2022","$100,000.00","Shweta Jain, Marie-Helen Maras","acarpi@jjay.cuny.edu","524 West 59th Street","New York","NY","100191069","2122378449","CSE","8080","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>John Jay College of Criminal Justice, a senior college of the City University of New York that is both a Minority-Serving and a Hispanic-Serving Institution, is creating a comprehensive blueprint for improving cyber infrastructure to advance instruction, learning, and research in the critical areas of forensics, cyber security, and bioinformatics. An interdisciplinary team of faculty, research staff, and information technology professionals is convening to identify, enumerate, and prioritize the college?s cyberinfrastructure needs, and produce a blueprint for meeting them. The goal of John Jay?s Campus Cyberinfrastructure planning initiative is to facilitate the development of a John Jay College Research Campus Cyber Infrastructure Resource Cluster. <br/><br/>This team is undertaking a comprehensive needs assessment focusing on the college?s overall management of cyberinfrastructure resources; requirements for hardware and high-performance computing; requirements for software, database, and simulation applications; applications specific to the college?s unique criminal justice focus; and professional development training needs for faculty, staff and students. This rigorous set of planning activities will produce an investment plan for an improved set of research computing resources to facilitate sophisticated faculty research across the disciplines. It further ensures the continued competitiveness of John Jay students in computing-focused career fields, particularly within the justice and security realms. Project deliverables include a five-year cyberinfrastructure investment plan that forms the basis for future proposal submissions to the NSF MRI and CC* programs to seek partial support for implementation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126582","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu","08/01/2021","07/02/2021","Jie Gao","NJ","Rutgers University New Brunswick","Continuing Grant","Sylvia Spengler","03/31/2022","$83,776.00","","jg1555@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","099Y","062Z, 9102","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835818","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes","10/01/2018","09/12/2018","Jerad Bales","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Seung-Jong Park","09/30/2022","$196,235.00","","jdbales@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392267445","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835643","RUI: Framework: Data - An Open Semantic Data Framework for Data-Driven Discovery","OAC","Data Cyberinfrastructure, Big Data Science &Engineering","01/01/2019","09/07/2018","Stuart Chalk","FL","University of North Florida","Standard Grant","Amy Walton","12/31/2022","$605,849.00","","schalk@unf.edu","1 UNF Drive","JACKSONVILLE","FL","322242645","9046202455","CSE","7726, 8083","026Z, 062Z, 077Z, 7925, 9229","$0.00","The project makes contributions to the ease of annotating, sharing, and searching heterogeneous data sets.  It focuses upon undergraduate training, emphasizing data science capabilities applied to a range of science problems.  <br/><br/>The project enables aggregation, search, and inference with heterogeneous datasets using a structured framework allowing data and metadata to be linked by encoding the framework as a JavaScript Object Notation (JSON) for Linked Data (JSON-LD) document.  The approach builds on existing developments such as the Scientific Data (SciData) framework and associated ontology that has been developed by the PI, and Shape Constraint Language (SHACL) shapes to provide efficient searching, browsing, and visualization of data.  The result extends existing approaches to link data and metadata and make data easily discoverable.  The activity emphasizes Research in Undergraduate Institutions (RUI), training more than 30 undergraduates, graduate students and a post-doctoral student in the application of data science techniques to an array of science problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934668","Collaborative Research: Knowledge Guided Machine Learning: A Framework for Accelerating Scientific Discovery","OAC","HDR-Harnessing the Data Revolu","09/01/2019","07/02/2020","Imme Ebert-Uphoff","CO","Colorado State University","Continuing Grant","Eva Zanzerkia","08/31/2022","$399,745.00","Elizabeth Barnes","iebert@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","099Y","062Z","$0.00","The success of machine learning (ML) in many applications where large-scale data is available has led to a growing anticipation of similar accomplishments in scientific disciplines. The use of data science is particularly promising in scientific problems involving processes that are not completely understood. However, a purely data-driven approach to modeling a physical process can be problematic. For example, it can create a complex model that is neither generalizable beyond the data on which it was trained nor physically interpretable. This problem becomes worse when there is not enough training data, which is quite common in science and engineering domains.  A machine learning model that is grounded by explainable theories stands a better chance at safeguarding against learning spurious patterns from the data that lead to non-generalizable performance. This is especially important when dealing with problems that are critical and associated with high risks (e.g., extreme weather or collapse of an ecosystem).  Hence, neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for knowledge discovery in complex scientific and engineering applications. This project is developing novel techniques to explore the continuum between knowledge-based and ML models, where both scientific knowledge and data are integrated synergistically. Such integrated methods have the potential for accelerating discovery in a range of scientific and engineering disciplines. This project will train interdisciplinary scientists who are well versed in such methods and will disseminate results of the project via peer-reviewed publications, open-source software, and a series of workshops to engage the broader scientific community.<br/><br/>This project aims to develop a framework that uses the unique capability of data science models to automatically learn patterns and models from data, without ignoring the treasure of accumulated scientific knowledge. Specifically, the project builds the foundations of knowledge-guided machine learning (KGML) by exploring several ways of bringing scientific knowledge and machine learning models together using pilot applications from four domains: aquatic ecodynamics, climate and weather, hydrology, and translational biology. These pilot applications were selected because they are at tipping points where knowledge-guided machine learning can have a transformative effect.  KGML has the potential for providing scientists and engineers with new insights into their domains of interest and will require the development of innovative new machine learning approaches and architectures that can incorporate scientific principles. Scientific knowledge, KGML methods, and software developed in this project could potentially be extended to a wide range of scientific applications where mechanistic (also known as process-based) models are used.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931306","Collaborative Research: Framework: Machine Learning Materials Innovation Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2019","09/09/2019","Benjamin Blaiszik","IL","University of Chicago","Standard Grant","Seung-Jong Park","09/30/2023","$400,000.00","","blaiszik@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7295, 7925, 8004, 9216, 9263","$0.00","Machine learning is rapidly changing our society, with computers recently gaining skills in many new tasks. These tasks range from understanding language to driving cars. Materials science and engineering is also being transformed. Many tasks are becoming increasingly accessible to machine learning algorithms. These range from predicting new data to analyzing images. Many basic machine learning algorithms are readily available. However the overall workflow involved in the application of machine learning for materials problems is still largely executed by hand. Getting results out is still done by traditional methods like publishing articles. There is an enormous opportunity to accelerate the growth and impact of machine learning in materials research. This requires improved cyberinfrastructure. This project will develop an approach to accelerate the entire machine learning workflow. Its output will include tools to easily develop datasets, manage model development, and output models. These will be reusable and reproducible for future use. This project will enable materials scientists and engineers to rapidly develop and deploy machine learning models. More importantly, the entire materials community will be able to quickly access these models. It will transform how we discover and develop advanced materials.<br/><br/>The project will have three major technical components: (i) A MAterials Simulation Toolkit for Machine Learning (MAST-ML) with workflow tools that will enable local or cloud-based multistep, automated execution of complex machine learning data analysis and model training, codified best practices, increased access to machine learning methods for non-experts, and accelerated model development; (ii) The Foundry Materials Informatics Environment that will provide flexible, integrated, cloud-based management of machine learning materials science and engineering projects, from organizing data to developing models to disseminating results that are machine and human accessible and reproducible in ways that support a networked materials innovation ecosystem, (iii) Representative science applications of machine learning materials science and engineering projects that will support infrastructure development and promotion, as well as demonstrate best practices on state-of-the-art materials science and engineering problems. In addition to its impact on materials science and engineering, this project will develop students and young researchers with the interdisciplinary skills of machine learning and materials science and engineering, and promote these new ideas to the broader materials community. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839746","CICI: SSC: Development of a Secure and Privacy-Preserving Workflow Architecture for Dynamic Data Sharing in Scientific Infrastructures","OAC","Cybersecurity Innovation","09/01/2018","10/01/2021","Xukai Zou","IN","Indiana University","Standard Grant","Robert Beverly","08/31/2022","$599,998.00","Huanmei Wu, Saptarshi Purkayastha","xkzou@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8027","","$0.00","Scientific cyberinfrastructures embrace collaborative workflows where users can access and share heterogeneous data and computing resources to perform research and education tasks, which catalyze scientific discovery. One such cyberinfrastructure, JetStream, is the first production cloud funded by the NSF for general-purpose science and engineering research and education. Although Jetstream provides basic data storage security and web authentication, its security features do not satisfy the strict requirements involving sensitive data, such as healthcare data with protected health information (PHI). This project builds a secure, holistic and resilient cybersecurity architecture on JetStream so that collaborative research and education projects can share PHI securely between its users.<br/><br/>The secured infrastructure provides comprehensive multi-level protection for the PHI and its workflows through user authentication, fine-tuned data access control, confidentiality, integrity, and traceability. The project implements role-wise passwordless authentication and authorization, cryptography-based hierarchical access control, dual-level key management, and secure digital provenance integrity protection. By employing these, JetStream VMs can guarantee the security, privacy, and integrity of scientific workflows and associated data, thus protecting data and computing resources from internal and external attacks. When applied to healthcare and life-science cyberinfrastructures, it enables sensitive health data to be shared securely, which is an essential requirement for accelerating life science research. The project promotes the use of real clinical data in training to produce enormous educational impacts. The developed secure architecture is generic and applicable to other data and resource sharing environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931266","Elements:Collaborative Proposal: A task-based code for multiphysics problems in astrophysics at exascale","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2019","07/26/2019","Mark Scheel","CA","California Institute of Technology","Standard Grant","Seung-Jong Park","09/30/2022","$295,000.00","","scheel@tapir.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","107y, 1253, 8004","026Z, 077Z, 7569, 7923, 8004","$0.00","Upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales.  The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments.  The new code will be made available as open-source community cyberinfrastructure.  The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Early-career researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.<br/><br/>The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multiphysics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astrophysics. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state.  The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.<br/><br/>This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829698","CyberTraining CIP: Cyberinfrastructure Expertise on High-throughput Networks for Big Science Data Transfers","OAC","CyberTraining - Training-based, Campus Cyberinfrastructure","10/01/2018","11/13/2019","Jorge Crichigno","SC","University of South Carolina at Columbia","Standard Grant","Alan Sussman","09/30/2022","$499,959.00","Nasir Ghani, Elias Bou-Harb","jcrichigno@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","044Y, 8080","026Z, 062Z, 7361, 9150","$0.00","This project establishes the Cyberinfrastructure Network of Expertise (CNE) for teaching, training, and research on networking technologies including Science Demilitarized Zone (Science DMZ). The Science DMZ is a network specifically designed to facilitate the transfer and sharing of very large scientific data (big data) across geographically separated sites. The project serves the national interest, as it addresses the shortage of skilled research personnel with specialized skills to support networks carrying big science data among research institutions, universities, and national laboratories. This initiative also fosters skills toward the development of a national ""cyber-highway"" system to better facilitate the sharing of big science data, hence promoting collaboration and national competitiveness in science and engineering, aligned with NSF's mission. CNE is composed of universities, industry organizations, national and state agencies in multiple states, and a national laboratory. This broad initiative ensures a strong adoption of best cyberinfrastructure (CI) practices in sharing terabytes of data and more, thereby enabling new modes of discovery and collaboration. Additionally, the project develops advanced instructional material for Science DMZ, which is being integrated into undergraduate courses in information technology, computer science, and engineering in multiple universities. <br/><br/>CNE is anchored at the University of South Carolina (USC) and has several thrusts. First, the instructional material is enriched with a large number of virtual laboratories (vLabs), designed in partnership with domain experts and the Network Development Group (NDG), a company in virtualized training. The material covers key aspects of the Science DMZ, including wide area networks, routers and switches, TCP attributes for big data transfers, and cybersecurity for friction-free environments. Second, the CNE partnership also includes Florida Atlantic University (FAU) and University of South Florida (USF), which are incorporating the vLabs and companion material into their degree programs. These enriched programs provide skills to fulfill pressing demands in both South Carolina and Florida. Third, in conjunction with local industry and Savannah River National Laboratory, CNE incorporates internships and industry-sponsored capstone projects in student training to strengthen the cooperation between academia and industry and to bolster national security related to big data transfers. Fourth, CNE organizes workshops to provide critical training and education on Science DMZs in partnership with Internet2 and state agencies, namely South Carolina Cyber (SC Cyber) and the Florida Center for Cybersecurity (FC2). Finally, by leveraging the extensive dissemination channels of all project partners, CNE plans to make vLabs and manuals available to the wider CI community for online self-paced self-training delivery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940188","Collaborative Research: Atomic Level Structural Dynamics in Catalysts","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Mahmoud Moradi","AR","University of Arkansas","Continuing Grant","Pui Ho","09/30/2022","$324,793.00","","moradi@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","099Y, 1978","062Z, 9263","$0.00","Catalysts help make chemical reactions go faster and their development impact areas such as energy, the environment, biotechnology, and drug design. The vision of this project is to harness computational tools from modern statistics and machine learning to perform data-driven discovery of new catalysts. To this end, a collaborative team is assembled with the complementary expertise in catalysts, materials science, biophysics, computational modelling, statistics, signal processing, and data science. How a reaction is accelerated depends on the dynamic changes in the structure and shape of a catalyst and its associated chemical reactants (a catalytic system). The goal of this project is to explore, describe, and quantify the dynamic structures of enzyme and nanoparticle catalysts at the atomic level. Recent advances in microscopy and spectroscopy now make it possible to measure with great detail dynamic changes in time and in dimensional space. This project combines recent advances in data science with these new experimental tools to extract features that describe the dynamic behaviour of catalytic systems. In addition, the project will enhance the development of educational infrastructure for data-intensive and interdisciplinary science, contribute to workforce development, promote gender equality in the sciences, and disseminate scientific knowledge. <br/><br/>The guiding hypothesis of this research is that catalytic functionality cannot be fully understood without describing the atomic-level structural changes triggered by the molecular interactions of reactants with the catalyst. This hypothesis is tested by utilizing experimental datasets obtained from electron microscopy and single-molecule fluorescence resonance energy-transfer spectroscopy to explore structural dynamics in nanoparticles and enzymes. A data-analysis workflow, which integrates denoising, dimensionality reduction, clustering, and dynamic Markovian modelling, enables descriptions and classifications of the complex dynamical evolutions in spatiotemporally resolved measurements. The research develops and applies advanced methodologies to process noisy, high-dimensional data - a crucial bottleneck for the analysis of dynamic systems. The information extracted from experimental data guides the computational sampling of the conformational space of proteins and nanoparticles within a statistical physics framework, using supercomputer technology. This information facilitates the development of physical models that probe phenomena that are currently experimentally inaccessible, such as picosecond nuclear motions, as well as protein conformational changes and their coupling with chemical events. The transformative impact is to better understand catalysis by establishing a link between dynamic system response and catalytic functionality. The computational approaches developed through this project have the potential to be generally applied to many fundamental problems in materials science and structural biology where dynamic behaviours are important.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2153813","Detail between Naval Postgraduate School and National Science Foundation for Dr. Robert Beverly.(Year 2-continuation)","OAC","","10/01/2021","09/23/2021","Robert Beverly","CA","Naval Postgraduate School","Contract Interagency Agreement","Carl Anderson","04/01/2022","$141,337.00","","rbeverly@nps.edu","1 University Circle","Monterey","CA","939435000","8316562271","CSE","0119","","$0.00",""
"1920103","MRI: Acquisition of an Advanced Computing Instrument to Integrate Data-Driven Research and Data intensive computing at Johns Hopkins University","OAC","CYBERINFRASTRUCTURE","10/01/2019","10/26/2020","Dennice Gayme","MD","Johns Hopkins University","Standard Grant","Alejandro Suarez","09/30/2022","$2,795,025.00","Mark Robbins, Rigoberto Hernandez, Michael Schatz, Jaime Combariza, Margaret Johnson","dennice@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7231","075Z, 1189","$0.00","The project funds the purchase of a high-performance computing and storage system to create a Data Intensive Scientific Computing (DISCO) environment that will be located at Johns Hopkins University (JHU). DISCO will support research in a range of science and engineering areas including wind energy, ocean circulation, materials by design, additive manufacturing, big data, genomics, and biological function on membrane to organ scales. It provides a framework for creating new data from simulations, analyzing data, and making data available for analysis by remote users. DISCO will also enable the development and sharing of new code and data analysis methods across institutions. This project addresses two of NSF's big ideas: Growing Convergence Research and Harnessing the Data Revolution, as well as the Materials Genome initiative. DISCO will immediately support over 40 faculty research projects involving over 200 scientists. DISCO will be managed in collaboration with Morgan State University, a historically black university, which will be allocated at least 5% of resources to support its educational and computational research programs. Other users nationally will be offered 20% of the resources to be managed through a partnership with XSEDE. Opportunities for training on a diverse set of scientific computing topics will be directly integrated into regular courses across JHU and Morgan State. Courses on computational chemistry, genomics, molecular dynamics, machine learning and protein chemistry are planned. Course materials will be made available through the web and the existing set of Massive Open Online Courses (MOOCs) at JHU will be expanded. These materials will ensure training in the proper use of resources, share best practices between different disciplines and promote interdisciplinary collaboration.<br/><br/>The objective of this Major Research Instrumentation (MRI) project is to create a Data Intensive Scientific Computing (DISCO) environment that integrates high performance computing with tools for generating, analyzing and disseminating data sets of ever increasing size. The cluster will contain over 5 petabytes of storage and heterogeneous compute nodes optimized for different research projects and complex, optimized work flows: standard dual processor nodes, large memory nodes, and Graphics Processing Unit (GPU) enhanced nodes. An instance of SciServer software will enhance analysis and provide a platform for disseminating data through a web portal. This robust capability will become a powerful resource for developing and sharing code and infrastructure tools used by interdisciplinary data scientists at Johns Hopkins University, Morgan State University and beyond. DISCO will enable computational fluid dynamics studies of wind farms, ocean flow and physiological fluid dynamics. Research in multiscale modeling of materials will use machine learning to tailor material properties and integrate physical models at different scales to improve performance of additive manufacturing and connect defects and microstructures to macroscopic performance of crystals, polymers and other soft materials. DISCO will also enable sequencing of new genomes, study of variance within species and research on the microbiome. On larger scales, research on the dynamics of biological systems will examine function of proteins, membranes, cells and organs, including the impact of nanoparticles on function. This new integrated environment will reshape computational research practices and will be conducive to radical transformative research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017767","CyberTraining: Implementation: Small: Developing a Best Practices Training Program in Cyberinfrastructure-Enabled Machine Learning Research","OAC","CyberTraining - Training-based","09/01/2020","07/16/2020","Mary Thomas","CA","University of California-San Diego","Standard Grant","Alan Sussman","08/31/2024","$500,000.00","Robert Sinkovits, Mai Nguyen, Peter Rose, Martin Kandes","mpthomas@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","044Y","075Z, 079Z","$0.00","This project is targeted towards the NSF research workforce who need to develop machine learning applications that will run on the national cyberinfrastructure (CI). At the heart of this project is the CI-enabled Machine Learning (CIML) training system and repository that will support the development of cyberliteracy in the ML space. Unlike much of current ML-related training material found easily online, CIML material will be centered on science and engineering applications that make use of CI-enabled ML techniques and will be used to train a research workforce that is capable of understanding the challenges of working with CI, new HPC architectures, software, and applications. The user community for CIML training material will include students (undergraduate, graduate), postdocs, PIs, researchers, educators, and HPC trainers, each with their own diverse backgrounds and application requirements. The project will support national educational goals by ensuring that the CI modules run on advanced CI tools and resources, and that core literacy and discipline appropriate skills in advanced CI will be integrated into curricula and instructional material. CIML will support national security concerns by facilitating a workforce capable of developing ML applications in scientific domains such as climate and weather, the biosciences, physics, and chemistry. As a result of outreach and extension of the training efforts, this program will impact thousands of users and help develop the next generation of the CI research workforce. CIML training material will be available online, so the project has a huge potential to reach beyond the NSF cyber workforce to impact other communities including hospital and medical treatment systems, transportation and electrical monitoring systems, stock market monitoring systems, and disaster response systems. <br/> <br/>The Cyberinfrastructure-enabled Machine Learning (CIML) training system and repository will use a ?best practices? approach to develop a unique program targeted towards the research workforce who use machine learning (ML) and big data analytics methods for their domain specific applications or instructional material on large-scale cyberinfrastructure. The project will apply methods of Cyber Literacy and HPC Competencies to define a set of core ML and domain specific literacy areas as a function of the dimensions of learning ranging from a technological focus to a problem-solving focus or a focus on ML or computational science. Sources for the CIML system will be drawn from the work of HPC training, existing HPC researchers and users, collaborators, as well as new code and methods. The materials developed will be available via the CIML repository, which includes a web site, documentation, GitHub repositories for code, data, and related materials. CIMIL will become a useful tool for 2 communities: users who want to understand what technologies and skills they need to master in order to run a particular ML application, what systems to use, and suggested software libraries; and trainers who need to know what topics to teach.  The outcome of these efforts will result in a community of machine learning and data analytics CI Users (CIU) and Contributors (CIC) who actively contribute to the training material repository and incorporate the materials into their projects and courses. As a result of these efforts, the CIML program will extend the scope of the ongoing education and training across the research  workforce by developing cyberinfrastructure-based materials that will utilize and contribute to training material developed for XSEDE training, higher education, and other programs, and will impact thousands of existing and new users, including students (undergrads/grads), postdocs, PIs, researchers, and educators, each with their own diverse backgrounds and application requirements. CIML training material will be available online, so the project has a huge potential to reach beyond the cyber workforce and to impact many communities, including hospital and medical treatment systems, transportation and electrical monitoring systems, stock and market monitoring systems, and disaster response systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931297","Collaborative Research: Elements: Advancing Data Science and Analytics for Water (DSAW)","OAC","Hydrologic Sciences, Special Initiatives, EnvS-Environmtl Sustainability, Software Institutes, EarthCube","10/01/2019","04/03/2020","Jeffery Horsburgh","UT","Utah State University","Standard Grant","Alan Sussman","09/30/2022","$568,496.00","Alfonso Torres-Rua, Brian Crookston, Tianfang Xu","jeff.horsburgh@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","1579, 1642, 7643, 8004, 8074","026Z, 077Z, 7923, 8004","$0.00","Scientific challenges in hydrology and water resources such as understanding impacts of variable climate, sustainability of water supply with population growth and land use change, and impacts of hydrologic change on ecosystems and humans are increasingly data intensive. The volume of data produced by environmental scientists to study hydrologic systems requires advanced software tools for effective data visualization, analysis, and modeling. Scientists spend much of their time accessing, organizing, and preparing datasets for analyses, which can be a barrier to efficient analyses and hinders scientific inquiries and advances. This project will develop new software that will enhance scientists' ability to apply advanced data visualization and analysis methods (collectively referred to as ""data science"" methods) in the hydrology and water resources domain. The project will promote standardized software tools and data formats to help scientists enhance the consistency, share-ability, and reproducibility of the analyses they perform - all of which are important in building trust in scientific results. The software developed in the project will make data loading and organization for analysis easier, reducing the time spent by scientists in choosing appropriate data structures and writing computer code to read and parse data. It will enable users to automatically retrieve data from the HydroShare system, which is a hydrology domain data repository, as well as from important national water data sources like the United States Geological Survey's National Water Information System. The software will automatically load data from these sources into standardized and high performance data structures targeted to specific scientific data types and that integrate with visualization, analysis, and other data science capabilities commonly used by scientists in the hydrology and water resources domains. The project will also reduce the technical burden for water scientists associated with creating a computational environment within which to execute their analyses by installing and maintaining the Python packages developed within the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare-linked JupyterHub environment. Finally, the project will demonstrate the functionality and use of the software by producing a set of educational modules based on real water-data science applications that provide a specific mechanism for delivering the software to the community and promoting its use in classroom and research environments.<br/><br/>Scientific and related management challenges in the water domain are inherently multi-disciplinary, requiring synthesis of data of multiple types from multiple domains. Many data manipulation, visualization, and analysis tasks performed by water scientists are difficult because (1) datasets are becoming larger and more complex; (2) standard data formats for common data types are not always agreed upon, and, when they are, they are not always mapped to an efficient structure for visualization and/or analysis within an analytical environment; and (3) water scientists generally lack training in data intensive scientific methods that would enable them to use new and existing tools to efficiently tackle large and complex datasets. This project will advance Data Science and Analytics for Water (DSAW) by developing: (1) an advanced object data model that maps common water-related data types to high performance data structures within the object-oriented Python language and analytical environment based upon standard file, data, and content types established by the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare system; (2) two new Python packages that enable users to write Python code for automating retrieval of desired water data, loading it into high performance memory objects specified by the object data model designed in the project, and performing analysis in a reproducible way that can be shared, collaborated around, and formally published for reuse. The project will use domain-specific data science applications to demonstrate how the new Python packages can be paired with the powerful data science capabilities of existing Python packages like Pandas, numpy, and scikit-learn to develop advanced analytical workflows within cloud and desktop environments. The project aims to extend the data access, collaboration, and archival capabilities of the HydroShare data and model repository and promote its use as a platform for reproducible water-data science. The project also aims to overcome barriers associated with accessing, organizing, and preparing datasets for data science intensive analyses. Overcoming these barriers will be an enabler for transforming scientific inquiries and advancing application of data science methods in the hydrology and water resources domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834740","Elements: NSCI-Software -- A General and Effective B-Spline R-Matrix Package for Charged-Particle and Photon Collisions with Atoms, Ions, and Molecules","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","03/01/2019","02/15/2019","Oleg Zatsarinny","IA","Drake University","Standard Grant","Bogdan Mihaila","02/28/2022","$476,284.00","Klaus Bartschat","oleg.zatsarinny@drake.edu","2507 University Avenue","Des Moines","IA","503114505","5152713788","CSE","1253, 7244, 8004","026Z, 077Z, 7569, 7923, 8004","$0.00","This project concerns the development and subsequent distribution of a suite of computer codes that can accurately describe the interaction of charged particles (mostly electrons) and light (mostly lasers and synchrotrons) with atoms and ions. The results are of importance for the understanding of fundamental collision dynamics, and they also fulfill the urgent practical need for accurate atomic data to model the physics of stars, plasmas, lasers, and planetary atmospheres. With the rapid advances currently seen in computational resources, such studies can now be conducted for realistic systems, as opposed to idealized models. In particular, it has become possible to describe very complex targets, such as transition metals and other open-shell systems. Examples include the excited states of the inert gases beyond helium, as well as neutral and lowly-ionized iron.  These systems are of significant importance for plasma diagnostics and astrophysics, respectively.  The source code will be made publicly available. The project will support a post-doctoral researcher. A website devoted to user-developer interaction will be developed and maintained together with the neccessary code documentation and training materials.<br/><br/>The numerical calculations will be based upon the non-perturbative R-matrix (close-coupling) method.  A particular strength of the implementation pursued in this project is the use of a highly flexible B-spline basis with non-orthogonal orbital sets.  The major advantage of the approach compared to traditional methods is the fact that an accurate target description can be achieved with a much-reduced configuration-interaction (CI) expansion if the orthogonality requirements on the individual orbitals are relaxed.  This is critical for complex targets, where the valence orbitals in particular are known to be strongly term-dependent. Using a sufficiently small but still accurate enough CI expansion for the target states is essential for the feasibility and quality of the subsequent collision calculation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931435","Collaborative Research: Frameworks: An open source software ecosystem for plasma physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes, Space Weather Research","10/01/2019","05/20/2020","Tulasi Parashar","DE","University of Delaware","Standard Grant","Seung-Jong Park","09/30/2024","$273,452.00","","tulasi@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1253, 7244, 8004, 8089","026Z, 077Z, 4444, 7569, 7925, 8004, 9150","$0.00","Software is crucial to all areas of modern plasma physics research.  Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas.  Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions.  Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education.  PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy.  PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality.  The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.<br/><br/>The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics.  The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities  include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data.  They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835704","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes, EarthCube","10/01/2018","09/12/2018","Catherine Olschanowsky","ID","Boise State University","Standard Grant","Seung-Jong Park","09/30/2022","$700,000.00","Alejandro Flores","cathie@cs.boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","8004, 8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104052","Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED)","OAC","Software Institutes, EarthCube","09/01/2021","08/24/2021","Carl Tape","AK","University of Alaska Fairbanks Campus","Standard Grant","Tevfik Kosar","08/31/2025","$647,985.00","","ctape@alaska.edu","West Ridge Research Bldg 008","Fairbanks","AK","997757880","9074747301","CSE","8004, 8074","077Z, 7925, 8004, 9150","$0.00","Seismology is the most powerful tool for investigating the interior structure of Earth ? from its surface down to the inner core ? and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. <br/><br/>The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940202","Collaborative Research: MEMONET: Understanding memory in neuronal networks through a brain-inspired spin-based artificial intelligence","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","10/15/2020","Takaki Komiyama","CA","University of California-San Diego","Continuing Grant","Sylvia Spengler","09/30/2022","$435,478.00","","tkomiyama@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","099Y, 1640","062Z","$0.00","The brain is arguably the most sophisticated and the most efficient computational machine in the universe. The human brain, for example, comprises about 100 billion neurons that form an interconnected circuit with well over 100 trillion connections. Understanding how a multitude of brain functions emerge from the underlying neuronal circuit will give insights into the operating principles of the brain. In this award, a multidisciplinary team of systems biologist, computational biologist, material scientist, neuroscientist, and machine learning expert will work synergistically to leverage the data revolution in neuroscience to answer a fundamental question: How does the brain learn, store, and process information?  The team will develop and apply advanced data analysis algorithms to harness the great volume of neuronal data generated by the latest imaging and molecular profiling technologies, for elucidating the neuronal circuits driving brain functions. Computer simulations of a spin-electronic (spintronic) device will further serve as a platform to validate and emulate important operational characteristics of such neuronal circuits. The award sets the groundwork for an interdisciplinary data science research and educational program that will bring a new and powerful paradigm for studying brain functions as well as for designing transformative brain-inspired devices for information processing, data storage, computing, and decision making.<br/><br/>The project has a specific focus on an essential function of the brain: motor-skill learning. This function emerges from the underlying circuitry of neurons that governs the activities of molecular signal transmission and neuronal firing. Importantly, the neuronal circuit in a mammalian brain is highly plastic and dynamic, features that endow animals with the ability to respond to myriad external stimulations through learning. By harnessing the latest data revolution in neuronal imaging, single neuron molecular profiling, spintronic device simulation, network inference, and machine learning, a team of multidisciplinary investigators will be supported by this award to investigate the fundamental principle of neuronal circuit rewiring that drives brain?s learning function. More specifically, the team sets out to achieve the following specific tasks: (A) Infer learning-induced rewiring of large-scale neuronal networks from two-photon calcium imaging data through the development of novel and powerful network inference algorithms; (B) Build biochemical-based models of neuronal circuits by integrating molecular profiling with neuron firing and connectome dynamics; and (C) Develop a spintronic material network model that emulates learning and memory formation by exploiting the spin dynamics in spintronic materials. The project seeks to lay the foundation for the creation of an interdisciplinary data-intensive brain-to-materials initiative that will be applied to understand and emulate the operational principles of brain neuronal circuits underlying learning, cognition, memory formation, and other behaviors. The outcomes of the initiative will have a paramount impact on the society, not only in our understanding of the brain and its functions, but also in overcoming current bottlenecks of existing computing architectures.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940178","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2019","08/06/2020","Mark Wilson","MA","University of Massachusetts Amherst","Continuing Grant","Sylvia Spengler","09/30/2022","$213,091.00","","wilson.mark.c@gmail.com","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","099Y, 1640","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004302","Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science","OAC","Software Institutes, EarthCube","10/01/2020","08/03/2020","Abani Patra","MA","Tufts University","Standard Grant","Alan Sussman","09/30/2025","$574,537.00","","abani.patra@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","8004, 8074","026Z, 077Z, 7925, 8004","$0.00","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.<br/><br/>The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940162","Collaborative Research: MEMONET: Understanding memory in neuronal networks through a brain-inspired spin-based artificial intelligence","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc, Info Integration & Informatics","10/01/2019","10/15/2020","Rudiyanto Gunawan","NY","SUNY at Buffalo","Continuing Grant","Sylvia Spengler","09/30/2022","$387,256.00","","rgunawan@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","099Y, 1640, 7364","062Z","$0.00","The brain is arguably the most sophisticated and the most efficient computational machine in the universe. The human brain, for example, comprises about 100 billion neurons that form an interconnected circuit with well over 100 trillion connections. Understanding how a multitude of brain functions emerge from the underlying neuronal circuit will give insights into the operating principles of the brain. In this award, a multidisciplinary team of systems biologist, computational biologist, material scientist, neuroscientist, and machine learning expert will work synergistically to leverage the data revolution in neuroscience to answer a fundamental question: How does the brain learn, store, and process information?  The team will develop and apply advanced data analysis algorithms to harness the great volume of neuronal data generated by the latest imaging and molecular profiling technologies, for elucidating the neuronal circuits driving brain functions. Computer simulations of a spin-electronic (spintronic) device will further serve as a platform to validate and emulate important operational characteristics of such neuronal circuits. The award sets the groundwork for an interdisciplinary data science research and educational program that will bring a new and powerful paradigm for studying brain functions as well as for designing transformative brain-inspired devices for information processing, data storage, computing, and decision making.<br/><br/>The project has a specific focus on an essential function of the brain: motor-skill learning. This function emerges from the underlying circuitry of neurons that governs the activities of molecular signal transmission and neuronal firing. Importantly, the neuronal circuit in a mammalian brain is highly plastic and dynamic, features that endow animals with the ability to respond to myriad external stimulations through learning. By harnessing the latest data revolution in neuronal imaging, single neuron molecular profiling, spintronic device simulation, network inference, and machine learning, a team of multidisciplinary investigators will be supported by this award to investigate the fundamental principle of neuronal circuit rewiring that drives brain?s learning function. More specifically, the team sets out to achieve the following specific tasks: (A) Infer learning-induced rewiring of large-scale neuronal networks from two-photon calcium imaging data through the development of novel and powerful network inference algorithms; (B) Build biochemical-based models of neuronal circuits by integrating molecular profiling with neuron firing and connectome dynamics; and (C) Develop a spintronic material network model that emulates learning and memory formation by exploiting the spin dynamics in spintronic materials. The project seeks to lay the foundation for the creation of an interdisciplinary data-intensive brain-to-materials initiative that will be applied to understand and emulate the operational principles of brain neuronal circuits underlying learning, cognition, memory formation, and other behaviors. The outcomes of the initiative will have a paramount impact on the society, not only in our understanding of the brain and its functions, but also in overcoming current bottlenecks of existing computing architectures.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1941085","EAGER: The Next Generation of Smart Cyberinfrastructure: Efficiency and Productivity Through Artificial Intelligence","OAC","CYBERINFRASTRUCTURE","10/01/2019","08/15/2019","Valerio Pascucci","UT","University of Utah","Standard Grant","Bogdan Mihaila","09/30/2022","$299,712.00","","pascucci@acm.org","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7231","7916","$0.00","Efficient cyberinfrastructure (advanced computing, data, software and networking infrastructure) is a critical component of the support that NSF provides for new discoveries in science and engineering. Cyberinfrastructure is complex and traditionally requires years of human hand-tuning to fully achieve maximal performance for scientific users.  We propose to introduce Artificial Intelligence (AI) as a way to automatically and quickly optimize the performance and broadest use of recent NSF-supported advanced computing resources. Through this pilot effort our ultimate aim is to enable and accelerate scientific advances in widely diverse fields such as biology, chemistry, oceanography, materials science, climate modeling, and cosmology.<br/><br/>As the research cyberinfrastructure grows rapidly in scale and complexity, it is essential to integrate new technologies based on Machine Learning (ML) and AI to ensure that the investments in new hardware and software components result in proportional improvements in performance and capability. This project will undertake a transformative research activity targeting: (1) scaling ML algorithms to make them easily available to the scientific community; and (2) improving cyberinfrastructure efficiency through AI-based predictive models. This technical work will be complemented and informed by a community engagement effort to jointly catalog the state of the art and identify future challenges and opportunities in enabling a new smart cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761792","Spokes: MEDIUM: SOUTH: Collaborative: Integrating Biological Big Data Research into Student Training and Education","OAC","BD Spokes -Big Data Regional I","10/01/2018","09/14/2018","Donald Adjeroh","WV","West Virginia University Research Corporation","Standard Grant","Earnestine Psalmonds","09/30/2022","$150,000.00","Gianfranco Doretto, Ivan Martinez, Timothy Driscoll, Aaron Robart","donald.adjeroh@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","024Y","028Z, 8083, 9102","$0.00","The project is a collaborative effort among the University of Tennessee Chattanooga, Tuskegee University, Spelman College, and West Virginia University to integrate and automate biological big data into student training and education. Leveraging the team's expertise in computer science and ecology, the project will offer training workshops on using network models to integrate heterogeneous genomic big data and heterogeneous ecological big data to address life sciences questions. The team will engage faculty and students in developing a protocol to automate field data collection. The team also will prototype automated methods to enhance plant digitization, leveraging the collection of digitized plant images and meta-information at the Southeast Regional Network of Expertise and Collections, as well as the ecological datasets in collaboration with the Encyclopedia of Life.<br/><br/>The project objectives are to (1) enhance faculty expertise in big biological data through summer workshops; (2) catalyze interdisciplinary collaboration on big biological data research and education through hackathons, working groups, and community-building via a Video Education Faculty Network; and (3) develop hands-on, constructively peer-evaluated learning modules incorporating high-quality video tutorials. The proposed activities will address challenges surrounding the integration and automation of big biological data into education and training at predominantly undergraduate institutions and Historically Black Colleges and Universities. The project will help bridge the gaps between big biological data and the fields of systems biology, ecology and evolution, and environmental sciences. Overall, the project will catalyze collaborations among diverse institutions and disciplines while increasing diversity in big data. <br/><br/>This award is co-funded by the Improving Undergraduate STEM Education: Education and Human Resources (IUSE): EHR Program (NSF 17-590). IUSE supports projects that are designed to improve student learning through development of new curricular materials and methods of instruction and development of new assessment tools to measure student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018074","CC* Integration-Large: An 'On-the-fly' Deeply Programmable End-to-end Network-Centric Platform for Edge-to-Core Workflows","OAC","Special Projects - CNS, CISE Research Resources","10/01/2020","06/11/2021","Michael Zink","MA","University of Massachusetts Amherst","Standard Grant","Deepankar Medhi","09/30/2022","$757,198.00","Ewa Deelman, Prasad Calyam, Anirban Mandal","zink@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1714, 2890","9102, 9178, 9251","$0.00","Unmanned Aerial Vehicles (also known as drones) are becoming popular in the sky. Their application reaches from hobby drones for leisurely activities to life-critical drones for organ transport to commercial applications such as air taxis. The safe, efficient, and economic operation of such drones poses a variety of challenges that have to be addressed by the science community. For example, drones need very detailed, close to the ground weather information for safe operations, and data processing and energy consumption of drones need to be intelligently handled. This project will provide tools that will allow researchers and drone application developers to address operational drone challenges by using advanced computer and network technologies.<br/><br/>This project will provide an architecture and tools that will enable scientists to include edge computing devices in their computational workflows. This capability is critical for low latency and ultra-low latency applications like drone video analytics and route planning for drones. The proposed work will include four major tasks. First, cutting edge network and compute infrastructure will be integrated into the overall architecture to make them available as part of scientific workflows. Second, in-network processing at the network edge and core will be made available through new programming abstractions. Third, enhanced end-to-end monitoring capabilities will be offered. Finally, the architecture will leverage the Pegasus Workflow Management System to integrate in-network and edge processing capabilities.<br/><br/>Providing best practices and tools that enable the use of advanced cyberinfrastructure for scientific workflows will have a broad impact on society in the long term. The science drivers that will be supported by this project have the potential to increase the safety and efficiency of drone applications, an area that will grow in significance in the foreseeable future. The project team will enable access to a rich set of resources for researchers and educators from a diverse set of institutions (historically black colleges and universities (HBCU), community colleges, women?s colleges) to further democratize research. In addition, collaboration with the NSF REU (Research Experience for Undergraduates) Site in Consumer Networking will promote participation of under-served/under-represented students in project activities.<br/><br/>Information about the project will be available at http://www.flynet-ci.org to provide information on overall project activities, outreach activities, publications, tools and software, and the project?s team members. The project website will be preserved for at least three years after the project ends.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931393","Collaborative Research: Frameworks: An open source software ecosystem for plasma physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes, Space Weather Research","10/01/2019","09/07/2019","David Schaffner","PA","Bryn Mawr College","Standard Grant","Seung-Jong Park","09/30/2024","$187,240.00","","dschaffner@brynmawr.edu","101 N. Merion Avenue","Bryn Mawr","PA","190102899","6105265496","CSE","1253, 7244, 8004, 8089","026Z, 077Z, 4444, 7569, 7925, 8004","$0.00","Software is crucial to all areas of modern plasma physics research.  Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas.  Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions.  Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education.  PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy.  PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality.  The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.<br/><br/>The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics.  The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities  include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data.  They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835794","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes, EarthCube","10/01/2018","04/30/2021","Laura Condon","AZ","University of Arizona","Standard Grant","Seung-Jong Park","09/30/2022","$699,587.00","Michelle Strout","lecondon@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8004, 8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835569","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes","10/01/2018","09/12/2018","David Tarboton","UT","Utah State University","Standard Grant","Seung-Jong Park","09/30/2022","$339,985.00","","dtarb@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833846","EAGER: The Xpert Network: Synergizing National Expert-Assistance and Tool-Support Teams for Computational and Data-Intensive Science","OAC","Software Institutes","10/01/2018","08/15/2018","Rudolf Eigenmann","DE","University of Delaware","Standard Grant","Seung-Jong Park","09/30/2022","$298,521.00","","eigenman@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","8004","026Z, 7916, 8004, 9150","$0.00","Today's science frontiers are being advanced, to an increasing degree, by researchers who use large amounts of computational power to simulate models of our world's processes and to analyze large volumes of observed and collected data. Doing so requires expertise not only in the involved domain sciences, such as physics, chemistry, and biology, but also in the computational tools that implement the needed simulation and data analysis methods.  Many research projects have recognized the importance of pairing domain scientists with computational experts.  While the domain scientists pursue their individual research objectives, there is much commonality is the supporting software and hardware methods as well as in the needed cyberinfrastructure tools and resources. This project aims to increase the productivity of the many computational support teams - referred to as Xperts - through (i) the exchange of best practices and discussion of open problems, and (ii) the provision of advanced programming support environments.<br/><br/>To achieve the first objective - creating synergy among Xpert teams - this award will engage computational experts from some of the large, national cyberinfrastructure projects, including XSEDE and the NSF Software Institutes, as well as from university campus efforts that have recognized the value of computational research support teams. Events for exchanging best practices and discussing open issues will be organized at special workshops, through online meetings, and at established annual symposia, such as the Supercomputing and PEARC conferences. The same gatherings will be used to bring together users and developers of programming environments, pursuing the second objective. Among these groups are those who develop automatic, and semi-automatic program optimization instruments, performance analysis environments, and data management tools. This effort will not only expose Xpert teams to today's advanced programming support environments, but also gather important feedback from key user groups - those who push today's science frontiers - to help advance the programming environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931348","CSSI Elements: DataSwarm: A User-Level Framework for Data Intensive Scientific Applications","OAC","Software Institutes","09/01/2019","07/23/2019","Douglas Thain","IN","University of Notre Dame","Standard Grant","Amy Walton","08/31/2022","$562,994.00","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","077Z, 7923","$0.00","This project creates a capability that will support the construction of large, data intensive scientific applications that must run on top of national cyberinfrastructure, such as large campus clusters, NSF extreme-scale computing facilities, the Open Science Grid, and commercial clouds.  The new capability (DataSwarm) brings data requirements and software dependencies to the target cyberinfrastructure systems, and deploys them as and when required, rather than having these requirements pre-installed on the target systems.  The motivation comes from applications in high energy physics, molecular dynamics, and quantum chemistry.<br/><br/>The main motivation of the work is the challenge of scalable computing frameworks.  Based on a prior development by the Principal Investigator (Work Queue), the current project provides technical innovation in three areas: <br/>(1) Molecular Task Composition.  Molecular task composition is used as an abstraction for the precise construction of tasks that require a custom software environment, large data input, and a scratch data area to capture the outputs. By expressing these aspects explicitly instead of implicitly, the project improves the storage efficiency of large numbers of tasks. <br/>(2) In-Situ Data Management.  In-situ storage management is performed to offset the increased storage consumption likely to occur under molecular task composition, avoiding unpredictable failures of tasks due to storage exhaustion. <br/>(3) Precision Provenance.  Precision provenance of both data objects and task components enables the efficient re-use of resources across multiple runs, as well as precise incremental changes to complex workflows.<br/>For this project, the three key elements addressed are the software environment, input data, and a scratch data area. These elements are usually independently managed; here, they are bound together to form temporary ""molecules"" for task execution.  The three applications included in this project represent three typical types of complex data and complex software dependencies. They include custom late-stage data analysis codes in high energy physics, complex multidimensional optimization, and ensemble molecular dynamics, respectively.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039575","Data CI Pilot: CI-Based Collaborative Development of Data-Driven Interatomic Potentials for Predictive Molecular Simulations","OAC","DMR SHORT TERM SUPPORT, CESER-Cyberinfrastructure for","10/01/2020","10/14/2020","Ellad Tadmor","MN","University of Minnesota-Twin Cities","Standard Grant","Tevfik Kosar","09/30/2022","$1,127,993.00","Ryan Elliott, Stefano Martiniani","tadmor@aem.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1712, 7684","020Z, 054Z, 062Z, 095Z, 6863, 7569","$0.00","The project addresses a pressing need of the molecular simulation community by providing materials researchers with a powerful new ability to efficiently synthesize all available data and knowledge related to their particular problem of study. It convenes a consortium of materials researchers around the common goal of increasing interoperability of existing and emerging materials cyberinfrastructures supported by NSF and others. The PIs will develop a new computational framework that enables researchers to rapidly develop and deploy data-driven interatomic potentials for complex material systems, by connecting existing cyberinfrastructure resources of first-principles calculations and experimental data. This will provide qualitative insights into material behavior, as well as predictive capability necessary to design new materials and nanostructures.<br/><br/>This project aims to accelerate the adoption of data-driven interatomic potential (DDIP) technology by removing existing barriers faced by materials researchers. Few groups in academia, government and industry have the capacity to develop DDIPs as this requires simultaneous expertise in the physics of the material being modeled, in ?first principles?  (FP) calculations, and in machine learning techniques, as well as access to extensive computational resources needed to construct the extremely large and diverse training sets required to fit high-quality DDIPs. To address this, a computational infrastructure called ?ColabFit? is being developed that will enable researchers to collaborate on DDIP development by pooling their knowledge and data. Researchers will be able to train state-of-the-art DDIPs using a supported fitting code of choice, seamlessly access training data from existing cyberinfrastructure (CI) resources of FP data, and exchange DDIPs in a standard format through the Open Knowledgebase of Interatomic Models (OpenKIM) project so that they can build on each other?s work. This effort fills a pressing need in the materials simulation community, as evidenced by the large consortium of leading DDIP developers, CI projects, and materials standards organization that has been assembled to support it. To keep development of the ColabFit framework focused on real-world materials research needs, it will be organized around a target application of DDIP development for phase transformations in 2D transition metal dichalcogenides.<br/><br/>This project is jointly supported by the Office of Advanced Cyberinfrastructure in the Computer and Information Sciences Directorate, and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017699","CyberTraining: Implementation: Small: Enabling Dark Matter Discovery through Collaborative Cybertraining","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","10/01/2020","10/20/2020","Christopher Tunnell","TX","William Marsh Rice University","Standard Grant","Alan Sussman","09/30/2023","$170,010.00","","tunnell+nsf@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","044Y, 7244","7569","$0.00","Detecting dark matter in the lab would be transformational for physics, and such a difficult measurement requires providing a foundation for early-career scientists in advanced data analytics.   The science question being pursued is generally acknowledged to be one of the most important questions in particle physics and astrophysics and is key to understanding what makes up the vast majority of the universe.  Effective training in good computing practices is required for major research advances in this field.  The project will consolidate and strengthen training efforts in scientific software development and data analysis within the field of experimental dark matter research.  Scientifically, the training will enable discovery that will come from a world-wide effort consisting of hundreds of junior scientists  searching for  extremely-rare events on petabytes of data - effectively looking for a needle in a haystack the size of Texas.  The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure, and will support STEM disciplines with critical software training that is much needed both in scientific fields and in industry.<br/><br/>The dark matter community consists of more than a thousand scientists at the frontier of ultra-rare event searches whose efforts support more than twenty different experiments.  Searching for dark matter in multiple ways has resulted in disparate and often inadequate computational training. This project addresses the training problem to maximize impact across the field.  Representing three leading dark matter experiments, the project investigators will develop educational material and training workshops for systematic data science education to ensure early career scientists can harness the data volumes being produced by modern experiments.  The project will host two training workshops per year, toward the goal of developing a community of instructors and also a set of training materials for free distribution and reuse. Beyond domain-specific training in rare-event searches, foundational computational knowledge will be developed when necessary by working with partners such as the Software and Data Carpentries.  The project includes specific goals to engage women and underrepresented minorities in the training activities and broaden their advancement within the field.  Additionally, the project will provide mentors for advanced students through hackathons. These trainings will directly contribute to broader STEM workforce development while training students such that they can pursue careers in data science and/or data-intensive research.  This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839013","EAGER: Enhanced Robust Persistent Identification of Data (E-RPID)","OAC","NSF Public Access Initiative","10/01/2018","08/08/2018","Robert Quick","IN","Indiana University","Standard Grant","Martin Halbert","09/30/2021","$299,989.00","Laurence Lannom, Marlon Pierce","rquick@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7414","7916","$0.00","An increasing challenge in modern research is the rapid growth in ability to collect and store research data; this growth is much more rapid than the ability to catalog, make accessible, and reuse these data. Major challenges facing the scientific research enterprise include making data discoverable, accessible, and re-usable at Internet scales; and making it possible to use data to replicate analyses done in published research. The importance of this issue is at the center of many recent initiatives in Open Science and Open Data. The objective of this project is to address these deficiencies and to enable more options for data interoperability and reusability in the current research data landscape. One aim is to demonstrate how technical solutions for data accessibility and management should be designed, implemented, operated, and usable by a broad range of researchers. Another aim is to ultimately increase the publication, discovery, and reuse of data, thus contributing to the greater integrity of the scientific enterprise.<br/><br/>This project aims to create a full implementation of the Digital Object Architecture (DOA) concept to improve data interoperability and reusability and potentially transform the way research in many disciplines is conducted. The approach is to integrate existing and new software components into the Robust Persistent Identification of Data (RPID) testbed which itself is a 2-year pilot project supported by NSF (Project ID 1659310). Components to be added include a Digital Object Interface Protocol (DOIP) which defines common operations performed on digital objects; and a Repository Mapping Service which would allow mapping the contents of existing repositories to the Digital Object Architecture (DOA) environment. With the new components, the ensemble ""Enhanced-RPID"" (E-RPID) testbed will allow managing of data objects by assigning individual identifiers toward making data FAIR (findable, accessible, interoperable, and reusable). This technical effort along with planned educational material are designed to lower the barrier for adoption of Persistent Identification (PID)-centric data management throughout the data lifecycle, from initial data collection to long-term use and reuse. The E-RPID testbed will be housed on the NSF-funded Jetstream computational resource, will be applied to a number of important scientific use cases, and will contribute to activities fostered by the Research Data Alliance (RDA).  The outcomes of this project are aimed to broadly benefit disciplinary and multi-disciplinary research and to facilitate better access to and utilization of existing data repositories.<br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940208","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Ryan McGranaghan","CO","Atmospheric & Space Tech Research Associates","Standard Grant","Amy Walton","09/30/2022","$170,000.00","","rmcgranaghan@astraspace.net","282 Century Place","Louisville","CO","800271676","2108343475","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029221","Collaborative Research: IRNC: Testbed: BRIDGES - Binding Research Infrastructures for the Deployment of Global Experimental Science","OAC","CYBERINFRASTRUCTURE, International Res Ret Connect","10/01/2020","09/13/2021","Bijan Jabbari","VA","George Mason University","Continuing Grant","Kevin Thompson","09/30/2023","$1,449,957.00","Jerome Sobieski","bjabbari@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7231, 7369","","$0.00","The BRIDGES project is developing an innovative, dynamic, high-performance trans-Atlantic network testbed that interconnects research communities and their resources in the US with collaborating partners and facilities in Europe. It explores advanced virtualized network architectures enabling rapidly reconfigurable global cyber-infrastructure to address changing research requirements of the collaborators to explore new concepts and to create new project workflows securely and with consistent, predictable performance.<br/><br/>The BRIDGES facility consists of two geographically separate optical links spanning the North Atlantic and terrestrial optical links in US and Europe to form an intercontinental optical network ring topology, capable of carrying up to 200 Gbps of science data between major nodes in Washington, New York, Paris, and Amsterdam. The BRIDGES facility is a binding platform for research projects, providing a flexible research-oriented infrastructure connecting laboratories and universities in the US to their counterparts in Europe. Science applications in high energy physics, deep space communications, and even biomedical programs are collaborating with BRIDGES to leverage them investment in this technology to reach digital resources such as globally distributed or remote sensors and instruments, distributed data storage, analytical processing resources, to create new global applications tailored to the science at hand. BRIDGES is led by George Mason University and East Carolina University and is working with and connecting an initial community of nearly 40 networking and science research projects across the US and Europe. The project is expected to have a far-reaching impact beyond the testbed or interconnect facility and lead to innovative use of cyber-infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118102","CyberTraining: Pilot: Cyberinfrastructure Training in Computer Science and Geoscience","OAC","CyberTraining - Training-based","10/01/2021","09/13/2021","Bing Wang","CT","University of Connecticut","Standard Grant","Alan Sussman","09/30/2023","$299,362.00","Sanguthevar Rajasekaran, Chuanrong Zhang, Wei Wei, Suining He","bing@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","044Y","7231, 7301, 9102","$0.00","Computation, data, and workforce development are critical components of cyberinfrastructure (CI). This project will help create future CI professionals from the Computer Science and Engineering (CSE) and Geography Departments who can use, develop, deploy, and maintain advanced CI. CSE students have little experience with spatio-temporal data, while Geography students have training in spatio-temporal data but are unfamiliar with advanced computing techniques and data analytics techniques. To fill these gaps in curricula, this project will develop a set of training materials covering the full workflow of data acquisition, transfer, synthesis, computation, and visualization of CI, with a focus on handling large-scale spatio-temporal data. <br/><br/>To encompass the interdisciplinary nature of CI, this project will develop training materials by faculty with complementary expertise in both CSE and Geography, targeting undergraduate and graduate students, postdocs and researchers with these substantive interests. The training materials will be at three levels that are in increasing depth. The first level is a set of basic training modules on the main components and their interactions in a CI system. The second level is a set of open-ended course projects that uses CI to solve challenging research problems, while the third level is an intensive competition-based workshop that will provide the students with the experience of solving real-world problems using CI through interdisciplinary collaboration. The context of the training materials will be in urban computing, an interdisciplinary field that relies heavily on spatio-temporal data and has profound societal impacts. This project will provide students with improved understanding of CI systems, contribute to new knowledge and discoveries using CI, and expand the CI community of interdisciplinary collaborators. Training materials be made publicly available and will be broadly disseminated through various outreach programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117519","Collaborative Research: CyberTraining: Pilot: A Cybertraining Program to Advance Knowledge and Equity in the Geosciences","OAC","CyberTraining - Training-based","10/01/2021","09/09/2021","Julie Libarkin","MI","Michigan State University","Standard Grant","Alan Sussman","09/30/2023","$75,000.00","","libarkin@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","044Y","7231, 7301, 9102, 9179","$0.00","This project will build and evaluate sustainable pathways for participation in cybertraining within the Earth science fields. The project will enhance inclusivity by focusing on underrepresented geoscience students for training in cyberinfrastructure (CI), including computer programming, data analysis, and modeling, which are not typically parts of the geoscience curriculum. The project will enhance programming education for geoscience students and increase access and mentoring relationships in STEM fields, thus building sustainable CI for geoscience programing. Consequently, this project will lower barriers to entry for STEM students while developing networks and mentoring relationships in the geosciences.  <br/><br/>This project will provide geoscience graduate students with skills training that promote knowledge proliferation in software development and science communication. Students will learn to automate workflows, test code, manage code repositories, visualize data, and communicate research effectively, which will accelerate their research foundation and provide future professional opportunities. Given the size of the Spanish-speaking STEM student population in the United States, the current effort will focus on bi-lingual cybertraining. This CI project will provide opportunities to enhance coding skills and communication abilities, while expanding students? professional network and establishing mentors at multiple career stages. Cybertraining will occur in a series of virtual and in-person terms, where students will learn Python coding and data analysis, and will be exposed to a Hackathon to provide practical application of these skills. Course materials will be open source, which will create a sustainable cybertraining program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017289","CyberTraining: Pilot: Modular experiential learning for secure, safe, and reliable AI (MELSSRAI)","OAC","CyberTraining - Training-based, Secure &Trustworthy Cyberspace","08/15/2020","04/30/2021","Alvis Fong","MI","Western Michigan University","Standard Grant","Joseph Whitmeyer","07/31/2022","$314,257.00","Ajay Gupta, Steven Carr, Shameek Bhattacharjee","alvis.fong@wmich.edu","1903 West Michigan Avenue","Kalamazoo","MI","490085200","2693878298","CSE","044Y, 8060","9251","$0.00","In this pilot project, core literacy and advanced skills at the intersection of Secure, Safe, Reliable (SSR) Computing, High Performance Computing (HPC), and artificial intelligence (AI) are integrated into educational curricula and training materials to prepare faculty, undergraduate, and graduate students at institutions with relatively low rates of advanced cyberinfrastructure (CI) adoption for large-scale secured data analytics. From self-driving vehicles to smart digital personal assistants and real-time multilingual translators, applications of AI have become omnipresent in our daily lives.  There is an urgent need to ensure that current and future scientists who advance AI, as well as practitioners who use AI, understand the limitations of AI and how to develop robust and dependable AI.  The long-term goals of this project are to contribute to a pipeline for a SSR AI-minded CI workforce and a self-sustaining advanced CI ecosystem. <br/><br/>In this project, inspired by authoritative sources such as Open AI and Partnership on AI, curricular modifications and materials are developed to educate computer science (CS) students in SSR techniques from the outset.  Intensive, multi-faceted, modular, experiential learning units are designed to upgrade the skills of current and future CI users rapidly, so they can apply their new skills to their tasks. The loosely coupled modules can be integrated into existing classes, including elementary CS classes taken by non-CS STEM students. Students participate in research activities, which train next generation interdisciplinary scientists, including many from underrepresented groups.  Universities at varied levels and varied locations as well as community colleges are included in the project.  Using a collective impact plan, a group of multi-discipline, public-private-sector experts provide guidance and participate in train-the-trainer activities to multiply the effect. Lessons learned and best practices are codified into blueprints for reusability and widespread future adoption across STEM disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005506","Category I: Jetstream 2: Accelerating Science and Engineering On-Demand","OAC","Innovative HPC","10/01/2020","08/16/2021","David Hancock","IN","Indiana University","Cooperative Agreement","Robert Chadduck","09/30/2025","$22,500,000.00","Gwen Jacobs, Matthew Vaughn, Marlon Pierce, Nirav Merchant","dyhancoc@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7619","","$0.00","The frontiers of science are rapidly evolving in regard to availability of data to be analyzed and the breadth and variety of analytical tools that researchers use. To effectively analyze and make sense of this ever-growing cache of information, and to make it possible to leverage new artificial intelligence tools for research, researchers need on-demand, interactive, and programmatic cyberinfrastructure (CI) delivered via the cloud. Jetstream 2 is a system that will be easy to expand and reconfigure, and capable of supporting diverse modes of on-demand access and use, The system will also revolutionize the national cyberinfrastructure (CI) ecosystem by enabling ?AI for Everyone? with virtual GPU capabilities and widespread outreach through the five partners, led by Indiana University. The project promises to enable the research community to use a greater variety of computational resources and to expand its reach into student populations, drawn from a broad range of disciplines, thus contributing to building the future STEM workforce.<br/><br/>Jetstream 2 will be an 8 PetaFLOPS (PFLOPS) cloud computing system using next-generation AMD ?Milan? CPUs and the latest NVIDIA Tensor Core GPUs with 18.5 petabytes (PB) of storage. Consisting of five computational systems, Jetstream 2?s primary system will be located at Indiana University, with four modest regional systems deployed nationwide at Arizona State University (ASU), Cornell University, the University of Hawai?i (UH), and the Texas Advanced Computing Center (TACC). Additional partnerships with the University of Arizona, Johns Hopkins University, and University Corporation for Atmospheric Research (UCAR) will contribute to Jetstream 2?s unparalleled usability and support for a broad range of scientific efforts.<br/><br/>The Jetstream team has been at the forefront of training the research community to transition from batch computing methods to adopt cloud-style usage. Jetstream 2 will continue this path and will ease the transition between academic and commercial cloud computing. Some of the advanced features include push-button virtual clusters, advanced high-availability science gateways services (including commercial cloud integration), federated authentication for JupyterHubs, bare metal and virtualization within the same system through programmable CI, support for on-demand data intensive workloads in addition to on-demand computation, high-performance software-defined storage, and advanced multi-platform orchestration capabilities.<br/><br/>Jetstream 2 will have far-reaching societal benefits. As enhanced educational infrastructure, it will serve more students, from traditional undergraduates to domain-science experts desiring training in computational techniques, than any other NSF-funded CI resource. These students will be better equipped to fully participate in the evolving STEM workforce. In addition to enabling new research, discovery, and innovation across many disciplines, Jetstream 2 will advance the national CI ecosystem and extend the broader impacts of existing NSF investments. Jetstream 2?s ?Core Services? will demonstrate a practical model of distributed cloud computing that will give academic institutions an incentive to invest their own funds in new advanced CI facilities. Although modest in scale, these facilities will represent the state of the art in reconfigurable computing. The implementation of Jetstream 2 will also demonstrate that colleges and universities can invest sustainable amounts of their own funds in highly-effective, flexible CI resources that generate a significant return on investment. In sum, Jetstream 2 will transform the national CI landscape and greatly benefit the nation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934752","A Framework for Data Intensive Discovery in Multimessenger Astrophysics","OAC","HDR-Harnessing the Data Revolu, WoU-Windows on the Universe: T","09/01/2019","09/15/2019","Patrick Brady","WI","University of Wisconsin-Milwaukee","Standard Grant","Amy Walton","08/31/2022","$2,800,000.00","David Kaplan, Mario Juric, Chad Hanna","prbrady@uwm.edu","P O BOX 340","Milwaukee","WI","532010340","4142294853","CSE","099Y, 107Y","062Z, 069Z","$0.00","This is the conceptualization phase for a Scalable Cyberinfrastructure Institute for Multi-Messenger Astrophysics (SCIMMA).  SCIMMA is a collaboration between data scientists, computer scientists, astronomers, astro-particle physicists, and gravitational wave physicists.  It leverages NSF investments in astronomical and multi-messenger facilities, and in advanced cyberinfrastructure.  Along with achieving key scientific goals and consulting with relevant collaborations and stakeholders, the conceptualization of SCIMMA includes developing algorithms, databases, and computing and networking cyberinfrastructure, to support both observations and their interpretation.  This phase includes prototyping a SCIMMA education and outreach center, and presenting accessible digests of the goals and achievements of the institute.  The team will be preparing novel curricula at the undergraduate and graduate levels, and the group will support graduate student fellowships for short duration projects.<br/><br/>Multi-messenger astrophysics (MMA) is a data-intensive science in its infancy, but already providing revolutionary insights.  At the current tipping point, timely investments in cyberinfrastructure for key MMA facilities will enable a torrent of new discoveries.  A key goal for SCIMMA is the efficient, rapid, and routine discovery of electromagnetic emission via coordinated follow-up of gravitational wave events.  This real-time science needs co-analysis of disparate datasets and the autonomous coordination of diverse follow-up in a seamless fashion.  SCIMMA balances rapid prototyping, novel algorithm development, and software sustainability.  The rapid-prototype team, tasked to use existing resources to meet immediate goals, is closely integrated with the cyberfoundations team, which will architect new algorithms and deploy new technologies.  Software sustainability is a foundational element of the design process, since the institute and its supporting cyberinfrastructure will persist for a decade or more, and both teams will adopt this principle.  SCIMMA in full operation will provide the community with consulting services and bring both existing and new MMA activities to maximal interoperability.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR) and Windows on the Universe - The Era of Multi-Messenger Astrophysics (WoU-MMA).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912444","SI2-SSE:   GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","08/31/2018","02/11/2019","Emre Brookes","MT","University of Montana","Standard Grant","Seung-Jong Park","09/30/2020","$197,151.00","","emre.brookes@umontana.edu","32 CAMPUS DRIVE","Missoula","MT","598120001","4062436670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747483","Collaborative Research: Building the Community for the Open Storage Network","OAC","BD Spokes -Big Data Regional I, EarthCube","06/15/2018","03/21/2019","Stanley Ahalt","NC","University of North Carolina at Chapel Hill","Standard Grant","Alejandro Suarez","05/31/2021","$431,786.00","Jay Aikat, Lea Shanley","ahalt@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","024Y, 8074","062Z","$0.00","The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?<br/><br/>Many universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117956","MRI: Acquisition of a GPU/CPU computing cluster for research and education in computational chemistry and materials","OAC","Major Research Instrumentation","10/01/2021","09/21/2021","Maosheng Miao","CA","The University Corporation, Northridge","Standard Grant","Alejandro Suarez","09/30/2024","$455,237.00","Jussi Eloranta, Kah Chun Lau, Abdelaziz Boulesbaa, Joseph Teprovich","maosheng.miao@csun.edu","18111 Nordhoff Street","Northridge","CA","913308309","8186771403","CSE","1189","102Z, 1189","$0.00","This Major Research Instrumentation award supports California State University Northridge (CSUN), a primarily undergraduate and minority-serving institution, to acquire its first mixed GPU/CPU high performance computing (HPC) cluster. The requested HPC cluster will enable CSUN research labs to expand the current scope and pursue new research ideas that need the support of large-scale simulations on a self-administrated machine. Specifically, it will allow the exploration of novel chemistry and new state of matter under extreme conditions; the design of novel functional and low-dimensional materials for the harvest and storage of renewable energy; the high-throughput screening of candidate organic and inorganic compounds; and the large-scale simulation studies of physical, chemical and electrochemical processes in materials synthesis and applications. The HPC cluster will greatly enhance the STEM education program at CSUN by providing necessary facilities for existing and newly created courses and by establishing research programs that broaden the participation of a diverse student population in advanced materials research. Computer simulations can greatly enhance the learning experience and efficiency by visualizing the atomic and electronic structures and relating them with the properties and functions of the molecules and materials. <br/><br/>The acquired HPC cluster will greatly enhance the activities and outcome of the research labs at CSUN by enabling large-scale screening of candidate compounds, high-throughput structure searches, constructions of ternary and quaternary phase diagrams, and thermodynamic modeling of multi-phase systems. Specifically, the labs of the PIs, the senior personnel, and other faculty members will explore the following areas of materials and solid-state chemistry: 1) the study of novel oxidation states of elements, the transformations of structures and properties of materials and minerals under high pressure and other conditions in the interior of large planets; 2) large-scale atomistic simulations of electrochemical processes and the corresponding evolution of nanostructures in metal-oxygen, metal-sulfur and new aqueous redox flow batteries; 3) developing computational methods to model the thermodynamic and fluid dynamic processes of nanoparticle formation and assembly in quantum and classical liquids during laser ablation; 4) designing new 2D materials, polymer electrolytes, carbon quantum dots, metallic glasses, hydroflurocarbon replacements and new hydrogenation catalysts through large-scale screening of the candidate materials and the understanding of related chemical processes and mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828467","MRI: Development of An Instrument for Secure Cyber Physical Systems Analytics","OAC","Major Research Instrumentation","09/01/2018","08/24/2018","Murat Kantarcioglu","TX","University of Texas at Dallas","Standard Grant","Alejandro Suarez","08/31/2022","$601,797.00","Latifur Khan, Bhavani Thuraisingham, Alvaro Cardenas","muratk@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1189","1189","$0.00","This project aims to develop a secure and scalable instrument that can efficiently perform big data analytics on data collected from Internet of Things (IoT) devices while providing tools for preserving data security and privacy. IoT devices are monitoring and controlling systems that interact with the physical world by collecting, processing and transmitting data using the internet. IoT devices include home automation systems, smart grid, transportation systems, medical devices, building controls, manufacturing and industrial control systems. With the increase in deployment of IoT devices, the amount of data generated by these devices also increases.  There is thus a need for large-scale, and secure data processing systems to process and extract information for efficient and impactful decision-making. The issue of trustworthiness in computation and data security arises when the IoT data contains sensitive information. For example, data collected using the home health devices such as a wireless blood-pressure monitor may need to be analyzed and correlated with other information. In these cases, data owners may need to protect their data and demand guaranties about data security and integrity. Development of this instrument will enable new research projects that require efficient and secure processing of IoT data. Consequently, this may allow the creation of novel IoT data processing tools and services which are not feasible today due to security and privacy concerns. <br/><br/>The proposed instrument will integrate two important components in a novel and unique way. First, an IoT data gathering component that can collect various data from IoT devices including the industrial IoT (IIoT) devices, will be developed. This component will create the necessary data gathering part of the instrument. It will allow researchers to adjust data collection frequency and granularity to enable different types of data collection activities for various research projects. The second component of the instrument will be the secure data analytics layer that can process the potentially sensitive IoT data including the network packets, sensor data, etc. As part of this component, recently developed secure data processing techniques which leverage trusted execution environments (TEEs) will be implemented. In addition, these TEE-based techniques will be tailored for different types of IoT data to increase their efficiency and limit any sensitive data leakage. During this project, these components will be integrated using custom developed software which will be open sourced at the final stage. Furthermore, these components will be integrated to test various research tools with respect to scalability, security and data privacy. The developed instrument will be made available to our collaborators and larger scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829553","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Liran Ma","TX","Texas Christian University","Standard Grant","Alan Sussman","08/31/2022","$250,000.00","Richard Alexander","l.ma@tcu.edu","2800 South University Drive","Fort Worth","TX","761290001","8172577516","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2022042","Collaborative Research: Biology-guided neural networks for discovering phenotypic traits","OAC","ICB: Infrastructure Capacity f, HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","01/15/2020","10/13/2020","Paula Mabee","OH","Battelle Memorial Institute","Continuing Grant","Peter McCartney","09/30/2022","$424,386.00","","mabee@battelleecology.org","505 KING AVE","Columbus","OH","432012696","6144244873","CSE","085Y, 099Y, 7231","062Z, 1165, 7231, 9150","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751143","CAREER: Reliable and Efficient Data Encoding for Extreme-Scale Simulation and Analysis","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","04/15/2018","10/15/2020","Seung Woo Son","MA","University of Massachusetts Lowell","Continuing Grant","Alan Sussman","03/31/2023","$500,000.00","","SeungWoo_Son@uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","1045, 7231","026Z, 062Z, 1045","$0.00","Transformative research in science and engineering to address challenges of our time, such as designing new combustion systems, depends on progressively sophisticated computational models and simulations that operate on high performance computing systems.  These simulations and analyses are increasingly constrained by the massive volumes of data that they must use, generate, and analyze.  To manage this enormous amount of data, this project explores innovative mechanisms to optimize the performance of these simulations by reducing data movement and maximizing the use of computing power, while minimizing errors and information loss.  Such performance improvements support NSF's mission to advance emerging, data-intensive science discovery and contribute to solving the world's most pressing and complex contemporary science and engineering problems.  This project implements comprehensive outreach and education to train the next-generation of professional workers and researchers in the latest computing architectures and programming methodologies, and provides rich opportunities for student engagement, research, and employment.  It leverages multiple campus and national resources and implements proven, research-based interventions to attract, retain, and educate female and underrepresented minority populations in computer engineering, which furthers the US national goal of increased participation in engineering. <br/><br/>The research goal of this project is to adapt techniques and formats for compressing video data to the investigation of novel data encoding and decoding schemes to optimize data movement and computation in data-intensive simulation and analyses.  Innovative new mechanisms have the potential to efficiently reduce the volume of data generated and transferred while also enabling rapid execution of various analysis kernels using compressed data, and permitting seamless scaling of their performance on current and future extreme-scale platforms.  The research objectives are to investigate data encoding/decoding of scientific datasets and harness encoded data, employ and scale encoded datasets seamlessly within current extreme-scale scientific workflows, and optimize machine learning and data mining algorithms with the goal of maximizing the use of computing power while minimizing errors.  These new mechanisms are applied to an evaluation framework and validated on multiple extreme-scale data-driven scientific applications, including climate, multiphysics, and fluid dynamics.  This approach is expected to transform data representation and encoding while incurring minimal disturbance to existing applications, responding to the trends in hardware architecture and dataset characteristics.  It is anticipated to improve the overall performance of computational scientists' workloads by reducing defensive and productive I/O costs, respectively, up to 100x and 200x data reduction spatially and temporally, potentially resulting in up to an overall 50x I/O cost improvement.  The project leverages multiple collaborations in order to establish the governing principles for system co-design and scalable system software layers for better data encoding within world-class computational infrastructures.  This project strengthens the University of Massachusetts Lowell computer engineering curriculum, broadens participation in computer engineering, and creates a collaborative, interdisciplinary research program geared toward exploiting ever-evolving computing paradigms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004879","Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","WoU-Windows on the Universe: T, Software Institutes","07/01/2020","04/01/2020","Roland Haas","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","06/30/2024","$683,514.00","Gabrielle Allen, Helvi Witek","rhaas@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","107Y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835903","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","XC-Crosscutting Activities Pro, Software Institutes","10/01/2018","09/12/2018","Reed Maxwell","CO","Colorado School of Mines","Standard Grant","Stefan Robila","11/30/2020","$901,394.00","","reedmaxwell@princeton.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","7222, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107548","Collaborative Research: OAC Core: Enabling Extremely Fine-grained Parallelism on Modern Many-core Architectures","OAC","OAC-Advanced Cyberinfrast Core","07/01/2021","06/28/2021","Ioan Raicu","IL","Illinois Institute of Technology","Standard Grant","Seung-Jong Park","06/30/2024","$333,684.00","","iraicu@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","090Y","026Z, 7923","$0.00","Computer systems are becoming increasingly complex: multisocket systems with many-core processors and general graphic processors have the potential to address the needs of demanding applications at the node level. Programmability and efficiency are often not easy to find together due to the hardware growing several orders of magnitude in degree of parallelism to thousands of computing units on a chip. Task parallelism is an important type of parallelism in which computation is broken down into a set of inter-dependent tasks which can be executed concurrently on various computing units. To achieve strong scaling and high levels of effective parallelism, there is a growing need in today's parallel languages with supporting over-decomposition (many more tasks than cores) in order to improve performance, hide latency caused by blocking operations, and otherwise achieve maximum speedup. By enabling the efficient support of fine-grained parallelism across the growing range of scales seen in modern and future hardware, it is expected that the productivity of parallel programmers will be enhanced. Trends show evidence that most of the Top500 high-performance computing systems will likely employ hardware that this work directly targets. The project aims to conduct a high-impact education program in distributed parallel programming with broad reach, encouraging student internships grounded in real-world challenges, and paving the way for technology transfer from research to open-source projects. Special emphasis is placed on engaging women and underrepresented minorities. This education facet will create a new and more accessible foundation for fluency in parallel computing for scientists and engineers.<br/><br/>This work explores novel data-structures and algorithms that allow for scalable runtime and execution models for fine-grained parallelism at sub-microsecond timescales. Preliminary work by the PIs at the language and runtime levels suggests a path to achieving this. The project objectives are: 1) unifying runtime enabling task granularities measured in cycles: design, analysis, and implementation of building blocks for efficient fine-grained computing on diverse node hardware; 2) evaluating performance of these building blocks in the context of real parallel systems and application kernels on a range of computer architectures; 3) measuring performance and scalability impact of runtime on benchmark kernels and real applications; and 4) integrating this research with education programs from undergraduate to graduate levels through new course material on parallel computing. This high-risk/high-reward research is geared towards yielding transformative improvements in the ease and efficiency of programming parallel machines at every scale. The contributions lie in the realization of productive, implicitly parallel high-level languages optimized for single node deployments with many-core architectures to support fine-grained parallelism measured in cycles, enabling an entirely new class of many-task computing applications. The dataflow architecture makes implicit parallelism tractable with a programming model whose impact could rival that of MATLAB, R, and Python, with the added benefit that the same code could also run in a distributed system or large-scale HPC systems. Thus, the scientist would be able to write a program once, run it at any suitable scale, and have it seamlessly use the most appropriate granularity for each component of the hardware. This work?s innovations in dataflow architecture will be broadly applicable to a number of existing parallel programming systems such as OpenMP, Swift/Parsl, and CUDA/OpenCL, in terms of both efficiency in executing fine grained parallelism and adding support for implicit parallelism where possible. Target hardware includes Intel/AMD x86, ThunderX/2 ARM, IBM Power9, and NVIDIA/AMD GPUs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018975","CC* Team: Piloting a CI-Enabled Tribal College and University Research Collaboration","OAC","CYBERINFRASTRUCTURE","10/01/2020","10/20/2020","Algirdas Kuslikis","VA","American Indian Higher Education Consortium","Standard Grant","Kevin Thompson","09/30/2022","$1,399,997.00","Dana Skow, Chad Davis","akuslikis@aihec.org","121 Oronoco Street","Alexandria","VA","223142015","7038380400","CSE","7231","","$0.00","The American Indian Higher Education Consortium and institutional partners Sitting Bull College, Nueta Hidatsa Sahnish College, United Tribes Technical College Turtle Mountain Community College, Cankdeska Cikana Community College, and North Dakota State University are partnering on this Cyber Team project that establishes the socio-technical foundations of a cyberinfrastructure (CI)-enabled collaboration that will both support current STEM research and education programs and provide the framework for aggressive research program development. The project involves faculty, students and IT staff at the six participating institutions collaborating on CI improvements, IT staff capacity-building, and distributed STEM programming focused on environmental science, eventually including engineering, digital manufacturing and other STEM disciplines.<br/><br/>The project will provide professional development, support, mentoring and collaborative research opportunities for faculty and students at all North Dakota Tribal Colleges and Universities (TCUs). The project will enhance opportunities for economic development and enhance the design and delivery of health, education, and other services developed through CI-enabled basic and applied research. It will serve as a model for engaging TCUs nationally in the adoption of CI resources necessary to support implementation of a significantly broader range of research and education activities, particularly involving collaborations with the larger research community.<br/><br/>The project?s collaborative capacity-building framework implements the connectivist learning model in which networked human and physical resources constitute a complex adaptive system that generates actionable knowledge (both technical and domain-specific) sustainably. The project tests the hypothesis that a connectivist framework can be transformative in building the research and education capacity of small resource challenged institutions and organizations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029309","IRNC: ENgage: Strengthening Global Cyberinfrastructure Ecosystems to Advance International Science Collaboration","OAC","International Res Ret Connect","10/01/2020","10/14/2020","Steven Huter","OR","University of Oregon Eugene","Continuing Grant","Kevin Thompson","09/30/2025","$1,977,577.00","Dale Smith, William Allen","sghuter@nsrc.org","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7369","","$0.00","The Network Startup Resource Center (NSRC) develops network communications infrastructure and local engineering capacity in areas of the world where inadequate research and education network (REN) connectivity poses a significant barrier to international collaborations with US scientists and educators. By helping build physical network infrastructure and the human technical capacity to maintain and upgrade it, NSRC assists the US science community by fostering state-of-the-art global cyberinfrastructure (CI), ubiquitous access to global scientific communities, remote scientific instruments, and shared data resources. NSRC exemplifies the NSF's stated strategic goal of ""encouraging collaborative research and education across organizations, disciplines, sectors, and international boundaries."" <br/> <br/>NSRC incubates and builds sustainable CI consistent with the guiding principles of the International Research Network Connections (IRNC) program, including: technical training and direct engineering assistance to improve operational infrastructure from university campuses to national and regional RENs; engaging RENs to participate in globally interconnected testbed facilities with the IRNC initiative; disseminating information about best practices for design, development, and operation of CI resources; and amplifying numerous training programs to foster a new generation of network engineers and leaders in underserved communities, while providing experiential learning opportunities for undergraduate and graduate students. NSRC achieves this through targeted capacity building activities and partnerships with universities, RENs, Internet Service Providers, industry, government and supranational agencies in Africa, Asia-Pacific, the Middle East, Latin America-Caribbean, and North America. Substantial financial support from industry and international partners, facilitated by NSRC, leverages NSF funds to augment the impact of its work with the global R&E community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931278","Collaborative Research: Elements: Advancing Data Science and Analytics for Water (DSAW)","OAC","Software Institutes","10/01/2019","09/09/2019","Anthony Castronova","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Alan Sussman","09/30/2022","$29,443.00","","acastronova@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392267445","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Scientific challenges in hydrology and water resources such as understanding impacts of variable climate, sustainability of water supply with population growth and land use change, and impacts of hydrologic change on ecosystems and humans are increasingly data intensive. The volume of data produced by environmental scientists to study hydrologic systems requires advanced software tools for effective data visualization, analysis, and modeling. Scientists spend much of their time accessing, organizing, and preparing datasets for analyses, which can be a barrier to efficient analyses and hinders scientific inquiries and advances. This project will develop new software that will enhance scientists' ability to apply advanced data visualization and analysis methods (collectively referred to as ""data science"" methods) in the hydrology and water resources domain. The project will promote standardized software tools and data formats to help scientists enhance the consistency, share-ability, and reproducibility of the analyses they perform - all of which are important in building trust in scientific results. The software developed in the project will make data loading and organization for analysis easier, reducing the time spent by scientists in choosing appropriate data structures and writing computer code to read and parse data. It will enable users to automatically retrieve data from the HydroShare system, which is a hydrology domain data repository, as well as from important national water data sources like the United States Geological Survey's National Water Information System. The software will automatically load data from these sources into standardized and high performance data structures targeted to specific scientific data types and that integrate with visualization, analysis, and other data science capabilities commonly used by scientists in the hydrology and water resources domains. The project will also reduce the technical burden for water scientists associated with creating a computational environment within which to execute their analyses by installing and maintaining the Python packages developed within the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare-linked JupyterHub environment. Finally, the project will demonstrate the functionality and use of the software by producing a set of educational modules based on real water-data science applications that provide a specific mechanism for delivering the software to the community and promoting its use in classroom and research environments.<br/><br/>Scientific and related management challenges in the water domain are inherently multi-disciplinary, requiring synthesis of data of multiple types from multiple domains. Many data manipulation, visualization, and analysis tasks performed by water scientists are difficult because (1) datasets are becoming larger and more complex; (2) standard data formats for common data types are not always agreed upon, and, when they are, they are not always mapped to an efficient structure for visualization and/or analysis within an analytical environment; and (3) water scientists generally lack training in data intensive scientific methods that would enable them to use new and existing tools to efficiently tackle large and complex datasets. This project will advance Data Science and Analytics for Water (DSAW) by developing: (1) an advanced object data model that maps common water-related data types to high performance data structures within the object-oriented Python language and analytical environment based upon standard file, data, and content types established by the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare system; (2) two new Python packages that enable users to write Python code for automating retrieval of desired water data, loading it into high performance memory objects specified by the object data model designed in the project, and performing analysis in a reproducible way that can be shared, collaborated around, and formally published for reuse. The project will use domain-specific data science applications to demonstrate how the new Python packages can be paired with the powerful data science capabilities of existing Python packages like Pandas, numpy, and scikit-learn to develop advanced analytical workflows within cloud and desktop environments. The project aims to extend the data access, collaboration, and archival capabilities of the HydroShare data and model repository and promote its use as a platform for reproducible water-data science. The project also aims to overcome barriers associated with accessing, organizing, and preparing datasets for data science intensive analyses. Overcoming these barriers will be an enabler for transforming scientific inquiries and advancing application of data science methods in the hydrology and water resources domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118171","Collaborative Research: CyberTraining: Pilot: A Cybertraining Program to Advance Knowledge and Equity in the Geosciences","OAC","CyberTraining - Training-based","10/01/2021","09/09/2021","Mark Piper","CO","University of Colorado at Boulder","Standard Grant","Alan Sussman","09/30/2023","$51,556.00","","mark.piper@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","044Y","7231, 7301, 9102","$0.00","This project will build and evaluate sustainable pathways for participation in cybertraining within the Earth science fields. The project will enhance inclusivity by focusing on underrepresented geoscience students for training in cyberinfrastructure (CI), including computer programming, data analysis, and modeling, which are not typically parts of the geoscience curriculum. The project will enhance programming education for geoscience students and increase access and mentoring relationships in STEM fields, thus building sustainable CI for geoscience programing. Consequently, this project will lower barriers to entry for STEM students while developing networks and mentoring relationships in the geosciences.  <br/><br/>This project will provide geoscience graduate students with skills training that promote knowledge proliferation in software development and science communication. Students will learn to automate workflows, test code, manage code repositories, visualize data, and communicate research effectively, which will accelerate their research foundation and provide future professional opportunities. Given the size of the Spanish-speaking STEM student population in the United States, the current effort will focus on bi-lingual cybertraining. This CI project will provide opportunities to enhance coding skills and communication abilities, while expanding students? professional network and establishing mentors at multiple career stages. Cybertraining will occur in a series of virtual and in-person terms, where students will learn Python coding and data analysis, and will be exposed to a Hackathon to provide practical application of these skills. Course materials will be open source, which will create a sustainable cybertraining program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007181","OAC Core: Small: Efficient and scalable tools for design and analysis of active matter systems","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","08/26/2021","Tong Gao","MI","Michigan State University","Standard Grant","Alan Sussman","09/30/2023","$513,999.00","Shanker Balasubramaniam, Hasan Metin Aktulga","gaotong@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","090Y","7923, 9251","$0.00","The term ""active matter"" denotes a novel class of non-equilibrium materials made up of constituents that are self-driven, powered by converting the energy in the environment (typically chemical energy) to mechanical work (locomotion or swimming). They are abundant in nature, from the animate to the inanimate; this terminology can be used to describe flocks of birds or swarms of bacteria that self-organize to chemically active colloidal particles. The common characteristics of active matter are collective motion, anomalous fluctuations, and mechanical properties that cannot be explained by equilibrium physics. To date, most studies have been on small systems or a limited number of particles with the goal of understanding the underlying behavior with over-simplified property descriptors. This project seeks to develop a framework that will enable both understanding and exploiting the properties of active matter systems; to take this engineering leap forward, the project team intends to develop a publicly available virtual laboratory, the Fast Active Matter Simulator (FAMS), that will enable prototyping of novel active matter systems via efficient discrete particle methods. To enable widespread dissemination, the project will create local K-12 outreach programs, leverage REU opportunities for undergraduate students, recruit under-represented students, and incorporate computational techniques into our undergraduate and graduate curriculum.<br/> <br/>The proposed research will transform the state-of-the-art in active matter research, from understanding simple canonical systems to design tools that would enable engineering/manipulation of active matter to build systems. To do so, one needs to account for the morphology of particles, ambient environment, external forces, etc. As the problem is inherently multiscale, one needs to develop rigorous methods that are efficient across these scales, and fully resolve the long- and short-range interactions by incorporating the details of particle shapes, complex geometries of obstacles, and confinement boundaries. To realize the above objectives, the project team will perform research and development in four different areas; (a) higher-order representation of both geometry and physics on the geometry via isogeometric methods so as to guarantee fidelity without high cost, (b) casting these representations within a boundary integral equation based framework, (c) integrating with a set of acceleration techniques to reduce memory and computational bottlenecks to facilitate analysis of realistic aggregates, and (d) integration with existing libraries to leverage parallel algorithms for linear algebra (dense and sparse). The project will use this framework to characterize material properties and collective dynamics. These new methods will transform the state-of-the-art in active matter research, from understanding simple canonical systems to building tools that would enable engineering/manipulation of active matter to build virtual ?living? systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031623","NSF Frontera Allocation Travel Grant","OAC","Leadership-Class Computing","09/01/2020","05/12/2020","Aleksei Aksimentiev","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2022","$8,190.00","","aksiment@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031661","Collaborative Research: Travel Supplement for Frontera's ""Multi-scale, MHD-Kinetic Modeling of the Solar Wind and its Interaction with the Local Interstellar Medium""","OAC","Leadership-Class Computing","06/01/2020","05/12/2020","Vadim Roytershteyn","CO","SPACE SCIENCE INSTITUTE","Standard Grant","Edward Walker","05/31/2022","$2,120.00","","vroytershteyn@spacescience.org","4765 Walnut Street","Boulder","CO","803012575","7209745888","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117449","MRI: Acquisition of a High-Performance Computing Cluster for Science and Engineering Research at the University of Kansas","OAC","Major Research Instrumentation, Information Technology Researc, EPSCoR Co-Funding","10/01/2021","09/17/2021","Brian Laird","KS","University of Kansas Center for Research Inc","Standard Grant","Alejandro Suarez","09/30/2024","$687,060.00","Ward Thompson, Yinglong Miao, Suzanne Shontz","blaird@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457552","7858643441","CSE","1189, 1640, 9150","1189, 9102, 9150","$0.00","A High Performance Computing (HPC) cluster resource will be built at the University of Kansas (KU) that will advance computational science and engineering at KU and selected Primarily Undergraduate Institutions (PUIs). This cluster, nicknamed BigJay, will be part of the KU Community Cluster housed in the state-of-the-art Advanced Computing Facility and maintained by the KU Center for Research Computing (CRC). The major users form a multidisciplinary group from 11 different academic departments/programs at KU: Aerospace Engineering, Bioengineering, Chemistry, Chemical and Petroleum Engineering, Computational Biology, Electrical Engineering and Computer Science, Geography, Geology, Mathematics, Molecular Biosciences, and Physics & Astronomy. In addition, faculty and undergraduate researchers from four different Primarily Undergraduate Institutions (PUIs) with ties to KU (The College of St. Scholastica, Prairie View A&M University, Hobart & William Smith College, and the University of Southern Indiana) will have access to the enabled computational resources. Additionally, workshops will be offered by the CRC and participants on cluster use and advanced computational methods, that will also be available to students (and faculty) from KU and the participating PUIs. The proposed HPC resources will enable exposure to HPC techniques to students in graduate and advanced undergraduate courses. Finally, the BigJay cluster will stimulate increased interdisciplinary collaborations centered around HPC, including with the PUI faculty participants. <br/><br/>BigJay will be made up of a mix of CPU, high-memory CPU, and GPU-enabled processors and include gigabit ethernet and high-speed Infiniband networking. The proposed configuration was chosen to reflect the diverse computing need of the participants. The fundamental research enabled by BigJay will span algorithm development, computational methodology, machine learning, and theoretical descriptions of physical systems. The work will impact problems of importance to society ranging from the development of battery materials to cancer recognition to optimization of catalytic systems, as well as improved theories of physical and biological systems. Interdisciplinary collaborations will be facilitated by the workshops that will introduce students and faculty across disciplines. These will be used as an opportunity to identify commonalities in research topics, numerical methods, and computational tools. Collaborations will also be encouraged by instituting a Computational Research Symposium, consisting of posters and short talks, that will be held biannually for the participants, as well as others who are engaged in computational research on the KU campus. The proposed purchase will substantially enhance HPC research efforts at KU and the PUIs and make participating research groups potentially more competitive for funding from federal, non-profit, or industrial sources. It will further be used to enhance student training in scientific and technical computing through research, the CRC and participant-led workshops, and inclusion of HPC components in graduate and advanced undergraduate courses.<br/><br/>This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103780","Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science","OAC","Software Institutes","10/01/2021","09/17/2021","Jerad Bales","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Continuing Grant","Alan Sussman","09/30/2026","$104,033.00","","jdbales@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392267445","CSE","8004","077Z, 7925, 8004","$0.00","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges.  The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.<br/><br/>The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the  Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.<br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126266","CC* Integration-Large: In-Network Distributed Infrastructure for Advanced Network Applications","OAC","CISE Research Resources","10/01/2021","09/16/2021","Douglas Swany","IN","Indiana University","Standard Grant","Deepankar Medhi","09/30/2023","$938,479.00","","swany@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","2890","","$0.00","This project makes the Internet ""smarter"".  The ""end to end"" principle of Internet design states that the routers, switches, WiFi base stations, etc. that comprise the network should only forward packets. There are many instances in which small amounts of processing at intermediate stages can make Internet applications more efficient. Cloud computing services have had a significant impact on society. Many envision an even more powerful Internet with cloud-like services distributed throughout and all the way to the edge of the network. This project is exploring a smart network with small servers embedded in the fabric of the Internet.<br/><br/>This project brings together a novel lightweight execution model suitable for efficient in-network microservices, and a topology aware coordination and control layer, with a policy language to express desired application services and requirements. The execution model, called InLocus, is designed to bridge the gap between network packet processors and stateful network services. InLocus runs on conventional microprocessors and microcontrollers, as well as on accelerators like Field Programmable Gate Arrays (FPGAs). This enables necessary functionality with minimal resources. The coordination framework monitors the network (with extensions to the PerfSONAR network monitoring system) and can make intelligent service placement and instantiation decisions. This project will integrate and evaluate two important application environments, one in Earth Sciences and one applicable to distributed machine learning. Each of these will work in conjunction with NSF cyberinfrastructure resources.<br/><br/>This project stands to impact scientific and commercial applications and their users. Just as the Internet has touched many lives in the past years, a better Internet will continue to drive innovation and users' qualities of experience. Techniques like the ones we are exploring are widely regarded as necessary to fully realize autonomous vehicles and smart cities, as well as user-oriented capabilities like augmented reality. Additionally, we teach many of the tenets of this project in classes and seminars at various levels, as they embody the history and the future of the Internet.<br/><br/>This project's web site is at http://indiana.open.luddy.indiana.edu/ and a project overview, component descriptions, and links to source code repositories can be found there. This project builds, and integrates, previously funded projects. These previous projects remained open source even after their period of funding ended. Further, the core functionality has been maintained as other work depends on it. This project's data and code will be maintained as long as possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103680","Frameworks: MUSES, Modular Unified Solver of the Equation of State","OAC","PHYSICS AT THE INFO FRONTIER, Software Institutes","10/01/2021","09/16/2021","Nicolas Yunes","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tevfik Kosar","09/30/2026","$4,421,367.00","Jorge Noronha, Jacquelyn Noronha-Hostler, Veronica Dexheimer, Claudia Ratti","nyunes@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","107y, 7553, 8004","069Z, 075Z, 077Z, 7483, 7569, 7925, 8004, 9102","$0.00","The recent detection of X-rays from hot spots on the surface of rotating neutron stars and the observation of the gravitational waves emitted when neutron stars collide hold the promise of revealing the mysteries of nuclear astrophysical phenomena in a previously inaccessible regime. Meanwhile, in the laboratory, heavy-ion collision experiments, akin to miniature neutron star mergers, can reveal complementary information about the properties of matter at extreme temperatures and densities. A core group of research teams at the University of Illinois Urbana-Champaign, the National Center for Supercomputing Applications, the University of Houston, Kent State University, and several other auxiliary institutions are creating a new cyberinfrastructure that can rapidly and efficiently describe nuclear matter across vastly different densities and temperatures. This open cyberinfrastructure allows users to understand the nature of the building blocks of matter through comparisons with data from NASA?s Neutron Star Interior Composition Explorer, the National Science Foundation?s Laser Interferometer Gravitational-wave Observatory, the Department of Energy?s fixed target Solenoid Tracker at the Relativistic Heavy Ion Collider, and other international facilities. The project provides outreach to the community through an online science portal that makes high-end computations more approachable for end users with additional tutorials, extended showcases, and detailed documentation.<br/><br/>The research plan creates a cyberinfrastructure, MUSES (Modular Unified Solver of the EoS), that provides various scientific communities with novel tools to help answer critical interdisciplinary questions in nuclear astrophysics, gravitational wave astrophysics, and heavy-ion experiments. The investigators are creating modern, efficient and parallelizable code to generate equation-of-state modules in different regimes of density, pressure, and temperature in either 2, 3, or 4 dimensions. These modules are then integrated into a single, standardized global calculational engine that allows the fast generation of equations-of-state across the entire phase diagram of quantum chromodynamics ? the fundamental theory of strong interactions. The project creates a web interface with a collection of application programming interfaces that allows external users to run the MUSES cyberinfrastructure remotely and deploy it in high-performance computing clusters. In particular, the plug-and-play operability of MUSES allows external users to select parameters in the equation-of-state package to run a given set of modules and generate a global EoS with a chosen set of observable byproducts. This project advances the goals of the Division of Physics and the Office of Advanced Cyberinfrastructure of the National Science Foundation.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe NSF Big Idea program, the Physics at the Information Frontier (PIF) program in the Division of Physics (PHY), and the Division of Astronomical Sciences (AST).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118285","HDR Institute: HARP- Harnessing Data and Model Revolution in the Polar Regions","OAC","HDR-Harnessing the Data Revolu, ANT Integrated System Science, Polar Cyberinfrastructure, CYBERINFRASTRUCTURE","01/01/2022","09/15/2021","Maryam Rahnemoonfar","MD","University of Maryland Baltimore County","Cooperative Agreement","Amy Walton","12/31/2026","$5,000,000.00","Jan Lenaerts, Jianwu Wang, Shashi Shekhar, Mathieu Morlighem","maryam@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","099Y, 5292, 5407, 7231","062Z, 1079, 5294, 7231, 9102","$0.00","Climate-change induced loss of polar ice sheets impacts many lives and increases coastal flooding by rising sea level and affecting ocean circulation. However, it remains difficult to accurately predict how quickly the ice sheets will continue to shrink. In particular, we are still challenged by a limited understanding of transdisciplinary processes that determine ice sheet change, such as the role of subglacial topography and ice-atmosphere-ocean interactions. Timely investment in machine learning and data intensive research can revolutionize the way that scientists currently answer questions related to ice dynamics. This HDR Institute serves as a research hub where experts in data science, Arctic and Antarctic science, and cyberinfrastructure in academia, government, and private sectors come together to develop transformative and integrative data science solutions to reduce uncertainties in projecting future sea-level rise and climate change. i-HARP researchers investigate the potential of novel physics-aware data science and machine learning approaches to address national priorities and challenges on Navigating the New Arctic, climate change, and sea-level rise.<br/><br/>The HDR Institute aims to harness massive heterogeneous, noisy, and discontinuous data in space and time and integrate data with numerical and physical models. Researchers at i-HARP are investigating novel data science techniques including deep generative adversarial networks, graph neural networks, meta learning, hybrid networks, physics-informed machine learning, causal artificial intelligence, data assimilation, spatiotemporal deep learning, and scalable algorithms. Due to the fundamental nature of data science problems that i-HARP addresses, the solutions can be translated to other disciplines such as remote sensing, medicine, and autonomous driving. Moreover, the convergence team champions multiple clusters of research-integrated educational initiatives, with a specific focus on facilitating cross-disciplinary collaborations, training next-generation multi-disciplinary researchers and engaging the public in scientific inquiry as related to climate change and data science. In partnership with related communities, i-HARP designs curricula, and offers hands-on community workshops, lecture series, conference tutorials, and training. i-HARP engages students from underrepresented minority groups by leveraging several existing organizations for underrepresented minorities.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR).  This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Section for Antarctic Sciences and the Section for Arctic Sciences within the NSF Office of Polar Programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117997","HDR Institute: Accelerated AI Algorithms for Data-Driven Discovery","OAC","HDR-Harnessing the Data Revolu, WoU-Windows on the Universe: T","10/01/2021","09/15/2021","Shih-Chieh Hsu","WA","University of Washington","Cooperative Agreement","Amy Walton","09/30/2026","$4,500,000.00","Mark Neubauer, Song Han, Michael Coughlin, Kate Scholberg","schsu@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","099Y, 107Y","062Z, 069Z, 075Z, 1206, 7483, 9102","$0.00","The data revolution is dramatically accelerating the acquisition rate of new information, creating a vast amount  of data. Artificial intelligence (AI) has emerged as a solution for rapid processing of complex datasets.  New hardware such as graphics processing units (GPUs) and field-programmable gate arrays (FPGAs) allow AI algorithms to be greatly accelerated.  To take full advantage of fast AI, the Institute of Accelerated AI Algorithms for Data-Driven Discovery (A3D3) targets fundamental problems in three fields of science: high energy physics, multi-messenger astrophysics, and systems neuroscience.  A3D3 works closely within these domains to develop customized AI solutions to process large datasets in real-time, significantly enhancing their discovery potential.  The ultimate goal of A3D3 is to construct the institutional knowledge essential for real-time applications of AI in any scientific field. Through dedicated outreach efforts, A3D3 will empower scientists with new tools to deal with the data deluge.  Students mentored through A3D3 research will interact closely with industry partners, creating new career opportunities and strengthening synergies between academia and industry.<br/><br/>The approach of A3D3 is to tightly couple AI algorithm innovations, heterogeneous computing platforms, and science-driven application development informed through close collaboration with domain scientists within physics, astronomy, and neuroscience.  The common theme  across domains is  the development of AI strategies accelerated by emerging processor technology, employing hardware-AI co-design as a transformative solution to a wide range of scientific challenges.  Hardware architectures such as GPUs and FPGAs have emerged as promising technologies to address many of the challenges in data-intensive science because they provide highly-performant, parallelizable, and configurable data processing pipeline capabilities.  When combined with AI algorithms, these architectures significantly accelerate scientific workflows compared to CPU-only computing platforms.  Building on the existing Fast Machine Learning community, A3D3 cultivates an ecosystem where scientists across domains collaborate to meet critical challenges, forming a central hub of excellence for innovation in accelerated AI for science.  The work is extended to the public at large through a diverse set of educational training programs and by mentoring next-generation scientists.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR) and Windows on the Universe - The Era of Multi-Messenger Astrophysics (WoU-MMA).  This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Divisions of Astronomical Sciences and of Physics within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939987","Collaborative Research: MEMONET: Understanding memory in neuronal networks through a brain-inspired spin-based artificial intelligence","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc, Info Integration & Informatics","10/01/2019","10/15/2020","Linbing Wang","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Sylvia Spengler","09/30/2022","$399,859.00","","wangl@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","099Y, 1640, 7364","062Z","$0.00","The brain is arguably the most sophisticated and the most efficient computational machine in the universe. The human brain, for example, comprises about 100 billion neurons that form an interconnected circuit with well over 100 trillion connections. Understanding how a multitude of brain functions emerge from the underlying neuronal circuit will give insights into the operating principles of the brain. In this award, a multidisciplinary team of systems biologist, computational biologist, material scientist, neuroscientist, and machine learning expert will work synergistically to leverage the data revolution in neuroscience to answer a fundamental question: How does the brain learn, store, and process information?  The team will develop and apply advanced data analysis algorithms to harness the great volume of neuronal data generated by the latest imaging and molecular profiling technologies, for elucidating the neuronal circuits driving brain functions. Computer simulations of a spin-electronic (spintronic) device will further serve as a platform to validate and emulate important operational characteristics of such neuronal circuits. The award sets the groundwork for an interdisciplinary data science research and educational program that will bring a new and powerful paradigm for studying brain functions as well as for designing transformative brain-inspired devices for information processing, data storage, computing, and decision making.<br/><br/>The project has a specific focus on an essential function of the brain: motor-skill learning. This function emerges from the underlying circuitry of neurons that governs the activities of molecular signal transmission and neuronal firing. Importantly, the neuronal circuit in a mammalian brain is highly plastic and dynamic, features that endow animals with the ability to respond to myriad external stimulations through learning. By harnessing the latest data revolution in neuronal imaging, single neuron molecular profiling, spintronic device simulation, network inference, and machine learning, a team of multidisciplinary investigators will be supported by this award to investigate the fundamental principle of neuronal circuit rewiring that drives brain?s learning function. More specifically, the team sets out to achieve the following specific tasks: (A) Infer learning-induced rewiring of large-scale neuronal networks from two-photon calcium imaging data through the development of novel and powerful network inference algorithms; (B) Build biochemical-based models of neuronal circuits by integrating molecular profiling with neuron firing and connectome dynamics; and (C) Develop a spintronic material network model that emulates learning and memory formation by exploiting the spin dynamics in spintronic materials. The project seeks to lay the foundation for the creation of an interdisciplinary data-intensive brain-to-materials initiative that will be applied to understand and emulate the operational principles of brain neuronal circuits underlying learning, cognition, memory formation, and other behaviors. The outcomes of the initiative will have a paramount impact on the society, not only in our understanding of the brain and its functions, but also in overcoming current bottlenecks of existing computing architectures.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939263","HDR: DIRSE-IL: Collaborative Research: Harnessing data advances in systems biology to design a biological 3D printer: the synthetic coral","OAC","HDR-Harnessing the Data Revolu","10/01/2019","10/15/2020","Lenore Cowen","MA","Tufts University","Continuing Grant","Sylvia Spengler","09/30/2022","$260,081.00","","cowen@eecs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","099Y","062Z, 9102","$0.00","Corals are important natural resources that are key to the ocean's vast biodiversity and provide economic, cultural, and scientific benefits. As a result of human activities, locally and globally, coral reefs are declining rapidly. The complexity of corals makes conserving and restoring reefs very challenging. Corals are made up of thousands of different organisms, including the animal host and the algae, bacteria, viruses, and fungi that coexist as a so-called holobiont. Thus, corals are more like cities than individual animals, as they provide factories, housing, restaurants, nurseries, and more for an entire ecosystem. This project brings together experts in computer science, materials science, and biology to harness the data revolution in biology with machine learning to study how corals grow and function, when viewed as if they were manufacturing sites in the ocean. The study will focus on three key coral capabilities: (1) they create calcium carbonate skeletons that provide 3D structures for diverse sea life to live in, (2) they can heal damage to their tissues, and, (3) they live with the other organisms in a process referred to as symbiosis. Through these remarkable abilities, corals can 'print' resources for themselves and hundreds of thousands of other species, just like a 3D printer. The goal of this project is to understand these processes well enough to control them in the lab. This project may allow finding new ways to help coral survival, by deciphering the reasons why certain conditions damage them and find ways of repairing them. Furthermore, by synthetically growing corals, new types of materials may be identified for manufacturing. This project offers an opportunity to educate a diverse scientific workforce and the public by creating and disseminating the outcomes of a convergent research environment and will train postdoctoral researchers, graduate, and undergraduate students. Results of this research will be made available to the broader scientific community through web interfaces, peer-reviewed publications and workshops/conferences and shared with the public through outreach activities online, at schools, and public aquariums.<br/>    <br/>Through convergence of three disciplines, computer science, material science and biology, this project will provide a data-driven framework and toolset to learn from, control, engineer, and manufacture a combined form of living material, the 'synthetic coral', thereby opening new avenues for material synthesis and manufacturing. The research methodology will offer new analytical approaches to identify and quantify the parameters that govern coral growth and foster innovative new tools for controlling their growth. To understand the key functions of coral biology of biomineralization, wound healing, and symbiosis, this research will : (1) harness and analyze large amounts of coral '-omics' data to decipher critical molecules and their interactions for the aforementioned key functions, (2) experimentally validate the resulting predictions in coral individuals and cell lines, (3) manipulate the material properties of the calcium carbonate structures of the coral individuals and cell lines, and (4) test the biological and physical interactions in a network model of the 'synthetic coral'. This project develops and integrates fundamental building blocks that are essential for  an integrated computational and experimental validation system. Specifically, using machine learning, diverse data will be harnessed to identify physical conditions (e.g., surface characteristics), environmental conditions (e.g., temperature, pH), and key biological constituents (e.g., small molecule ligands and proteins encoded in the DNA) that are correlated to key structural and functional properties of the coral holobiont. These predicted conditions and molecules will be verified experimentally by perturbing individual coral nodes in a network of a 3D printed array of intact corals or their constituent cells and measuring their effects on the network of interactions and resulting structures. The results from this prediction-validation cycle will then be transferred back as input to manufacture novel adaptive materials fully embracing the organic/inorganic interface. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934405","Collaborative Research: Framework for Integrative Data Equity Systems","OAC","CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Bill Howe","WA","University of Washington","Continuing Grant","Sylvia Spengler","08/31/2022","$656,000.00","","billhowe@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","099y, 7231","062Z, 7231","$0.00","Data Science continues to have a transformative impact on Science and Engineering, and on society at large, by enabling evidence-based decision making, reducing costs and errors, and improving objectivity. The techniques and technologies of data science also have enormous potential for harm if they reinforce inequity or leak private information.  As a result, sensitive datasets in the public and private sector are restricted from research use, slowing progress in those areas that have the most to gain: human services in the public sector.  Furthermore, the misuse of data science techniques and technologies will disproportionately harm underrepresented groups across race, gender, physical ability, sexual orientation, education, and more. These data equity issues are pervasive, and represent an existential risk for the use of data-driven methods in science and engineering. This project will establish a  Framework for Integrative Data Equity Systems (FIDES): an Institute for the study of systems that enable research on sensitive data while preventing misuse and misinterpretation. <br/><br/>FIDES will enable interdisciplinary community convergence around data equity systems, with an initial study in critical domains such as mobility, housing, education, economic indicators, and government transparency, leading to the development of a novel data analytics infrastructure that supports responsibility in integrative data science.  Towards this goal, the project will address several technically challenging problems: (1) To be able to use data from multiple sources, risks related to privacy, bias, and the potential for misuse must be addressed. This project will develop principled methods for dataset processing to overcome these concerns.  (2) Individual datasets are difficult to integrate for use in advanced multi-layer network models.  This project considers methods to create pre-trained tensors over large collections of spatially and temporally coherent datasets, making them easier to incorporate while controlling for fairness and equity.  (3) Any dataset or model must be equipped with sufficient information to determine fitness for use, communicate limitations, and describe underlying assumptions.  This project will develop tools and techniques to produce ""nutritional labels"" for data and models, formalizing and standardizing ad hoc metadata approaches to provenance, specialized for equity issues. In addition to supporting methodological innovation in data science, the Institute will become a focal point for sharing expertise in data equity systems.  It will do so by establishing interfaces for interaction between data science and domain experts to promote expertise development and sharing of best practices, and by consistently supporting efforts on diversity and equity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2139358","CICI: UCSS: Towards Secure and Usable Push Notification Authentication for Collaborative Scientific Infrastructures","OAC","Cybersecurity Innovation","10/01/2021","07/09/2021","Nitesh Saxena","TX","Texas A&M Engineering Experiment Station","Standard Grant","Robert Beverly","11/30/2024","$499,934.00","","nsaxena@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","8027","7923, 8027, 9150","$0.00","Second factor (2FA) or passwordless authentication based on notifications pushed to a user's personal device (e.g., a phone) that the user can simply approve (or deny) has become widely popular due to its convenience, especially to protect scientific resources at Universities and similar organizations. This project is studying the premise that the effortlessness of this approach gives rise to a fundamental design vulnerability arising from concurrent login sessions (one initiated by the user and the other initiated by the attacker), and then redesigning push-based authentication systems that can counter the identified vulnerability without degrading the overall usability of the approach. The proposed new design attempts to address the concurrent login attacks by establishing a unique binding between the user?s browser session and the push notification.<br/><br/>The research consists of three inter-related activities: (1) formalization and study of a fundamental vulnerability against standard push notification authentication schemes; (2) design and implementation of low-effort push-based authentication schemes that can defeat the identified vulnerability without undermining the usability; and (3) formal studies of the proposed new push-based authentication schemes, conducted in lab settings and field environments. The developed resilient push authentication system designs are expected to offer an improved level of protection, accessibility and usability to everyday users in scientific and collaborative settings. The research prototypes are expected to be of broader value in future research on building resilient and usable authentication services in practice. The project is emphasizing technology transfer by working with major players in the push-based authentication domain. The proposed research is being integrated with educational activities in the form of advanced curriculum development and student mentoring in the broad domains of Authentication and Human-Computer Interaction, and the involvement of high school and K-12 students and minority populations are broadening the reach of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017506","CyberTraining: Implementation: Small: Enabling Dark Matter Discovery through Collaborative Cybertraining","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","10/01/2020","10/20/2020","Andrew Renshaw","TX","University of Houston","Standard Grant","Alan Sussman","09/30/2023","$162,118.00","","arenshaw@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","044Y, 7244","7569","$0.00","Detecting dark matter in the lab would be transformational for physics, and such a difficult measurement requires providing a foundation for early-career scientists in advanced data analytics.   The science question being pursued is generally acknowledged to be one of the most important questions in particle physics and astrophysics and is key to understanding what makes up the vast majority of the universe.  Effective training in good computing practices is required for major research advances in this field.  The project will consolidate and strengthen training efforts in scientific software development and data analysis within the field of experimental dark matter research.  Scientifically, the training will enable discovery that will come from a world-wide effort consisting of hundreds of junior scientists  searching for  extremely-rare events on petabytes of data - effectively looking for a needle in a haystack the size of Texas.  The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure, and will support STEM disciplines with critical software training that is much needed both in scientific fields and in industry.<br/><br/>The dark matter community consists of more than a thousand scientists at the frontier of ultra-rare event searches whose efforts support more than twenty different experiments.  Searching for dark matter in multiple ways has resulted in disparate and often inadequate computational training. This project addresses the training problem to maximize impact across the field.  Representing three leading dark matter experiments, the project investigators will develop educational material and training workshops for systematic data science education to ensure early career scientists can harness the data volumes being produced by modern experiments.  The project will host two training workshops per year, toward the goal of developing a community of instructors and also a set of training materials for free distribution and reuse. Beyond domain-specific training in rare-event searches, foundational computational knowledge will be developed when necessary by working with partners such as the Software and Data Carpentries.  The project includes specific goals to engage women and underrepresented minorities in the training activities and broaden their advancement within the field.  Additionally, the project will provide mentors for advanced students through hackathons. These trainings will directly contribute to broader STEM workforce development while training students such that they can pursue careers in data science and/or data-intensive research.  This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835613","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Arthi Jayaraman","DE","University of Delaware","Standard Grant","Bogdan Mihaila","09/30/2022","$235,702.00","","arthij@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005123","Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning","OAC","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","10/01/2020","10/15/2020","Pedram Hassanzadeh","TX","William Marsh Rice University","Standard Grant","Alan Sussman","09/30/2025","$1,144,020.00","","ph25@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","5740, 8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid.  They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify.  Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices.  These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.<br/><br/>The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004572","Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning","OAC","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","10/01/2020","10/15/2020","Edwin Gerber","NY","New York University","Standard Grant","Alan Sussman","09/30/2025","$1,194,936.00","Lakshminarayan Subramanian","gerber@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","5740, 8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid.  They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify.  Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices.  These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.<br/><br/>The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919752","MRI: Acquisition of Heterogeneous Computer System for Machine Learning","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2019","09/11/2019","Steven Skiena","NY","SUNY at Stony Brook","Standard Grant","Alejandro Suarez","09/30/2022","$571,975.00","Arie Kaufman, Dimitrios Samaras, Michael Ferdman, Zhenhua Liu","skiena@cs.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","1189, 7231","075Z, 1189","$0.00","This award supports the purchase, deployment, and operation of a heterogeneous computer system to advance the research interests of many faculty in the Stony Brook University's Artificial Intelligence research community. The shared research instrument will enable discovery in a multitude of areas. For example: in Geosciences, the cluster will be used to process satellite imagery to map remote parts of the Arctic and Antarctic; in Biosciences it will be employed to build neural networks that will advance cardiac imagery and fMRI research; in Social Sciences it will help produce methods and tools that can illuminate social and psychological determinants of population health. The provisioning of the instrument in the Stony Brook University campus will greatly benefit hundreds of graduate and undergraduate students taking courses in machine learning and artificial intelligence each year. The proposed cluster will offer professionally-managed resources providing a standardized environment with adequate technical support. Further, student demand is anticipated to continue to grow through several ongoing initiatives including an undergraduate CS concentration in artificial intelligence, with new courses in NLP and data science, a recently approved graduate CS concentration in Data Science, and a proposed ""Data Science+X"" undergraduate sequence integrating with several majors in the College of Arts and Sciences (CAS).<br/><br/>The cluster will comprise of three categories of nodes: (1) a large Graphics Processing Unit (GPU) machine, (2) GPU server machines with large amounts of memory and (3) Field Programmable Gate Arrays (FPGA) machines thus providing a forward-looking computing environment focused on supporting research on and enabled by AI. It will constitute the research instrumentation catalyst for the Institute for AI-Driven Discovery and Innovation (AI Institute) and significantly overhaul the compute capability of the institution. This system?s heterogeneity will allow for a more versatile shared environment with excellent performance. will support research in three directions: 1) Artificial Intelligence (AI) including Natural Language Processing, Computer Vision, and Machine Learning (ML) 2) applications of AI and ML on a variety of scientific disciplines including Biomedical Informatics, Chemistry, Ecology, and Linguistics, 3) Computer Science (CS) research on HPC such as visualization, neural network compilation, and process scheduling. Furthermore, the strong educational and outreach activities that engage high school students and teachers, as well as undergraduate and graduate students have the potential to support the building of a skilled workforce of developers, researchers, staff and users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003808","Elements: DeepPDB: An open-source automated framework to enable high-fidelity atomistic simulations in unexplored material space","OAC","DMR SHORT TERM SUPPORT, Software Institutes","11/01/2020","04/17/2020","Wissam Saidi","PA","University of Pittsburgh","Standard Grant","Seung-Jong Park","10/31/2023","$600,000.00","","alsaidi@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","1712, 8004","054Z, 075Z, 077Z, 079Z, 7923, 8004, 9216","$0.00","A key requirement to address the world?s energy challenges is the development of new energy-efficient and smart materials. To aid in this process, state-of-the-art and revolutionary computational tools can be used to simulate new materials, thus providing a thorough understanding of their characteristics and behavior. Such materials simulation efforts can drastically reduce the time-to-market of new materials from decades to months. Traditional simulation approaches can accurately predict the behavioral properties of materials both at the smallest possible scale (atomic) and at the macroscopic level, i.e. millimeter or larger. Many critical materials properties are defined and needed at scales ranging from a few nanometers to micrometers, yet simulations are lacking at these levels. While algorithms do exist to simulate properties at these scales, often we lack the fundamental parameters, termed force-fields, for novel materials such as those for next-generation solar cells, batteries and jet turbine alloys. These force-fields are laborious to determine using traditional methods, requiring significant expertise and thus restricted by the human-in-the-loop. The primary goal of the proposed Deep Potential DataBase (DeepPDB) will be to offer an open-source toolkit with the ability to automatically generate estimates of force-fields parameters using advanced empirical-based computational tools.  We will also curate and disseminate a validated repository of first-principles datasets and their corresponding potentials for inorganic materials. DeepPDB will serve both the materials science and machine learning communities, by providing the former with critical parameters to solve materials challenges and the latter by benchmark datasets for machine learning development. The resulting synergy will enable artificial intelligence and machine learning to play a greater role in computing critical materials properties for next-generation challenges.  DeepPDB will also serve a critical educational objective, allowing the budding of a new generation of materials scientists, who understand how deep learning can be used to solve materials science challenges.                                                                                                                             <br/><br/>DeepPDB aims to build a database of deep neural network potentials (DNP) for the simulation of inorganic materials. In the process DeepPDB will: (1) develop automated workflows that given a target composition, will run the necessary density functional theory (DFT) calculations, train DNPs, validate against metrics imposed by the training data, identify the input-data space with the largest uncertainty and iterate until an optimal DNP is trained; (2) openly disseminate the training DFT data along with the pre-trained DNPs; (3) develop transparent automated validation that encompasses both traditional DNP based methods as well as fully integrated tests that include target metrics. To accomplish this, DeepPDB will build a toolkit based on careful software engineering practices: a combination of feature- and sprint-based development cycles; constant continuous integration using unit-tests and integration tests as milestones; and a database-oriented approach to data and workflow management. The resulting open-source toolkit will serve as a foundational tool to investigate the properties of hitherto-unseen materials at length- and time-scales previously not possible.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931425","Elements: Can Empirical SE be Adapted to Computational Science?","OAC","Software Institutes","10/01/2019","09/05/2019","Timothy Menzies","NC","North Carolina State University","Standard Grant","Robert Beverly","09/30/2022","$592,129.00","","timm@ieee.org","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","8004","026Z, 077Z, 7923","$0.00","Today the computer is just as important a tool for chemists as the test tube.   For example, the 2013 Nobel Prize was awarded to chemists using computer models to explore very fast chemical reactions during photosynthesis. Other scientific areas where software is used intensively are astronomy, astrophysics, chemistry, weather prediction, economics, genomics, molecular biology, oceanography, physics, political science, and many other engineering fields. It is important to ensure the quality of these software-driven fields since its results    accelerate global innovations by improving quality and quantity of computational scientific studies. But many software developers in this area have not formally studied computer science or software engineering. This proposal will create SEnTRY, a workbench containing methods adapted from empirical software engineering, that would help bridge the skill gap via automatic agents by suggesting to developers when they should investigate or redo part of their code. <br/><br/><br/>Software is used intensively in scientific areas such as astronomy, astrophysics, chemistry, weather prediction, economics, genomics, molecular biology, oceanography, physics, political science, and many other engineering fields. It is important to ensure the quality of these software-driven fields since its results  accelerate global innovations by improving quality and quantity of computational scientific studies. But many software developers in this area have not formally studied computer science or software engineering. This proposal will create SEnTRY, a workbench containing methods adapted from empirical software engineering, that would help bridge the skill gap via automatic agents by suggesting to developers when they should investigate or redo part of their code. To achieve these goals, methods developed for traditional kinds of software must be extensively adapted for computational science. For example,  language models describing software defects must be created, especially for the  computational science community; test case prioritization algorithms must be re-tuned to appropriately prioritize   ""tests"" that are  really ""tests of scientific concepts""; and  static code analysis warnings have to be re-engineered to manage the  kinds of software tools used within the computational science community. To that end, this project will apply data miners, hyperparameter optimizers and active learning to project data from the computational science community. When successful, SEnTRY will reduce the associated cost (time, money, etc.) required to handle many of the large and more tedious aspects of software development. This will free up more time of the computational scientists, and allow them to focus on core scientific issues. As an additional benefit, SEnTRY will also ensure the reproducibility and credibility of the computational science researches which, in turn, will naturally encourage more adoption of current work as well as adaptation and innovation in future work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760950","Spokes: MEDIUM: WEST: Breaking down barriers for reproducible neuroimaging data analyses","OAC","BD Spokes -Big Data Regional I, Cognitive Neuroscience, Robust Intelligence","09/15/2018","06/23/2020","Russell Poldrack","CA","Stanford University","Standard Grant","Kenneth Whang","08/31/2022","$599,904.00","Krzysztof Gorgolewski","poldrack@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","024Y, 1699, 7495","028Z, 040Z, 8083, 8089, 8091","$0.00","This project will enable the reproducible analysis and sharing of large human brain imaging datasets.  Imaging the brain using magnetic resonance imaging (MRI) is an essential tool for the study of the human brain, but the processing of large neuroimaging datasets is often a difficult and time-consuming process. This project will improve the ability of researchers to analyze these data more effectively and share the results.  This work will build upon the OpenNeuro project, which provides researchers with the ability to easily upload and share neuroimaging data. The first aim of this project is to develop the ability to process these datasets using national supercomputing resources. Because these resources are both much more powerful and more cost-effective than commercial cloud computing resources, this extension will allow researchers to process larger datasets using more sophisticated analysis procedures.  The second aim is to extend the reach of data storage and processing beyond standard neuroimaging datasets to include heterogeneous datasets with clinical and psychological data in addition to brain imaging data. A large study of human brain development will be used as a test case for this extension. The third aim of this project is to engage researchers and software developers in order to develop a broad community that will further extend the reach and capabilities of the proposed technical developments and promote the sustainability of the resources beyond the grant period.<br/><br/>The first aim will implement the ability to execute computational workflows using national supercomputing resources through a Science-As-A-Service model, via technologies such as the Agave API and the Singularity container system.  This will provide the ability for researchers to execute complex containerized workflows using large datasets, providing a long-term sustainable computational platform for analysis of open data. The second aim will involve the extension of the current OpenNeuro platform to represent data from a large longitudinal neurodevelopmental study, in order to allow the joint analysis of imaging, clinical, psychological, and genetic data.  The third aim will engage the relevant user and developer communities through workshops and code sprints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115107","CICI: UCSS: Towards Secure and Usable Push Notification Authentication for Collaborative Scientific Infrastructures","OAC","Cybersecurity Innovation","08/15/2021","05/11/2021","Nitesh Saxena","AL","University of Alabama at Birmingham","Standard Grant","Robert Beverly","09/30/2021","$499,934.00","","nsaxena@tamu.edu","1720 2nd Avenue South","Birmingham","AL","352940111","2059345266","CSE","8027","7923, 8027, 9150","$0.00","Second factor (2FA) or passwordless authentication based on notifications pushed to a user's personal device (e.g., a phone) that the user can simply approve (or deny) has become widely popular due to its convenience, especially to protect scientific resources at Universities and similar organizations. This project is studying the premise that the effortlessness of this approach gives rise to a fundamental design vulnerability arising from concurrent login sessions (one initiated by the user and the other initiated by the attacker), and then redesigning push-based authentication systems that can counter the identified vulnerability without degrading the overall usability of the approach. The proposed new design attempts to address the concurrent login attacks by establishing a unique binding between the user?s browser session and the push notification.<br/><br/>The research consists of three inter-related activities: (1) formalization and study of a fundamental vulnerability against standard push notification authentication schemes; (2) design and implementation of low-effort push-based authentication schemes that can defeat the identified vulnerability without undermining the usability; and (3) formal studies of the proposed new push-based authentication schemes, conducted in lab settings and field environments. The developed resilient push authentication system designs are expected to offer an improved level of protection, accessibility and usability to everyday users in scientific and collaborative settings. The research prototypes are expected to be of broader value in future research on building resilient and usable authentication services in practice. The project is emphasizing technology transfer by working with major players in the push-based authentication domain. The proposed research is being integrated with educational activities in the form of advanced curriculum development and student mentoring in the broad domains of Authentication and Human-Computer Interaction, and the involvement of high school and K-12 students and minority populations are broadening the reach of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808553","CDS&E: Collaborative Research: A Computational Framework for Reconstructing and Visualizing Myocardial Active Stresses","OAC","CDS&E-MSS, CDS&E","10/01/2018","07/02/2021","Suzanne Shontz","KS","University of Kansas Center for Research Inc","Standard Grant","Tevfik Kosar","09/30/2022","$343,568.00","","shontz@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457552","7858643441","CSE","8069, 8084","026Z, 028E, 8084, 9102, 9150, 9251, 9263","$0.00","The normal heart functions by contracting and pushing the blood from the left ventricle into the rest of the body. Due to various diseases, the contraction capabilities of the heart become diminished in certain regions of the heart chamber wall, compromising the overall function of the heart. In order to identify and select optimal treatment, it is critical to identify the regions of the heart wall that exhibit reduced contractions. Unfortunately, contractions cannot be easily measured. This project will estimate the stress (contraction power) developed within the heart muscle by combining medical imaging and mechanical modeling of the heart. These stresses will serve as a quantitative measure of the contractile function of the heart and help detect and localize disease. Therefore, this research has the potential to evolve into a future tool to diagnose cardiac function. This project will also feature a synergistically integrated education and outreach program. We will foster research opportunities for graduate and undergraduate students in computer science, biomedical engineering, mathematics, and imaging science at Rochester Institute of Technology and the University of Kansas. The PIs will develop innovative hands-on workshops to inspire and educate K-12 students from underrepresented groups on biomedical computing and medicine.<br/> <br/>This project proposes to develop a method that enables non-invasive appraisal and visualization of the active stresses developed in the myocardium to serve as a direct means to assess the bio-mechanical function of the heart. The PIs will develop open-source cyberinfrastructure and integrate it into a novel computational framework for cardiac biomechanics that will reconstruct the active stresses from cardiac deformations. The PIs will accomplish this goal by developing and integrating techniques for medical image computing, high-order meshing, and inverse-problem bio-mechanical modeling. This research will address a currently unexplored niche in the cardiac modeling field, specifically the reconstruction and visualization of myocardial active stresses, to enable direct appraisal of cardiac function. This research will contribute to medical image computing through the development of algorithms for medical image processing and visualization. The PIs will develop and implement novel high-order meshing techniques and integrate them with the fiber architecture to enable accurate and efficient scientific modeling and computing. The project will contribute new knowledge in mathematical modeling and simulation by implementing efficient nonlinear least-squares solutions for inverse cardiac biomechanics. Lastly, the PIs will release the resulting cyberinfrastructure to the scientific computing community for research and education use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939992","Collaborative Research: MEMONET: Understanding memory in neuronal networks through a brain-inspired spin-based artificial intelligence","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc, Info Integration & Informatics","10/01/2019","10/15/2020","Ying Zhang","RI","University of Rhode Island","Continuing Grant","Sylvia Spengler","09/30/2022","$399,550.00","","yingzhang@uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","099Y, 1640, 7364","062Z, 9102, 9150","$0.00","The brain is arguably the most sophisticated and the most efficient computational machine in the universe. The human brain, for example, comprises about 100 billion neurons that form an interconnected circuit with well over 100 trillion connections. Understanding how a multitude of brain functions emerge from the underlying neuronal circuit will give insights into the operating principles of the brain. In this award, a multidisciplinary team of systems biologist, computational biologist, material scientist, neuroscientist, and machine learning expert will work synergistically to leverage the data revolution in neuroscience to answer a fundamental question: How does the brain learn, store, and process information?  The team will develop and apply advanced data analysis algorithms to harness the great volume of neuronal data generated by the latest imaging and molecular profiling technologies, for elucidating the neuronal circuits driving brain functions. Computer simulations of a spin-electronic (spintronic) device will further serve as a platform to validate and emulate important operational characteristics of such neuronal circuits. The award sets the groundwork for an interdisciplinary data science research and educational program that will bring a new and powerful paradigm for studying brain functions as well as for designing transformative brain-inspired devices for information processing, data storage, computing, and decision making.<br/><br/>The project has a specific focus on an essential function of the brain: motor-skill learning. This function emerges from the underlying circuitry of neurons that governs the activities of molecular signal transmission and neuronal firing. Importantly, the neuronal circuit in a mammalian brain is highly plastic and dynamic, features that endow animals with the ability to respond to myriad external stimulations through learning. By harnessing the latest data revolution in neuronal imaging, single neuron molecular profiling, spintronic device simulation, network inference, and machine learning, a team of multidisciplinary investigators will be supported by this award to investigate the fundamental principle of neuronal circuit rewiring that drives brain?s learning function. More specifically, the team sets out to achieve the following specific tasks: (A) Infer learning-induced rewiring of large-scale neuronal networks from two-photon calcium imaging data through the development of novel and powerful network inference algorithms; (B) Build biochemical-based models of neuronal circuits by integrating molecular profiling with neuron firing and connectome dynamics; and (C) Develop a spintronic material network model that emulates learning and memory formation by exploiting the spin dynamics in spintronic materials. The project seeks to lay the foundation for the creation of an interdisciplinary data-intensive brain-to-materials initiative that will be applied to understand and emulate the operational principles of brain neuronal circuits underlying learning, cognition, memory formation, and other behaviors. The outcomes of the initiative will have a paramount impact on the society, not only in our understanding of the brain and its functions, but also in overcoming current bottlenecks of existing computing architectures.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923980","Collaborative Research: CyberTraining: Pilot: Semi-Automatic Assessment of Parallel Programs in Training of Students and Faculty","OAC","CyberTraining - Training-based, IUSE","09/01/2019","07/10/2019","Sheikh Ghafoor","TN","Tennessee Technological University","Standard Grant","Bogdan Mihaila","08/31/2022","$82,000.00","Ada Haynes","sghafoor@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","044Y, 1998","8209, 8244, 9150, 9178","$0.00","Modern computers allow a computer program to be decomposed into multiple activities or threads that can execute concurrently.  Emerging big-data scientific, health, social, and engineering applications require such concurrency to give results in a timely manner. The key to matching the computation needs of these applications to the available computing resources is training of a workforce to develop, maintain, and configure concurrent programs. Ongoing work on developing toolkits for teaching concurrency is challenging because instruction is particularly labor-intensive, and thus, these toolkits, on their own, cannot help instructors meet the demands for such instruction. Specifically, concurrent programs are notoriously difficult to write, and substantial instructor effort is required to evaluate the performance and correctness of these programs, and identify potential problems. This project will extend an existing instructional toolkit with a new software framework to automate assessment of concurrent programs, and using instructional workshops and university courses to validate the extended toolkit. Successful execution of the project will improve the workforce development and promote the progress of science.<br/><br/>The main research question we are exploring is: What should be the nature of a rule-based software framework for assessing concurrent programs written in multiple programming languages that improves the productivity and learning, respectively, of trainers and trainees? The key novel steps we are taking to explore this question are (a) development of a semi-automatic assessment model in which manual evaluation, integrated with automatic rules, reduces false positives and negatives of the automated checks; (b) identification of  new protocols and associated architectures that leverage the capabilities of several existing powerful tools that have not been used before to address our question, (c) creation of new techniques based on the insight that solutions to a concurrent programming assignment often have a prescribed code-structure and algorithm, (d) support for layered techniques that allow rule-writers to tradeoff assessment quality for low rule-writing effort, (e) development of a meta-assessment framework to train the trainers to write rules, (f) use of the meta-assessment and assessment framework in instructional workshops and university course offerings, respectively, and (g) evaluation of the usability, programmability, effectiveness and learning gain of the frameworks through diverse mechanisms including pre-post surveys, course exit interviews, and focus groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841456","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Mark Neubauer","IL","University of Illinois at Urbana-Champaign","Standard Grant","Bogdan Mihaila","09/30/2022","$499,872.00","Daniel Katz","msn@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923621","Collaborative Research: CyberTraining: Implementation: Small: Integrating core CI literacy and skills into university curricula via simulation-driven activities","OAC","CyberTraining - Training-based","10/01/2019","06/25/2019","Henri Casanova","HI","University of Hawaii","Standard Grant","Alan Sussman","09/30/2022","$261,293.00","","henric@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","044Y","026Z, 9150","$0.00","Scientific and societal progress in the 21st century relies on a large, heterogeneous, and evolving ecosystem of Parallel and Distributed Computing (PDC) technologies. And yet, most college students graduating today from computing curricula have little exposure to PDC concepts and practices.  There is thus an imminent risk that the emerging scientific workforce will be ill-prepared for using and developing those computing infrastructures that are key to progress.  Teaching PDC early and effectively in university curricula is notoriously difficult, in part due to the need to provide students with access to and meaningful hands-on learning opportunities on actual PDC platforms.  This project addresses this challenge directly by relying on simulation technology: it provides students with hands-on learning opportunities that do not require access to any PDC platforms. This makes it possible to teach the full gamut of PDC conceptual and practical topics effectively and at any higher education institution in the nation.  The pedagogic activities being developed in this project can be integrated into existing university courses and also provide a sound basis for developing new courses, starting at freshman levels.  By supporting education in a view to modernizing the scientific workforce, this project promotes the progress of science, as stated by NSF's mission.<br/><br/>Years of Cyberinfrastructure research and development have resulted in a rich set of abstractions and interoperable software implementations that can leverage a wide range of hardware platforms. It is crucial to provide students with hands-on pedagogic activities through which they can acquire the PDC conceptual and practical knowledge necessary for them to join a workforce that develops and uses this Cyberinfrastructure.  Requiring that these activities be conducted on actual hardware and software stacks limits participation because only few institutions have access to secure representative, stable, and possibly large deployments that can be used for educational purposes. The main insight behind this work is that simulation promotes both participation and pedagogy because it allows students to experience arbitrary Cyberinfrastructure scenarios, only requiring that they have access to a standard laptop computer.  This is feasible due to the recent development of simulation frameworks for easily developing simulators of complex distributed systems that afford simulations that are both pedagogically accurate and scalable.  Given this insight and this recent development, this project develops simulation-driven interactive pedagogic activities for a spectrum of Student Learning Objectives (SLOs), ranging from standard PDC SLOs as well as SLOs relevant to current and emerging Cyberinfrastructure practices.  The activities are organized in modules with a prerequisite structure, and come with guidelines for integration into existing university courses, starting at freshman levels.  Several pedagogic strategies are employed through which students execute interactive simulations with configurable levels of details along various narrative paths.  Research questions include determining which strategies, with which levels of simulation details, work best for which SLOs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841531","Virtual Data Set Services Enabling New Science at NSF Facilities","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/15/2018","Ian Foster","IL","University of Chicago","Standard Grant","Bogdan Mihaila","09/30/2021","$1,500,000.00","Steven Tuecke","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7684","020Z, 062Z","$0.00","Scientific facilities supported by the National Science Foundation such as the Daniel K. Inouye Solar Telescope (DKIST), National Center for Atmospheric Research (NCAR), and National Ecological Observatory Network (NEON) collect enormous quantities of valuable data about the world in which we live. These data can be used for scientific and societal benefit: to make breakthrough discoveries about sun's behavior and magnetic field, and our changing environment; to improve the speed and accuracy of forecasting of severe storms and destructive wildfires; to predict disruptions to electrical systems from solar flares; and many other purposes. Before such vital scientific data can be used effectively, they must be delivered rapidly, efficiently, and reliably to the people who need them. Researchers need interactive community access to hard-to-obtain data located in large data archives. Because the NCAR, DKIST, and NEON data archives cannot feasibly provide the computing resources needed for all analyses, end-user scientists need to be able to define, navigate, download, and analyze data subsets. Current web-based tools are not up to these tasks. The Virtual Data Set Services Enabling New Science at NSF Facilities project will tackle this challenge by developing new methods for organizing, packaging, and rapidly transporting data. <br/><br/>A key innovation will be the development of methods for defining, sharing, and manipulating ""virtual data sets,"" data collections extracted ""on the fly"" from the vast holdings of scientific facilities for a specific purpose. A researcher may define a virtual data set much as a shopper assembles products in an online ""shopping cart."" Once defined, a virtual data set can then be transferred to a remote computer for analysis, shared with colleagues, or extended for future projects. We will develop new services to (a) enable definition of, navigation over, and selective access to virtual data sets from petascale data archives of the scientific facilities, and (b) ensure reliable, automated, efficient, and secure replication and access of entire data sets or data subsets between a petascale data archive and other locations, to include both end user computers and remote mirrors intended to accelerate data access by community members. These new services will be constructed on top of the Globus platform, already heavily used within NCAR's Research Data Archive (RDA) and many other research data centers. The ultimate aim is to integrate the new services into operational systems in collaboration with DKIST, NEON, and NCAR/RDA. The results will be evaluated in the context of demanding science applications in partnership with solar physics, atmospheric science, and ecology researchers.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841487","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Robert Gardner","IL","University of Chicago","Standard Grant","Bogdan Mihaila","09/30/2021","$400,000.00","","rwg@hep.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761735","Spokes: MEDIUM: SOUTH: Collaborative: Integrating Biological Big Data Research into Student Training and Education","OAC","BD Spokes -Big Data Regional I","10/01/2018","09/14/2018","Fan Wu","AL","Tuskegee University","Standard Grant","Earnestine Psalmonds","09/30/2022","$149,996.00","Guohao He, Channapatna Prakash, Jay Bhuyan","fwu@tuskegee.edu","1200 W Montgomery Road","Tuskegee Institute","AL","360881923","3347278970","CSE","024Y","8083, 9102","$0.00","The project is a collaborative effort among the University of Tennessee Chattanooga, Tuskegee University, Spelman College, and West Virginia University to integrate and automate biological big data into student training and education. Leveraging the team's expertise in computer science and ecology, the project will offer training workshops on using network models to integrate heterogeneous genomic big data and heterogeneous ecological big data to address life sciences questions. The team will engage faculty and students in developing a protocol to automate field data collection. The team also will prototype automated methods to enhance plant digitization, leveraging the collection of digitized plant images and meta-information at the Southeast Regional Network of Expertise and Collections, as well as the ecological datasets in collaboration with the Encyclopedia of Life.<br/><br/>The project objectives are to (1) enhance faculty expertise in big biological data through summer workshops; (2) catalyze interdisciplinary collaboration on big biological data research and education through hackathons, working groups, and community-building via a Video Education Faculty Network; and (3) develop hands-on, constructively peer-evaluated learning modules incorporating high-quality video tutorials. The proposed activities will address challenges surrounding the integration and automation of big biological data into education and training at predominantly undergraduate institutions and Historically Black Colleges and Universities. The project will help bridge the gaps between big biological data and the fields of systems biology, ecology and evolution, and environmental sciences. Overall, the project will catalyze collaborations among diverse institutions and disciplines while increasing diversity in big data. <br/><br/>This award is co-funded by the Improving Undergraduate STEM Education: Education and Human Resources (IUSE): EHR Program (NSF 17-590). IUSE supports projects that are designed to improve student learning through development of new curricular materials and methods of instruction and development of new assessment tools to measure student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747552","Collaborative Research: Building the Community for the Open Storage Network","OAC","CYBERINFRASTRUCTURE, EarthCube","06/15/2018","08/10/2020","James Glasgow","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alejandro Suarez","05/31/2021","$472,033.00","Michelle Butler, Kenton McHenry, James Glasgow","glasgow@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7231, 8074","062Z","$0.00","The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?<br/><br/>Many universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1747493","Collaborative Research: Building the Community for the Open Storage Network","OAC","BD Spokes -Big Data Regional I, Data Cyberinfrastructure","06/15/2018","06/07/2018","Alexander Szalay","MD","Johns Hopkins University","Standard Grant","Alejandro Suarez","05/31/2021","$165,185.00","","aszalay1@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","024Y, 7726","062Z","$0.00","The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales--from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?<br/><br/>Many universities have several of the necessary pieces for Data Intensive Science in place--reasonably sized computing clusters, a few PB of storage and even a high-speed connection--yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals, could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API and build a variety of delivery services for the educational community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1943002","EAGER: Citizenly: Empowering Communities by Democratizing Urban Data Science","OAC","CYBERINFRASTRUCTURE","10/01/2019","09/23/2019","Naveen Sharma","NY","Rochester Institute of Tech","Standard Grant","Seung-Jong Park","09/30/2022","$298,734.00","Pradeep Murukannaiah, M. Ann Howard","nxsvse@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7231","7916","$0.00","Governments around the nation have been using data-driven approaches to drive improvements in city operations and citizens' quality of life. Large cities such as New York, Chicago, and Boston have reported impressive results from the use of data in addressing urban issues. Smaller cities such as Rochester, NY have mostly lagged behind. Data has not played a large role in addressing problems of urban neighborhoods. Midsize cities are large enough to have data collection systems in place. But they cannot afford a team of data scientists to make use of them. The open data sets, provided by data.gov and many other organizations, contain raw data. This data requires specialized skills to access, make sense of, and to compute with. Most established open data sets are most suitable for conducting large data analysis. They require significant efforts in filtering data down to specific scenario of local interests. Citizens and community leaders are key consumers of open data. But they are not able to access, decipher, and use this data in meaningful ways. While many tools and algorithms for conducting data science exist, most of these technologies cannot simply be used by anyone. Democratizing data science is the notion that anyone, with little to no technical expertise, can do data science. But they must be provided the right data and user-friendly tools. This project will put urban data science into the hands of citizens and community leaders. It will integrate citizens into the development of urban policy and solutions for local issues. Community leaders would like to see data filtered down to their community levels. The resulting analysis should be made relevant to citizens. A major impact of the work is that it can significantly lower the barrier to entry for community leaders and citizens to meaningfully leverage urban data. Of particular significance is the engagement of neighborhood youths. They will become neighborhood innovators, designing technology applications to support neighborhood-based self-sufficiency strategies. Moreover, the project will develop infrastructure that can be replicated for other/similar midsize cities as well pave the way to democratize data science in other domains.<br/><br/>This project will develop the underlying scientific and engineering foundations necessary to create the Citizenly cyberinfrastructure to democratize data science amongst citizens and city governments. This project is multidisciplinary and it will bring together a team of investigators, with expertise in big data analytics, program synthesis, active learning, and social sciences. Key partners, all in the city of Rochester, NY, include the Office of Innovation and Strategic Initiatives and the Office of Research and Analytics at the Common Ground Health. The project will address the following key scientific and technical challenges: (1) Community-focused Data Infrastructure: Design and implementation of a lightweight citizen sensor. Design anexpressive and efficient model for community dataset and implement a scalable Extract-Transform-Load (ETL) system to automate its creation. (2) Citizen-centric Programing: Design domain specific language for citizens and community leaders to access community datasets and to express the intent and constraints for desired urban application. (3) Urban Data Science Applications: Using the Citizenly approach, develop: (a) predictive models for the influence of presence of vacant lots on city's property values; and (b) assessment models for health impact from socio-economic factors. Citizenly will integrate community data and common services along with necessary systems and algorithm innovations to provide next generation cyberinfrastructure for city leaders and citizens. Citizenly will enable research efforts broadly in urban data science and community-based participatory research by providing Cyberinfrastructure (CI) services to a diverse scientific community or communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931339","Elements: Community portal for high-precision atomic physics data and computation","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","10/01/2019","08/19/2019","Marianna Safronova","DE","University of Delaware","Standard Grant","Seung-Jong Park","09/30/2022","$559,999.00","Rudolf Eigenmann","msafrono@physics.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1253, 7244, 8004","026Z, 077Z, 7569, 7923, 8004, 8005, 9150","$0.00","Many engineering products and science projects depend on knowledge of exact characteristics of the atoms that make up the used materials. Today's Global Positioning Systems (GPS) are an example of such engineering products. This project will develop an easy-to-use web portal that provides the atomic physics and other communities with much-needed precision information about atomic properties. Research in quantum information, degenerate quantum gases, atomic clocks, precision measurements of atoms and molecules, plasma physics, astrophysics, and studies of fundamental physics all rely on precise knowledge of atomic properties. Experimental measurements are impossible or infeasible in many cases. Releasing all codes to the public and creating a portal with easy access to high-precision computations, now limited to only group of experts, will enable and accelerate a broad range of physics research and engineering projects.<br/><br/>The portal is powered by a database of high-precision information, including matrix elements and polarizabilities of frequently used atoms. The database is coupled with a package of computational applications capable of computing on-demand a wider range of properties and for atoms not yet in the database. The code package has already demonstrated these capabilities, but will be developed into community codes that run efficiently on available high-performance computing systems. For atoms with many valence electrons, computing precise properties is not yet feasible, as the computational complexity is exponential. This project will develop new methods and algorithms for computing these properties, enabling precision atomic research to be done with atoms not possible thus far. Finally, the development of the portal and parallel applications addresses important computer science challenges, such as the efficient coupling of databases with complex computations that automatically ingest new data. Thus, this project will also create new knowledge in the key issue of combining data and simulation.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934634","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Arindam Banerjee","MN","University of Minnesota-Twin Cities","Continuing Grant","Amy Walton","05/31/2021","$385,238.00","","arindamb@illinois.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","099Y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923539","Collaborative Research: CyberTraining: Implementation: Small: Integrating core CI literacy and skills into university curricula via simulation-driven activities","OAC","CyberTraining - Training-based","10/01/2019","09/14/2021","Loic Pottier","CA","University of Southern California","Standard Grant","Alan Sussman","09/30/2022","$238,707.00","","lpottier@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","044Y","026Z, 9150","$0.00","Scientific and societal progress in the 21st century relies on a large, heterogeneous, and evolving ecosystem of Parallel and Distributed Computing (PDC) technologies. And yet, most college students graduating today from computing curricula have little exposure to PDC concepts and practices.  There is thus an imminent risk that the emerging scientific workforce will be ill-prepared for using and developing those computing infrastructures that are key to progress.  Teaching PDC early and effectively in university curricula is notoriously difficult, in part due to the need to provide students with access to and meaningful hands-on learning opportunities on actual PDC platforms.  This project addresses this challenge directly by relying on simulation technology: it provides students with hands-on learning opportunities that do not require access to any PDC platforms. This makes it possible to teach the full gamut of PDC conceptual and practical topics effectively and at any higher education institution in the nation.  The pedagogic activities being developed in this project can be integrated into existing university courses and also provide a sound basis for developing new courses, starting at freshman levels.  By supporting education in a view to modernizing the scientific workforce, this project promotes the progress of science, as stated by NSF's mission.<br/><br/>Years of Cyberinfrastructure research and development have resulted in a rich set of abstractions and interoperable software implementations that can leverage a wide range of hardware platforms. It is crucial to provide students with hands-on pedagogic activities through which they can acquire the PDC conceptual and practical knowledge necessary for them to join a workforce that develops and uses this Cyberinfrastructure.  Requiring that these activities be conducted on actual hardware and software stacks limits participation because only few institutions have access to secure representative, stable, and possibly large deployments that can be used for educational purposes. The main insight behind this work is that simulation promotes both participation and pedagogy because it allows students to experience arbitrary Cyberinfrastructure scenarios, only requiring that they have access to a standard laptop computer.  This is feasible due to the recent development of simulation frameworks for easily developing simulators of complex distributed systems that afford simulations that are both pedagogically accurate and scalable.  Given this insight and this recent development, this project develops simulation-driven interactive pedagogic activities for a spectrum of Student Learning Objectives (SLOs), ranging from standard PDC SLOs as well as SLOs relevant to current and emerging Cyberinfrastructure practices.  The activities are organized in modules with a prerequisite structure, and come with guidelines for integration into existing university courses, starting at freshman levels.  Several pedagogic strategies are employed through which students execute interactive simulations with configurable levels of details along various narrative paths.  Research questions include determining which strategies, with which levels of simulation details, work best for which SLOs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118305","CyberTraining: Implementation: Medium: Collaborative Research: Computational and Data-Centric Ecology Training","OAC","CyberTraining - Training-based","01/01/2022","09/08/2021","Naupaka Zimmerman","CA","University of San Francisco","Standard Grant","Alan Sussman","12/31/2024","$79,514.00","","nzimmerman@usfca.edu","Contracts and Grants","San Francisco","CA","941171080","4154225203","CSE","044Y","7231, 7301","$0.00","While the scientific community has ever cheaper and more rapid access to large amounts of data with respect to ecology and other issues, the training necessary to take full advantage of these large data streams has not kept up. This project will create an online learning and community platform known as Data4Ecology to support the integration of computing, statistics, and data science into undergraduate ecology curriculum and courses. To do so, this proposal will build a freely accessible website enabling students and others in ecology and allied STEM disciplines to develop the skills in computational and statistical reasoning necessary to address problems related to ecology and data science. <br/><br/>This project, Data4Ecology, will develop and deploy a website to curate, assemble and embed open educational and learning resources that will serve as training for a multitude of undergraduate students in ecology and environmental science at various college and universities. In turn, this project will integrate computing, statistics, and data science into an ecology curriculum. The project will build and make accessible collaboration and community resources while providing professional development and disseminating project products. The project will serve the larger scientific community by providing researchers and educators with a dedicated repository of data-centric ecology data, resources, and curriculum. Further, the learning resources from this project will be ported to STATS4STEM, an NSF-funded project that collects educational resources on data, computing, and statistics for use by educators and students at many levels, thus exposing users beyond the project to an expanded pool of ecology-based data-centric learning resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118222","CyberTraining: Implementation: Medium: Cyberinfrastructure Training to Advance Environmental Science","OAC","CyberTraining - Training-based","10/01/2021","09/07/2021","Gwen Jacobs","HI","University of Hawaii","Standard Grant","Alan Sussman","09/30/2024","$999,326.00","Thomas Giambelluca, Jason Leigh, Helen Turner, Sean Cleveland","gwenj@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","044Y","7231, 7301, 9102, 9150, 9179","$0.00","The project will support Cyberinfrastructure (CI) training for environmental science research, education, and practice in the Hawaii-Pacific region. Environmental issues in the region affect health, economic stability, and ways of life for Indigenous and other communities. This project will create workshops and curriculum modules for undergraduate and graduate students to increase CI skills across the environmental science domain. By providing the next generation of environmental scientists and practitioners with advanced CI skills, these future professionals will have the ability to use CI to transform science in the region across academic research institutions, public agencies, and community stakeholders. <br/><br/>The project will build a multi-level capacity enhancement program for CI training and skills development that will serve CI users, contributors, and professionals in the environmental and climate science communities. The training materials in the workshops and curriculum modules will be incorporated in undergraduate and graduate courses in order to promote CI skills development and awareness. The project will increase the pool of CI professionals and facilitate knowledge transfer from existing CI professionals to the environmental and climate science communities. As well, it will advance domain-specific CI skills development to position students as CI contributors in their future professional careers. The project will deliver important impacts to the environmental and climate science communities in the Pacific Island region, as there are unique aspects to the region, as well as to others in and beyond the region.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018886","CC* Networking Infrastructure: Science DMZ for Data-enabled Science, Engineering, and Health","OAC","Campus Cyberinfrastructure","07/15/2020","06/29/2020","Douglas Jennewein","AZ","Arizona State University","Standard Grant","Kevin Thompson","06/30/2022","$494,273.00","Lalitha Sankar, James McCabe, Chris Kurtz, Barbara Munk","douglas.jennewein@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8080","","$0.00","Drawing upon its mission to enable access to discovery and scholarship in science, engineering, and health, Arizona State University is deploying an advanced research network employing the Science DMZ architecture. While advancing knowledge of deploying 21st century cyberinfrastructure in a large public research university, this project also advances how network cyberinfrastructure supports research and education in science, engineering, and health. This effort is being carried out by a partnership of campus cyberinfrastructure experts to: 1) Improve campus network connectivity to enable high speed data movement for STEM research and education activities, by enabling friction-free access to wide area networks. 2) Ensure secure and performant data movement for STEM research and education activities. 3) Increase STEM research and education productivity.<br/><br/>The project incorporates national best practices in network architecture, security, and federated authentication and identity management. Replacing existing edge network equipment and installing an optimized, tuned Data Transfer Node provides friction-free wide area network path and streamlined research data movement. A strict router access control list and Intrusion Detection System provide security within the Science DMZ, and end-to-end network performance measurement via perfSONAR guards against issues such as packet loss. Science data flows are supported by a process incorporating user engagement, iterative technical improvements, training, documentation, and follow-up. Network design and implementation are guided by an external advisory board consisting of experts from the Energy Sciences Network, Internet2, the Engagement Performance and Operations Center (EPOC), The Quilt, and Arizona's ?Sun Corridor? research network.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925681","CC* Team: Great Plains Regional CyberTeam","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2019","01/29/2021","Grant Scott","MO","University of Missouri-Columbia","Continuing Grant","Kevin Thompson","06/30/2022","$1,399,479.00","David Swanson, Daniel Andresen, James Deaton, Kevin Brandt, Douglas Jennewein, George Louthan, Carrie Brown, Derek Weitzel","GrantScott@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","7231, 8080","9150","$0.00","Advances in science and technology fields are increasingly accomplished as part of multidisciplinary and multi-institutional collaborations that require complex cyberinfrastructure. A regional CyberTeam led by the Great Plains Network will support and advance the computational and data-intensive research across the region through the development of specific cyberinfrastructure resources, workforce training, and the development of unique, mutual, and cross-institutional support methodologies and agreements. The project advances the adoption and experience of advanced computing and data resources by developing a model built upon best and emerging practices for cross training and researcher outreach, pairing an experienced mentor at one institution with a mentee at another.<br/><br/>The project objectives are to: 1) Improve campus awareness and adoption of advanced cyberinfrastructure. 2) Increase the number of campus research computing and data professionals at mentored institutions, especially for institutions with small IT staffs with many job duties. 3) Increase the capabilities of campus cyberinfrastructure resources. 4) Enable development, deployment, and operation of cyberinfrastructure to make science efficient, trusted, and reproducible. The CyberTeam is a cross-institutional team consisting of technical leaders in the region paired with new members of the workforce, graduate and undergraduate students interested in joining the cyberinfrastructure workforce, and the institutional research computing leadership for regional research universities. It provides a model for distributed support teams to support cyberinfrastructure and aid in the development of a cyberinfrastructure engineering and facilitation workforce. Generalized best practices for a regional team of CI mentors including specific mentorship plans, retrospectives, and reference materials are disseminated.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934600","Collaborative Research: Knowledge Guided Machine Learning: A Framework for Accelerating Scientific Discovery","OAC","HDR-Harnessing the Data Revolu","09/01/2019","07/02/2020","Aidong Zhang","VA","University of Virginia Main Campus","Continuing Grant","Eva Zanzerkia","08/31/2022","$342,000.00","Kevin Janes","aidong@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","099Y","062Z","$0.00","The success of machine learning (ML) in many applications where large-scale data is available has led to a growing anticipation of similar accomplishments in scientific disciplines. The use of data science is particularly promising in scientific problems involving processes that are not completely understood. However, a purely data-driven approach to modeling a physical process can be problematic. For example, it can create a complex model that is neither generalizable beyond the data on which it was trained nor physically interpretable. This problem becomes worse when there is not enough training data, which is quite common in science and engineering domains.  A machine learning model that is grounded by explainable theories stands a better chance at safeguarding against learning spurious patterns from the data that lead to non-generalizable performance. This is especially important when dealing with problems that are critical and associated with high risks (e.g., extreme weather or collapse of an ecosystem).  Hence, neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for knowledge discovery in complex scientific and engineering applications. This project is developing novel techniques to explore the continuum between knowledge-based and ML models, where both scientific knowledge and data are integrated synergistically. Such integrated methods have the potential for accelerating discovery in a range of scientific and engineering disciplines. This project will train interdisciplinary scientists who are well versed in such methods and will disseminate results of the project via peer-reviewed publications, open-source software, and a series of workshops to engage the broader scientific community.<br/><br/>This project aims to develop a framework that uses the unique capability of data science models to automatically learn patterns and models from data, without ignoring the treasure of accumulated scientific knowledge. Specifically, the project builds the foundations of knowledge-guided machine learning (KGML) by exploring several ways of bringing scientific knowledge and machine learning models together using pilot applications from four domains: aquatic ecodynamics, climate and weather, hydrology, and translational biology. These pilot applications were selected because they are at tipping points where knowledge-guided machine learning can have a transformative effect.  KGML has the potential for providing scientists and engineers with new insights into their domains of interest and will require the development of innovative new machine learning approaches and architectures that can incorporate scientific principles. Scientific knowledge, KGML methods, and software developed in this project could potentially be extended to a wide range of scientific applications where mechanistic (also known as process-based) models are used.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807622","Collaborative Research: CDS&E: ReaxFF2:  Efficient and Scalable Methods for Long-time Reactive Molecular Dynamics Simulations","OAC","DMR SHORT TERM SUPPORT, Chem Thry, Mdls & Cmptnl Mthds, CDS&E","09/01/2018","08/31/2018","Hasan Metin Aktulga","MI","Michigan State University","Standard Grant","Tevfik Kosar","08/31/2022","$250,818.00","","hma@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1712, 6881, 8084","026Z, 054Z, 7926, 8084, 9216, 9263","$0.00","This project aims to enable long-time simulations of reactive molecular systems through efficient<br/>and scalable techniques. Long-time reactive simulations are critical for several scientific problems<br/>such as catalysis, battery interfaces, biological simulations involving water, and emerging<br/>areas like surface oxidation and chemical vapor deposition (CVD) growth. However, progress on<br/>these fronts is limited because long-time simulations of large-scale systems are very difficult, if<br/>not impossible, to perform using existing methods. The Reactive Force Field (ReaxFF) method is in principle  <br/>ideally suited for this purpose. However, the short time steps required in current ReaxFF simulations <br/>and the computationally expensive force field formulation limit ReaxFF's temporal capabilities to <br/>narrow simulation time ranges. This project aims to overcome such limitations by creating ReaxFF2,<br/>which will extend time scales by one to two orders of magnitude - thus making large-scale, <br/>long-time RMD simulations accessible to a wide community. Codes developed will be made publicly <br/>available and results from this project will be highlighted on a dedicated website, and they will also be <br/>incorporated into workshops by the PIs.<br/><br/>In creating ReaxFF2, the PIs will enhance the Reax force field formulation significantly, and develop <br/>innovative algorithms and software implementations for scalable simulations. More specifically, <br/>alternative ReaxFF interactions will be formulated to eliminate sharp derivatives in energy terms <br/>and enhance ReaxFF time step lengths by at least a factor of four. To accelerate the dynamic charge <br/>distribution models needed in RMD, scalable parallel preconditioning techniques for the iterative <br/>solvers will be developed. A task parallel approach to compute interactions, hierarchical problem <br/>decomposition, vectorization of the key kernels, and use of mixed precision arithmetics constitute<br/>the main techniques that will be utilized to fully leverage the performance capabilities of large<br/>computer clusters. Finally, capabilities of accelerated RMD concepts in the proposed ReaxFF2 <br/>formulation will be evaluated and inlined trajectory analysis tools for RMD will be developed <br/>to facilitate the study of long-time RMD simulations. This project will significantly enhance the PIs' <br/>software development, community building,  and sustenance efforts for the RMD community. Codes, <br/>functional forms, and parameter sets developed will be made publicly available, enabling fast and <br/>accurate modeling of diverse reactive systems beyond the scope of this project. For community outreach, <br/>results from this project will be highlighted on a dedicated website, and they will also be incorporated <br/>into workshops by the PIs.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research <br/>and the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1908144","OAC Core: Small: Collaborative Research: Scalable Run-Time for Highly Parallel, Heterogeneous Systems","OAC","OAC-Advanced Cyberinfrast Core","07/01/2019","06/19/2019","Marc Snir","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alan Sussman","06/30/2022","$250,000.00","","snir@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","090Y","026Z, 9179","$0.00","Supercomputing has become an essential tool in many scientific fields, including advances in engineering and medicine, and contributes to national security. Progress in many areas depends on continued improvements in the performance of supercomputers and their usability. Communication between processes is a critical component of this effort and is the target of this project. This project departs from the traditional communication protocols. Rather, the project  focuses on providing middle ground solutions between hardware and software. This approach potentially  reduces communication overheads and better matches the functionality of the communication library to the capabilities of modern communication adapters and also improves the match between the requirements of modern parallel computing frameworks and applications. By improving the communication capabilities of computational platforms, this project will promote faster and more flexible communication capabilities and will improve the time to completion of scientific applications.  It, therefore, increases the scientific throughput of existing and future cyberinfrastructure platforms. The research and educational outcomes of this project are closely related, resulting in highly trained new generations of researchers and engineers leading to a more efficient and globally competent workforce. Therefore, this project aligns with the NSF's mission to promote the progress of science and to advance national prosperity and welfare through science, and serves the national interest.<br/> <br/>This project brings together a multidisciplinary team and aims at breaking away from the limitation of standards such as Message Passing Interface and pointing the way for handling the needs of future computational frameworks and high-end systems. To this end the project (1) designs and implements a communication library with new communication primitives to enable fast coordination with no serial bottleneck, to manage irregular, fine grain communication, and to provide new efficient synchronization mechanisms; (2) demonstrates the value of this library by using it to accelerate multiple task-based runtimes (Legion, PaRSEC) and communication libraries (MPI and GasNET); (3) demonstrates the value of hardware support by porting key components to a programmable NIC; and (4) delivers improvements and extensions to mainstream communication libraries to provide the new functionality. This work puts a special emphasis on emerging programming models, such as Legion or PaRSEC, and on emerging application domains, such as graph analytics. It aims at an orthogonal design where different mechanisms for associating producer buffer with consumer buffer can be composed with different mechanisms for synchronizing producer and consumer; and where mechanisms can be specialized so as to allow efficient hardware support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018841","CC* Compute:  GROWTH - Gateway for Increased Research Output at a West Texas Higher-education Campus","OAC","CYBERINFRASTRUCTURE","09/01/2020","10/20/2020","Anirban Pal","TX","West Texas A&M University","Standard Grant","Kevin Thompson","08/31/2022","$297,538.00","Emily Hunt, Vinitha Hannah Subburaj","apal@wtamu.edu","2501 4th Avenue","Canyon","TX","790160001","8066512732","CSE","7231","","$0.00","West Texas A & M University (WTAMU) is expanding its computing resources to meet the rapidly growing needs of faculty, researchers, and students across four colleges on its campus. The university?s current objective is to attain doctoral status and become a Regional Research University to solve the complex and demanding problems associated with the Texas Panhandle region. This project is aimed at tackling the leading issues affecting the region: water and energy. The computing cluster enables scientific and engineering high performance computing (HPC) and data-intensive artificial intelligence (AI). These are key paradigms that are spearheading multiple science drivers across campus in water management, wind energy harvesting and energy storage. This program broadens and fortifies educational opportunities for a regional Hispanic Serving Institution with a recently established engineering graduate program, contributing to the development of an HPC & AI literate workforce. This cluster strongly impacts research and education communities associated with WTAMU, community colleges and industries in the Panhandle region, and Texas A&M Agrilife Research station in Amarillo. In addition to providing local computing resources, this cluster contributes towards the Open Science Grid, which provides high throughput computing resources across the nation. <br/><br/>The compute cluster is equipped with 1024 compute CPU cores and 10240 GPU cores to enable massively parallel HPC workflows in computational fluid dynamics, computational materials science computational mechanics, machine learning and data visualization. It complements on campus materials characterization techniques via a computational-empirical interface and engages undergraduate and graduate students in high performance computing and big data applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018822","CC* Compute: A High Performance GPU Cluster at Syracuse University","OAC","CYBERINFRASTRUCTURE","09/01/2020","10/20/2020","Samuel Scozzafava","NY","Syracuse University","Standard Grant","Kevin Thompson","08/31/2022","$393,128.00","Duncan Brown, Eric Sedore","sjscozza@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7231","","$0.00","Syracuse University is constructing a new compute cluster using Graphics Processing Units. This production cluster will serve the needs of the Syracuse University research community and the broader scientific community through integration with the Open Science Grid. The use of Graphics Processing Units to solve problems in science and engineering has grown significantly over recent years. Campus cyberinfrastructure providers have seen increasing demand for access to Graphics Processing Units in lieu of more traditional compute architectures. Syracuse University has created a strong partnership among campus-level cyberinfrastructure experts as well as at the national level through the University's contributions to the Open Science Grid. This cluster provides a significant resource for enabling science and cyberinfrastructure development at Syracuse University. The application drivers come from a variety of domains including computational forensics, high-energy physics, development of smart vision systems, computational chemistry, biomedical engineering, soft-matter physics, and gravitational-wave physics.<br/><br/>Access to the Graphics Processing Units will be an important resource for education of undergraduate and graduate students. Given the growth in related Graphics Processing Unit computing, Syracuse University faculty are introducing Graphics Processing Unit programming to undergraduate students from a variety of backgrounds. The capabilities and availability of this cluster will allow for broader adoption within the classroom environment, as well as provide compute power to researchers. Undergraduates from across the College of Engineering and Computer Science and the College of Arts and Sciences can use these resources, giving them early preparation for new computing schemes. Access to the resources via the Open Science Grid will allow researchers from across the U.S. access to the compute resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835864","Collaborative Research: Framework: Software: NSCI: Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","EarthCube","10/01/2018","09/12/2018","Paul Constantine","CO","University of Colorado at Boulder","Standard Grant","Seung-Jong Park","09/30/2022","$350,000.00","","paul.constantine@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835909","Elements: libkrylov, a Modular Open-Source Software Library for Extremely Large Eigenvalue and Linear Problems","OAC","Software Institutes","09/01/2018","08/28/2018","Filipp Furche","CA","University of California-Irvine","Standard Grant","Bogdan Mihaila","08/31/2022","$599,852.00","","filipp.furche@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","8004","026Z, 077Z, 7923, 8004, 8005, 9216","$0.00","Strongly coupled linear equation systems or eigenvalue problems with extremely large numbers of unknowns are a critical bottleneck for computational solutions to many grand challenges in science and engineering. For example, computational design of light emitting or photovoltaic materials from first principles requires the solution of tens of millions of strongly coupled linear equations within minutes to be practical. This project aims to develop, implement, test, and deploy libkrylov, a robust, efficient, and general open-source library of ""on-the-fly"" Krylov space methods suitable for solving such extremely large, dense problems. libkrylov will deliver the latest innovations in Krylov-space methods to the scientific and engineering communities by providing a uniform, reproducible, and user-friendly software standard. Coupled with electronic structure codes, the library will enable large-scale simulations of molecular time-dependent X-ray absorption spectra of organometallic and bio-inorganic systems. This project will promote computational literacy through student training and workforce education at University of California, Irvine and San Diego State University, and enhance national software infrastructure through collaboration with the NSF-funded Molecular Sciences Software Institute (MolSSI) in Blacksburg, VA.<br/><br/>The PI and his group have recently developed nonorthonormal Krylov space methods for solving extremely large dense eigenvalue and linear problems ""on-the-fly"", i.e., without explicit storage or access of coefficient matrices, with demonstrated efficiency and stability. This project aims to transform this methodology into robust, efficient, and sustainable software infrastructure freely accessible to the public. Key features include (i) unique capability to solve extremely large problems, (ii) a highly flexible interface to matrix-vector multiplication ""engines"" (iii) ultrahigh efficiency by minimizing the number and cost of matrix-vector multiplications, (iv) outstanding robustness by dynamic control of errors and condition, and stabilization methods, (v) versatility by exploiting symmetry and special structure, real and complex arithmetic, (vi) configurable precision, convergence control, preconditioning, memory and disk usage, (vii) portability to broad range of platforms, environments, and languages, (viii) flexible and user-friendly generic interfaces, documentation, and testing capabilities, (ix) extensibility through object orientation and modularity, (x) reproducibility through a dedicated test suite, (xi) community involvement and sustainability by collaboration with MolSSI and deployment of a public issue and feature request tracker.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117681","MRI: Acquisition of Cutting-Edge GPU and MPI Nodes for the Interdisciplinary Pitt Center for Research Computing","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2021","09/21/2021","Geoffrey Hutchison","PA","University of Pittsburgh","Standard Grant","Alejandro Suarez","09/30/2024","$1,187,606.00","Lillian Chong, Inanc Senocak, David Koes","geoffh@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","1189, 1640","1189, 9102","$0.00","Computational science and engineering spans research and education across many disciplines, and state-of-the-art cyberinfrastructure resources are needed to tackle large problems and enable innovative strategies in data-enabled science and engineering. This project will greatly expand the interdisciplinary University of Pittsburgh Center for Research Computing (CRC), the core facility for scientific computing and research at Pitt. CRC supports the work of over 800 users in 59 departments across the entire university. The new expanded hardware will advance both undergraduate and graduate courses and educational experience across a similarly broad range of departments and courses. These expanded resources will enable Pitt to expand a synergy between research and education at all levels, reaching beyond the university to faculty, staff, and students at Howard University, other historically black colleges and universities (HBCUs), and many undergraduate faculty and students both regionally and nationwide. The new resources will also expand scientific computing to students in the Pittsburgh Public School district, including nearby Pittsburgh Science and Technology Academy and Pittsburgh Public Allderdice, both urban schools with diverse student populations.<br/><br/>The funded resources will consist of 16 state-of-the-art graphics processing unit (GPU) computing nodes including NVIDIA Ampere A100 GPU accelerators. Each GPU node will be ~2x faster than previous generation GPUs and 14-50x faster on scientific computing software than current CPU nodes, and will enable increased machine learning productivity. An additional 36 state-of-the-art MPI nodes containing AMD ?Milan? cores and high system memory will enable complex computational simulations. The availability of the new resource will dramatically expand the access and opportunity to GPU and message passing interface (MPI) computing, offering significant speed improvements for an immense range of scientific computing, from machine learning and big data, to quantum chemistry, protein molecular dynamics, energy conversion, nanoparticle catalysis design, weather/wind forecasting, astronomical data analysis, atomistic tunneling electron microscopy measurements, and computer vision. Beyond simple acceleration, the resources will enable transformative research with vastly more accurate weather grids, new machine learning surrogates for quantum chemical calculations of molecular and materials energies and properties, rare-event sampling in protein folding and binding, fMRI neuroscience, and next generation digital astronomy. The resources will immediately benefit over 30 NSF-funded research groups, leveraging over $18 million in research and training grants. The resources will support research in all areas including Chemistry, Computational Biology, Chemical Engineering, Materials Science, Psychology, Astrophysics, Weather Forecasting, Computer Science, and research centers focusing on energy, sustainability, and other key areas of science and engineering. Workshops and courses associated with the new resources will focus on adapting existing software and developing new software for MPI and GPU-computing including a wide range of machine learning methods enabled by the transformation in numeric processing with these expanded resources. These resources will be shared with collaborators at Howard University, other HBCUs, and with regional and national undergraduate schools to broaden participation in computational science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2001789","CICI: SSC: Real-Time Operating System and Network Security for Scientific Middleware","OAC","Cybersecurity Innovation","11/01/2019","04/24/2020","Gedare Bloom","CO","University of Colorado at Colorado Springs","Standard Grant","Robert Beverly","09/30/2022","$708,581.00","","gbloom@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","8027","9251","$0.00","Remote monitoring and control of industrial control systems are protected using firewalls and user passwords. Cyberattacks that get past firewalls have unfettered access to command industrial control systems with potential to harm digital assets, environmental resources, and humans in proximity to the compromised system.  To prevent and mitigate such harms in scientific industrial control systems, this project enhances the security of open-source cyberinfrastructure used for high energy physics, astronomy, and space sciences. The results of this project enhance the security of scientific instruments used in particle accelerators, large-scale telescopes, satellites, and space probes. The benefits to science and the public include greater confidence in the fidelity of experimental data collected from these scientific instruments, and increased reliability of scientific cyberinfrastructure that reduces the costs associated with accidental misconfigurations or malicious cyberattacks.<br/><br/>The objective of this project is to enhance the security of the open-source Real-Time Executive for Multiprocessor Systems (RTEMS) real-time operating system and the Experimental Physics and Industrial Control System (EPICS) software and networks; RTEMS and EPICS are widely used cyberinfrastructure for controlling scientific instruments. The security enhancements span eight related project activities: (1) static analysis and security fuzzing as part of continuous integration; (2) cryptographic security for the open-source software development life cycle; (3) secure boot and update for remotely-managed scientific instruments; (4) open-source cryptographic libraries for secure communication; (5) real-time memory protection; (6) formal modeling and analysis of network protocols; (7) enhanced security event logging; and (8) network-based intrusion detection for scientific industrial control systems. The project outcomes provide a roadmap for enculturating cybersecurity best practices in open-source, open-science communities while advancing the state-of-the-art research in cyberinfrastructure software engineering and industrial control system security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839900","CICI: SSC: Integrity Introspection for Scientific Workflows (IRIS)","OAC","Cybersecurity Innovation","09/01/2018","04/20/2020","Anirban Mandal","NC","University of North Carolina at Chapel Hill","Standard Grant","Robert Beverly","08/31/2022","$999,575.00","Ewa Deelman, Von Welch, Yufeng Xin, Cong Wang","anirban@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8027","","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines in different geographical locations. It has been observed that sometimes during this complex process, scientific data is unintentionally modified or accidentally tampered with, with errors going undetected and corrupt data becoming part of the scientific record. The IRIS project tackles the problem of detecting and diagnosing these unintentional data errors that might occur during the scientific processing workflow. The approach is to collect data relevant to the correctness and integrity of the scientific data from various parts of the computing and network system involved in the processing, and to analyze the collected data using machine learning techniques to uncover errors in the scientific data processing. The solutions are integrated into Pegasus, a popular ""workflow management system"" - a software used to describe the complex process in a user-friendly way and that handles the details of processing for the scientists. The research methods will be validated on national computing resources with exemplar scientific applications from gravitational-wave physics, earthquake science, and bioinformatics. These solutions will allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>Data-driven science workflows often suffer from unintentional data integrity errors when executing on distributed national cyberinfrastructure (CI). However, today, there is a lack of tools that can collect and analyze integrity-relevant data from workflows and thus, many of these errors go undetected jeopardizing the validity of scientific results. The goal of the IRIS project is to automatically detect, diagnose, and pinpoint the source of unintentional integrity anomalies in scientific workflows executing on distributed CI. The approach is to develop an appropriate threat model and incorporate it in an integrity analysis framework that collects workflow and infrastructure data and uses machine learning (ML) algorithms to perform the needed analysis. The framework is powered by novel ML-based methods developed through experimentation in a controlled testbed and validated in and made broadly available on NSF production CI. The solutions will be integrated into the Pegasus workflow management system, which is used by a wide variety of scientific domains. An important part of the project is the engagement with science application partners in gravitational-wave physics, earthquake science, and bioinformatics to deploy the analysis framework for their workflows, and to iteratively fine tune the threat models, ML model training, and ML model validation in a feedback loop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2138811","OAC: Piloting the National Science Data Fabric: A Platform Agnostic Testbed for Democratizing Data Delivery","OAC","CYBERINFRASTRUCTURE","10/01/2021","09/17/2021","Valerio Pascucci","UT","University of Utah","Standard Grant","Bogdan Mihaila","09/30/2024","$5,609,259.00","Frank Wuerthwein, Alexander Szalay, John Allison, Michela Taufer","pascucci@acm.org","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7231","","$0.00","Ongoing investments from NSF and other agencies into shared experimental and computing facilities increase data generation by orders of magnitude and presents a challenge for universal, easy, and fast access to data by users and may limit the scientific impact of such facilities. This pilot seeks to demonstrate a trans-disciplinary National Science Data Fabric (NSDF) integrating access to and use of shared storage, networking, computing, and educational resources and, in doing so, will help democratize data-driven sciences through the development of a cyberinfrastructure (CI) platform designed for equitable access. This pilot connects an open network of researchers gathered around earth science, astronomy, biology, chemistry, physics, and materials science, to deploy a testbed for individual and shared scientific use. Supporting the IceCube neutrino observatory and the XenonNT dark matter detector will advance the understanding of the evolution of galaxies and the nature of dark matter and dark energy. Supporting the Materials Common enables the fast-paced design of new materials in critical fields such as energy, security, environment, and healthcare.  Active involvement of Historically Black Colleges, the Minority Serving Cyberinfrastructure Consortium, and of Hispanic Serving Institutions assures true democratization of data-driven science and unleashes the intellectual potential of a genuinely diverse scientific community presenting the best potential for US innovation.<br/><br/>The National Science Data Fabric (NSDF) pilot builds a testbed experimenting with critical technology needed to democratize data-driven sciences by constructing a CI platform designed for equitable access. In particular, NSDF experiments with key technologies that empower user communities to develop their solutions and support domain-specific requirements while avoiding duplication of technology. A programmable Content Delivery Network (CDN) will be a central component that interoperates with different appliances and storage solutions ranging from leadership-class computing facilities, campus-wide computing resources, commercial cloud, and research labs of individual investigators. With this strategy, NSDF connects storage, compute, and networking components with a software stack that empowers end-users with scalable tools that are easy to use, integrate and scale. Community-driven education and outreach will guarantee equitable access to all resources and engage an open network of universities, including minority-serving institutions in a federated data fabric configurable for individual and shared scientific use. By offering a shared, modular, containerized data delivery environment, operating at the best economies of scale, the NSDF pilot will demonstrate a key technology to fill the ?missing middle? in the national computational infrastructure and will help address the ?missing millions? challenge of American talent in STEM.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117575","MRI: Acquisition of Dolly Sods GPU Cluster for Accelerated High-Performance Computing and Applications in Machine Learning and Artificial Intelligence in West Virginia","OAC","Major Research Instrumentation","10/01/2021","09/17/2021","Blake Mertz","WV","West Virginia University Research Corporation","Standard Grant","Alejandro Suarez","09/30/2024","$1,099,448.00","Werner Geldenhuys, Sarah Spolaor, Piyush Mehta, Gianfranco Doretto","blake.mertz@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","1189","1189, 9102, 9150","$0.00","This project will enable West Virginia University (WVU) to acquire a special-purpose graphics processing unit (GPU) cluster called Dolly Sods. Dolly Sods will be a critical driver of WVU's goal of developing capabilities in utilizing big data, artificial intelligence (AI), and machine learning (ML) to enable transformational research in a broad range of fields encompassing drug development, interstellar phenomena, biometrics, material design, and business logistics and management. In conjunction with other institutions of higher learning in West Virginia, development of these capabilities will in turn lead to training an AI-ready West Virginian workforce that can leverage their skills to strengthen current relationships with regional high performance computing (HPC) centers as well as forging new relationships with federal and industrial partners. The creation of training opportunities for first generation college students, female students, and those from marginalized communities will aid in the diversification of the computationally intensive workforce and will be invaluable to West Virginia. Finally, this acquisition will contribute to keeping the United States at the forefront of AI development.<br/><br/>The rapid adoption of hardware accelerators (e.g., GPUs) in the research computing community has facilitated massively increased compute capability and application of ML and AI approaches to solving big data problems in every STEM field and non-traditional fields like business data analytics. The requested acquisition of the Dolly Sods cluster will enable cutting-edge research for efforts in diagnostic imaging of tumors, high-throughput screening of small molecule drug design, on-the-fly detection of interstellar phenomena, design and optimization of data compression algorithms used in space flight, computer vision of medical images, and informatics of business-based managerial decisions and system/process analysis, among others. This research will put WVU at the forefront of the movement to integrate ML and AI into the fabric of data-driven research, allowing for timely and relevant scientific and societal issues to be addressed as well as providing training opportunities for the next generation of data scientists, led by 24 faculty in conjunction with 92 postdocs, graduate students, and undergraduate students. Dolly Sods will also serve as an integral teaching resource for a newly formed undergraduate degree in Data Science, which will incorporate ML and AI throughout the curriculum. Combined with outreach to statewide institutions of higher learning, Dolly Sods will drive continued efforts in training a diverse HPC workforce, drawing from communities historically underrepresented in STEM, including first generation college students, women, and other underrepresented groups. The project will strengthen partnerships with regional federal labs (DOE National Energy Technology Laboratory, NOAA Environmental Security Computing Center, NASA Independent Validation and Verification Facility, and the FBI Criminal Justice Services Division), industrial partners (Leidos and PPG), and the nation?s leading center for ML and AI (Pittsburgh Supercomputing Center (PSC)). It will further WVU?s long-term goal of transforming the economically disadvantaged region of Appalachia into a top-level destination for investment from the technology sector.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126148","CC* Integration-Large: Prototyping a Secure Distributed Storage Infrastructure for Accelerating Big Science","OAC","CISE Research Resources","10/01/2021","09/16/2021","Susmit Shannigrahi","TN","Tennessee Technological University","Standard Grant","Deepankar Medhi","09/30/2023","$973,221.00","Lixia Zhang, Alexander Afanasyev, FRANK FELTUS","sshannigrahi@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","2890","9102","$0.00","Big science communities routinely publish and share vast amounts of data. Scalable, resilient, interoperable and controllable access to published data is essential to support data ­intensive science and engineering research. To facilitate scientific data publication and accessibility, this project aims to create a secure, resilient, and distributed data storage framework, Hydra, that enables scientific communities to build a loose federation of data repositories potentially owned by multiple administrative entities.<br/><br/>By taking a data-centric approach and leveraging the Named Data Networking (NDN) primitives, Hydra provides a generic software framework that allows scientists to publish datasets easily with data-centric security, automated access control, and automated data replication. It also facilitates access to data published on Hydra as well as other existing data repositories. The project team aims to test Hydra using NSF's FABRIC testbed and utilize its built-in logging capability to verify and validate its usability, scalability, and security.<br/><br/>Hydra enables secure and scalable data access to alleviate the publication, data access, and security problems currently faced by a large number of scientific communities. Hydra has the potential to benefit scientific communities such as climate, meteorology, astrophysics, geology, and more through streamlined data publication, built-in data authenticity and access control, improved availability, and reduced network traffic.<br/><br/>The publicly available website for this project is https://hydra-repo.io. This website maintains pointers to various repositories that host up-to-date codebase, user documentation, hydra-repo operational data logs, and publications. This website will be available at least three years beyond the project's duration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2053626","Collaborative Research: From Brains to Society: Neural Underpinnings of Collective Behaviors Via Massive Data and Experiments","OAC","HDR-Harnessing the Data Revolu, Information Technology Researc","10/01/2020","12/18/2020","Brandon Behlendorf","MA","Massachusetts Institute of Technology","Continuing Grant","Sylvia Spengler","09/30/2021","$398,311.00","","bbehlendorf@albany.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","099Y, 1640","062Z","$0.00","Despite thousands of investigations on the neural basis of individual behaviors and even more studies on collective behaviors, a clear bridge between the organization of individual brains and their combinational impact on group behaviors, such as cooperation and conflict and ultimately collective action, is lacking. To address the grand challenge of inferring group cooperation from the functional neuroarchitecture of individual brains, this project will harness advances in data, experiment and computation. Specifically, it will integrate, for the first time, existing large-scale human functional neuroimaging data, prospectively collected individual and group behavioral data from a large cohort, with cutting-edge machine learning tools, hierarchical models and large-scale simulations. This is a collaborative effort between a team of neuroscientists, social scientists and data scientists, that aims to elucidate the neural basis of cooperation, a fundamental process in a functioning society and at the core of social environments. <br/><br/>The project will first harness the combined wealth of existing neuroimaging and behavioral data from large-scale studies, including the Human Connectome-Lifespan (HCP-L) and the Adolescent Brain Cognitive Development (ABCD) and will leverage recent breakthroughs in machine learning to characterize the diversity, individuality and commonality of neural circuits (the connectome) supporting cognitive function across the lifespan. It will then conduct large-scale (~10,000 individuals) online behavioral experiments to identify connections between individual behaviors, decisions and group behaviors during a Public Goods Game. The experiments will measure individual proclivity towards cooperation and the social welfare obtained by cooperation, leading to potentially transformative insights into the emergence of cooperation within groups via individual behaviors. The resulting first-of-its-kind dataset may become a very valuable resource to the research community. Large-scale simulations based on statistical models estimated from this and the assembled neuroimaging datasets will then assess the direct or indirect relationships between individual connectomes and cooperation in group settings, and will elucidate the role of group processes in amplifying or ameliorating individual differences towards collective outcomes. Findings from this project may have a transformative impact on the scientific community's currently incomplete understanding of how individual brains shape societal behavior via cognitive, social, and interactive mechanisms.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031563","Travel Grant for NSF Frontera LRAC Award: Technical Coordination with TACC and Attendance to a PI Meeting","OAC","Leadership-Class Computing","08/01/2020","05/06/2020","Cheng-Chien Chen","AL","University of Alabama at Birmingham","Standard Grant","Edward Walker","07/31/2022","$8,108.00","","chencc@uab.edu","1720 2nd Avenue South","Birmingham","AL","352940111","2059345266","CSE","7781","9150","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931419","Collaborative Research: Frameworks: Cyber Infrastructure for Shared Algorithmic and Experimental Research in Online Learning","OAC","ECR-EHR Core Research, Software Institutes","10/01/2019","04/12/2021","Ryan Baker","PA","University of Pennsylvania","Standard Grant","Robert Beverly","09/30/2024","$1,399,995.00","Rebecca Stein, Peter Decherney","ryanshaunbaker@gmail.com","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7980, 8004","026Z, 075Z, 077Z, 7925","$0.00","This project, RAILKaM, will create new technology that will enable twenty researchers during the grant period to run large-scale field experiments where they study basic principles in education and educational psychology in the context of both K-12 mathematics learning and university Massive Online Open Courses (MOOCs). The experiments will be delivered through adaptive learning technology embedded in learning systems already being used by over 100,000 K-12 students and hundreds of thousands of MOOC learners each year. RAILKaM will also support 75 data scientists in conducting analyses on student data after the fact, using carefully redacted datasets that protect student privacy. In facilitating high-power, replicable experiments with diverse student populations and extensive measurement, this infrastructure increases the efficiency and ease of conducting high-quality educational research in online learning environments, bringing 21st-century research methods to education for the long-term betterment of learner outcomes.<br/><br/>This project, RAILKaM, will support researchers in more easily running scaled, highly instrumented studies on education and educational psychology, both in K-12 and university Massive Online Open Courses (MOOCs). RAILKaM will leverage ASSISTments, an online learning platform for middle school mathematics homework and classwork used by more than 100,000 students each year. In addition, RAILKaM will build functionality atop the ASSISTments platform so that educational experiments involving scaffolded problem-solving can be easily built into MOOC courses. ASSISTments will use open source APIs to integrate with MOOCs offered by the University of Pennsylvania, branching capacity for investigation to higher education while enabling richer student interactions and data collection than is typically feasible in MOOC courses. These capacities will enable researchers to run online field experiments to test interventions designed to increase student learning and engagement with a focus on how adaptive learning experiences can be optimized. These experiments will be augmented by rich data collection on learners, extending MOOC log data and ASSISTments data with several indicators of learning and engagement not previously available for research at scale. This project will develop the software infrastructure necessary to conduct experiments and collect enriched data, as well as the social infrastructure necessary to select and refine study ideas while maintaining instructor control over the activities that students experience. The combined software and social infrastructure will enable us to engage with researchers who are interested in these issues but who currently lack the infrastructure, technical capacity, or access to learners necessary to conduct high-powered or complex randomized controlled trials. This infrastructure will help these researchers to improve scientific understanding of the principles of human learning, providing a unique shared resource for learning scientists that will have considerable potential for broader impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934565","Collaborative Research: Framework for Integrative Data Equity Systems","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","09/01/2019","06/08/2021","H. Jagadish","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Sylvia Spengler","08/31/2022","$762,301.00","Olutayo Fabusuyi, Margaret Levenstein, Robert Hampshire","jag@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","099Y, 7231","062Z, 7231, 9102","$0.00","Data Science continues to have a transformative impact on Science and Engineering, and on society at large, by enabling evidence-based decision making, reducing costs and errors, and improving objectivity. The techniques and technologies of data science also have enormous potential for harm if they reinforce inequity or leak private information.  As a result, sensitive datasets in the public and private sector are restricted from research use, slowing progress in those areas that have the most to gain: human services in the public sector.  Furthermore, the misuse of data science techniques and technologies will disproportionately harm underrepresented groups across race, gender, physical ability, sexual orientation, education, and more. These data equity issues are pervasive, and represent an existential risk for the use of data-driven methods in science and engineering. This project will establish a  Framework for Integrative Data Equity Systems (FIDES): an Institute for the study of systems that enable research on sensitive data while preventing misuse and misinterpretation. <br/><br/>FIDES will enable interdisciplinary community convergence around data equity systems, with an initial study in critical domains such as mobility, housing, education, economic indicators, and government transparency, leading to the development of a novel data analytics infrastructure that supports responsibility in integrative data science.  Towards this goal, the project will address several technically challenging problems: (1) To be able to use data from multiple sources, risks related to privacy, bias, and the potential for misuse must be addressed. This project will develop principled methods for dataset processing to overcome these concerns.  (2) Individual datasets are difficult to integrate for use in advanced multi-layer network models.  This project considers methods to create pre-trained tensors over large collections of spatially and temporally coherent datasets, making them easier to incorporate while controlling for fairness and equity.  (3) Any dataset or model must be equipped with sufficient information to determine fitness for use, communicate limitations, and describe underlying assumptions.  This project will develop tools and techniques to produce ""nutritional labels"" for data and models, formalizing and standardizing ad hoc metadata approaches to provenance, specialized for equity issues. In addition to supporting methodological innovation in data science, the Institute will become a focal point for sharing expertise in data equity systems.  It will do so by establishing interfaces for interaction between data science and domain experts to promote expertise development and sharing of best practices, and by consistently supporting efforts on diversity and equity.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937369","Mentoring the Next Generation of Parallel Processing Researchers at IEEE-CSTCPP Sponsored Conferences","OAC","EDUCATION AND WORKFORCE","09/01/2019","07/12/2019","Jaroslaw Zola","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","08/31/2022","$50,000.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7361","7556, 9150","$0.00","Student mentoring programs play an important role in providing younger attendees with the necessary tools to take advantage of their participation at conferences. From their attendance to conferences, students can benefit from professional connections and establish collaborations, market themselves to potential employers, get exposed to a wider network of mentors, and find inspiration for novel research avenues. Student programs have emerged in several leading conferences, but are still far from being widely adopted. In particular the International Parallel and Distributed Processing Symposium (IPDPS) has been hosting a Ph.D. forum and student program for over a decade. This program has evolved into a comprehensive training workshop that includes sessions on career planning and hands on tutorials on writing and presentation skills; it also provides effective opportunities for student networking. This project seeks to expand the IPDPS mentoring and outreach model into other conferences that fall under the broad field of parallel and distributed processing and are sponsored by IEEE-CS Technical Committee on Parallel Processing. Parallel and distributed processing has become increasingly important for a variety of disciplines where traditional computational methods lack the mechanisms to deal with large data volumes or expensive computations. As stated by NSF's mission, this project supports education and diversity, while promoting the progress of science by mentoring the next-generation workforce in the high performance computing discipline.<br/><br/>This project has two concrete goals: 1) supporting student participation at the IEEE International Parallel and Distributed Processing Symposium. IPDPS is an international conference for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation. 2) promoting the adoption of student mentoring programs in other IEEE TCPP conferences, such as DS-RT, HiPC, PACT, and PerCom. The beneficiary conferences are required to facilitate an introductory session where students network with peers and mentors. Additionally, conference organizers are provided with the logistic plan of IPDPS's PhD Forum and are encouraged to have similar proven activities; including a poster session and mentoring workshops. Only students currently studying at U.S. universities are eligible to receive support from this NSF-funded project. This project targets especially students that typically cannot attended these conferences without financial support, such as undergraduate students, graduate students in their first years, and students attending their first conference. In order to increase the diversity of attendees, the project strongly encourages the participation of females and other underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755464","CRII: OAC: Scalable Cyberinfrastructure for Big Graph and Matrix/Tensor Analytics","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","06/01/2018","05/23/2018","Da Yan","AL","University of Alabama at Birmingham","Standard Grant","Alan Sussman","05/31/2022","$170,941.00","","yanda@uab.edu","1720 2nd Avenue South","Birmingham","AL","352940111","2059345266","CSE","026Y, 9150","026Z, 8228, 9150","$0.00","The existing distributed graph and matrix analytics frameworks are designed with data-intensive workloads in mind, rendering them inefficient for compute-intensive applications such as graph mining and scientific computing. The goal of this project is to develop novel big data frameworks for two compute-intensive tasks, graph mining and matrix/tensor computations, respectively. The two frameworks advance the field of big data analytics by motivating future systems for compute-intensive analytics, and promoting their application in various scientific areas to improve research productivity. The two systems will be available for public use, and can serve several cross-disciplinary projects in computer forensics, computational physics, and bioinformatics. The project includes mentoring graduate students and training K-12 students through summer internships, as well as related new course materials and outreach activities to help the public learn big data technologies. Thus, the project aligns with the NSF's mission to promote the progress of science and to advance the national health and prosperity.<br/><br/>The graph mining system and the matrix/tensor platform share the design of (i) a tailor-made storage subsystem providing efficient and flexible data access, and (ii) a computation subsystem with fine-grained task control for data-reuse-aware task assignment and load balancing. The graph mining system, called G-thinker, aims to facilitate the writing of distributed programs which mine from a big graph those subgraphs that satisfy certain requirements. Such mining problems are useful in many applications like community detection and subgraph matching. These problems usually have a high computational complexity, and existing serial algorithms tackle these problems by backtracking in a duplication-free vertex-set numeration tree, which recursively partitions the search space. G-thinker adopts an intuitive programming interface that minimizes the effort of adapting an existing serial subgraph mining algorithm for distributed execution. The subgraphs to mine are spawned from individual vertices and they grow their frontiers as needed, and memory overflow is avoided by spilling subgraphs to disks when needed. In each machine, vertices and edges shared by multiple subgraphs need only be transmitted and cached once, which minimizes communication (and hence data waiting) so that CPU cores are better utilized. To address the load-balancing problem of power-law graphs, G-thinker explores recursive decomposition and work stealing to allow idle machines to steal subgraphs for mining from heavily-loaded machines. The project also explores a distributed matrix/tensor storage and computing framework, where matrix/tensor partitions are stored in multiple replicas using different storage schemes to efficiently support all kinds of submatrix access operations. This flexible storage scheme offers the upper-layer computations much more opportunities for fine-grained optimizations, including smarter task scheduling and in-situ updates. The use of this framework is exemplified by matrix multiplication and LU factorization. Both of the proposed frameworks can help build a cyberinfrastructure for collaborations with scientists in science, medicine, and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761990","Spokes: MEDIUM: NORTHEAST: Collaborative: Advancing a Data-Driven Discovery and Rational Design Paradigm in Chemistry","OAC","BD Spokes -Big Data Regional I, OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, PROJECTS","09/01/2018","08/23/2018","Johannes Hachmann","NY","SUNY at Buffalo","Standard Grant","Lin He","08/31/2022","$700,000.00","Alan Aspuru-Guzik, Geoffrey Hutchison, Marcus Hanwell","hachmann@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","024Y, 1253, 1712, 1978","028Z, 054Z, 062Z, 7433, 8083","$0.00","With support from the Division of Chemistry (CHE) and the Division of Materials Research (DMR) in the Directorate for Mathematical and Physical Sciences (MPS) and the Directorate for Computer & Information Science & Engineering (CISE), this project aims to advance the use of modern data science in chemistry. In particular, the project will advance the field of data-driven chemical research by promoting the use of machine learning and other data mining techniques in the molecular sciences and by fostering and coalescing a community of stakeholders. The work of the project and the community it represents aims to transform chemistry's ability to tackle challenging discovery and design problems. This approach can dramatically accelerate and streamline the process that leads to chemical innovation -- an important factor in economic and technological advancement -- and thus result in an improved return on public and private investments. The project also addresses corresponding questions of training and workforce development needed in chemistry, thus insuring the US's international competitiveness. <br/><br/>The mission of this project is to assert the role of big data research in the chemical domain, i.e., to promote, enable, and advance the ideas of data-driven discovery and rational design. The project aims to create a community-driven roadmap as well as facilitate concrete solutions that are beyond the scope of the disjointed efforts of its individual stakeholders. The Big Data Hubs and Spokes ecosystem is the ideal framework to realizing this vision and accelerating progress in this high-priority area of research. The effort at hand sets out to implement some of the key findings of the recent NSF Division of Chemistry workshop on Framing the Role of Big Data and Modern Data Science in Chemistry. The four signature initiatives of this Spoke project include (i) the planning, coordination, integration, and consolidation of community-developed software tools for big data research in chemistry as well as the formulation of guidelines, best practices, and standards; (ii) the organization of workshops for community building, to connect solution seekers with solution providers, and to address questions ranging from strategic to technical; and (iii) the creation and dissemination of community-developed teaching materials as well as the formulation of course, program, and curricular recommendations for education and workforce development that reflect the changing, data-centric approach in chemical research; (iv) providing access to a shared hardware infrastructure for community data sets, on-site data mining capacity, and the exploration of domain specific method and hardware issues.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931388","Collaborative Research: Frameworks: An open source software ecosystem for plasma physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes, Space Weather Research","10/01/2019","09/07/2019","Nicholas Murphy","MA","Smithsonian Institution Astrophysical Observatory","Standard Grant","Seung-Jong Park","09/30/2024","$1,439,530.00","","namurphy@cfa.harvard.edu","60 Garden St","Cambridge","MA","021381516","6174967923","CSE","1253, 7244, 8004, 8089","026Z, 077Z, 4444, 7569, 7925, 8004","$0.00","Software is crucial to all areas of modern plasma physics research.  Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas.  Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions.  Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education.  PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy.  PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality.  The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.<br/><br/>The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics.  The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities  include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data.  They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117439","MRI: Acquisition of Artificial Intelligence Super Computer (AISC) for Accelerating Scientific Discovery","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","10/01/2021","09/18/2021","Vipin Chaudhary","OH","Case Western Reserve University","Standard Grant","Alejandro Suarez","09/30/2024","$694,449.00","Jonathan Haines, Anant Madabhushi, Roger French, Roger Bielefeld","vipin@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","1189, 7231","1189","$0.00","The project funds the acquisition of an Artificial Intelligence Super Computer (AISC) which will enable researchers, entrepreneurs, educators, and policymakers to leverage previously unimaginable resources to make intelligence-¬informed decisions from data. AISC will enable transformative advances in myriad scientific, engineering, and healthcare fields that rely on artificial intelligence and machine learning (AI/ML) techniques and high-performance computing (HPC) technologies. AISC will support researchers from across Case Western Reserve University and their collaborating institutions. The spectrum of research enabled by AISC can be classified into four broad themes: (i) Cyberinfrastructure (CI) and computer science; (ii) material science; (iii) engineering systems; and (iv) biomedical engineering. In addition to supporting a broad spectrum of research, the project will build a vibrant multidisciplinary community around accelerating AI/ML computing and will conduct classes, tutorials, and workshops to engage undergraduates, graduates, post-docs and faculty in computer science and discipline sciences within the local Cleveland area and nationally.<br/><br/>AISC adopts Tensor Core graphics processing units (GPUs) with large, fast, and high bandwidth memory along with cutting edge technologies such as Non-Volatile Memory Express (NVMe) based storage, and a high-speed interconnect. AISC can be partitioned into multiple instances each fully isolated with their own high-bandwidth memory, cache, and compute cores allowing support of varying size jobs with guaranteed quality of service (QoS) for every job, optimizing utilization and thereby extending the reach of AISC to more users. Broader impacts from AISC include not only research findings and new technologies, but transfer to practice and wide dissemination of advances to broad communities beyond the CI, AI and specific scientific domains. To fully realize the project?s broader impact vision, the following activities are envisioned: (i) Build STEM talent and improve public interest in science and computing; (ii) Advance discovery and understanding while promoting teaching, training, and learning; (iii) CI workforce development that is conversant with AI; (iv) Yearly domain-specific hackathons; and (v) Broaden participation of underrepresented groups by partnering with multiple major NSF-sponsored programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103878","Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science","OAC","Software Institutes","10/01/2021","09/17/2021","Mark Piper","CO","University of Colorado at Boulder","Continuing Grant","Alan Sussman","09/30/2026","$95,701.00","","mark.piper@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8004","077Z, 7925, 8004, 9102","$0.00","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges.  The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.<br/><br/>The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the  Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.<br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126246","CC* Integration-Large: MAINTLET: Advanced Sensory Network Cyber-Infrastructure for Smart Maintenance in Campus Scientific Laboratories","OAC","CISE Research Resources","10/01/2021","09/16/2021","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Deepankar Medhi","09/30/2023","$1,000,000.00","Mark McCollum, Gianni Pezzarossi, John Dallesasse, Mauro Sardela","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2890","9102","$0.00","Studies show that in some industry and scientific environments, between 15 and 60 percent of the total costs originate in maintenance activities, and about 33 cents of every dollar spent on maintenance in the US is wasted because of unnecessary and preventable maintenance activities. The cost of scientific instruments? maintenance is even greater in universities because the scientific instruments, and associated support equipment, such as vacuum pumps, serve diverse students, staff, and faculty populations for educational and research purposes over much longer periods with smaller budgets than in industry. Instruments? down-time greatly limits research productivity and programs. Hence, MAINTLET investigates an advanced sensory network cyber-infrastructure with modern AI-guided big data methods that helps the campus scientific laboratories to see patterns that indicate the right time to purchase kits, parts, and services, and minimize opportunity cost due to down-time and all repairs and maintenance.<br/><br/>MAINTLET enables cost-effective, scalable, and sustainable reactive, preventive and predictive maintenance solutions for scientific instruments. MAINTLET provides two important indicators.  For preventive and predictive maintenance, simulations identify potential instrument failures, using data from instruments? surrounding sensors such as acoustic sensors, water flow sensors, and contact water temperature sensors. These data help predict in real-time, using AI techniques, when a pump may need condition-based preventive maintenance.  For reactive maintenance, trained failure detectors detect failures in real-time. MAINTLET includes sensors; edge devices such as Raspberry Pis executing reactive maintenance services; WiFi and Zigbee access points and networks interconnecting sensors, edge and cloud devices; and a private cloud with predictive and preventive maintenance services.  <br/><br/>The impact of MAINTLET is in terms of decreased instrument failures and down-time and hence speed-up and accuracy of scientific discoveries, and in terms of security (as uncertainty about failed scientific lab equipment can cause both cyber and physical harm).  MAINTLET?s various insights are taught in undergraduate and graduate courses to students from Materials Science & Engineering, Computer Science, and other departments. MAINTLET is presented at the Advanced Materials Characterization Workshop with instrument vendors? exhibit, ?Nano at Illinois? event, and other scientific venues. During the summers, the Worldwide Youth in Science and Engineering program for high school students, and other outreach programs, organized within the Grainger College of Engineering, receive a series of MAINTLET lectures.<br/><br/>MAINTLET?s website https://t2c2.csl.illinois.edu/projects/maintlet includes links to data, code, results, and simulations as they are developed. The project-related information will be accessible for at least five years after the project ends.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118329","HDR Institute: Geospatial Understanding through an Integrative Discovery Environment","OAC","HDR-Harnessing the Data Revolu, Hydrologic Sciences, XC-Crosscutting Activities Pro, CYBERINFRASTRUCTURE, Info Integration & Informatics, CZO-Critical Zone Obsrvatories","10/01/2021","09/15/2021","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Amy Walton","09/30/2026","$6,845,994.00","Mohan Ramamurthy, Deanna Hence, Xiaohui Carol Song, David Tarboton","shaowen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","099Y, 1579, 7222, 7231, 7364, 7693","054Z, 062Z, 5294, 7231, 8400, 9102","$0.00","This project establishes the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE). In today?s interconnected world, disasters such as floods and droughts are rarely isolated events, and their cascading effects are often felt far beyond their locations of origin. I-GUIDE creates an integrative discovery environment that enables the harnessing of geospatial data for understanding interconnected interactions across diverse socioeconomic-environmental systems to enhance community resilience and environmental sustainability. I-GUIDE nurtures a diverse and inclusive geospatial discovery community across many disciplines by bridging disciplinary digital data divides with broader impacts amplified through a well-trained and diverse workforce and proactive engagement of minority and underrepresented groups. The project leverages existing collaborations with its diverse member and partner organizations, representing academic, governmental, and industrial institutions, thereby extending its reach across the U.S. and the globe. The influence of I-GUIDE?s new knowledge frontiers impact solutions to real-world problems and geospatial decisions, with significance ranging from the nation?s economic development to security. I-GUIDE builds upon relationships with the museum and informal science education communities as well as libraries and news media to raise public awareness about the contributions of the geospatial data revolution to society. <br/><br/>Globalization has intensified and extended the impacts of socioeconomic-environmental interactions across long distances, a process known as telecoupling. I-GUIDE?s integrative discovery environment is vital to transform innovative theories, concepts, methods, and tools focusing geospatial synthesis that drive novel capabilities for addressing scientific questions of how to harness geospatial data for multi-scale and telecoupling discoveries to enhance community resilience and environmental sustainability. Two substantive and convergent scientific problems are addressed: (1) Water security, to evaluate geospatial and socioeconomic impacts of hydroclimatic extremes that are related to environmental and infrastructure sustainability, and (2) Biodiversity and food security, to enhance basic understanding of biodiversity dynamics in the face of nearby and distant disasters, global changes, international trade, and dynamic land use transitions. Transformative geospatial understanding gained from solving these interrelated problems are translated into digital resources and tools made available online through an open I-GUIDE platform for a variety of educational and training activities to serve a broad and diverse audience. Through this platform, the next-generation workforce can acquire geospatial knowledge coupled with data-driven technological skills for decision-making and problem-solving experiences. <br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR).  The award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Behavioral and Cognitive Sciences, the Division of Social and Economic Sciences, and the Office of Multidisciplinary Activities within the Directorate for Social, Behavioral and Economic Sciences; the Division of Earth Sciences within the Directorate for Geosciences; the Division of Mathematical Sciences within the Directorate for Mathematical and Physical Sciences, and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118240","HDR Institute: Imageomics: A New Frontier of Biological Information Powered by Knowledge-Guided Machine Learning","OAC","HDR-Harnessing the Data Revolu","10/01/2021","09/15/2021","Tanya Berger-Wolf","OH","Ohio State University","Cooperative Agreement","Amy Walton","09/30/2026","$4,000,000.00","Henry Bart, Hilmar Lapp, Charles Stewart, Anuj Karpatne","berger-wolf.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","099Y","062Z, 1165, 9102","$0.00","The traits that characterize living organisms, in particular, their morphology, physiology, behavior and genetic make-up, enable them to cope with forces of the physical as well as the biological and social environments that impinge on them. Moreover, since function follows form, traits provide the raw material upon which natural selection operates, thus shaping evolutionary trajectories and the history of life. Interestingly, most living organisms, from microscopic microbes to charismatic megafauna, reveal themselves visually and are routinely captured in copious images taken by humans from all walks of life.   The resulting massive amount of image data has the potential to further understanding of how multifaceted traits of organisms shape the behavior of individuals, collectives, populations, and the ecological communities they live in, as well as the evolutionary trajectories of the species they comprise. Images are increasingly the currency for documenting the details of life on the planet, and yet traits of organisms, known or novel, cannot be readily extracted from them. Just like with genomic data two decades ago, our ability to collect data far outstrips our ability to extract biological insight from it. The Institute will establish a new field of Imageomics, in which biologists utilize machine learning (ML) algorithms to analyze vast stores of existing image data?especially publicly funded digital collections from national centers, field stations, museums and individual laboratories?to characterize patterns and gain novel insights on how function follows form in all areas of biology to expand our understanding of the rules of life on Earth and how it evolves. <br/><br/>This Institute will introduce structured knowledge from the biological sciences to guide and structure ML algorithms to enable biological trait discovery from images, establishing the field of Imageomics. With images captured and annotated by scientists and the public serving as the basis for the work, the Institute?s convergent approach uses structured biological knowledge to provide scientifically validated inductive biases and rich supervision for ML, and ML will in turn enrich the body of biological knowledge. The resulting ML models and tools will help to make what was hidden visible, so that scientists from a wide range of biological communities can discover and infer the traits of organisms; assess shared similarities and differences between individuals, populations, and species; and come to see the world in new ways. Imageomics will accelerate and transform the biomedical, agricultural, and basic biological sciences as they seek to understand and control genes that relate to specific phenotypes and enable an overarching understanding of how the genome evolved in tandem with the organismal phenome. Because traits are the essential links between genes and the environment, using ML to help characterize them will lead to emergent understandings of how they function.  Harnessing the insights that arise from these new visualizations will stimulate the use of new genetic technologies, such as CRISPR gene editing, and more nuanced ecological practices, such as modified land use schemes that emerge from better understanding the connections between individual decision-making within species and their impact on their population dynamics.  With the emergence of new and better targeted practices that generate fewer unintended consequences, the new linkages resulting from a better understanding of traits and their consequences will bolster the nation?s bioeconomy. In addition, by leveraging and expanding existing diverse, inclusive and intellectually wide-ranging collaborative networks, the Institute will also educate the next generation of scientists and engage the broader public in scientific inquiry and knowledge discovery so that Imageomics can transform and democratize science for public good.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR).  This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Directorate for Biological Sciences, and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029306","IRNC Core Improvement: Accelerating Scientific Discovery & Increasing Access - Enhancing & Extending the Pacific Wave Exchange Fabric","OAC","International Res Ret Connect","10/01/2020","10/14/2020","Louis Fox","CA","Corporation for Education Network Initiatives in California","Continuing Grant","Kevin Thompson","09/30/2025","$3,124,565.00","Thomas DeFanti, Howard Pfeffer, Ronald Johnson","lfox@cenic.org","16700 Valley View Ave, Suite 400","La Mirada","CA","906385830","7142203400","CSE","7369","","$0.00","Pacific Wave (PacWave) is a distributed, Research and Education (R&E)-focused, open Internet Exchange. It provides for very high-performance Internet connectivity among US Science and Engineering R&E institutions and their international partners and is critical infrastructure for high-performance access to internationally supported instruments and large-scale data sources and repositories. PacWave enables large-scale scientific workflows to accelerate discovery in all areas of science and engineering, including high-energy physics, earth sciences, astronomy and astrophysics, biology and biomedical engineering, as well as scalable visualization, virtual reality, machine learning, and artificial intelligence. Its strategic location on the US West Coast facilitates connection of Trans-Pacific undersea cables to US National and Regional Research Networks, such as Internet2, ESnet, and others. In collaboration with Internet2 and other US-based R&E Exchanges (StarLight, Atlantic Wave, MANLAN, and WIX), the US Science and Engineering community is also connected to Europe, Central and South America, Africa, and beyond, thereby supporting a truly global high-performance research platform.<br/><br/>The operation, expansion, and technology of PacWave with primary points of presence in Los Angeles, Sunnyvale, and Seattle will be enhanced over the next 5 years, with advanced measurement, monitoring, and analytic tools. Performance data gathered from passive and active measurements will help characterize network traffic, monitor incident pathology and provide alerting messages and visualizations. PacWave?s enhancements aim to meet demands for higher data rates (400 Gbps, 800 Gbps, and beyond), making possible exploration and adoption of newly emerging technologies for improved performance, security, measurement, and monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003720","CDS&E: Collaborative Research: Hierarchical Kernel Matrices for Scientific and Data Applications","OAC","CDS&E-MSS, CDS&E","10/01/2020","10/15/2020","Yuanzhe Xi","GA","Emory University","Standard Grant","Tevfik Kosar","09/30/2023","$305,767.00","","yxi26@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","8069, 8084","8084, 9263","$0.00","Kernel matrices in machine learning and scientific computing describe the relationships between collections of points which may represent various types of information.  The increasing size of data sets in various disciplines and the increasing computational capability of computer hardware make it essential that our algorithms and software for kernel matrices are scalable, and that the time it takes for their execution grows linearly or close to linearly, with the problem size. Otherwise, such large-scale data problems may not be tractable.  This project addresses the scaling bottlenecks associated with handling the kernel matrix by exploiting a hierarchical structure that is often found in these matrices.  By accelerating computations with kernel matrices, this research enables large-scale data analysis and scientific simulation in diverse areas such as uncertainty quantification, integral equation problems, particle simulations, and geostatistics.  High-performance software implementing the newly developed methods will be developed in an open-source environment.<br/><br/>This project specifically addresses high-dimensional problems, the use of specialized kernel functions in machine learning, and the high initial computational cost of constructing a hierarchical representation for a kernel matrix.  New methods developed will be applied to large-scale cases in a scientific application and a machine learning application: Brownian dynamics and Gaussian process regression.  In machine learning, the new methods will complement existing large-scale approaches for Gaussian processes.  High-performance software will address specific scaling challenges in constructing hierarchical matrices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939249","HDR: DIRSE-IL:  Collaborative Research: Harnessing data advances in systems biology to design a biological 3D printer: the synthetic coral","OAC","","10/01/2019","10/15/2020","Jinkyu (JK) Yang","WA","University of Washington","Continuing Grant","Sylvia Spengler","09/30/2022","$358,816.00","","jkyang@aa.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","099y","062Z","$0.00","Corals are important natural resources that are key to the ocean's vast biodiversity and provide economic, cultural, and scientific benefits. As a result of human activities, locally and globally, coral reefs are declining rapidly. The complexity of corals makes conserving and restoring reefs very challenging. Corals are made up of thousands of different organisms, including the animal host and the algae, bacteria, viruses, and fungi that coexist as a so-called holobiont. Thus, corals are more like cities than individual animals, as they provide factories, housing, restaurants, nurseries, and more for an entire ecosystem. This project brings together experts in computer science, materials science, and biology to harness the data revolution in biology with machine learning to study how corals grow and function, when viewed as if they were manufacturing sites in the ocean. The study will focus on three key coral capabilities: (1) they create calcium carbonate skeletons that provide 3D structures for diverse sea life to live in, (2) they can heal damage to their tissues, and, (3) they live with the other organisms in a process referred to as symbiosis. Through these remarkable abilities, corals can 'print' resources for themselves and hundreds of thousands of other species, just like a 3D printer. The goal of this project is to understand these processes well enough to control them in the lab. This project may allow finding new ways to help coral survival, by deciphering the reasons why certain conditions damage them and find ways of repairing them. Furthermore, by synthetically growing corals, new types of materials may be identified for manufacturing. This project offers an opportunity to educate a diverse scientific workforce and the public by creating and disseminating the outcomes of a convergent research environment and will train postdoctoral researchers, graduate, and undergraduate students. Results of this research will be made available to the broader scientific community through web interfaces, peer-reviewed publications and workshops/conferences and shared with the public through outreach activities online, at schools, and public aquariums.<br/>    <br/>Through convergence of three disciplines, computer science, material science and biology, this project will provide a data-driven framework and toolset to learn from, control, engineer, and manufacture a combined form of living material, the 'synthetic coral', thereby opening new avenues for material synthesis and manufacturing. The research methodology will offer new analytical approaches to identify and quantify the parameters that govern coral growth and foster innovative new tools for controlling their growth. To understand the key functions of coral biology of biomineralization, wound healing, and symbiosis, this research will : (1) harness and analyze large amounts of coral '-omics' data to decipher critical molecules and their interactions for the aforementioned key functions, (2) experimentally validate the resulting predictions in coral individuals and cell lines, (3) manipulate the material properties of the calcium carbonate structures of the coral individuals and cell lines, and (4) test the biological and physical interactions in a network model of the 'synthetic coral'. This project develops and integrates fundamental building blocks that are essential for  an integrated computational and experimental validation system. Specifically, using machine learning, diverse data will be harnessed to identify physical conditions (e.g., surface characteristics), environmental conditions (e.g., temperature, pH), and key biological constituents (e.g., small molecule ligands and proteins encoded in the DNA) that are correlated to key structural and functional properties of the coral holobiont. These predicted conditions and molecules will be verified experimentally by perturbing individual coral nodes in a network of a 3D printed array of intact corals or their constituent cells and measuring their effects on the network of interactions and resulting structures. The results from this prediction-validation cycle will then be transferred back as input to manufacture novel adaptive materials fully embracing the organic/inorganic interface. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931523","Collaborative Research: Frameworks: Cyber Infrastructure for Shared Algorithmic and Experimental Research in Online Learning","OAC","ECR-EHR Core Research, Software Institutes","10/01/2019","06/25/2021","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Robert Beverly","09/30/2024","$1,891,611.00","Anthony Botelho, Korinn Ostrow, George Heineman","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7980, 8004","026Z, 075Z, 077Z, 7925","$0.00","This project, RAILKaM, will create new technology that will enable twenty researchers during the grant period to run large-scale field experiments where they study basic principles in education and educational psychology in the context of both K-12 mathematics learning and university Massive Online Open Courses (MOOCs). The experiments will be delivered through adaptive learning technology embedded in learning systems already being used by over 100,000 K-12 students and hundreds of thousands of MOOC learners each year. RAILKaM will also support 75 data scientists in conducting analyses on student data after the fact, using carefully redacted datasets that protect student privacy. In facilitating high-power, replicable experiments with diverse student populations and extensive measurement, this infrastructure increases the efficiency and ease of conducting high-quality educational research in online learning environments, bringing 21st-century research methods to education for the long-term betterment of learner outcomes.<br/><br/>This project, RAILKaM, will support researchers in more easily running scaled, highly instrumented studies on education and educational psychology, both in K-12 and university Massive Online Open Courses (MOOCs). RAILKaM will leverage ASSISTments, an online learning platform for middle school mathematics homework and classwork used by more than 100,000 students each year. In addition, RAILKaM will build functionality atop the ASSISTments platform so that educational experiments involving scaffolded problem-solving can be easily built into MOOC courses. ASSISTments will use open source APIs to integrate with MOOCs offered by the University of Pennsylvania, branching capacity for investigation to higher education while enabling richer student interactions and data collection than is typically feasible in MOOC courses. These capacities will enable researchers to run online field experiments to test interventions designed to increase student learning and engagement with a focus on how adaptive learning experiences can be optimized. These experiments will be augmented by rich data collection on learners, extending MOOC log data and ASSISTments data with several indicators of learning and engagement not previously available for research at scale. This project will develop the software infrastructure necessary to conduct experiments and collect enriched data, as well as the social infrastructure necessary to select and refine study ideas while maintaining instructor control over the activities that students experience. The combined software and social infrastructure will enable us to engage with researchers who are interested in these issues but who currently lack the infrastructure, technical capacity, or access to learners necessary to conduct high-powered or complex randomized controlled trials. This infrastructure will help these researchers to improve scientific understanding of the principles of human learning, providing a unique shared resource for learning scientists that will have considerable potential for broader impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1909379","OAC Core: Small: Enabling High-fidelity Turbulent Reacting-Flow Simulations through Advanced Algorithms, Code Acceleration, and High-order Methods for Extreme-scale Computing","OAC","OAC-Advanced Cyberinfrast Core","10/01/2019","06/10/2019","Matthias Ihme","CA","Stanford University","Standard Grant","Alan Sussman","09/30/2022","$500,000.00","Alexander Aiken","mihme@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","090Y","026Z, 9179","$0.00","Accurate numerical simulations of turbulent flows are of practical importance for several applications, including gas turbines and internal-combustion engines for power generation and transportation, the risk mitigation associate with reactor safety, and for scientific discovery of novel energy-conversion strategies. However, commonly employed software employ simplifications and exhibit deficiencies in accurately representing the underlying physical processes. The so-called discontinuous Galerkin (DG) methods have been identified as a promising alternative. These methods are characterized by utilizing a formulation that significantly improves fidelity. Other advantages are the flexibility in representation complex physical processes and the excellent performance on high-performance computing systems. While the potential of these DG-methods has been recognized, major roadblocks to adoption include the lack of suitable cyberinfrastructure (CI) methods and tools for scientific discovery and engineering analysis as well as the need for innovative programming techniques to enable scalable simulations on modern machines. This project addresses these research challenges and develops novel numerical methods and advanced programming paradigms for high-performance simulations of turbulent reacting flows. Integrated into this research are several education and outreach activities that address the need for training the next generation of interdisciplinary scientists and engineers. High-school students participate in several research activities, and a mentoring program is established that brings together students from engineering and computer science to work together on interdisciplinary research problems. This project, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense.<br/><br/>The long runtime costs of simulating turbulent flows inhibit explorations and studies of realistic flames and the engineering analysis of complex combustion geometries. The approach to improving the quality and performance of turbulent flow simulations is to use high-order discontinuous Galerkin (DG) methods backed by high-performance algorithmic implementations suitable for execution on heterogeneous compute platforms. The work specifically uses task level parallelism coupled with load-balancing and adaptive techniques to achieve high throughput simulation capabilities on heterogeneous hardware. Research on advanced CI-ecosystems is conducted to develop task-based programming techniques for accelerating multi-physics flow simulations on heterogeneous computing systems. To this end, Legion is employed for the dynamic runtime mapping of compute-intense kernel functions to heterogeneous processors under consideration of computational load, data complexity, and heterogeneity of the computing system. Novel integration schemes and advanced adaptation techniques are developed to enable efficient simulations of turbulent reacting flows. These techniques are incorporated into a multi-physics DG-method that is made available to the research community as an open-source software platform for scientific discovery and engineering analysis. The close collaboration of graduate students with national laboratories and industrial partners facilitates an effective transition of the numerical methods and programming techniques that are developed in this project into other software environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029218","Collaborative Research: IRNC: Testbed: BRIDGES - Binding Research Infrastructures for the Deployment of Global Experimental Science","OAC","International Res Ret Connect","10/01/2020","09/08/2021","Ciprian Popoviciu","NC","East Carolina University","Continuing Grant","Kevin Thompson","09/30/2023","$304,787.00","","popoviciuc18@ecu.edu","Office Research Administration","Greenville","NC","278581821","2523289530","CSE","7369","","$0.00","The BRIDGES project is developing an innovative, dynamic, high-performance trans-Atlantic network testbed that interconnects research communities and their resources in the US with collaborating partners and facilities in Europe. It explores advanced virtualized network architectures enabling rapidly reconfigurable global cyber-infrastructure to address changing research requirements of the collaborators to explore new concepts and to create new project workflows securely and with consistent, predictable performance.<br/><br/>The BRIDGES facility consists of two geographically separate optical links spanning the North Atlantic and terrestrial optical links in US and Europe to form an intercontinental optical network ring topology, capable of carrying up to 200 Gbps of science data between major nodes in Washington, New York, Paris, and Amsterdam. The BRIDGES facility is a binding platform for research projects, providing a flexible research-oriented infrastructure connecting laboratories and universities in the US to their counterparts in Europe. Science applications in high energy physics, deep space communications, and even biomedical programs are collaborating with BRIDGES to leverage them investment in this technology to reach digital resources such as globally distributed or remote sensors and instruments, distributed data storage, analytical processing resources, to create new global applications tailored to the science at hand. BRIDGES is led by George Mason University and East Carolina University and is working with and connecting an initial community of nearly 40 networking and science research projects across the US and Europe. The project is expected to have a far-reaching impact beyond the testbed or interconnect facility and lead to innovative use of cyber-infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854828","Operations & Maintenance for the Endless Frontier","OAC","Leadership-Class Computing","10/01/2019","09/02/2021","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","09/30/2024","$60,249,999.00","Dhabaleswar Panda, Omar Ghattas, Tommy Minyard, John West","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","026Z","$0.00","The Texas Advanced Computing Center (TACC) has been funded to acquire and deploy a system called Frontera, a powerful new advanced computational instrument for Science and Engineering (S&E) research. The name of the system is inspired by Vannevar Bush's paper ""The Endless Frontier"", which led to the establishment of the National Science Foundation in 1950. Bush argued that science was humanity's last frontier, leading to an endless stream of innovation and discovery that would improve the human condition. Today, computation is one of the most critical tools to advance more deeply into the Endless Frontier. Frontera provides a system of unprecedented scale in the NSF advanced computational portfolio that will yield productive science on day one, while also preparing the community for the shift to even more capable future systems. When deployed, the system will provide more than three times the performance of the current NSF leadership-class computing resource and enable S&E discoveries across a broad range of domains that were not possible otherwise. This project supports the Frontera acquisition plan with the operations and maintenance plan during the production life of the system.<br/><br/>The plan for Frontera operations and maintenance is anchored by an experienced team of partners and vendors with a community-leading track record of performance. The project will support the system through a comprehensive project plan, including first-class user support and training, education, outreach, documentation, data management, visualization, analytics-driven application support, and research collaboration. The plan also features a dedicated, broad-based science team, selected to ensure that S&E applications run effectively on a resource of this scale. Frontera will support big science in virtually all S&E disciplines through a dedicated leadership-class computing science team, deep computational science engagements of the Collaborative Support team, and the innovative use of containers.  Additionally, the Frontera system will support a broad range of software services, including Application Programmer Interfaces (APIs) to support an evolving user base that will make more extensive use of science gateways, automated workflows, and web services.  The project will develop new expertise and techniques for leadership-class computing and data-driven applications that benefit future users worldwide through publications, training, and consulting.   Furthermore, the Frontera project will serve as a bridge for the S&E community to future systems in 2024 and beyond.   The project will leverage the team's unique approach to education, outreach, and training activities to prepare new and current users for leadership-class computing and to encourage, educate, and develop the next generation of computational researchers. The team includes leaders in campus bridging and minority-serving institution (MSI) outreach, and will use data technologies that will increase the number and diversity of people using leadership-class computing for traditional and data-driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835892","Elements:Software:NSCI: Empowering Data-driven Discovery with a Provenance Collection, Management, and Analysis Software Infrastructure","OAC","Software Institutes","10/01/2018","09/06/2018","Yong Chen","TX","Texas Tech University","Standard Grant","Robert Beverly","09/30/2022","$599,982.00","William Hase, Brian Ancell, Dong Dai","yong.chen@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Scientific breakthroughs are increasingly powered by advanced computing and data analysis capabilities delivered by high performance computing (HPC) systems. In the meantime, many scientific problems have moved to a level of complexity that the ability of understanding the results, auditing how a result is generated, and reproducing the important experiments or simulation results, is critical to scientists. Enabling such a capability in HPC systems requires a holistic collection, management, and analysis software infrastructure for ""provenance"" data, the metadata that describes the history of a piece of data. Such a software infrastructure does not exist yet, which motivates the proposed software development of a lightweight provenance service. With such a software element, many advanced data management functionalities such as identifying the data sources, parameters, or assumptions behind a given result, auditing data history and usage, or understanding the detailed process that how different input data are transformed into outputs can be possible. Responding to the National Strategic Computing Initiative, this project will provide an attractive software infrastructure to future national HPC systems to improve the productivity of science in complex HPC simulation and analysis cycles. The project team will also recruit underrepresented students, mentor graduate and undergraduate students, integrate results into curriculum, and publish and disseminate results.<br/><br/>The lightweight provenance service software on HPC systems will provide: 1) an always-on, background service that automatically and transparently collects and manages provenance for scientific applications, 2) captures comprehensive provenance with accurate causality to support a wide range of use cases, and 3) provides easy-to-use analysis tools for scientists to quickly explore and utilize the provenance. This project will integrate the development, education, and outreach efforts tightly together.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2131987","Mid-scale RI-1 (M1:DP): Designing a global measurement infrastructure to improve Internet security","OAC","Mid-scale RI - Track 1, Special Projects - CNS, CYBERINFRASTRUCTURE, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2021","09/21/2021","Kimberly Claffy","CA","University of California-San Diego","Continuing Grant","Kevin Thompson","09/30/2024","$6,865,527.00","David Clark, Bradley Huffaker","kc@caida.org","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","108Y, 1714, 7231, 7359","8812, 9102","$0.00","While the Internet has become critical infrastructure permeating all aspects of modern society, its security and trustworthy character are subject to constant threats and attacks.  Any enterprise or service can have its traffic deflected to a masquerading site that attempts to mimic the legitimate site, steal user credentials, disrupt security, or defraud users.  The security of the Internet infrastructure is a high priority for the security research community, but that community is greatly hindered by a lack of relevant data.  The goal of this project is a validated design for a transformative infrastructure to support collection, curation, archiving, and expanded sharing of data needed to advance critical and stunted scientific research on the security, stability, and resilience of Internet infrastructure.<br/><br/>The measurement infrastructure design effort will focus on the Internet transport layer: routing, domain naming, addressing, and key management (the Certificate Authorities).  Community workshops and interactive prototyping efforts will enable evaluation of a proposed design of a measurement infrastructure to acquire and share such security-relevant data at a macroscopic scale.  Due to the complexity, volume, and sensitivity of resulting data, the design will cover many facets of data management: curation, post-processing analytics, privacy preserving data analysis frameworks, data discovery and accessibility, and normalized data sharing agreements. To support use of and feedback on prototypes, the PIs will develop an on-line course on Network Infrastructure Data Science, that will facilitate responsible (ethical, privacy-respecting) use of the datasets and analytics, and STEM/cybersecurity workforce training. This design project will enable the application of data-intensive methods to data from the global Internet infrastructure, overcoming a key barrier to scientific and engineering advances to navigate current and future Internet-related harms. The project will contribute to a broad range of disciplines that now depend on data about the Internet, including network science, socioeconomic studies, international relations, and political science. The resulting capabilities to support data acquisition, curation, and sharing will have an inherent equalizing effect on the research community. Moreover, if successful, this cyberinfrastructure will increase the trustworthiness of the Internet for U.S. citizens, and serve as a model for rest of the world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019135","CC* Team: Texas Education and Research Cybertraining Center (TERCC)","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","09/21/2021","Christopher Simmons","TX","University of Texas at Dallas","Continuing Grant","Kevin Thompson","06/30/2023","$1,399,502.00","Frank Feagans, Akbar Kara, Kendra Ketchum, Jeffery Neyland","simmons@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7231, 8080","","$0.00","The Texas Research and Education Cyberinfrastructure Services (TRECIS) center establishes a regional hub that advances collaborative support for research computing and expands the use of advanced cyberinfrastructure (CI) and expertise throughout the University of Texas system. It effectively integrates previously siloed CI and computing support across three UT campuses and the Lonestar Education and Research Network (LEARN) and builds a unique model of embedded<br/>collaborative facilitation with scientific projects. TRECIS trains postdoctoral researchers with domain-specific expertise in high performance computing and other CI-related services who are then available for consultations and support. TRECIS creates new career paths in research facilitation, broadens CI access for research and education and engages students and underrepresented groups, including historically black colleges, other minority institutions, and<br/>organizations that promote gender equality in STEM.<br/><br/>TRECIS builds upon the model of shared CI and support developed at UT Dallas, extends fast connectivity and networking to other campuses, provides outreach and develops a model of research facilitation through training and collaboration. It initially engages seven scientific projects from the fields of biomedicine, chemistry, geosciences, and others. Most are multi-institution collaborations and have multiple CI needs, including parallel / distributed computing, data modeling, and storage and transfer of data. TRECIS helps these projects and other users of research computing in allocating CI resources and developing effective workflows, algorithms and pipelines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940236","Collaborative Research: Precision Learning: Data-Driven Experimentation of Learning Theories using Internet-of-Videos","OAC","HDR-Harnessing the Data Revolu, IUSE, Discovery Research K-12","10/01/2019","03/18/2020","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Finbarr Sloane","09/30/2022","$708,316.00","","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","099Y, 1998, 7645","062Z, 7645, 9178","$0.00","This is a project to study what works to help students learn more effectively in the context of the ASSISTments system. ASSISTments is an online system that provides both assistance to students and real time assessment data to teachers. ASSISTments now supports 100,000 students who have completed more than 12 million mathematics problems. The system uses teacher input and artificial intelligence to provide assistance to students who are attempting to solve mathematics problems. This project will increase the assistance provided by the teacher and machine learning by incorporating video suggestions, such as those produced by the Kahn academy, targeted to the needs of the student. The experimentation will take content from three Open Educational Resource textbooks that are openly licensed and free to schools.<br/><br/>More specifically, the researchers will identify a large collection of videos that address mathematics skills in the textbooks and will extract features of these videos including language complexity, speaking rate, and other features. These videos and features will be checked by both teachers and through a Mechanical Turk process for usability before they are presented to students. Additionally, the project will develop a suite of novel technologies for precision learning including fine grained video feature extraction, student feature learning from heterogeneous raw data, causal modeling, and fairness aware and causal relationship enhanced optimized personalized recommendation. The research will advance theoretical understanding of fundamental issues related to personalized learning and will enable data-driven experimentation of learning theories. Causal modeling will enable the researchers to learn the features of video that are correlated with learning effectiveness. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity and is co-funded by the Division of Undergraduate Education and the Division of Research on Learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126281","CC* Integration-Large: Bringing Code to Data: A Collaborative  Approach to Democratizing Internet Data Science","OAC","CISE Research Resources","10/01/2021","09/16/2021","Ramakrishnan Durairajan","OR","University of Oregon Eugene","Standard Grant","Deepankar Medhi","09/30/2023","$988,548.00","Arpit Gupta, David Teach, Reza Rejaie","ram@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","2890","","$0.00","Successful application of machine learning (ML) for networking problems depends on the availability of high-quality labeled data from real-world networks. Equally critical is the ability to share these datasets, respecting the data owners' privacy concerns. Unfortunately, short of sharing the data via today?s commonly-applied data-to-code paradigm, researchers lack a systematic framework for working with or benefiting from data collected and curated by third parties. Consequently, Internet Data Science as practiced today is ill-suited for applications such as (i) high-quality data labeling, (ii) rigorous evaluation of research artifacts such as learning models, and (iii) independent validation/reproducibility of reported research findings.<br/><br/>This collaborative project brings together researchers from University of Oregon, University of California-Santa Barbara, and NIKSUN, Inc., and will investigate an innovative collaborative data labeling and knowledge sharing framework in three thrusts. First, the project will investigate a novel code-to-data approach that entails sharing of programmatic representations of operators' domain knowledge to identify events of interest in the data. Second, the project will design and develop a new learning framework to enable the pursuit of Internet Data Science as a full-fledged collaborative effort. Third, the project will illustrate the capabilities of the proposed framework in the context of collaborative efforts between two participating universities (UO and UCSB) and demonstrate its ability to scale to any number of participants.<br/><br/>The resulting framework will serve as a driving force for advancing collaborative efforts in the emerging area of Internet Data Science. In addition to identifying some of the fundamental changes to how ML ought to be used in networking, the research findings will benefit both industry and academia and will ensure that tomorrow's workforce has the proper training to fully exploit the application of ML for network-specific problems. Also, the outcomes will catalyze the development of a roadmap for the adoption of Internet Data Science efforts by operators and the deployment of ensuing research artifacts in real-world production networks.<br/><br/>This project will maintain the following webpage: https://onrg.gitlab.io/projects/emerge.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118201","HDR Institute: Institute for Data Driven Dynamical Design","OAC","TRIPODS Transdisciplinary Rese, HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT, PROJECTS","10/01/2021","09/15/2021","Eric Toberer","CO","Colorado School of Mines","Cooperative Agreement","Amy Walton","09/30/2026","$6,100,000.00","Steven Lopez, Alvitta Ottley, Adji Bousso Dieng, Ryan Adams","etoberer@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","041Y, 099Y, 1712, 1978","054Z, 060Z, 062Z, 094Z, 095Z, 8037, 8249, 8396, 8399, 8400, 8604, 9102, 9216, 9263","$0.00","From molecules to robots, designing for dynamics has common theoretical underpinnings despite differences in length and time scale.  However, such research is often overwhelmed by the high dimensional design space.  The Institute for Data-Driven Dynamical Design addresses the challenge of prediction of dynamical processes in materials, including ion and molecular transport, catalytic pathways, and phase transformations in metamaterials, with a focus on discovering fundamentally new mechanisms and pathways.  This research represents a paradigm shift from traditional material efforts involving incremental improvements in ground-state and steady-state properties.  Developments in the data sciences target (i) strategies for encoding complex structures and mechanistic pathways for machine intelligence, (ii) new predictive capabilities for evolving systems, and (iii) advances in visualization and integrating machine and human expertise.  Fueling these data science developments are large-scale simulations of dynamical processes across high dimensional design spaces. Experimental validation of these large-scale simulations addresses both end-product prediction and mechanistic pathways therein.  The Institute's data science innovations may advance fields both within and beyond STEM involving complex time-evolving systems including molecular biology, atmospheric science, geophysics, and physical cosmology.  The Institute seeks to grow and unite the dispersed data-driven design community.  Long-term growth is sought through outreach activities involving (i) high school coding schools, (ii) undergraduate involvement in data-rich research, and (iii) a post-baccalaureate bridge program that introduces students to data sciences and motivate them to pursue higher degrees.  Data-driven design community activities include (i) interdisciplinary summer schools and workshops, (ii) a Fellows program to collaboratively grow and disseminate the Institute?s developments, and (iii) dedicated efforts to create open-source software for the design community.  Throughout these efforts, the Institute actively seeks to recruit, retain, and graduate a diverse array of students in STEM.  <br/><br/>This virtual Institute seeks to design complex dynamical materials and structures through the union of machine and human intelligence. To learn dynamical behavior and ultimately discover new mechanisms, three core data science needs are addressed: (i) new representations and learning architectures that capture and encode the spatial arrangement, interactions, and temporal evolution of complex materials and geometrical structures, (ii) efficient exploration of high dimensional, time-dependent design spaces, and (iii) new visual analytics tools to quantitatively incorporate human-in-the-loop design feedback. Advances in each of these areas form a virtuous cycle that accelerates discovery of new materials, driven by new mechanisms. This Institute converges an interdisciplinary team focused on four design spaces at their `tipping point', where large quantities of dynamical data can be readily created: (i) crystalline solids with tailored ion transport for fuel cells and batteries, (ii) pressure-sensitive metamaterials for robotics, (iii) light driven catalytic reactions for chemical production, and (iv) synthesis and assembly of porous frameworks for chemical separations. These four areas are testbeds for cyberinfrastructure development for the broader scientific community. Interwoven throughout these activities are dedicated activities to build a new generation of STEM talent at the intersection of data science and the physical sciences/engineering and to broaden participation in STEM through targeted outreach.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR).  The award by the Office of Advanced Cyberinfrastructure is jointly supported by the Divisions of Chemistry, Materials Research, and Mathematical Sciences within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106461","Collaborative Research: OAC Core: Large-Scale Spatial Machine Learning for 3D Surface Topology in Hydrological Applications","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/01/2021","Da Yan","AL","University of Alabama at Birmingham","Standard Grant","Alan Sussman","09/30/2024","$238,838.00","","yanda@uab.edu","1720 2nd Avenue South","Birmingham","AL","352940111","2059345266","CSE","090Y","075Z, 079Z, 7923, 9150","$0.00","Rapid advances in sensing technology and computer simulation have generated vast amounts of 3D surface data in various scientific domains, from high-resolution geographic terrains to electrostatic surfaces of proteins. Analyzing such emerging 3D surface big data provides scientists an opportunity to study problems that were not possible before, such as mapping detailed surface water flow and distribution for the entire continental US. Despite its vast transformative potential, machine learning tools to analyze large volumes of 3D surface data are not readily available. The project aims to fill this gap by designing a novel parallel spatial machine learning framework for 3D surface topology and implementing the system in a distributed computing environment. The system can produce high-quality observation-based flood inundation maps derived from satellite images. In collaboration with federal agencies (e.g., U.S. Geological Survey, NOAA), the project will enhance situational awareness for flood disaster response and improve flood forecasting capabilities of the NOAA National Water Model by filling in the gap of lacking observations in model calibration and validation. The proposed software tools will be open-source to enhance the research infrastructure for the broad geoscience communities. Educational activities include curriculum development, mentoring a group of high school students in data science seminars at K-12 Summer Camps, and year-long projects for selected high school students in regional Science Fair competitions. <br/><br/>The project will transform spatial machine learning research by enhancing terrain awareness through modeling large-scale 3D surface topology. Specifically, the project will bring about the following cyberinfrastructure innovations. First, the project will design a topography-aware spatial probabilistic model called hidden Markov contour forest, which advances existing machine learning tools by incorporating physical constraints of heterogeneous 3D terrains into zonal tree structures in the model representation. Second, the project will investigate a parallel inference framework by decomposing both intra-zone dependency and inter-zone dependency. Finally, the project will implement the proposed parallel learning framework in a distributed computing environment by addressing challenges related to task partitioning, load balancing, and dynamic task scheduling. The proposed system will be deployed for real-world rapid flood disaster response and the validation and calibration of the National Water Model through collaboration with the U.S. Geological Survey and NOAA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115094","CICI: UCSS: Blockchain Based Assured Open Scientific Data Sharing and Governance","OAC","Cybersecurity Innovation","07/15/2021","08/23/2021","Murat Kantarcioglu","TX","University of Texas at Dallas","Standard Grant","Robert Beverly","06/30/2024","$499,692.00","Yulia Gel, David Lary","muratk@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","8027","7923, 8027, 9102","$0.00","Despite being one of the key driving forces behind scientific discovery and playing a critical role for the well-being of our society as a whole, scientific data sharing remains an issue fraught with security and integrity risks. This is because certain restrictions on data usage (e.g., maintaining appropriate audit logs for the data modifications) must be enforced rigorously, which is difficult to accomplish with existing technologies in a seamless manner.  The need to share scientific data while honoring usage restrictions, protecting data integrity, and capturing provenance for scientific repeatability requires development of new software tools that can support these capabilities across multiple organizations? boundaries. The novel blockchain based data sharing system built as a part of this project may result in advancing the knowledge base on how to automatically balance the conflicting goals of security and integrity requirements and information sharing in the context of open scientific discovery process. This system in return can significantly increase volume and variety of data shared for research purposes across a broad range of disciplines, thereby directly addressing the NSF Big Idea on Harnessing the Data Revolution, by further advancing capabilities of modern cyberinfrastructure and fostering novel opportunities for interdisciplinary data-intensive research. Furthermore, by actively incorporating our research results into the curriculum development, at both undergraduate and graduate levels, we foster the next generation of interdisciplinary scientists in a new era for open data.<br/><br/>To achieve the above stated goals, this project builds an innovative system where blockchain techniques are leveraged for assured scientific data sharing. First, the project develops a policy specification framework tailored for open science data sharing. Using this framework, scientists may easily specify any usage restriction (e.g., do not  re-identify  human  subjects,  do  not  modify  data,  notify  us  if  the  data  is  lost due  to  hacking).  Later on, using the blockchain framework, these restrictions will be captured,  and stored via smart contracts.  One novel aspect of the proposed framework is that, using machine learning techniques, it can analyze the smart contract based provenance information to automatically detect potentially anomalous data modifications. To test the capabilities of our framework,  the project uses a multi-sensor based data collection and data sharing project for understanding the impact of urban pollution as a case study.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104102","Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities","OAC","XC-Crosscutting Activities Pro, GEOINFORMATICS, Software Institutes, EarthCube","10/01/2021","09/13/2021","Gregory Tucker","CO","University of Colorado at Boulder","Standard Grant","Tevfik Kosar","09/30/2026","$2,562,303.00","Albert Kettner, Eric W Hutton, Irina Overeem, Julia Moriarty","gtucker@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7222, 7255, 8004, 8074","077Z, 7925, 8004, 9102","$0.00","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.<br/><br/>As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940181","Collaborative Research: Autonomous Computing Materials","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Takaki Komiyama","CA","University of California-San Diego","Continuing Grant","Daryl Hess","09/30/2022","$345,915.00","","tkomiyama@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","099Y, 1978","062Z, 9263","$0.00","The recent explosion in worldwide data together with the end of Moore's Law and the near-term limits of silicon-based data storage being reached are driving an urgent need for alternative forms of computing and data storage/retrieval platforms. In particular, exabyte-scale datasets are increasingly being generated by the biological sciences and engineering disciplines including genomics, transcriptomics, proteomics, metabolomics, and high-resolution imaging, as well as disparate other scientific fields including climate science, ecology, astronomy, oceanography, sociology, and meteorology, amongst others. In this data revolution, the continuously increasing size of these datasets requires a concomitant increase in available computational power to store, process, and harness them, which is driving a need for revolutionary new, alternative substrates for, and forms of, computing and data storage. Unlike traditional data storage and computing materials such as silicon, the human brain offers a remarkable ability to sense, store, retrieve, and compute information in a manner that is unrivaled by any human-made material. In this research project, analogous modes of information sensing, data storage, retrieval, and computation will be explored in non-traditional computing molecular systems and materials. The over-arching goal of the research is to discover revolutionary new modes of data storage/retrieval, sensing, and computation that rival conventional silicon-based technology, for deployment to benefit society broadly across all domains of data science. Graduate students and postdocs across five institutions will be trained and mentored in a highly interdisciplinary manner to attain this goal and prepare the next-generation of data scientists, chemists, physicists, and engineers to harness the ongoing data revolution. The research will be disseminated to a broad community through news outlets and integration of high school student internships in participating research laboratories. <br/><br/>Large-scale datasets from spatial-temporal calcium imaging of the mouse brain will be recorded into DNA-based, nanoparticle-based, and phononic 2D and 3D soft and hard materials. Continuous spatial-temporal data will first be transformed into discrete data for mapping onto DNA-conjugated fluorophore networks, dynamic barcoded nanoparticle networks, and phononic 2D and 3D materials. Sensing, computation, and data storage/retrieval will be demonstrated as proofs-of-principle in exploiting the chemical properties of molecular networks and materials to recover the encoded neuronal datasets and their sensing and computing processes. Success with any of these three prototypical materials would revolutionize the ability to encode arbitrarily complex, large-scale datasets into complex molecular systems, with the potential to scale across diverse data domains and materials frameworks. The investigators' Autonomous Computing Materials framework will thereby enable the encoding of arbitrary ""big data"" sets into diverse materials for data storage, sensing, and computing. This project maximizes opportunities for disruptive new computing and data science concepts to emerge from a multi-disciplinary, collaborative team spanning data science, neuroscience, materials science, chemistry, physics, and biological engineering. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1911229","OAC Core: Small: Scalable Graph Analytics on Emerging Cloud Infrastructure","OAC","OAC-Advanced Cyberinfrast Core","06/01/2019","05/20/2019","Viktor Prasanna","CA","University of Southern California","Standard Grant","Seung-Jong Park","05/31/2022","$481,837.00","Sanmukh Rao Kuppannagari","prasanna@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","090Y","026Z, 9179","$0.00","Graphs are powerful tools for representing real world networked data in a wide range of scientific and engineering domains. As examples, graphs are used to represent people and their interactions in social networks, or proteins and their functionality in biological networks, landmarks and roads in transportation networks, etc. Understanding graph properties and deriving hidden information by performing analytics on graphs at extreme scale is critical for the progress of science across multiple domains and solving real world impactful problems. Cloud platforms have been adopted to perform extreme scale graph analytics. This has led to exponential increase in the workloads while at the same time the rate of performance improvements of cloud platforms has slowed down. To address this, cloud platforms are being augmented with accelerators. However, the expertise required to realize high performance from such accelerator enhanced cloud platforms will limit their accessibility to the broader scientific and engineering community.  To address this issue, this project will research and develop a toolkit to provide Graph Analytics as a Service to enable researchers to easily perform extreme scale graph analytics workflows on accelerator enhanced cloud platforms. This will significantly increase the productivity of the researchers as i) the researchers will avoid the steep learning curve of developing parallel implementation of graph analytics algorithms, and ii) the increased size and scale of graph analytics will allow researchers to analyze significantly large datasets at reduced latency thereby enriching the quality of the domain research. Moreover, the techniques developed in this project will also be applicable for performing streaming graph analytics at the ""edge"" for applications such as autonomous vehicles, smart infrastructure, etc. The toolkit is expected to be used in many engineering and science disciplines including power systems engineering, network biology, preventive healthcare, smart infrastructure, etc. The research conducted in this project will also constitute materials appropriate for inclusion in graduate and undergraduate courses.<br/><br/>The project will research and develop high performance graph analytics algorithms and software for key graph workflows and kernels spanning multiple scientific and engineering domains. The target platform will be accelerator enhanced cloud platforms consisting of emerging node architectures comprising of multi-core processors, Field Programmable Gate Arrays (FPGAs) and high bandwidth memory (HBM) with cache coherent interface. An integrated optimization framework consisting of memory optimizations and partitioning and mapping techniques will be developed to exploit the heterogeneity of the target platforms. Specifically, techniques for optimal memory data layout and integrated optimizations for cloud execution will be developed to realize scalable performance in accelerator enhanced cloud platforms. The memory data layout optimization seeks to fully exploit the high bandwidth provided by HBM by ensuring data reuse for a broad class of graph analytics problems. The proposed software will ensure seamless parallel processing of the entire graph on a single heterogeneous node architecture as well as cloud platforms with multiple heterogeneous nodes. The integrated optimization framework will be developed into a scalable, deployable, robust Cyber Infrastructure (CI) toolkit to provide Graph Analytics as a Service (GAaaS). The framework will be developed using state-of-the-art heterogeneous platforms. By accelerating graph analytics workflows on cloud platforms, this project will enable researchers to perform extremely large-scale graph analytics workflows which are key components of many scientific and engineering domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2039142","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","06/17/2020","06/26/2020","Ritu Ritu","TX","University of Texas at San Antonio","Standard Grant","Seung-Jong Park","04/30/2021","$203,817.00","","ritu.arora@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8004","077Z, 7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1823385","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Von Welch","IN","Indiana University","Standard Grant","Bogdan Mihaila","04/30/2021","$74,410.00","","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOÕs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118099","Collaborative Research: CyberTraining: Implementation: Medium: Cyber Training on Materials Genome Innovation for Computational Software (CyberMAGICS)","OAC","CyberTraining - Training-based","09/01/2021","08/30/2021","Tao Wei","DC","Howard University","Standard Grant","Alan Sussman","08/31/2025","$400,000.00","Pratibha Dev","tao.wei@howard.edu","2400 Sixth Street N W","Washington","DC","200599000","2028064759","CSE","044Y","1594, 9102, 9179, 9219","$0.00","The computing landscape is evolving rapidly. Exascale computers can perform unprecedented mathematical operations per second, while quantum computers have surpassed the computing power of the fastest supercomputers. Concomitantly, artificial intelligence (AI) is transforming every aspect of science and engineering. To address these rapid changes and challenges, this project will train a new generation of materials cyberworkforce, who will solve challenging materials genome problems through innovative use of advanced cyberinfrastructure (CI) at the exa-quantum/AI nexus. Further, the project will foster the adoption of exa-quantum/AI nexus technologies by a broad research community and beyond through a unique dual-degree PhD/MS program, undergraduate research to close the research-education gap, and broadening participation of women and underrepresented groups.<br/><br/>This project will develop training modules for a new generation quantum materials simulator named AIQ-XMaS (AI and quantum-computing enabled exascale materials simulator), which integrates exa-scalable quantum, reactive and neural-network molecular dynamics simulations with unique AI and quantum-computing capabilities to study a wide range of materials and devices of high societal impact such as optoelectronics and pandemic preparedness. CyberMAGICS (cyber training on materials genome innovation for computational software) portal will be developed as a single-entry access point to all training modules that include step-by-step instructions in Jupyter notebooks and associated tutorial slides/videos, while providing online cloud service for those who do not have access to computing platform. The modules will be incorporated into the open-source AIQ-XMaS software suite as tutorial examples, and they will be piloted in classroom and workshop settings to directly train 1,200 CI users at the University of Southern California (USC) and Howard University, with a strong focus on underrepresented groups. Broader reach and training will be accomplished through the portal and nanoHUB. Students trained in the dual-degree program will earn a PhD in materials science or physics; they will also earn either an MS in computer science specialized in high-performance computing and simulations, MS in quantum information science, or MS in materials engineering with machine learning. Undergraduate students will be mentored and trained by academic scholars in multidisciplinary fields as well as by scientists at national labs and industry. The project will further broaden participation through USC?s Women in Science and Engineering (WiSE) program and undergraduate research by underrepresented groups jointly supervised by USC and Howard faculty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118061","Collaborative Research: CyberTraining: Implementation: Medium: Cyber Training on Materials Genome Innovation for Computational Software (CyberMAGICS)","OAC","CyberTraining - Training-based","09/01/2021","08/30/2021","Aiichiro Nakano","CA","University of Southern California","Standard Grant","Alan Sussman","08/31/2025","$600,000.00","Priya Vashishta, Ken-ichi Nomura","anakano@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","044Y","9102, 9179","$0.00","The computing landscape is evolving rapidly. Exascale computers can perform unprecedented mathematical operations per second, while quantum computers have surpassed the computing power of the fastest supercomputers. Concomitantly, artificial intelligence (AI) is transforming every aspect of science and engineering. To address these rapid changes and challenges, this project will train a new generation of materials cyberworkforce, who will solve challenging materials genome problems through innovative use of advanced cyberinfrastructure (CI) at the exa-quantum/AI nexus. Further, the project will foster the adoption of exa-quantum/AI nexus technologies by a broad research community and beyond through a unique dual-degree PhD/MS program, undergraduate research to close the research-education gap, and broadening participation of women and underrepresented groups.<br/><br/>This project will develop training modules for a new generation quantum materials simulator named AIQ-XMaS (AI and quantum-computing enabled exascale materials simulator), which integrates exa-scalable quantum, reactive and neural-network molecular dynamics simulations with unique AI and quantum-computing capabilities to study a wide range of materials and devices of high societal impact such as optoelectronics and pandemic preparedness. CyberMAGICS (cyber training on materials genome innovation for computational software) portal will be developed as a single-entry access point to all training modules that include step-by-step instructions in Jupyter notebooks and associated tutorial slides/videos, while providing online cloud service for those who do not have access to computing platform. The modules will be incorporated into the open-source AIQ-XMaS software suite as tutorial examples, and they will be piloted in classroom and workshop settings to directly train 1,200 CI users at the University of Southern California (USC) and Howard University, with a strong focus on underrepresented groups. Broader reach and training will be accomplished through the portal and nanoHUB. Students trained in the dual-degree program will earn a PhD in materials science or physics; they will also earn either an MS in computer science specialized in high-performance computing and simulations, MS in quantum information science, or MS in materials engineering with machine learning. Undergraduate students will be mentored and trained by academic scholars in multidisciplinary fields as well as by scientists at national labs and industry. The project will further broaden participation through USC?s Women in Science and Engineering (WiSE) program and undergraduate research by underrepresented groups jointly supervised by USC and Howard faculty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103508","Collaborative Research: Elements: Simulation-driven Evaluation of Cyberinfrastructure Systems","OAC","Software Institutes","08/01/2021","09/14/2021","Loic Pottier","CA","University of Southern California","Standard Grant","Tevfik Kosar","07/31/2024","$315,027.00","","lpottier@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8004","077Z, 7923, 8004, 9102","$0.00","Most scientific breakthroughs and discoveries are now preconditioned on performing complex processing of vast amounts of data as conveniently, reliably, and efficiently as possible. This requires high-end interconnected compute and storage resources, as well as software systems to automate the processing on these resources. An enormous amount of effort has been invested in producing such ""cyberinfrastructure"" software systems. And yet, developing and evolving these systems so that they are as efficient as possible, while anticipating future cyberinfrastructure opportunities and needs, is an open challenge.  This project transforms the way in which these systems are evaluated, so that their capabilities can be developed and evolved judiciously. The traditional evaluation approach is to observe executions of these systems on real-world hardware resources. Although seemingly natural, this approach suffers from many shortcomings. Instead, this project focuses on simulating these executions. Simulation has tremendous, and untapped, potential for transforming the development cycle of cyberinfrastructure systems. Specifically, this project produces software elements that can be easily integrated into existing and future systems to afford them with simulation capabilities.  These capabilities make it possible for developers to put their systems through the wringer and observe their behaviors for arbitrary operating conditions, including ones that go beyond current hardware platforms and scientific applications. Simply put, these capabilities will make it possible to establish a solid experimental science approach for the development of cyberinfrastructure systems that support current and future scientific endeavors that are critical to the development of our society.<br/><br/>The cyberinfrastructure has been the object of intensive research and development, resulting in a rich set of interoperable software systems that are used to support science. A key challenge is the development of systems that can execute application workloads efficiently, while anticipating future cyberinfrastructure opportunities and needs. This project aims to transform the way in which these systems are evaluated, so that their capabilities can be evolved based on a sound, quantitative experimental science approach. The traditional evaluation approach is to use full-fledged software stacks to execute application workloads on actual cyberinfrastructure deployments. Unfortunately, this approach suffers from several shortcomings: real-world experiments are time- and labor-intensive, and they are limited to currently available hardware and software configurations. An alternative to real-world experiments that does not suffer from these shortcomings is simulation, i.e., the implementation and use of a software artifact that models the functional and performance behaviors of software and hardware stacks of interest. This project uses simulation to transform the way in which cyberinfrastructure systems are evaluated as part of their long-term development cycles.  This is achieved via software elements for enhancing production cyberinfrastructure systems with simulation capabilities so as to enable quantitative evaluation of these systems for arbitrary execution scenarios. Creating these scenarios requires little labor, and executions can be simulated accurately and orders of magnitude faster than their real-world counterparts. Furthermore, simulations are perfectly reproducible and observable. While this approach is general, its effectiveness will be demonstrated by applying it to a number of production systems, namely, workflow management systems.  This project capitalizes on the years of development invested in the SimGrid and WRENCH simulation frameworks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925764","CC* Team: SWEETER -- SouthWest Expertise in Expanding, Training, Education and Research","OAC","Information Technology Researc, CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2019","10/15/2020","Dhruva Chakravorty","TX","Texas A&M University","Continuing Grant","Kevin Thompson","06/30/2022","$1,416,000.00","Diana Dugas, JoAnn Browning, Emily Hunt, Timothy Cockerill","chakravorty@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","1640, 7231, 8080","9251","$0.00","The efficacy of an  interdisciplinary research team is frequently limited by a researcher's ability to draw together a cohort of collaborators with needed scientific expertise.  SWEETER: South West Expertise in Expanding Training, Education and Research is a network of resources, both training and personnel, that collaborate and foster cooperation across the boundaries of disciplines and institutions.  SWEETER unites not-for-profits, community colleges, minority serving institutions, research-intensive universities, and industry from multiple states to develop this research network.  The collaboration leverages expertise in several facets of computational sciences to address long-standing issues encountered in sharing resources by employing a model of resource sharing where each institution is a provider and receiver of resources.<br/><br/>Research specialists from various sites will support domain scientists at site workshops, SWEETER annual events, and online avenues. To encourage these relationships, SWEETER offers a web-portal and shared cyberinfrastructure that will help remove resource barriers faced by community colleges and smaller institutions. SWEETER strengthens efforts for researcher preparation by offering remote and in-person training opportunities, and propagating elements of its learning environment to formal curricular efforts. Importantly, SWEETER employs an innovative evaluation strategy to assess its successes in the aspects of research, training and education. By fostering diverse partnerships developed through existing efforts, SWEETER will significantly broaden the involvement of traditionally underrepresented populations in computing at the K-12, undergraduate and graduate levels of learning. In all, SWEETER offers a flexible, portable and scalable framework to develop communities of practice that will inform the design of future research networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934584","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Pradeep Ravikumar","PA","Carnegie-Mellon University","Continuing Grant","Amy Walton","08/31/2022","$297,347.00","","pradeepr@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","099Y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032486","Frontera Travel Grant: Enhanced Atomistic Simulations for Predictive Multi-Scale Modeling Safety Pharmacology Pipeline","OAC","Leadership-Class Computing","09/01/2020","06/05/2020","Igor Vorobyov","CA","University of California-Davis","Standard Grant","Edward Walker","08/31/2022","$9,999.00","","ivorobyov@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835543","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","08/09/2018","Noah Fahlgren","MO","Donald Danforth Plant Science Center","Standard Grant","Amy Walton","08/31/2023","$592,999.00","","nfahlgren@danforthcenter.org","975 N. Warson Rd.","St. Louis","MO","631322918","3145871041","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829752","Collaborative Research: CyberTraining: CIU: Toward Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","06/11/2020","Prasun Dewan","NC","University of North Carolina at Chapel Hill","Standard Grant","Alan Sussman","08/31/2022","$471,286.00","Sreekalyani Bhamidi, Alison LaGarry","dewan@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","044Y","026Z, 062Z, 7361, 9179, 9251","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829724","CyberTraining: CIU: Preparing the Public Sector Research Workforce to Impact Communities through Data Science","OAC","CyberTraining - Training-based","09/01/2018","06/29/2018","Libby Hemphill","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Alan Sussman","08/31/2022","$498,778.00","Clifford Lampe, Lynette Hoelter, Christopher Brooks","libbyh@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","044Y","026Z, 062Z, 7361","$0.00","The ability to share data among researchers, citizens, government agencies, and  educators creates potential for new research collaborations with significant real-world impact. However, cities are often unprepared to use cyberinfrastructure to support research that would impact their citizens and communities, and researchers often do not have access to or awareness of the kinds of data and questions that are relevant for communities. This project develops innovative and scalable instructional materials, for both in-person and online courses, to increase data science literacy to meet the public sector's emerging needs for experts in computational and data science. The materials emphasize the types of data necessary for communities to make informed decisions (e.g., administrative data on land use, constituent service requests, and crime statistics) and applies them to pressing issues presented by community partners, providing a real-world context for learning. The project leverages the University of Michigan School of Information's Citizen Interaction Design program and the Summer Program in Quantitative Methods of Social Research at the Inter-university Consortium for Political and Social Research (ICPSR) to train undergraduate students, graduate students, and public sector researchers in collecting, extracting, cleaning, annotating, and analyzing data generated and used by government organizations. The project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; by enabling cities to conduct research that improves their communities and making the instructional materials and data used in the courses available for use by other interested educators, communities, and citizens. <br/><br/>The project addresses bottlenecks in scientific and engineering research workforce development by developing and offering three instructional activities: (a) a project-based course in which students work directly with Michigan communities to design cyberinfrastructure tools, and (b) two massive, open online courses (MOOCs) in which students learn the fundamentals of data science for work in the public sector. These courses provide scalable training and education programs to increase cyberinfrastructure-enabled research in the public sector that leverages administrative data. Course development occurs in collaboration between educators and community partners in cities throughout the Midwest, in order to ensure diversity of topics and audiences, timeliness, relevance, and direct application. This reliance on real data and immediate application to community issues is a novel approach in data science instruction. Despite being offered virtually, the MOOCs retain the pedagogical benefits of working on meaningful projects with real stakeholders. All three courses will be offered and evaluated at least once over the course of the project, and the course materials will be made publicly available through the University of Michigan?s School of Information, ICPSR, and other forums (e.g., the University's institutional repository, Deep Blue) for maximum impact. The long-term goals are to broaden engagement between researchers and communities to leverage advanced cyberinfrastructure to support public sector STEM research and to contribute to the infrastructure for online, dynamic, personalized lessons and certifications. Through partnership with the Midwest Big Data Hub, the project ensures that communities in the Midwest have access to resources for workforce development and opportunities to refine educational materials to serve their specific needs now and in the future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029176","Collaborative Research: IRNC: Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","09/01/2020","09/14/2021","Robert Gardner","IL","University of Chicago","Continuing Grant","Kevin Thompson","08/31/2023","$180,000.00","","rwg@hep.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7369","","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019136","CC* CIRA: Building Research Innovation at Community Colleges","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","Dhruva Chakravorty","TX","Texas A&M University","Continuing Grant","Kevin Thompson","06/30/2022","$250,000.00","Honggao Liu, Sarah Janes, Timothy Cockerill","chakravorty@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","7231, 8080","","$0.00","Two-year colleges and smaller institutions of higher education play an important role in shaping the country's economic and computing workforce.  Burgeoning interest in fields such as cloud computing and smart manufacturing have paved the path for adoption of advanced cyberinfrastructure practices in research and academic pursuits at these institutions.  Expanding on the collective experience of groups engaged in various aspects of campus computing, the Building Research Innovation at Community Colleges (BRICCs) approach examines the research and educational needs from advanced cyberinfrastructure in such institutional settings. Led by CyberTeams, BRICCs offers an inclusive platform to develop and extend efforts in this space to the national level.  <br/><br/>BRICCs is a unique opportunity to study campus computing characteristics at smaller institutions and community colleges. BRICCs adopts a multi-pronged approach that focuses on learning about the problems at hand, partnering with institutions to enable solutions, and finally communicating its findings to the broader research community. Fostering partnerships between knowledgeable cyberinfrastructure professionals and these institutions is a critical aspect of BRICCs. Toward achieving this, BRICCs will host virtual and in-person community workshops to explore avenues to broaden the impact of advanced cyberinfrastructure on campus computing at all levels. BRICCs will produce learning resources, workshop reports, support future funding efforts, and propose campus networking models that align with the  research and academic pursuits of smaller institutions. As a collective of computing expertise, BRICCs serve as a collaborative space to address similar challenges in advancing cyberinfrastructure adoption in research and educational settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031650","Direct Numerical Simulation and Analysis of Turbulent Pipe Flow at High Reynolds Numbers","OAC","Leadership-Class Computing","09/01/2020","05/06/2020","Fazle Hussain","TX","Texas Tech University","Standard Grant","Edward Walker","08/31/2022","$9,117.00","Jie YAO","fazle.hussain@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842054","EAGER: GreenDataFlow: Minimizing the Energy Footprint of Global Data Movement","OAC","Data Cyberinfrastructure, Campus Cyberinfrastructure","09/01/2018","04/14/2020","Tevfik Kosar","NY","SUNY at Buffalo","Standard Grant","Amy Walton","08/31/2022","$407,595.00","","tkosar@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7726, 8080","062Z, 7916, 9251","$0.00","This project fills an important gap in the understanding of data transfer energy efficiency.  The models, algorithms and tools developed as part of this project will help increase performance and decrease power consumption during end-to-end data transfers, which should save significant quantities of resources (estimated to be gigawatt-hours of energy and millions of dollars in the US economy alone).  The applicability and efficiency of these novel techniques will be evaluated in actual applications, in a collaborative partnership with IBM.<br/><br/>The project explores options for minimizing the energy-use footprint of global data movement. The effort is focused on saving energy at the end systems (sender and receiver nodes) during data transfer.  It explores a novel approach to achieving low-energy end-to-end data transfers, through application-layer energy-aware throughput optimization. The research team investigates and analyzes the factors that affect performance and energy consumption in end-to-end data transfers, such as CPU frequency scaling, multi-core scheduling, I/O block size, TCP buffer size, and the level of parallelism, concurrency, and pipelining, along with the data transfer rates at the network routers, switches, and hubs.   How these parameters decrease energy consumption in the end systems and networking infrastructure, without sacrificing transfer performance, are assessed.  The project will create novel application-layer models, algorithms, and tools for:<br/> - predicting the best combination of end-system and protocol parameters for optimal data transfer throughput with energy-efficiency constraints; <br/> - accurately predicting the network device power consumption due to increased data transfer rate on the active links, and dynamic readjustment of the transfer rate to balance the energy performance ratio; and <br/> - providing service level agreement (SLA) based energy-efficient transfer algorithms to service providers. <br/>The models, algorithms and tools developed as part of this project will help increase performance and decrease power consumption during end-to-end data transfers, saving significant quantities of resources.  Since the tools focus on the application layer, they will not require changes to the existing infrastructure, nor to the low-level networking stack, and wide deployment of the developed system should be readily attainable.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827210","CC* Network Design and Implementation for Small Institutions: Rural Campus Connectivity for Research and Teaching on the Prairie","OAC","Campus Cyberinfrastructure","07/01/2018","06/25/2018","Channon Visscher","IA","Dordt University, Incorporated","Standard Grant","Kevin Thompson","12/31/2020","$362,589.00","Kari Sandouka, Nathan Tintle, Nick Breems, Brian Van Donselaar","cvisscher@spacescience.org","498 4th Avenue NE","Sioux Center","IA","512501606","7127226339","CSE","8080","","$0.00","This project provides cyberinfrastructure upgrades at Dordt College for an Internet2 connection and enhanced network connectivity between campus locations, data sources, and computing resources. Through the application of data-intensive multi-omics and biostatistics research (such as high-throughput genotyping and phenotyping) at agricultural and prairie sites, these upgrades are designed to equip ongoing research to better understand, first hand, land-use impacts on local ecology and microbiome, prairie genetics, soil erosion, nutrient uptake and loss, carbon sequestration, drought susceptibility, and water quality. The Rural Campus Connectivity for Research and Teaching on the Prairie project also connects Dordt College to national research and education resources and enhances its STEM education impact by providing cutting-edge research, teaching, and outreach opportunities through collaborative, interdisciplinary, and data-driven science for undergraduate students and STEM teachers-in-training.<br/> <br/>The primary goal of this cyberinfrastructure project is the installation and connection of a fiber optic line from the rurally-situated Dordt College campus to an Internet2 connection point. Campus network upgrades also include the development of a Science DMZ and associated services such as Federated ID Management, data transfer management, performance monitoring through the implementation of a PerfSONAR measurement node, and wireless connectivity to a rural satellite campus. A new network switch will be used as the hub of the 10Gbps DMZ to connect local storage, compute clusters and instrumentation to the existing campus network and Internet2.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934759","Collaborative Research: Near Term Forecasts of Global Plant Distribution, Community Structure, and Ecosystem Function","OAC","HDR-Harnessing the Data Revolu","09/01/2019","10/15/2020","Amy Frazier","AZ","Arizona State University","Continuing Grant","Peter McCartney","08/31/2022","$294,356.00","","Amy.Frazier@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","099Y","062Z","$0.00","This project is the first to explore how plant species distributions across the entire globe may respond to global change. The project brings together ecologists, environmental engineers, data scientists, and conservation stakeholders to determine optimal ways to integrate these data sources to make near term forecasts for all plants globally by addressing changes in (1) species' abundance and geographic distribution, (2) community structure, and (3) ecosystem function. This three-pronged approach is designed to span a range of approaches to understand the spectrum of possible futures consistent with current knowledge while integrating knowledge across scales of biological organization. These forecasts will be used along with input from conservation stakeholders to assess how differing conservation decisions can minimize the impacts of global change responses. An ultimate goal of the project is to automate a pipeline to ingest new incoming data, update forecasts, and serve these to end-users to enable a near-real time forecasting workflow to provide best-available predictions at any given time to inform conservation decisions. <br/><br/>A key aspect of these forecasts is their reliance on novel environmental information that better characterize the conditions that influence plant performance, including soil moisture and extreme weather events based on NASA satellite observations. These species-level predictions will be linked to community demography models that integrate a variety of relatively untapped data sources for understanding global change, including plant trait data, community plot data across the globe, highly detailed plot data from National Ecological Observatory Network (NEON) and Long Term Ecological Research (LTER) sites, and global biomass data from NASA's Global Ecosystem Dynamics Investigation (GEDI) mission. By integrating this wide variety of data sources, the mechanistic understanding needed to make robust near term forecasts can be made, to understand ecosystem properties like Net Primary productivity, Carbon stock, and resilience. Based on workshops with conservation stakeholders, researchers will determine how best to use this unique suite of forecasts to best inform different conservation questions in different regions of the world. The project will also result in an open, cleaned and curated database on global plant distributions. This will aid others in exploring data and predictions by delivering and visualizing complex future scenarios in an easy to use portal. All results of the project can be found at the website for the Biodiversity Informatics and Forecasting Institute or BIFI, at https://enquistlab.github.io/BIFI .<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924117","CyberTraining: Implementation: Small: Collaborative Research: Easy-Med: Interdisciplinary Training in Security, Privacy-Assured Internet of Medical Things","OAC","CyberTraining - Training-based, CYBERCORPS: SCHLAR FOR SER","09/01/2019","05/27/2021","Prabha Sundaravadivel","TX","University of Texas at Tyler","Standard Grant","Alan Sussman","08/31/2022","$248,597.00","Premananda Indic, Jimi Francis","psundaravadivel@uttyler.edu","3900 University Boulevard","Tyler","TX","757990001","9035655670","CSE","044Y, 1668","026Z","$0.00","The combination of a network of physical devices embedded with electronics, Internet connectivity, and other hardware, such as sensors, that can communicate and interact with others over the Internet is known as the Internet of Things.  One familiar application of this technology is the ""smart home"" in which people monitor and control their lights, thermostats and security systems remotely using smart phones or smart speakers. An Internet of Things-based framework for the healthcare industry is called the Internet of Medical Things.  This technology connects patients to their physicians and supports the transfer of medical data over the Internet.  Concerns about the privacy of data transmitted over the Internet and network security are challenges facing all applications of the Internet of Things concept, but they can be exacerbated by the knowledge gap between the designers of the frameworks and the end users in the medical field.  As the healthcare industry grows to meet the needs of an aging population, the workforce that designs Internet of Medical Things devices and the networks that connect them must be ready to address these privacy and security concerns. The project is addressing this gap, and thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>Easy-Med is a multi-disciplinary training program designed to improve core literacy of cyber infrastructure for students at the undergraduate level in northeast Texas. The six-week-long mentored program provides immersive training to increase the students' ability to develop and use secure, privacy-assured sensing healthcare frameworks.  A different training module is provided each week and each day of the module includes four hours of lecture and three hours of hands-on lab exercises.  Training modules introduce the students to the different aspects involved in designing devices and networks for the Internet of Medical Things.  The six training modules, components of which are also provided online, include: 1) types of biosensors and Internet of Medical Things components, 2) system-level modeling of Internet of Medical Things networks, 3) signal and data analytics used in healthcare, 4) security and privacy assurance in Internet of Medical Things technology, 5) applications of biosensors in the healthcare industry, and 6) use of and ethics involved in using the Internet of Medical Things in the community setting.  Students who participate in Easy-Med during the summer are encouraged to further their knowledge and provide outreach about the program by participating in a Build-a-Thon activity during the following fall semester and a research symposium in the following spring semester.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829554","CyberTraining: CIC: Widening the CI Workforce On-ramp by Exposing Undergraduates to Heterogeneous Computing","OAC","CyberTraining - Training-based","09/01/2018","08/24/2021","David Bunde","IL","Knox College","Standard Grant","Alan Sussman","08/31/2022","$132,026.00","","dbunde@knox.edu","2 East South Street","Galesburg","IL","614014999","3093358860","CSE","044Y","026Z, 062Z, 7361, 9229","$0.00","In keeping with NSF's mission of promoting scientific progress, this project strengthens the workforce of future cyberinfrastructure researchers and professionals by preparing undergraduate students to program and employ heterogeneous computing systems. The need for increased performance per watt and demands of processing diverse workloads have triggered a major industry shift towards systems containing different kinds of specialized components. These heterogeneous architectures are quickly becoming the dominant platform in high-performance computing (HPC), cloud computing, and Internet of Things (IoT) networks. As such, it is imperative that the scientific workforce in advanced cyberinfrastructure have a deep understanding of heterogeneity in computing systems. Current undergraduate computer science curricula lack sufficient coverage of heterogeneous computing concepts and there is an imminent risk that tomorrow's scientific workforce will be ill-equipped to program the complex heterogeneous systems of the future.  This project addresses this by developing and disseminating modules covering heterogeneous computing concepts. Module development is complemented with out-of-class training camps that will serve as a springboard for undergraduates to pursue further training and career opportunities.  <br/><br/>The project takes an early-and-often, module-driven approach to curricular integration. A collection of modules is being designed to cover a range of heterogeneous computing concepts including heterogeneous architectures, hybrid algorithms, and heterogeneous programming models such as CUDA and OpenCL. For easy adoption, modules are designed to be self-contained with all requisite teaching material including in-class interactive exercises, problem sets and solutions, and pedagogical notes.  Each module is designed to exploit heterogeneous context within the curriculum and introduce topics at an appropriate level of abstraction. The summer training camps are designed to be immersive experiences for undergraduates that (i) reinforce and extend the material covered in the modules, (ii) present the role of cyberinfrastructure in advancing scientific research, and (iii) expose students to opportunities in the fields of computational and data science.  A key component of this project is forging alliances with industry partners, who are engaged throughout the design process.  A detailed evaluation plan is in place to evaluate student learning outcomes and engagement.  Both internal and external evaluators are involved in carrying out the evaluation plan.  The modules and training activities are expected to impact over 1,100 computer science undergraduates enrolled at Texas State, Knox College, and Concordia University Texas.  Extensive dissemination efforts through faculty training workshops are expected to further broaden the impact of this work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2139112","NSF Student Travel Grant for IEEE Cluster 2021","OAC","EDUCATION AND WORKFORCE","09/01/2021","08/30/2021","Sunita Chandrasekaran","DE","University of Delaware","Standard Grant","Alan Sussman","09/30/2022","$3,750.00","","schandra@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7361","026Z, 7556, 9150, 9179","$0.00","The Cluster 2021 Conference is a premier conference in the field of cluster computing and in the use of cluster systems for scientific and commercial applications.  The conference brings together researchers, developers and users from academia, industry, laboratories, and commerce to discuss recent advances and trends in many areas related to cluster computing, including applications, architectures, programming, data and visualization.  The conference will be held online from September 7-10, 2021.  The project will fund students to participate in the conference, to engage deeply with the Cluster research community.  The Cluster student participation program provides a comprehensive means for students to improve their overall research skills and career planning efforts, beyond presenting posters and papers.  The project serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Due to Covid-19 safety issues, the conference will be fully virtual. The student program committee for the conference is recruiting students with an emphasis on diversity and inclusion of underrepresented groups and from a diverse set of institutions.  The conference will run specific student-centric sessions during the conference, including sessions on research presentation training, research career guidance, industry interaction, and a meeting/discussion with the authors of the best paper candidates from the conference.  While the project will enable funding a significant fraction of the students attending the conference, some students are presenting papers and posters at the conference and will be given priority for funding.  The funding provided by NSF will have a significant impact on the careers of the future generation of researchers in cluster computing, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103958","Elements: Cyberinfrastructure for spin and charge transport calculation of partially disordered alloys","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2021","08/30/2021","Michael Widom","PA","Carnegie-Mellon University","Standard Grant","Alan Sussman","08/31/2024","$482,252.00","Yang Wang","widom@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1712, 8004","054Z, 077Z, 6863, 7923, 8004, 9216","$0.00","Metallic alloys are ubiquitous in high technology, in industry, and even in our households. Alloys, which form when two or more chemical species are combined to create a single metallic phase, offer the chance to improve upon the properties of pure metallic elements. Some of the properties that can be altered through alloying include mechanical strength, magnetism, melting temperature, and oxidation resistance. This project focuses on the property of electrical conductivity, by developing computer codes to evaluate the quantum mechanical scattering of electrons off of atoms. To accurately predict the scattering we must know where the atoms are located, and in an alloy that means understanding how the different chemical species arrange in space. Often these arrangements are random, but even if the elements are randomly distributed, there will be correlations in the positions of certain species relative to others as a result of chemical bonding preferences. The code will contain features that enable it to predict these correlations and reveal how the correlations influence the conductivity. In addition to developing computer codes, this project will develop a user base of scientists interested and able to run the code and to contribute to its further development. Outreach to high school students and their teachers will enhance the pipeline of prospective scientists. Inclusion of scattering theory in college and graduate level courses taught by the PIs will prepare Physics and Materials Science students to understand and apply the codes. Presentations at scientific society conferences will inform the existing community of the capabilities, while workshops and webinars and webinars will provide specific training for active users. <br/><br/>Electronic density functional theory (DFT) has flourished as a practical tool for calculating energies, forces and electronic structure; its use is now widespread both in basic science and in engineering. Charge and spin transport calculation is a capability that has not yet reached the broader user community, partly because codes that incorporate these effects are not widely available and partly because these properties are highly sensitive to the degree of crystalline order. Basic knowledge of the degree of order is often lacking, as it can be temperature dependent, and thermal effects are not captured by most DFT codes. To address this need, a code will be developed that is easy to use (capable of running on a desktop computer), that can predict the degree of chemical order or disorder as a function of temperature, and that can calculate the resulting charge and spin conductivities. This will be achieved by building upon innovations in electronic structure calculation, coupled with methods of statistical mechanics to address thermal disorder. Specifically, we will modify the Coherent Potential Approximation (CPA) to incorporate the effects of short range order by unifying the resulting total energies with the Cluster Variation Method (CVM) to predict temperature dependent disorder. The modified CPA will express the total energy as a function of interatomic correlation functions, while the CVM will express the entropy in the same terms, allowing the determination of correlations that balance the energy against the entropy. This approach to density functional theory employs multiple scattering as implemented in the public domain code MuST. This method determines the electronic Green?s functions, and consequently it integrates naturally with the Kubo and Greenwood formulas for charge and spin conductivity. This internally consistent combination of approximations will achieve both high accuracy and high performance. <br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103708","CRII:OAC: Machine Learning- Enhanced Multiscale Simulation of Fiber Composites","OAC","CRII CISE Research Initiation","09/01/2021","08/30/2021","Ramin Bostanabad","CA","University of California-Irvine","Standard Grant","Alan Sussman","08/31/2023","$175,000.00","","Raminb@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","026Y","079Z, 8228","$0.00","Many engineered materials such as fiber composites have a hierarchical structure that spans multiple length scales. The analysis and design of these materials rely on multiscale simulations whose computational costs significantly increase if the structure is large and if the material deformation depends on its loading history. These high costs prohibit computationally intensive studies such as uncertainty propagation and design optimization. To tackle this challenge, the project employs recent advances in high performance computing and machine learning to accelerate multiscale simulations by orders of magnitude without compromising accuracy. The developed methods and tools are applicable to many materials systems and the testbed on fiber composites benefits a wide range of academic and industrial efforts since these materials are heavily used in, for example, the automobile and aerospace industries.<br/><br/>This work develops cyberinfrastructure foundations that will enable acceleration of multiscale simulations while (1) minimizing the information loss incurred in inter-scale communication, and (2) considering various uncertainty sources such as spatial variation of microstructural properties and morphologies. The project builds mechanistic machine learning (ML) models that emulate complex and history-dependent microstructural deformations that embody a broad range of nanoscale and mesoscale effects to ensure transferability. The ML models are integrated with a message passing interface (MPI) design that leverages the hierarchical nature of the multiscale simulation to achieve two-level parallelism, both within and across the computational nodes of a compute cluster. The message passing is employed to manage inter-scale and intra-scale data transfer during multiscale simulation of a light-weight fiber composite whose microstructures spatially vary due to manufacturing uncertainties. The simulations on composite materials aim to increase understanding of how their properties are affected by inter- and intra- microstructural uncertainties, constituent properties, deformation history, and microstructure morphology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2050534","REU Site: Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM).","OAC","RSCH EXPER FOR UNDERGRAD SITES","09/01/2021","08/30/2021","Kwai Wong","TN","University of Tennessee Knoxville","Standard Grant","Alan Sussman","08/31/2024","$404,779.00","Stanimire Tomov","kwong@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1139","026Z, 9250","$0.00","The Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM) REU site program at the University of Tennessee (UTK) engages a group of ten undergraduate students each year to explore emerging interdisciplinary computational science models and techniques via a variety of compute-intensive and data-driven applications. The RECSEM program complements the growing importance of computational sciences in many advanced degree programs and provides scientific understanding and discovery to undergraduates with an intellectual focus on research projects using high performance computing (HPC). This program aims to deliver a real-world research experience to the students by partnering with teams of scientists engaged in scientific computing research at the Joint Institutes of Computational Sciences (JICS), the Innovative Computing Laboratory (ICL), and Oak Ridge National Laboratory. International students supported by several partnering universities in Hong Kong also participate in this program. Together the students of RECSEM work collaboratively to achieve their research goals, and at the same time share a unique opportunity for trading academic experiences, scientific ideas, and cultural and social activities in the ten week long programs.<br/><br/>The program is organized around ideas and practices common to many scientific disciplines. The research projects are categorized into three interrelated areas of research interests: engineering applications, numerical mathematics, and linear algebra software and tools. The scope of work for these projects puts emphasis on developing skill sets in scientific model derivation, software implementation, and evaluation of numerical experiments, under the guidance of a team of experts in each scientific domain. Projects include computationally oriented multi-scale materials science and biomechanics applications, simulation of traffic flow phenomena, and processing of images using different techniques and algorithms from machine learning and data analytics. The students have opportunities to perform large-scale scientific simulations on HPC clusters as well as world-class state-of-the-art supercomputers equipped with the latest hardware technologies provided by the Extreme Science and Engineering Discovery Environment (XSEDE) organization, and include computing units with graphical processing units (GPUs). The RECSEM program is organized into four major activities: HPC training, research formulation, project action, and scientific reporting. These stages aim to lead the students toward completing their research projects on time during the REU period, while providing the appropriate level of motivation and guidance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103632","Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities","OAC","Software Institutes","10/01/2021","09/13/2021","Erkan Istanbulluoglu","WA","University of Washington","Standard Grant","Tevfik Kosar","09/30/2026","$112,501.00","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8004","077Z, 7925, 8004","$0.00","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.<br/><br/>As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019194","CC* Compute: Central Computing with Advanced Implementation at San Diego State University","OAC","CYBERINFRASTRUCTURE","09/01/2020","10/20/2020","Jose Castillo","CA","San Diego State University Foundation","Standard Grant","Kevin Thompson","08/31/2022","$399,328.00","Guadalupe Ayala, Andrew Cooksy, Christopher Paolini","jcastillo@sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","7231","","$0.00","This project establishes a computing system that assists researchers at San Diego State University to accelerate their work through ongoing developments in computer processing hardware.  Faculty and students will develop code, most of it currently running on traditional processing units, to take advantage of the enhanced computing power of graphical processing units and field programmable gate arrays, which effectively allow the circuitry of the computer chip to be optimized for specific computing tasks.  Applications hosted on this system include simulations of subterranean carbon dioxide sequestration, the development of new computer programs to identify disease-causing and other organisms in biologically diverse environments, and studies of nuclear structure, brain imaging, the motion of viruses, and engine design.  One of the PIs directs the training of new users, and the infrastructure is incorporated into several courses available to undergraduates and graduate students.  External users can access the resource through the Pacific Research Platform.<br/><br/>A new, high-performance computing cluster offers advanced processor hardware to researchers at San Diego State University and throughout the region. The cluster is optimized to support transfer of existing software developed in-house to processors that promote distributed computing and/or electronic design automation. While accelerating projects and modernizing training in computational methods, this work also informs the University?s long-range cyberinfrastructure acquisition plans by quantifying the enhancement of research possible with advanced hardware. 20% of the cluster capacity is reserved for extramural use through the Pacific Research Platform, and 5% for training and coursework to advance workforce development.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019220","CC* Compute: A Customizable, Reproducible, and Secure Cloud Infrastructure as a Service for Scientific Research in Southern California","OAC","Campus Cyberinfrastructure","09/01/2020","10/20/2020","Carl Kesselman","CA","University of Southern California","Standard Grant","Kevin Thompson","08/31/2022","$399,800.00","Byoung-Do Kim","carl@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8080","","$0.00","________________________________________________________________________________________________________________<br/><br/><br/>This project creates a hybrid cloud infrastructure as a scientific computing gateway that promotes and supports inter-disciplinary, multi-institutional research in science, engineering, biomedicine, and the social sciences. The hybrid cloud platform also promotes regional and national research collaboration, as a portion of the resources is integrated into the Open Science Grid (OSG). Many institutes with multi-institutional research projects headquartered at University of Southern California (USC), along with their regional and national collaborators, benefit from use of the OSG. It also extends the impact of their research outcomes and the projects themselves, as the system offers various ways to share research outputs and knowledge with external collaborators. The planned support for regional universities and integration with OSG increases opportunities to serve a broader community.<br/><br/>A broad research community is supported by this system by providing access to public and private cloud services as well as local high performance computing (HPC) and data resources. The design of the hybrid cloud system facilitates the creation of customizable, virtualized platforms and reproducible, container-based application services that enable multi-dimensional computing and data solutions. Researchers are able to pick and choose from a standard service catalogue to build pre-defined virtual machines and containerized applications, or, if necessary, create their own specialized environments. Along with built-in security, reproducible service modules, and the capability of creating and sharing customized environments, the hybrid cloud system bridges multi-disciplinary research domains and enhances the usability of advanced cyberinfrastructure for improved research productivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004751","Frameworks: Developing CyberInfrastructure for Waterborne Antibiotic Resistance Risk Surveillance (CI4-WARS)","OAC","EnvE-Environmental Engineering, Special Initiatives, Software Institutes","07/01/2020","04/01/2020","Liqing Zhang","VA","Virginia Polytechnic Institute and State University","Standard Grant","Joseph Whitmeyer","06/30/2023","$1,299,036.00","Peter Vikesland, Amy Pruden, Ali Butt","lqzhang@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1440, 1642, 8004","077Z, 7925","$0.00","CI4-WARS is a cyberinfrastructure (CI) being developed as a key step towards establishing an efficient and integrated network of wastewater treatment plants (WWTPs) incorporating CI for antibiotic resistance (AR) surveillance.  AR is the ability of some bacteria to survive antibiotic treatment, a capability encoded by antibiotic resistance genes (ARGs).  AR rates are increasing globally, with the US Centers for Disease Control estimating 35,000 related deaths in the US per year in 2019, compared to 23,000 deaths per year in 2013.  It is a grand challenge that calls for an interdisciplinary approach to combat its spread.  Efficient and effective surveillance is needed to pinpoint where ARGs are spreading among bacteria and to inform ways to stop their spread. Combining next generation DNA sequencing with CI for monitoring patterns in ARG detection WWTPs is a promising and novel way to achieve this.  WWTPs aggregate antibiotics excreted by all people in a community undergoing antibiotic therapy, as well as any AR bacteria or ARGs that are present on their skin or eliminated by them.  Identifying anomalies in ARG patterns in wastewater could help identify potential outbreaks before they occur, better inform clinical use of antibiotics, and improve treatment practices to prevent release of ARGs to rivers and streams. <br/><br/>The objectives of this research are to: (1) Develop and demonstrate the CI4-WARS system using DNA sequencing data and associated metadata collected from a local WWTP; (2) Develop new computational tools to identify ARG occurrence patterns  in the DNA sequencing data that indicate the risk of AR spreading and develop and apply new algorithms for identifying anomalies in these indicators that are indicative of emergence of new AR bacteria or outbreaks; and (3) Integrate the developed computational tools into CI4-WARS, establishing it as a one-stop service for surveying, evaluating, communicating, and reporting/alerting indicators of AR risk. Broader impact activities include student and professional training, annual workshops, free online videos, tutorials, and making CI4-WARS freely available on the web to maximize the benefit of CI4-WARS and facilitate adoption by the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103918","Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities","OAC","Polar Cyberinfrastructure, Geomorphology & Land-use Dynam, Software Institutes","10/01/2021","09/13/2021","David Gochis","CO","University Corporation For Atmospheric Res","Standard Grant","Tevfik Kosar","09/30/2026","$135,508.00","","gochis@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","5407, 7458, 8004","077Z, 1079, 7925, 8004","$0.00","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.<br/><br/>As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840218","CICI: RDP: Open Science Chain (OSC) - A Novel Distributed Ledger-Based Framework for Protecting Integrity and Provenance of Research Data","OAC","Cybersecurity Innovation","09/01/2018","08/17/2018","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Robert Beverly","08/31/2022","$818,433.00","Viswanath Nandigam","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8027","","$0.00","Data sharing is an integral component of scientific research and associated publications. Researchers have the ability to extend and build upon prior research when they are able to efficiently access, validate, and verify the data referenced. Facilitating the future reuse of data in a secure and independently verifiable manner is critical to the advancement of research. Open Science Chain (OSC) allows a broad set of researchers to efficiently share metadata and easily verify authenticity of their scientific datasets in a secure manner, while preserving provenance and lineage information. <br/><br/>OSC is a web based cyberinfrastructure platform built using distributed ledger technologies that allows researchers to provide metadata and verification information about their scientific datasets and update this information as the datasets change and evolve over time in an auditable manner. The researchers are able to search, verify and validate scientific datasets and link datasets to show lineage information. OSC features a web-based portal with user-friendly interfaces for metadata registration, data search and verification capability. OSC has been designed and implemented using real world scientific datasets from a diverse set of use cases ensuring the broad applicability across scientific domains. OSC enables sharing and verification of datasets among wider research communities including large facilities, smaller labs, individual researchers and students while promoting good data documentation practices. OSC increases the confidence of the scientific results and promotes data sharing, which in turn increases productivity and promotes good science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923737","Collaborative Research: CyberTraining: Conceptualization: Planning a Sustainable EcoSystem for Incorporating Parallel and Distributed Computing into Undergraduate Education","OAC","CyberTraining - Training-based","09/01/2019","08/17/2019","Peter Keleher","MD","University of Maryland, College Park","Standard Grant","Almadena Chtchelkanova","12/31/2021","$22,160.00","","keleher@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","College Park","MD","207425141","3014056269","CSE","044Y","026Z","$0.00","In this era of pervasive multicore machines, GPUs, cloud services, big data, machine learning, and the Internet of Things, there is a critical need for an institute to create a sustainable, discipline-wide ecosystem for incorporating parallel and distributed computing (PDC) into undergraduate computing curricula. Such an institute would support the community of educators, students, and other stakeholders, with the goal of developing a workforce that is ready to meet the challenges of working with current and future computing fabrics. The investigators propose planning for such an institute (iPDC) that can help eliminate the longstanding barrier of the sequential computing paradigm such that, analogous to the establishment of the object oriented paradigm, the PDC paradigm is naturally integrated into Computer Science (CS) and Computer Engineering (CE) curricula across various institutions as recommended by the 2013 ACM/IEEE Computer Science Curricula and now by ABET.<br/><br/>Through the network of funded and unfunded collaborators, established contacts with instructors at institutions serving underrepresented groups, and outreach efforts, the project will robustly engage with stakeholder communities through four well-structured planning workshops, weekly teleconferences, and feedback and dissemination activities to formulate the key attributes of the institute.Broadening PDC education will further enable advances in science and engineering, which depend increasingly on PDC systems, by providing the next generation of practitioners and researchers with the necessary skills and knowledge to effectively exploit them. The curriculum standards, adoption, and dissemination activities will have synergistic international components. Overall, this project will facilitate a rich exchange of ideas within the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2138773","EAGER: Preserve/Destroy Decisions for Simulation Data in Computational Physics and Beyond","OAC","NSF Public Access Initiative","09/01/2021","08/26/2021","Victoria Stodden","CA","University of Southern California","Standard Grant","Bogdan Mihaila","03/31/2023","$85,456.00","","stodden@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7414","7916","$0.00","The scientific research community has been increasingly developing ways to share and re-use research data, thereby allowing more discoveries to be made from previous research investments. Much of the focus on sharing and reusability has been on experimental and observational data. This project addresses the equally vexing challenge of how to make best use and re-use of the massive data produced in computational simulations. Important research questions guiding this project include: the degree to which simulation results can be replicated; the advantages of storing the simulation data itself for others to reuse as compared to providing the computational software so that others can re-run the simulations; and understanding which software testing practices can facilitate the replication/reuse of simulation data and the simulation software that produces those data. The principal investigators will address these questions by performing extensive replication and software code testing on a set of computational physics simulation datasets and software code that they had gathered through a previous study. The project will produce publicly available, fully reproducible computational physics works as examples for publishing results in a way that the data and code are effectively reusable.<br/><br/>The principal investigators aim to improve understanding of, and increase, the reusability of the code and data associated with simulation-based research.  This project specifically aims to better inform data destroy/preservation decisions in the simulation context, toward improving the reusability and interoperability of simulation data and code. The project will also consider important questions such as how software engineering testing practices relate to computational physics practices, and how changes in computational environments affect code execution and the regeneration of simulation data. Ultimately, the results of this work are intended to guide the research community on how to best produce and disseminate research code. It is anticipated that the results for computational physics can be extended to develop general guidelines for simulation data and code sharing for other communities, the appropriate code testing to do so, and best practices for development of associated cyberinfrastructure and tools. <br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103701","Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED)","OAC","EarthCube","09/01/2021","08/24/2021","Marine Denolle","WA","University of Washington","Standard Grant","Tevfik Kosar","08/31/2025","$660,591.00","","marinedenolle@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8074","077Z, 7925, 8004, 9102","$0.00","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. <br/><br/>The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103621","Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED)","OAC","Geophysics, XC-Crosscutting Activities Pro, EarthCube","09/01/2021","08/24/2021","Hatice Bozdag","CO","Colorado School of Mines","Standard Grant","Tevfik Kosar","08/31/2025","$613,397.00","","bozdag@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1574, 7222, 8074","077Z, 7925, 8004, 9102","$0.00","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. <br/><br/>The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112606","AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE)","OAC","AI Research Institutes","11/01/2021","07/28/2021","Dhabaleswar Panda","OH","Ohio State University","Cooperative Agreement","Tevfik Kosar","10/31/2026","$11,999,998.00","Beth Plale, Eric Fosler-Lussier, Vipin Chaudhary, Raghu Machiraju","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","132Y","075Z, 7231, 8004, 9102","$0.00","Although the world is witness to the tremendous successes of Artificial Intelligence (AI) technologies in some domains, many domains have yet to reap the benefits of AI due to the lack of easily usable AI infrastructure. The NSF AI Institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) will develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments. It will advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. ICICLE brings together a multidisciplinary team of scientists and engineers, led by The Ohio State University in partnership with Case Western Reserve University, IC-FOODS, Indiana University, Iowa State University, Ohio Supercomputer Center, Rensselaer Polytechnic Institute, San Diego Supercomputer Center, Texas Advanced Computing Center, University of Utah, University of California-Davis, University of California-San Diego, University of Delaware, and University of Wisconsin-Madison. Initially, complex societal challenges in three use-inspired scientific domains will drive ICICLE?s research and workforce development agenda: Smart Foodsheds, Precision Agriculture, and Animal Ecology.  <br/><br/>ICICLE?s research and development includes: (i) Empowering plug-and-play AI by advancing five foundational areas: knowledge graphs, model commons, adaptive AI, federated learning, and conversational AI. (ii) Providing a robust cyberinfrastructure capable of propelling AI-driven science (CI4AI), solving the challenges arising from heterogeneity in applications, software, and hardware, and disseminating the CI4AI innovations to use-inspired science domains. (iii) Creating new AI techniques for the adaptation/optimization of various CI components (AI4CI), enabling a virtuous cycle to advance both AI and CI. (iv) Developing novel techniques to address cross-cutting issues including privacy, accountability, and data integrity for CI and AI; and (v) Providing a geographically distributed and heterogeneous system consisting of software, data, and applications, orchestrated by a common application programming interface and execution middleware. ICICLE?s advanced and integrated edge, cloud, and high-performance computing hardware and software CI components simplify the use of AI, making it easier to address new areas of inquiry. In this way, ICICLE focuses on research in AI, innovation through AI, and accelerates the application of AI. ICICLE is building a diverse STEM workforce through innovative approaches to education, training, and broadening participation in computing that ensure sustained measurable outcomes and impact on a national scale, along the pipeline from middle/high school students to practitioners. As a nexus of collaboration, ICICLE promotes technology transfer to industry and other stakeholders, as well as data sharing and coordination across other National Science Foundation AI Institutes and Federal agencies. As a national resource for research, development, technology transfer, workforce development, and education, ICICLE is creating a widely usable, smarter, more robust and diverse, resilient, and effective CI4AI and AI4CI ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126327","CC* Integration-Large: Democratizing Networking Research in the Era of AI/ML","OAC","CISE Research Resources","10/01/2021","08/30/2021","Arpit Gupta","CA","University of California-Santa Barbara","Standard Grant","Deepankar Medhi","09/30/2023","$999,913.00","Elizabeth Belding, Nicholas Feamster, Trinabh Gupta","arpitgupta@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","2890","9102","$0.00","The emerging area of self-driving networks provides network administrators at campus networks to automate most network-management tasks. Such an automation ensures that the network remains performant and reliable amidst various disruptions, while requiring minimal interventions from the network administrators. However, making significant contributions to self-driving network research requires developing artificial intelligence (AI) and machine learning (ML)-based tools and demonstrating that they work in practice. Unfortunately, in stark contrast to their counterparts in industry, most academic researchers have neither access to the proper data for developing learning-based tools nor have properly instrumented testbeds for road-testing the resulting tools in realistic settings. <br/><br/>This collaborative project brings together investigators from the University of California-Santa Barbara, University of Chicago, and NIKSUN Inc., to investigate how to use campus networks to overcome barriers to self-driving network research. First, it will deploy packet-processing pipelines at two campus networks to collect the proper network data at scale without compromising user privacy. It will then strategically place programmable network devices at campus networks to safely road-test newly developed learning models in production settings. Finally, it will illustrate the capabilities enabled by these newly-instrumented campus networks for developing, evaluating, and road-testing new learning models with different use cases.<br/><br/>This project is intended to seed a community effort that uses campus networks as vehicles for democratizing self-driving networks research, improving its transparency through reproducibility, and ensuring its success in practice by establishing trust. As such, it promises to be transformative not only for the network community as a whole but also for different campus network stakeholders (e.g., campus IT). Fully leveraging these campus networks' dual role as data source and testbed and seamlessly integrating it into the university's engineering curriculum suggests radically new approaches to teaching, training, and educating engineering students in the era of AI and ML.<br/><br/>The project information will be maintained at: https://democratize-netai.cs.ucsb.edu/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103836","Collaborative Research: Elements: SENSORY: Software Ecosystem for kNowledge diScOveRY - a data-driven framework for soil moisture applications","OAC","Hydrologic Sciences, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","06/01/2021","05/05/2021","Rodrigo Vargas","DE","University of Delaware","Standard Grant","Amy Walton","05/31/2024","$249,776.00","","rvargas@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1579, 7222, 8004, 8074","077Z, 7923","$0.00","Tools for gathering soil moisture data (such as in situ soil sensors and satellites) have differing capabilities.  In situ soil moisture data has fine-grained spatial and high temporal resolution, but is only available in limited areas; satellite data is available globally, but is more coarse in resolution. Existing software tools for studying the dynamic characteristics of soil moisture data are limited in their ability to model soil moisture at multiple spatial and temporal scales, and these limitations hamper scientists? ability to address urgent practical problems such as wildfire management and food and water security. Accurate gathering and effective modeling of soil moisture data are essential to address pressing environmental challenges. This interdisciplinary project designs, builds, and shares a data-driven software ecosystem for soil moisture applications. This software ecosystem models and predicts soil moisture at scales suitable to support studies in forestry, precision agriculture, and earth surface hydrology.<br/><br/>This project connects multi-disciplinary advances across the scientific community (such as generating datasets at scale and supporting cloud-based cyberinfrastructures) to develop a data-driven software ecosystem for analyzing, visualizing, and extracting knowledge from the growing data collections (from fine-grained, in situ soil sensor information to coarse-grained, global satellite measurements) and releasing this knowledge to applications in environmental sciences.  Specifically, this project (a) develops scalable methodologies to integrate and analyze soil moisture data at multiple spatial and temporal scales; (b) implements a data-driven software ecosystem to access complex information and provide basic and applied knowledge to inform researchers and stakeholders interested in soil moisture dynamics (scientists, educators,  government agencies, policy makers); and (c) builds cyberinfrastructures to support discovery on cloud platforms, lowering resource barriers to improve accessibility and interoperability.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103951","Elements:Open-source hardware and software evaluation system for UAV","OAC","Software & Hardware Foundation, Software Institutes","06/15/2021","06/10/2021","Hyesoon Kim","GA","Georgia Tech Research Corporation","Standard Grant","Tevfik Kosar","05/31/2024","$600,000.00","Arijit Raychowdhury","hyesoon@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798, 8004","077Z, 7923, 7942, 8004","$0.00","The usage of Robots and Unmanned Aerial Vehicles (UAVs) is increasing, and they are becoming a part of our everyday lives. In the process, robotic systems have ceased to be solely a mechanical design problem but now include substantial computing power as well. However, little system or architecture research has been done for robotics workloads, especially in realistic environments. The existing infrastructure is focused on drone flight algorithm tests, with little flexibility to configure the computing capability. To this end, this project develops a software-hardware co-design framework that includes an end-to-end, vertically integrated stack that enables reconfigurable hardware to be programmed and exposed to virtual reality environments for realistic drone navigation problems. It will establish a flexible software-hardware infrastructure of drones and open up exciting possibilities for more research in academia and industry. Since drones are nowadays widely used, the research has potential for impact across a wide range of applications.<br/> <br/>The project develops an end-to-end hardware and software infrastructure that can evaluate the computing requirements and test out new architectures and technologies. This includes: (1) profiling the workload for UAVs, which include both model-based as well as learning-based workloads; (2) developing an open-source hardware/software drone platform that can be used for testing computing architecture/systems with a real system; (3) developing a cyber-physical system infrastructure that can be connected with drones and flight simulation; and (4) developing a simulation infrastructure to offload high-end computation to other accelerators while running drone flight scenarios. The project will advance the state of the art in implementation of processors for robotics/UAV workloads. It will offer new opportunities in power-constrained platforms for applications including surveillance, automotive, environment, military, and disaster to name a few.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104115","Collaborative Research: Framework Implementation: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network  Dynamics","OAC","Software & Hardware Foundation, Software Institutes","09/01/2021","05/11/2021","Boyana Norris","OR","University of Oregon Eugene","Standard Grant","Tevfik Kosar","08/31/2025","$1,214,001.00","Peter Ralph","norris@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7798, 8004","077Z, 7925, 7942, 8004","$0.00","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.<br/><br/>This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103874","Frameworks: Advanced Cyberinfrastructure for Sustainable Community Usage of Big Data from Numerical Fluid Dynamics Simulations","OAC","FD-Fluid Dynamics, Physical & Dynamic Meteorology, PHYSICAL OCEANOGRAPHY, Special Initiatives, Software Institutes, EarthCube","07/01/2021","05/21/2021","Charles Meneveau","MD","Johns Hopkins University","Standard Grant","Amy Walton","06/30/2026","$3,992,109.00","Tamer Zaki, Randal Burns, Thomas Haine, Alexander Szalay","meneveau@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1443, 1525, 1610, 1642, 8004, 8074","077Z, 7925, 8004","$0.00","In most computer simulation-based research, the prevailing approach has been to perform large simulations that generate so much data that only some quantities can be computed during the simulation runs while at most a few representative snapshots are stored and shared for subsequent analysis. Storing the entire time evolution creates access and distribution bottlenecks that have been a pervasive challenge. This project addresses the challenge and broadens the impact of high-performance scientific computing in one of the disciplines at the frontier of high-performance scientific computing: fluid turbulence. Improved tools for turbulence research are required to model many natural processes in atmospheric and ocean sciences and to develop engineering applications. The project builds easily accessible and useable databases from world-class turbulence simulations that help bridge the increasing resource gap between top computer simulators and the wider turbulence data user community. <br/><br/>The project develops and implements an advanced cyberinfrastructure framework for turbulence databases that enables ground-breaking research on fluid turbulence in various engineering, atmospheric and ocean flows. Novel services include user-programmable server computation, efficient batch processing tools, easy-to-use inspections of the data that also allow users to store and query the locations of specific flow patterns. The system extends the applicability of notebook-based, AI-augmented data analyses accessing a diverse federation of structured databases with more complex data objects, and includes datasets for various new flows of engineering and geophysical interest at increasing Reynolds numbers. It contains backwards compatible elements with a legacy system, to support and maintain compatibility with an existing active community of users.  Research training and mentoring focus on the interplay between physical concepts and computer and computational science aspects of massive simulation-based datasets. Over 2.5 Petabytes of data will become available to support breakthroughs in turbulence research. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation Division of Chemical, Bioengineering, Environmental and Transport Systems within the Directorate for Engineering; and by the Physical Oceanography Program, Physical and Dynamic Meteorology Program, and the Division of Integrative and Collaborative Education and Research within the Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105345","Collaborative Research: Multifidelity Uncertainty Quantification Through Model Ensembles and Repositories","OAC","CDS&E","09/01/2021","05/21/2021","Alison Marsden","CA","Stanford University","Standard Grant","Tevfik Kosar","08/31/2024","$508,715.00","","amarsden@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8084","026Z, 8084","$0.00","Numerical simulations are increasingly used in clinical research and practice for diagnosis and treatment planning in cardiovascular disease, creating new demand for reliable simulation and analysis tools. Quantification of uncertainty in these simulations is crucial to increased clinical adoption but has previously been largely disregarded due to its excessive computational cost and complexity. To address these challenges, the project leverages a new class of multi-fidelity Monte Carlo estimators for direct and inverse problems, designed to mitigate computational complexity through the solution of a large number of inexpensive low-fidelity surrogates. It demonstrates the proposed approach in full-scale clinical problems including multiple uncertainty sources at a reasonable computational budget. The project?s main objective is to create an end-to-end advanced cyberinfrastructure ecosystem for uncertainty quantification (direct problem) and parameter estimation (inverse problem) in cardiovascular models incorporating realistic sources of uncertainty, able to leverage arbitrary low-fidelity models through advanced Monte Carlo estimators, while drastically reducing computational cost and complexity. The project?s interdisciplinary team is synergizing computational modeling, cardiovascular physiology, UQ and open-source software towards making UQ tractable in full-scale 3D cardiovascular simulations, leveraging multi-fidelity estimators for the solution of both direct and inverse problems. The project will produce seamless cyberinfrastructure linking two well-regarded open-source packages, Dakota and SimVascular, with sizable user communities.<br/><br/>The project is creating new cyberinfrastructure ecosystems for large-scale UQ tasks. Dissemination to industry/academia is performed through SimVascular, a leading open-source platform for cardiovascular modeling. It will leverage SimVascular and the proposed multi-fidelity estimators to create hands-on teaching material for graduate and undergraduate courses. Although the project focuses on cardiovascular modeling, its results are directly applicable to other engineering problems. The PIs will organize minisymposia and workshops at national conferences. They will lead outreach activities to local K-12 schools to attract girls and underrepresented minority (URM) students to STEM. The PIs will mentor URM summer students through the SURF program and women students through the Women in Mathematics, Scientific Computing and Engineering (WiMSCE) group at Stanford.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104025","Elements:  Towards a Robust Cyberinfrastructure for NLP-based Search and Discoverability over Scientific Literature","OAC","NSF Public Access Initiative, Data Cyberinfrastructure, Software Institutes","05/01/2021","05/21/2021","James Pustejovsky","MA","Brandeis University","Standard Grant","Amy Walton","04/30/2024","$399,566.00","Nancy Ide","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","7414, 7726, 8004","077Z, 7923, 8004","$0.00","This project creates an open platform for accessing and mining information from scientific texts that provides access to an array of software, computing resources, and publication data.  Current search technologies typically find many relevant documents, but do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. Natural Language Processing (NLP) strategies are a recognized means to approach this problem, and this project develops the cyberinfrastructure to support sophisticated search and retrieval from scientific publications, use and augmentation of facilities for advanced and well-established natural language processing and machine learning tools, and extraction and aggregation of data from scientific publications. The project leverages two NSF-funded projects: the Language Applications (LAPPS) Grid, which has already proven to be an effective platform for development of NLP applications; and University of Wisconsin?s xDD (formerly, GeoDeepDive), a scalable, dependable infrastructure capable of rapidly growing a digital library of scientific publications, currently including over 13 million documents from multiple distributed commercial and open-access providers. The effort significantly enhances the value of these existing NSF-funded infrastructures by providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property issues. Scientists may perform large-scale text retrieval and mining using the University of Wisconsin?s high performance computing (HPC) infrastructure through a web-based interface. Iterative domain adaptation capabilities allow scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The potential impact of the cyberinfrastructure is applicable to any community that relies on computational tools for mining large textual datasets, including researchers in sociology, psychology, economics, education, linguistics, digital media, and the humanities.<br/><br/>This project extends the LAPPS Grid to provide access to UW-xDD?s collection of scientific publications and UW?s High Performance Computing facilities, as well as means to rapidly adapt existing, well-established natural language processing and machine learning software tools to new domains and evaluate results. The LAPPs Grid provides a large collection of NLP tools from a wide variety of sources exposed as web services, together with multiple commonly used resources and a front-end document retrieval engine currently configured to access PubMed/PubMedCentral as well as nightly updates of the CORD-19 dataset. The LAPPS Grid is open source, and can be run from the web, on a user?s laptop or desktop, in the cloud, or as a self-contained docker image when it is necessary to protect sensitive or licensed data, when there is no network connection available, or for deployment on remote HPC facilities. All tools and resources can be used interoperably, eliminating the effort required to convert input and output formats to use a set of tools or resources together.  xDD is one of the world?s largest single repositories of scientific publications that spans all domains of knowledge, incorporates new documents automatically and updates API endpoints every hour.  xDD has accumulated millions of documents from multiple commercial and open-access publishers (over 13M publications).  The xDD infrastructure is an integral part of the developing UW-COSMOS pipeline, which consists of a suite of services supporting document processing, including ingestion and parsing of PDFs; extraction of individual document objects such as text sections, figures, tables, and captions; and recall, which creates searchable Anserini and ElasticSearch indexes on the contexts and objects to enable retrieval of information. Specific project activities include implementing efficient retrieval and analysis of xDD?s vast holdings of scientific publications; extending the NLP capabilities of the LAPPS Grid for scientific publication mining and domain adaptation; developing full interoperability between the Grid and xDD/COSMOS; scaling LAPPS Grid services to handle the very large textual datasets available from UW-xDD; and surveying visualization techniques and integrating them into the Grid.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering, and the NSF Public Access program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042155","CAREER: Exploiting Parallel Heterogeneous Architectures to Enable Time-domain Astronomy in the LSST era","OAC","CAREER: FACULTY EARLY CAR DEV","07/15/2021","07/09/2021","Michael Gowanlock","AZ","Northern Arizona University","Continuing Grant","Alan Sussman","06/30/2026","$288,421.00","","michael.gowanlock@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","1045","026Z, 1045","$0.00","Recent and near future scientific instruments will generate large amounts of data. One example of such an instrument is the Vera C. Rubin Observatory that will carry out the Legacy Survey of Space and Time (LSST) over a ten year period. This astronomical survey has the potential to advance many fields of astronomy, and may even lead to the development of new fields of scientific inquiry. However, the large data volume implies that many processors will need to be used to process the data within a reasonable amount of time. This project creates new technologies and algorithms that can utilize a large number of processors. In particular, the project harnesses the power of both standard central processing units (CPUs) and graphics processing units (GPUs) that are good at processing many data items simultaneously. The developed technologies are designed to use the data from LSST and find interesting events in the Solar System. Once an interesting event is detected on a given astronomical object, alerts are sent to the astronomy community so that they can use additional telescopes to further study these objects. Without the technologies developed in this project, astronomers will miss out on opportunities to study transient phenomena. The project integrates several teaching activities that ensure both computer scientists and astronomers receive the necessary training to exploit future generation computer systems. The project includes mentoring undergraduate and graduate students. In addition, the local community will be engaged through outreach activities that promote science, technology, engineering, and mathematical fields, particularly through activities targeting K-12 students.  The project serves the national interest, as stated by NSF's mission, by promoting the progress of science, and to advance the national health, prosperity, and welfare. <br/><br/>The Vera C. Rubin Observatory will have unprecedented time domain capabilities. However, LSST will generate large volumes of data that need to be examined in order to realize many scientific goals. This project focuses on LSST supporting cyberinfrastructure (CI) in the context of Solar System science. Fast outlier detection is needed to enable rapid follow up by other facilities to ensure that transient events in the Solar System and objects with intrinsically unusual properties are discovered.  To ensure rapid detection capabilities, the outlier detection algorithms will exploit heterogeneous CPU and GPU architectures. Furthermore, heterogeneous computing will be employed where the work is distributed between the CPU and GPU. Also, the project examines using application specific integrated circuits on modern GPU hardware, such as tensor and ray tracing cores as applied to a broader range of applications than matrix multiplication and ray tracing. Algorithmic transformations are needed to exploit these heterogeneous processors; consequently, a unifying framework is developed that models the performance of these algorithms as executed on these architectures. This framework and novel parallel and scalable algorithms provide foundational CI that will enable the LSST to successfully explore the Solar System, understand its origins, and identify potentially hazardous asteroids, among other scientific objectives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018705","MRI: Acquisition of a High-Performance Computing Cluster for Interdisciplinary Research and Teaching","OAC","Major Research Instrumentation","08/01/2020","07/21/2020","Manuela Ayee","IA","Dordt University, Incorporated","Standard Grant","Alejandro Suarez","07/31/2023","$249,254.00","Hailiang Zhu, Nathan Tintle, Nick Breems, Channon Visscher","Manuela.Ayee@dordt.edu","498 4th Avenue NE","Sioux Center","IA","512501606","7127226339","CSE","1189","1189, 9150","$0.00","The acquisition of a high-performance computing (HPC) cluster for use by faculty and undergraduate students at rurally located Dordt University will significantly impact several new and ongoing, multidisciplinary research projects in both STEM and non-STEM fields. This cluster will generate opportunities for Dordt faculty and other regional college and high school faculty to integrate high-performance computing into their research, training activities, and classrooms. Research projects covering a wide range of academic disciplines will be supported by the new HPC cluster, including computational biomedicine, medicinal chemistry, statistical genetics, planetary chemistry, and structural integrity under seismic loading. The project will also expand opportunities for HPC use within other fields such as social sciences statistical research, digital humanities, institutional research, and IT. The research endeavors of this team of undergraduate research mentors will be substantially catalyzed by the HPC cluster, leading to a diverse and interdisciplinary set of high-impact research findings, providing cutting-edge, high performance computing research experiences for undergraduate students, and acting as a model for cross disciplinary computational resource sharing at Primarily Undergraduate Institutions. Other faculty in computer science, data science, statistics, engineering, and chemistry will also utilize the cluster in upper-level courses as research training opportunities for students. <br/><br/>This Major Research Instrumentation award will provide infrastructure that will enable faculty to further develop and sustain research programs that already produce skillfully trained undergraduate researchers, generate peer reviewed publications, and attract external support. To accommodate the multidisciplinary range of the research applications of HPC users, this project will provide a high performance computing cluster consisting of a head node and 18 compute nodes in three standardized configurations: eight regular compute nodes, eight high RAM nodes, and two Graphics Processing Unit (GPU)-based nodes for applications, such as large molecular modeling systems, that will harness the massively parallel vector processing abilities of GPUs. Over the three years of the project, approximately fifty graduate school-bound undergraduate students (over half female; at least 10% minority) will receive intensive research training on the proposed HPC cluster by the primary faculty users. Furthermore, at least 1500 additional undergraduate, middle and high school students (many from rural, first generation college, or lower socio-economic strata), and faculty will use the cluster as part of STEM courses and outreach activities, including high school teacher training and faculty development workshops, as well as research/education programs for middle and high school students. The HPC resources will also be integral to a new initiative at Dordt University focused on recruitment and retention of academically talented students with financial barriers to education. This award will bring additional high-performance computing capability to a region of the country which currently has limited access to HPC resources, but has seen recent rapid growth in STEM. The HPC cluster will reach a diverse user community and maximize student impact, inspiring more students to pursue STEM careers and use computational tools across disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019129","MRI: Acquisition of FASTER - Fostering Accelerated Sciences Transformation Education and Research","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE","09/01/2020","07/21/2020","Honggao Liu","TX","Texas A&M University","Standard Grant","Alejandro Suarez","08/31/2023","$3,090,000.00","Zhangyang Wang, Zhe Zhang, Dilma Da Silva, Raymundo Arroyave","honggao@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","1189, 1640, 7231","075Z, 1189","$0.00","The project funds the acquisition of a composable high-performance data-analysis and computing instrument, named FASTER (Fostering Accelerated Scientific Transformations, Education, and Research). FASTER will enable transformative advances in scientific fields that rely on artificial intelligence and machine learning (AI/ML) techniques, big data practices, and high-performance computing (HPC) technologies. The FASTER platform removes significant bottlenecks in research computing by leveraging a technology that can dynamically allocate resources to support workflows. It will support researchers from across the Texas A&M University System and their collaborating institutions.  Thirty percent of FASTER?s computing resources will also be allocated to researchers nationwide by the National Science Foundation (NSF) XSEDE (Extreme Science and Engineering Discovery Environment) program. FASTER?s composable interface allows it to simultaneously support both emerging and traditional workloads in research computing. Transformative research projects benefiting from FASTER will include the development of AI/ML models, cybersecurity, health population informatics, genomics, bioinformatics, computer-aided drug design, agricultural sciences, life sciences, oil and gas simulations, de novo materials design, climate modeling, multi-scale simulations, quantum computing architectures, biomedical imaging, geosciences, and quantum chemistry. In addition to supporting a wide-range of fields of research, the project contributes to code development, education, and the workforce development goals of several NSF Big Ideas.<br/><br/>FASTER adopts the innovative Liqid composable software-hardware approach combined with cutting-edge technologies such as state of the art CPUs and GPUs, NVMe (Non-Volatile Memory Express) based storage, and thigh speed interconnect. Workflows on FASTER will be able to dynamically integrate disaggregated GPUs and NVMe to compose a single node, allowing them to scale beyond traditional hardware limits. The composable and configurable techniques will allow researchers to use resources efficiently, enabling more science. Best practices gathered from managing the resource will be shared with the community. FASTER will coordinate a three-pronged effort to effectively broaden participation in computing by focusing on training, education and outreach. FASTER will leverage existing efforts that promote STEM (Science, Technology, Engineering and Mathematics) and broaden participation in computing at the K-12, collegiate, and professional levels to have a transformative impact nationally. FASTER activities are designed to expand the participation of traditionally underrepresented groups in computing and STEM, particularly at minority-serving institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826993","CC* Networking Infrastructure: North Dakota Internet Leadership Level Infrastructure","OAC","Campus Cyberinfrastructure","08/01/2018","07/23/2018","Dana Skow","ND","North Dakota State University Fargo","Standard Grant","Kevin Thompson","07/31/2021","$495,524.00","Marc Wallman, Aaron Bergstrom","dane.skow@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","CSE","8080","9150","$0.00","This project creates data networking and storage infrastructure dedicated to scientific research and education at North Dakota State University (NDSU) and the University of North Dakota (UND). These network upgrades prioritize science data movement, allowing both institutions to more effectively participate in collaborative research across the globe. The project also prepares both schools for much higher capacity connectivity through North Dakota's research and education (R&E) state network. The networking upgrades enhance efforts to provide a strong and stable workforce equipped with the skills and knowledge necessary to support contemporary advanced research. Examples include the current North Dakota state EPSCoR Track I project, collaborations between researchers at NDSU/UND and the other North Dakota institutions and Tribal College schools, and student internship programs using the research computing facilities on each campus. <br/><br/>100Gbps connectivity is established from campus HPC systems to the campus network border. Science drivers for this project include Precision Agriculture and Digital Agriculture initiatives studying crop and livestock data collected using drone and satellite technologies; multi-campus research collaboration on 100TB class datasets used by UND?s Center for Regional Climate Studies; and predictive design of materials based in NDSU?s Materials and Nanotechnology program. These enable collaborations between researchers in chemistry, biochemistry, computer science, coatings and polymeric materials and mechanical engineering across the globe. The networking upgrades follow the scienceDMZ model and include perfsonar based end-to-end testing capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811101","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","Brent Cochran","MA","Tufts University","Standard Grant","Seung-Jong Park","12/31/2019","$29,816.00","","Brent.cochran@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003747","Collaborative Research: CDS&E: A framework for solution of coupled partial differential equations on heterogeneous parallel systems","OAC","CDS&E-MSS, CDS&E","09/01/2020","10/15/2020","Sandip Mazumder","OH","Ohio State University","Standard Grant","Tevfik Kosar","08/31/2023","$183,000.00","","mazumder.2@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8069, 8084","026Z, 8084, 9263","$0.00","This project is aimed toward the development of a transformative software framework to more easily calculate complex mathematical models for science and engineering research. There is currently significant burden on scientists who need to execute their mathematical models on different supercomputers; code developed for one supercomputer must often be re-developing or significantly adjusted to run on other supercomputers. Streamlining this development process will make modern hardware including supercomputers more accessible and impactful for scientific computations. Beyond the benefits to the computational science research agenda of this project, the new software infrastructure is expected to similarly help other domain scientists who develop similar types of mathematical models. All developed software will be publicly released with an open-source license.<br/><br/>Our goal is to develop a framework for code generation and efficient parallel solution of a large coupled set of partial differential equations (PDEs) with minimal user intervention. The Finite Volume Method will be used for discretization of the PDEs on unstructured meshes.  From user-specified PDEs and their boundary and initial conditions in symbolic form, and characteristics of the target hardware platform, the framework will perform efficient code generation and enable load-balanced parallel execution. The generated parallel code can either be compiled and used as is, or can be used as a starting point for the insertion of additional physical models and features by advanced users. The software will be demonstrated for two problem types: (1) computation of reacting flows, in which multiple space and time dependent PDEs are non-linearly coupled, and (2) solution of the Boltzmann Transport Equation (BTE) for phonons, in which the discretization of the 7-dimensional BTE results in thousands of space and time dependent coupled PDEs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004492","Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning","OAC","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","10/01/2020","10/15/2020","Aditi Sheshadri","CA","Stanford University","Standard Grant","Alan Sussman","09/30/2025","$1,189,926.00","","aditi_sheshadri@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","5740, 8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid.  They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify.  Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices.  These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.<br/><br/>The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005137","Collaborative Research: Frameworks: Community-Based Weather and Climate Simulation With a Global Storm-Resolving Model","OAC","Climate & Large-Scale Dynamics, Software Institutes, EarthCube","08/01/2020","07/23/2021","David Randall","CO","Colorado State University","Continuing Grant","Alan Sussman","07/31/2025","$2,789,185.00","James Hurrell","randall@atmos.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","5740, 8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Global Earth System Models (ESMs) use mathematical equations to simulate both weather and climate. ESMs include the dynamics of the atmosphere, oceans, land surface, ice, and vegetation. They can be used to make predictions of use to the public and policymakers. Today?s ESMs use coarse grids with cells about 100 km wide. Important weather systems like thunderstorms are too small to be simulated with such grids. One way to improve ESMs is to use finer grids that can directly simulate thunderstorms, but such models can only be run on very powerful computers. This project, called EarthWorks, will create an ESM capable of resolving storms by taking advantage of recent developments in high performance computing. EarthWorks will also use artificial intelligence to improve and speed up the model, and state-of-the-art methods to limit the amount of data produced as the model runs. The EarthWorks ESM will be built by spinning off and modifying a copy of the most recent version of the widely used Community Earth System Model. The modified model will represent the atmosphere, the oceans, and the land surface on a single very high-resolution grid, with grid cells about 4 km wide. It will have improved forecast skill, and produce more realistic simulations of past, present, and future climates. The project will make the model and its output openly available for use by all scientists.<br/><br/>The open-source Community Earth System Model (CESM) is both developed and applied to scientific problems by a large community of researchers. It is critical infrastructure for the U.S. climate research community. In the atmosphere and ocean components of the CESM, the adiabatic terms of the partial differential equations that express conservation of mass, momentum, and thermodynamic energy are solved numerically using what is called a dynamical core. Atmosphere and ocean models also include parametric representations, called parameterizations, that are designed to include the effects of storm and cloud processes that occur on scales too small to be represented on the model's grid. Despite decades of work by many scientists, today's parameterizations are still problematic and limit the utility of ESMs for many applications of societal relevance. Fortunately, recent advances in computer power have made it possible to parameterize less, by using grid spacings on the order of a few kilometers over the entire globe. These ""global storm-resolving models"" (GSRMs) can only be run on today's fastest computers. GSRMs are under very active development at a dozen or so modeling centers around the world. Unfortunately,  however, the current formulation of the CESM prevents it from being run as a GSRM. This project, called EarthWorks, will create a new, openly available GSRM by spinning off and intensively modifying a copy of the CESM. To accomplish this goal, the researchers will use recently developed and closely related dynamical cores for the atmosphere and ocean. All components of the model will use the same very high-resolution grid. This high resolution will make it possible to eliminate the particularly troublesome parameterization of deep cumulus convection (i.e., thunderstorms), and thereby reduce systematic biases that plague current ESMs. Earthworks will exploit the pre-exascale and exascale technologies now being brought to market by high performance computing vendors. The new exascale ESM will run the most computationally intensive components on powerful graphics processor units (GPUs), and exploit node-level task parallelism to execute the rest of the model asynchronously. The component model codes are close to completion and are currently being tested on GPUs. EarthWorks will use a simplified component-coupling approach, incorporate machine learning where feasible, and leverage lossy compression techniques and parallel I/O tools to deal with the enormous data volumes that will be generated as the model runs. The completed model will be simple, powerful, and well documented. The project will apply it to pressing scientific problems in both numerical weather prediction and climate simulation. The model and its input datasets will be made openly available to the broad research community, via GitHub.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032650","EAGER: Community Building and Workflows for Data Sharing with Publicly Accessible and Consumable Metadata","OAC","NSF Public Access Initiative","09/01/2020","07/22/2020","Brian Nosek","VA","Center for Open Science","Standard Grant","Martin Halbert","08/31/2022","$298,998.00","","nosek@cos.io","210 Ridge McIntire Road","Charlottesville","VA","229035083","4343521024","CSE","7414","7916","$0.00","In this exploratory activity, the Center for Open Science (COS) will create and pilot a result-reporting workflow for with Open Science Framework (OSF) study registration that includes reporting study outcomes, archiving study data and materials with permanent identifiers, and availability of metadata in a public API such as could be used by a federal agency for consumption (e.g., NSF Public Access Repository (PAR)).    <br/><br/>The team will conduct community building around data sharing in the product development process and in the review and evolution of data sharing standards established with the Transparency and Openness Promotion (TOP) Guidelines and badges for acknowledging open practices.  The project anticipates maximizing the discoverability of metadata of NSF-funded research, and increasing scientific rigor by helping link the products of research (data and software) to their associated publications through a user experience that supports transformative behavior change towards open-science aligned research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118217","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","10/01/2021","09/02/2021","Eric Jankowski","ID","Boise State University","Standard Grant","Alan Sussman","09/30/2025","$105,000.00","","ericjankowski@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","044Y","7231, 7301, 9150","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117834","CyberTraining: Implementation: Medium: Machine Learning Training and Curriculum Development for Earth Science Studies","OAC","CyberTraining - Training-based","09/01/2021","09/01/2021","Nicoleta Cristea","WA","University of Washington","Standard Grant","Alan Sussman","08/31/2024","$995,817.00","Ziheng Sun","cristn@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","044Y","7231, 7301, 9102","$0.00","Earth system science discoveries are increasingly affected by data management, analysis, and inference using powerful machine learning (ML) techniques. Yet, the skills required to perform these tasks, and training in cutting-edge, open-source technologies to build ML models and pipelines, big data, and cloud computing, are not covered by the traditional graduate curriculum in the geosciences. To fill these gaps, this project will develop the GeoScience MAchine Learning Resources and Training (GeoSMART) framework that will build a foundation in open-source scientific ecosystems and general ML theory, toolkits, and deployment on Cloud computing platforms. <br/> <br/>This project will include a team of geoscience and ML educators to create a novel ML curriculum with focus on seismology, cryosphere and hydrology applications. The training materials will be included in an enhanced curriculum that will broaden impact on emerging ML communities. The project?s implementation plan will provide training in open-source ML toolkits and data science skills. Further, the project will cultivate the development of discipline-specific ML libraries, workflows, and communities of practice to sustain future growth of ML cybertraining opportunities. By building tools using open-source and cloud-accessible platforms, and by partnering with colleges and institutions that lack computing resources for ML workflows, the project will increase access to cybertraining materials and help to solve geoscience challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017259","Collaborative Research: CyberTraining: Implementation: Small: INnovative Training Enabled by a Research Software Engineering Community of Trainers (INTERSECT)","OAC","CyberTraining - Training-based","08/15/2020","07/21/2020","Jeffrey Carver","AL","University of Alabama Tuscaloosa","Standard Grant","Alan Sussman","07/31/2023","$243,761.00","","carver@cs.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","044Y","026Z","$0.00","Software forms the backbone of much current research in a variety of scientific and engineering domains. The breadth and sophistication of software skills required for modern research software projects is increasing at an unprecedented pace. Despite this fact, an alarming number of researchers who develop software do not have adequate training in software development. Therefore, it is imperative for researchers who develop the software that will drive tomorrow?s critical research discoveries to have access to software engineering training at multiple stages of their career, to not only make them more productive, but also to make their software more robust, reliable, and sustainable. This project, INnovative Training Enabled by a Research Software Engineering Community of Trainers (INTERSECT), provides training on software development and engineering practices to research software developers who already possess an intermediate or advanced level of knowledge. To achieve this goal, INTERSECT provides expert-led training courses and workshops to help build the pipeline of computational researchers trained in best practices for research software development. This project serves the national interest and NSF's mission of promoting the progress of science by preparing a workforce who are trained in research software engineering best practices and addresses a gap in the current training and education available to research software developers.<br/><br/>The INTERSECT project provides training in research software engineering practices for researchers who are engaged in software development by (1) involving practicing Research Software Engineers (RSEs) in workshops to curate, develop, and refine training material and (2) using experienced RSEs to conduct training courses focused on that material. INTERSECT assembles experienced RSEs from the growing RSE community across the country who are interested in being instructors, to gather information about their  capabilities, knowledge, and expertise. In addition to developing materials, INTERSECT is curating the materials used in the workshops and training sessions into existing frameworks. This curation provides an open-source platform that allows RSEs to have ongoing engagement with the training material and helps coordinate efforts across the RSE-trainer community. To support the generation and curation of materials, INTERSECT sponsors multiple RSE-trainer workshops. These workshops connect practicing RSEs, who are also instructors, from across the country to both leverage the diversity of knowledge present in different institutions and to build and strengthen a national community of this increasingly important group of Cyberinfrastructure Professionals. By contributing to the growth of this community, INTERSECT will have a broad and long lasting impact on research software through increased access to peer support, resource and knowledge sharing, and networking opportunities for community members. By providing intermediate and advanced level training events and RSE workshops, INTERSECT will help to cultivate sophisticated Research Software Developers, bring awareness to the RSE career path, and strengthen the nascent national community of RSEs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118079","CyberTraining: Pilot: Employing Proper Orthogonal Decomposition (POD) and High-Performance Computing (HPC) in Advanced CI","OAC","CyberTraining - Training-based","09/01/2021","08/30/2021","Daqing Hou","NY","Clarkson University","Standard Grant","Alan Sussman","08/31/2023","$299,998.00","Ming-Cheng Cheng, Yu Liu","dhou@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","044Y","9179","$0.00","Proper Orthogonal Decomposition (POD) is a highly effective, data-driven learning algorithm for solving multi-dimensional Ordinary/Partial Differential Equations (ODEs/PDEs). However, POD is rarely covered in the typical graduate curriculum, and thus the nation is not fully leveraging this advanced algorithm in science and engineering research. To fill this void, this project will conduct a two-week online workshop for trainees in engineering and science-related disciplines and will integrate the developed instructional material into an existing graduate course on High-performance Computing (HPC). In doing so, this project provides for an educational ecosystem enabling computational and data-driven science for scientists and engineers. By training a diverse group of graduate students, post-docs, and faculty members in various disciplines, this project will help prepare the scientific workforce for advanced CI-enabled research, which will serve to enhance research productivity and enable researchers to effectively address complex societal problems. <br/><br/>The project will help develop the national research workforce in areas of critical need through intensive, integrated instruction on open-source computing platforms to solve ODEs/PDEs by use of POD models that employ HPC. The training will incorporate team-based interdisciplinary projects based on data-driven POD learning algorithm for computationally intensive multiphysics simulation problems in various science and engineering disciplines. The workshop will provide trainees with intensive instruction on POD and related topics, including open source platforms to solve ODEs/PDEs and eigenvalue problems. Training culminates in a research project where trainees learn advanced computational tools and implementation of HPC skills. The project will broaden the access and adoption of advanced CI while integrating CI skills into existing curriculum models and fostering inter-disciplinary and inter-institutional research collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104009","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","PHYSICAL OCEANOGRAPHY, Climate & Large-Scale Dynamics, Software Institutes","08/01/2021","07/15/2021","Mathieu Morlighem","CA","University of California-Irvine","Standard Grant","Tevfik Kosar","09/30/2021","$461,611.00","","Mathieu.Morlighem@dartmouth.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","1610, 5740, 8004","077Z, 1079, 4444, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019163","CC* Integration-Small: Error Free File Transfer for Big Science","OAC","CISE Research Resources","07/01/2020","07/01/2020","Craig Partridge","CO","Colorado State University","Standard Grant","Deepankar Medhi","06/30/2023","$470,384.00","Anton Betten, Susmit Shannigrahi","craig.partridge@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","2890","","$0.00","Scientific data transfers have gotten so large that previously rare transmission errors in the Internet are causing some scientific data transfers to be corrupted. The Internet's error checking mechanisms were designed at a time when a megabyte was a large file. Now files can contain terabytes. The old error checking mechanisms are in danger of being overwhelmed.  This project seeks to find new error checking mechanisms for the Internet to safely move tomorrow's scientific data efficiently and without errors.<br/><br/>This project addresses two fundamental issues. First, the Internet's checksums and message digests are too small (32-bits) and probably are poorly tuned to today's error patterns.  It is a little-known fact that checksums can (and typically should) be designed to reliably catch specific errors. A good checksum is designed to protect against errors that it will actually encounter. So the first step in this project is to collect information about the kinds of transmission errors currently happening in the Internet for a comprehensive study.  Second, today's file transfer protocols, if they find a file has been corrupted in transit, simply discard the file and transfer it again.  In a world in which the file is huge (tens of terabytes or even petabytes long), that's a tremendous waste.  Rather, the file transfer protocol should seek to repair the corrupted parts of the file.  As the project collects data about errors, it will also design a new file transfer protocol that can incrementally verify and repair files.<br/><br/>This project will improve the Internet's ability to support big data transfers, both for science and commerce, for decades to come.  Users will be able to transfer big files with confidence that the data will be accurately and efficiently copied over the network. This work will further NSF's Blueprint for a National Cyberinfrastructure Ecosystem by ensuring a world in which networks work efficiently to deliver trustworthy copies of big data to anyone who needs it. Additional information on the project is available at: www.hipft.net<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2002985","CDS&E: Collaborative Research: Private Data Analytics, Synthesis, and Sharing for Large-Scale Multi-Modal Smart City Mobility Research","OAC","CDS&E","07/01/2020","06/24/2020","Yuan Tian","VA","University of Virginia Main Campus","Standard Grant","Tevfik Kosar","06/30/2023","$165,000.00","","yt2e@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8084","026Z, 8084","$0.00","Given the trend towards urbanization, understanding real-time human mobility in urban areas has become increasingly important for many research areas from Mobile Networking, to Transportation/Urban Planning, Behavior Modeling, Emergency Response, to recent Pandemic Mitigation. Many analytical models have been proposed to understand human mobility based on mobility data. However, most of these data are proprietary and cannot be accessed by the research community at large. Fortunately, based on the latest expansion of urban infrastructures, such mobility data has been collected by city government agencies and some companies that are willing to share the data for social good. However, a key challenge is the privacy concern since such data usually have sensitive information and system design details for potential privacy and security issues. To address this issue, the project aims to generate realistic yet synthetic mobility data through machine learning based on the real mobility data analytics and then share these realistic synthetic data with the research community. The objective of the project is to lower the entry barriers for interdisciplinary researchers in mobility data-intensive research aimed at addressing major scientific/societal challenges related to urban mobility.<br/><br/>The core merit of the project lies in integrating two aims, i.e., privacy-preserving data synthesis and data integration, for large-scale smart city mobility research. For the first research aim, the project plans to utilize recent advances in Generative Adversarial Networks (GANs) to enable large-scale mobility data synthesis. The goal is to achieve the individual-level release of realistic synthetic mobility data by GAN-based models targeting key characteristics of human mobility. The GAN architecture proposed has novel technical components to augment basic GAN frameworks, which optimize the fundamental trade-off between privacy (regarding removing/obfuscating sensitive mobility features) and utility (in terms of preserving non-sensitive mobility features) with long-range dependencies (in terms of repeated mobility patterns) revealed. For the second research aim, the PIs plans to perform multi-modal data integration based on aligned multi-tensor decomposition under mobility semantics. The technical approach proposed is to enable multi-modal data integration based on synthetic single-modal data for comprehensive mobility modeling with a set of machine learning techniques including novel mobility semantic learning and multi-tensor decomposition with aligned spatiotemporal granularity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019541","Engaging Doctoral Students in Autonomic and Self-Organizing Computing Systems Research","OAC","EDUCATION AND WORKFORCE","07/01/2020","06/22/2020","Miaoqing Huang","AR","University of Arkansas","Standard Grant","Alan Sussman","06/30/2022","$7,500.00","","mqhuang@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","7361","026Z, 7556, 7923, 9150, 9179","$0.00","The new IEEE IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS) Conference, a merger of The IEEE International Conference on Autonomic Computing (ICAC) and the IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO), is a premier conference in the fields of autonomic computing, self-adaptation and self-organization.  The conference, to be held online in August 2020, brings together researchers and industry practitioners to develop self-adapting, self-organizing, and automatic systems for addressing a number of challenges in the field, for example developing novel modeling techniques for interactions between local and global behaviors.  The project will support a diverse array of students to participate in the doctoral symposium at the conference, to engage deeply with the ACSOS research community.  The students will also have the opportunity to participate in all other conference activities, which are being modified to an online format.  The project serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Recruitment efforts will focus on various underrepresented groups in science and engineering.  For the doctoral symposium, students will attend oral and poster presentations of research projects, and participate in a professional training and career development session to receive advice from experts in the field. That session will include tips on conducting research efficiently and ethically, writing and presenting research papers, seeking jobs in leading industry and academia after graduation, and realizing long-term professional and academic objectives. The funding provided by NSF will have a significant impact on the careers of the future generation of researchers in autonomic computing, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027007","RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response","OAC","COVID-19 Research","05/01/2020","04/20/2020","Viktor Prasanna","CA","University of Southern California","Standard Grant","Seung-Jong Park","04/30/2021","$158,592.00","Ajitesh Srivastava","prasanna@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","158Y","075Z, 077Z, 096Z, 7914","$0.00","The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?<br/><br/>Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005012","Elements: Data-Science Methods for Resource Allocation During Characterization of Dynamic Systems","OAC","Special Initiatives, DMR SHORT TERM SUPPORT, Software Institutes","06/15/2020","06/10/2020","Michael Groeber","OH","Ohio State University","Standard Grant","Amy Walton","05/31/2023","$596,785.00","Stephen Niezgoda","groeber.9@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","1642, 1712, 8004","054Z, 062Z, 077Z, 094Z, 7923, 9216","$0.00","The project develops the software infrastructure and its integration with physical infrastructure that is required to bring state-of-the-art data science, machine learning, and artificial Intelligence (AI) tools to novel materials science experiments at a national user facility. The work will create an openly available control package that will enable dynamic experiments informed by modeling in real-time. The developments will make more efficient use of scarce beam-time at national synchrotron user facilities, enabling higher scientific throughput for in-situ experiments probing the mechanical response of materials under load.<br/><br/>The effort evaluates the hypothesis that rare material failure events can be predicted from a small number of features that describe evolving local material states using machine learning solutions. The project is focusing on synchrotron x-ray scattering measurements of materials under mechanical load, and specifically integrating new and existing toolsets into a control package capable of dynamic resource allocation for hyper-efficient data collection at the Cornell High Energy Synchrotron Source (CHESS), a national user facility. The project integrates these toolsets to detect precursor signatures through real-time processing of data from user facilities such as CHESS, and suggests resource allocations to facilitate study of early stages of stochastic events in dynamic materials systems. The goal is to develop machine learning (ML) techniques and software infrastructure to inform the best, in a probabilistic sense, allocation of limited detector resources at material testing facilities, to better capture early stages of rare events in materials and the key factors for these events. The effort is also interested in applying the same resource allocation strategies to computational resource allocation in simulations of materials systems.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Civil, Mechanical and Manufacturing Innovation (CMMI) within the NSF Directorate for Engineering, and the Division of Materials Research (DMR) within the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004571","CSSI: Frameworks: X-Ion Collisions with a Statistically and Computationally Advanced Program Envelope (X-SCAPE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2020","04/28/2020","Abhijit Majumder","MI","Wayne State University","Standard Grant","Amy Walton","06/30/2024","$4,048,831.00","Loren Schwiebert, Joern Putschke, Chun Shen","abhijit.majumder@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1253, 7244, 8004","075Z, 077Z, 7569, 7925","$0.00","High-energy colliders around the world currently study the production of, and interaction between, a variety of subatomic particles and environments produced in the collision of protons with protons, with nuclei, or even between two nuclei, as at the Large Hadron Collider (LHC) at the European Center for Nuclear Research (CERN), or at the Relativistic Heavy-Ion Collider (RHIC) at Brookhaven National Laboratory (BNL). The future Electron Ion Collider (EIC), slated for construction at BNL, will study collisions of electrons with protons and nuclei. Interactions at all these experiments are dominated by the strong nuclear force, whose behavior is still not well understood. Direct comparison between theory and experiment requires sophisticated computer simulations, where each stage of a collision is modeled via a combination of established principles or candidate theories. A complete simulation, consisting of a number of sub-simulations, depends on several input parameters. The extracted values for these parameters represent fundamental properties of strongly interacting matter. A simultaneous determination of these parameters, in extensive comparisons with volumes of data from diverse experiments, requires an elaborate statistical and computational framework. The X-SCAPE collaboration, a multi-disciplinary team of physicists, computer scientists and statisticians, is engaged in the construction of such an open-source framework. The expertise required to operate, modify and extend this framework is disseminated to practicing scientists via a combination of dedicated short-term schools (in-person and virtual), topical workshops, web tutorials, conference presentations and detailed publications.<br/><br/>The X-SCAPE project encompasses the entire high-energy nuclear physics enterprise by providing a general purpose, modifiable, and modular framework, that incorporates all known interaction processes prevalent at these experiments. The developed software is designed to be portable onto hybrid architectures, where GPU acceleration can be applied to the most demanding computational tasks. Provided as a steadily improving package culminating in annual releases, X-SCAPE includes of i) the default distribution of baseline tools allowing for simplified simulations on commodity CPUs, ii) a refactored product for distributed architectures with GPUs, and iii) Bayesian statistical routines to both emulate and systematically explore the model space of parameters against diverse experimental data.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier (PIF) Program within the Division of Physics and the Office of Multidisciplinary Activities within the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1950052","REU Site: Data + Computing = Discovery","OAC","RSCH EXPER FOR UNDERGRAD SITES","06/01/2020","03/19/2020","Matthew Reuter","NY","SUNY at Stony Brook","Standard Grant","Alan Sussman","05/31/2023","$405,000.00","Alan Calder","matthew.reuter@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","1139","9250","$0.00","Data + Computing = Discovery (DCD) combines faculty from many programs at Stony Brook University to work with enthusiastic undergraduate students and advance knowledge in many disciplines, including life sciences, social sciences, engineering, and physical sciences. The unifying themes for the REU site are computational and data sciences, which are inherently interdisciplinary and becoming complementary avenues for scientific investigation alongside experiment and theory. Furthermore, the number of students, particularly undergraduates, who develop the required skills for success in computational and data sciences without access to training is very small. Thus, DCD offers resources and training to undergraduate students in computational and data sciences, which will drive new lines of scientific inquiry and research. Once at DCD, participants will learn computer programming skills, apply these skills to research, practice communicating ideas for broad audiences, and meet other outstanding young students. The primary impact of DCD is the lifetime of contributions to many fields of study from the participants that DCD helps assemble, train, and inspire.<br/><br/>To accomplish these goals, DCD will host undergraduate participants for a 9-week program every summer. Participants will be matched with Stony Brook faculty who are expert in the physical sciences, life sciences, engineering, and social sciences and have established records of high productivity both in computational research and in fostering student engagement. In addition to working on an original research project with these faculty members, every DCD participant will (i) participate in a course on computer programming with Python, (ii) complete a research methods workshop on the practice of research from ideation to publication, (iii) learn about careers and graduate school opportunities in computational and data sciences, (iv) practice communicating their ideas to a variety of technical and non-technical audiences, (v) present their research in a poster symposium, and (vi) enjoy social and networking activities with other students to form long-term collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818253","Computation for the Endless Frontier","OAC","CYBERINFRASTRUCTURE, Leadership-Class Computing","09/01/2018","10/15/2020","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","02/29/2024","$66,999,135.00","Dhabaleswar Panda, Omar Ghattas, Tommy Minyard, John West","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7231, 7781","026Z, 097Z, 7781","$0.00","Computation is critical to our nation's progress in science and engineering. Whether through simulation of phenomena where experiments are costly or impossible, large scale data analysis to sift the enormous quantities of digital data scientific instruments can produce, or machine learning to find patterns and suggest hypothesis from this vast array of data, computation is the universal tool upon which nearly every field of science and engineering relies upon to hasten their advance. This project will deploy a powerful new system, called ""Frontier"", that builds upon a design philosophy and operations approach proven by the success of the Texas Advanced Computing Center (TACC) in delivering leading instruments for computational science. Frontier provides a system of unprecedented scale in the NSF cyberinfrastructure that will yield productive science on day one, while also preparing the research community for the shift to much more capable systems in the future.  Frontier is a hybrid system of conventional Central Processing Units (CPU) and Graphics Processing Units (GPU), with performance capabilities that significantly exceeds prior leadership-class computing investments made by NSF.  Importantly, the design of Frontier will support the seamless transition of current NSF leadership-class computing applications to the new system, as well as enable new large-scale data-intensive and machine learning workloads that are expected in the future.  Following deployment, the project will operate the system in partnership with ten academic partners.  In addition, the project will begin planning activities in collaboration with leading computational scientists and technologists from around the country, and will leverage strategic public-private partnerships to design a leadership-class computing facility with at least ten times more performance capabilities for Science and Engineering research, ensuring the economic competitiveness and prosperity for our nation at large.<br/><br/>TACC, in partnerships with Dell EMC and Intel, will deploy Frontier, a hybrid system offering 39 PF (double precision) of Intel Xeon processors, complemented by 11 PF (single precision) of GPU cards for machine learning applications. In addition to 3x the per node memory of NSF's prior leadership-class computing system primary compute nodes, Frontier will have 2x the storage bandwidth in a storage hierarchy that includes 55PB of usable disk-based storage and 3PB of 'all flash' storage, to enable next generation data-intensive applications and support for the data science community.  Frontier will be deployed in TACC's state-of-the-art datacenter which is configured to supply 30% of the system's power needs from renewable energy.  Frontier will include support for science and engineering in virtually all disciplines through its software environment support for application containers, as well as through its partnership with ten academic institutions providing deep computational science expertise in support of users on the system. The project planning effort for a Phase 2 system with at least 10x performance improvement will incorporate a community-driven process that will include leading computational scientists and technologists from around the country and leverage strategic public-private partnerships.  This process will ensure the design of a future NSF leadership-class computing facility that incorporates the most productive near-term technologies, and anticipates the most likely future technological capabilities for all of science and engineering requiring leadership-class computational and data-analytics capabilities.  Furthermore, the project is expected to develop new expertise and techniques for leadership-class computing and data-driven applications that will benefit future users worldwide through publications, training, and consulting.  The project will leverage the team's unique approach to education, outreach, and training activities to encourage, educate, and develop the next generation of leadership-class computational science researchers. The team includes leaders in campus bridging, minority-serving institute (MSI) outreach, and data technologies who will oversee efforts to use Frontier to increase the diversity of groups using leadership-class computing for traditional and data-driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835877","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","07/02/2021","Barbara Minsker","TX","Southern Methodist University","Standard Grant","Amy Walton","08/31/2023","$584,151.00","Kenneth Berry, Jessie Zarazaga","minsker@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147684708","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2147601","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","PHYSICAL OCEANOGRAPHY, Climate & Large-Scale Dynamics, Software Institutes","09/15/2021","09/03/2021","Mathieu Morlighem","NH","Dartmouth College","Standard Grant","Tevfik Kosar","09/30/2025","$461,611.00","","Mathieu.Morlighem@dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","CSE","1610, 5740, 8004","077Z, 1079, 4444, 7925, 8004, 9150","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118204","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","10/01/2021","09/02/2021","Neeraj Rai","MS","Mississippi State University","Standard Grant","Alan Sussman","09/30/2025","$580,074.00","","neerajrai@che.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","CSE","044Y","7231, 7301, 9150","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2037661","COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products","OAC","Software Institutes","09/01/2020","07/22/2020","Ritu Ritu","TX","University of Texas at San Antonio","Standard Grant","Seung-Jong Park","08/31/2022","$175,000.00","","ritu.arora@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8004","077Z, 7916, 8004","$0.00","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination.<br/> <br/>The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2037656","COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products","OAC","Software Institutes","09/01/2020","07/22/2020","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Seung-Jong Park","08/31/2022","$124,585.00","","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","077Z, 7916, 8004","$0.00","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination.<br/> <br/>The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940231","Collaborative Research: Autonomous Computing Materials","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Mark Bathe","MA","Massachusetts Institute of Technology","Continuing Grant","Daryl Hess","09/30/2021","$334,231.00","","mark.bathe@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","099Y, 1978","062Z, 9263","$0.00","The recent explosion in worldwide data together with the end of Moore's Law and the near-term limits of silicon-based data storage being reached are driving an urgent need for alternative forms of computing and data storage/retrieval platforms. In particular, exabyte-scale datasets are increasingly being generated by the biological sciences and engineering disciplines including genomics, transcriptomics, proteomics, metabolomics, and high-resolution imaging, as well as disparate other scientific fields including climate science, ecology, astronomy, oceanography, sociology, and meteorology, amongst others. In this data revolution, the continuously increasing size of these datasets requires a concomitant increase in available computational power to store, process, and harness them, which is driving a need for revolutionary new, alternative substrates for, and forms of, computing and data storage. Unlike traditional data storage and computing materials such as silicon, the human brain offers a remarkable ability to sense, store, retrieve, and compute information in a manner that is unrivaled by any human-made material. In this research project, analogous modes of information sensing, data storage, retrieval, and computation will be explored in non-traditional computing molecular systems and materials. The over-arching goal of the research is to discover revolutionary new modes of data storage/retrieval, sensing, and computation that rival conventional silicon-based technology, for deployment to benefit society broadly across all domains of data science. Graduate students and postdocs across five institutions will be trained and mentored in a highly interdisciplinary manner to attain this goal and prepare the next-generation of data scientists, chemists, physicists, and engineers to harness the ongoing data revolution. The research will be disseminated to a broad community through news outlets and integration of high school student internships in participating research laboratories. <br/><br/>Large-scale datasets from spatial-temporal calcium imaging of the mouse brain will be recorded into DNA-based, nanoparticle-based, and phononic 2D and 3D soft and hard materials. Continuous spatial-temporal data will first be transformed into discrete data for mapping onto DNA-conjugated fluorophore networks, dynamic barcoded nanoparticle networks, and phononic 2D and 3D materials. Sensing, computation, and data storage/retrieval will be demonstrated as proofs-of-principle in exploiting the chemical properties of molecular networks and materials to recover the encoded neuronal datasets and their sensing and computing processes. Success with any of these three prototypical materials would revolutionize the ability to encode arbitrarily complex, large-scale datasets into complex molecular systems, with the potential to scale across diverse data domains and materials frameworks. The investigators' Autonomous Computing Materials framework will thereby enable the encoding of arbitrary ""big data"" sets into diverse materials for data storage, sensing, and computing. This project maximizes opportunities for disruptive new computing and data science concepts to emerge from a multi-disciplinary, collaborative team spanning data science, neuroscience, materials science, chemistry, physics, and biological engineering. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934633","Collaborative Research: Knowledge Guided Machine Learning: A Framework for Accelerating Scientific Discovery","OAC","HDR-Harnessing the Data Revolu","09/01/2019","07/02/2020","Paul Hanson","WI","University of Wisconsin-Madison","Continuing Grant","Eva Zanzerkia","08/31/2022","$341,854.00","Hilary Dugan","pchanson@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","099Y","062Z","$0.00","The success of machine learning (ML) in many applications where large-scale data is available has led to a growing anticipation of similar accomplishments in scientific disciplines. The use of data science is particularly promising in scientific problems involving processes that are not completely understood. However, a purely data-driven approach to modeling a physical process can be problematic. For example, it can create a complex model that is neither generalizable beyond the data on which it was trained nor physically interpretable. This problem becomes worse when there is not enough training data, which is quite common in science and engineering domains.  A machine learning model that is grounded by explainable theories stands a better chance at safeguarding against learning spurious patterns from the data that lead to non-generalizable performance. This is especially important when dealing with problems that are critical and associated with high risks (e.g., extreme weather or collapse of an ecosystem).  Hence, neither an ML-only nor a scientific knowledge-only approach can be considered sufficient for knowledge discovery in complex scientific and engineering applications. This project is developing novel techniques to explore the continuum between knowledge-based and ML models, where both scientific knowledge and data are integrated synergistically. Such integrated methods have the potential for accelerating discovery in a range of scientific and engineering disciplines. This project will train interdisciplinary scientists who are well versed in such methods and will disseminate results of the project via peer-reviewed publications, open-source software, and a series of workshops to engage the broader scientific community.<br/><br/>This project aims to develop a framework that uses the unique capability of data science models to automatically learn patterns and models from data, without ignoring the treasure of accumulated scientific knowledge. Specifically, the project builds the foundations of knowledge-guided machine learning (KGML) by exploring several ways of bringing scientific knowledge and machine learning models together using pilot applications from four domains: aquatic ecodynamics, climate and weather, hydrology, and translational biology. These pilot applications were selected because they are at tipping points where knowledge-guided machine learning can have a transformative effect.  KGML has the potential for providing scientists and engineers with new insights into their domains of interest and will require the development of innovative new machine learning approaches and architectures that can incorporate scientific principles. Scientific knowledge, KGML methods, and software developed in this project could potentially be extended to a wide range of scientific applications where mechanistic (also known as process-based) models are used.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940243","Collaborative Research: Integrating Physics and Generative Machine Learning Models for Inverse Materials Design","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","08/12/2020","Michael Lawler","NY","SUNY at Binghamton","Continuing Grant","Daryl Hess","05/31/2022","$338,320.00","","mlawler@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","099Y, 1712","054Z, 062Z, 8396, 8399","$0.00","This project is aimed to address a grand challenge in data-intensive materials science and engineering to find better materials with desired properties, often with the goal to enhance performance in specific applications. This project addresses this grand challenge with a specific focus on finding metal organic framework (MOF) materials that are used to separate gas mixtures and finding better battery materials for energy storage. The PIs will combine theoretical methods from statistical mechanics and condensed-matter physics, and physics-based models, to generate information-rich materials data which is integrated with generative machine learning (ML) algorithms to search a complex chemical design space efficiently and to train deep learning models for fast screening of materials properties. This project will be carried out by a multidisciplinary collaboration involving researchers from physics, materials science and engineering, computer science, and mathematics. The resulting multidisciplinary environment fosters training the next generation data savvy scientists who will engage in collaborative multidisciplinary research.  <br/><br/>Existing approaches for computational design of metal organic frameworks (MOF) and solid-state electrolyte materials are largely based on screening of known materials or enumerative search of hypothetical materials. This project develops a new approach that integrates first principles calculations, experimental data and abundant data generated by physics-based models to train generalized antagonistic network (GAN) models for efficient search of the materials design space, and to train deep convolutional neural network (DCNN) models for fast and accurate screening of properties of the GAN-generated candidate materials. Additionally, graph-based GAN models will be used for MOF topology exploration and can be applied to other nanomaterials designs. More specifically, the investigators will: 1) develop and exploit physics-based models for fast calculation of properties such as diffusivity, ion conductivity, and mechanical stability; 2) develop generative adversarial network (GAN) models with built-in physics rules for efficient exploration of the chemical design space for both MOF materials and solid electrolytes; 3) use persistence homology and Bravais lattice sequence representations of MOF materials and solid electrolytes, respectively, to build Deep Convolutional Neural Network (DCNN) models for fast and accurate prediction of the physical properties of generated materials; 4)  apply high-level quantum-mechanical calculations for verification of discovered materials. Accomplishments from this project will lead to accelerated discovery of novel nanostructured materials for gas separation and energy storage, materials for lithium-ion batteries, novel data-driven scheme for materials design, and theoretical methods enabling implementation of advanced data science techniques. The highly interdisciplinary collaboration will offer students unique opportunities to interact with a variety of disciplines, and training the next-generation scientists with the mindset for multidiscipline collaborations. Educational and outreach activities will be developed and undertaken in conjunction with the proposed research activities.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940166","Collaborative Research: Integrating Physics and Generative Machine-Learning Models for Inverse Materials Design","OAC","HDR-Harnessing the Data Revolu, DMR SHORT TERM SUPPORT","10/01/2019","10/15/2020","Yifei Mo","MD","University of Maryland, College Park","Continuing Grant","Daryl Hess","09/30/2022","$400,000.00","","yfmo@umd.edu","3112 LEE BLDG 7809 Regents Drive","College Park","MD","207425141","3014056269","CSE","099Y, 1712","054Z, 062Z, 8396, 8399","$0.00","This project is aimed to address a grand challenge in data-intensive materials science and engineering to find better materials with desired properties, often with the goal to enhance performance in specific applications. This project addresses this grand challenge with a specific focus on finding metal organic framework (MOF) materials that are used to separate gas mixtures and finding better battery materials for energy storage. The PIs will combine theoretical methods from statistical mechanics and condensed-matter physics, and physics-based models, to generate information-rich materials data which is integrated with generative machine learning (ML) algorithms to search a complex chemical design space efficiently and to train deep learning models for fast screening of materials properties. This project will be carried out by a multidisciplinary collaboration involving researchers from physics, materials science and engineering, computer science, and mathematics. The resulting multidisciplinary environment fosters training the next generation data savvy scientists who will engage in collaborative multidisciplinary research.  <br/><br/>Existing approaches for computational design of metal organic frameworks (MOF) and solid-state electrolyte materials are largely based on screening of known materials or enumerative search of hypothetical materials. This project develops a new approach that integrates first principles calculations, experimental data and abundant data generated by physics-based models to train generalized antagonistic network (GAN) models for efficient search of the materials design space, and to train deep convolutional neural network (DCNN) models for fast and accurate screening of properties of the GAN-generated candidate materials. Additionally, graph-based GAN models will be used for MOF topology exploration and can be applied to other nanomaterials designs. More specifically, the investigators will: 1) develop and exploit physics-based models for fast calculation of properties such as diffusivity, ion conductivity, and mechanical stability; 2) develop generative adversarial network (GAN) models with built-in physics rules for efficient exploration of the chemical design space for both MOF materials and solid electrolytes; 3) use persistence homology and Bravais lattice sequence representations of MOF materials and solid electrolytes, respectively, to build Deep Convolutional Neural Network (DCNN) models for fast and accurate prediction of the physical properties of generated materials; 4)  apply high-level quantum-mechanical calculations for verification of discovered materials. Accomplishments from this project will lead to accelerated discovery of novel nanostructured materials for gas separation and energy storage, materials for lithium-ion batteries, novel data-driven scheme for materials design, and theoretical methods enabling implementation of advanced data science techniques. The highly interdisciplinary collaboration will offer students unique opportunities to interact with a variety of disciplines, and training the next-generation scientists with the mindset for multidiscipline collaborations. Educational and outreach activities will be developed and undertaken in conjunction with the proposed research activities.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2140453","The Technical foundations of prosperity","OAC","CYBERINFRASTRUCTURE","09/01/2021","08/31/2021","Neil Thompson","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","08/31/2022","$98,691.00","","neil_t@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7231","7231, 7556","$0.00","The workshop brings together researchers at the intersection of technology and economics to help understand the large technical trends that are changing how computing is done, how these trends are evolving, and what this will mean for the benefits provided to society.  Participants include both those interested in the microeconomics of technology and those interested in the technological foundations of prosperity and economic growth.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117941","MRI: Acquisition of a GPU Cluster for Multi-Disciplinary Research and Education at University of Nevada, Las Vegas","OAC","Major Research Instrumentation, Information Technology Researc, EPSCoR Co-Funding","09/01/2021","08/31/2021","Mingon Kang","NV","University of Nevada Las Vegas","Standard Grant","Alejandro Suarez","08/31/2024","$432,269.00","","mingon.kang@unlv.edu","4505 MARYLAND PARKWAY","Las Vegas","NV","891549900","7028951357","CSE","1189, 1640, 9150","1189, 9150","$0.00","The project funds the purchase and commissioning of a high-performance graphical processing unit (GPU) cluster at the University of Nevada, Las Vegas (UNLV). The project will address emergent and longer-term needs, challenges, and opportunities in research and educational efforts across multiple disciplines: biomedical research, intelligent transportation systems and automated vehicles, genomics, astronomy, and physics. The project will help advance national initiatives in big data, strategic computing, artificial intelligence, and smart infrastructure systems. These will integrate, synthesize, model, and visualize large volumes of data from various sources as well as develop and apply Artificial Intelligence (AI) techniques to assist decision making. For applied and basic research aspects of the project, the GPU cluster will leverage advances in computing hardware, software, sensor networks, and communications systems. Some elements of the project will address near-term societal needs to preserve and enhance the quality of living of individuals and families, support economic competitiveness and growth of businesses, and foster the vitality of communities. These include topics related to public health, transportation, environment, and energy. The project?s longer-term initiatives will address explorations and innovations in basic research in these domains as well as in astronomy, physics, and genomics. These activities will include partnerships with academia, government entities, and private sector organizations. The project will support curricular and co-curricular activities for undergraduate and graduate students to increase their interests in related education, research, and career opportunities. As a Minority-Serving Institution and Hispanic Serving Institution, this grant will help UNLV to significantly expand such opportunities for students from varied socio-economic and socio-demographic communities. Thus, an outcome of the project will be to help develop skilled work-forces from diverse backgrounds. <br/><br/>The GPU cluster will support basic and applied research, as well as educational programs across multiple disciplines. Common elements for the research efforts include integrating, synthesizing, modeling, and visualizing large volumes of data along with the development and application of various AI techniques to support decision making. Efforts in Biomedicine will be to stratify individuals at risk for benzodiazepine and opioid overdose using interpretable deep learning techniques using publicly available pluripotency transcription factors datasets. Activities related to intelligent transportation systems and automated vehicles will address comprehensive trajectory prediction challenges for near real-time applications on transportation networks to help accelerate the deployment of Connected Automated Vehicles and Infrastructure Systems; they will use data from various in-vehicle, on-roadway, and roadside sensors. Research in Genomics will be to better understand the evolution of novel transcription factor (TF) binding sites originating from endogenous retrovirus (ERV) integration. Astronomy related endeavors will be to estimate planet mass from protoplanetary disk images using Convolutional Neural Networks (CNN). Efforts in Physics will be to develop rotationally equivariant CNN to simulate and evaluate force fields at atomistic scales of materials. Educational aspects of the project will include curricular and co-curricular initiatives at the undergraduate and graduate levels to help alert, engage, excite, and motivate students to pursue education, research, and career opportunities in related fields. The project will include partnerships with public and private sector organizations and academia.<br/><br/>This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032705","EAGER: Amplifying Community Readiness to Increase Public Access to Data","OAC","NSF Public Access Initiative","09/01/2020","07/20/2020","Melissa Cragin","CA","University of California-San Diego","Standard Grant","Martin Halbert","08/31/2022","$149,999.00","","mcragin@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7414","7916","$0.00","This exploratory project in open science will examine and address uptake of the NSF Public Access Repository (PAR) for deposition of metadata records for research datasets that support validation of results in publications, or for data resources described in publications. In particular, there is interest in how practices are changing for the use of data identifiers (e.g. DOIs). The project will partner with domain and technical organizations to extend the reach of this work. Studying researcher?s current data production, planning for deposit or exposing those data, and roles of their Data Management Plan (DMP), will provide insights on how to foster change ? away from the view of ?DMP as a box-checking? ? toward the use of the DMP as a value-add process for linking research project planning with increased transparency for the benefit of public access.<br/> <br/>This study will begin with two different research areas: Initial work will focus on the Geosciences, and computationally-based communities (e.g. Data Science and AI/Machine Learning). For the Geosciences, there are opportunities to improve data publishing practices, by obtaining DOIs and increasing data citation. At the intersections of the Geosciences, semantic technologies, computing and informatics, there are Open Knowledge Networks which have the potential to generate new data resources to support solutions to societal grand challenges. Across the Data Science and AI-related communities, there is a need for better access to high quality datasets, models, and infrastructure; currently, resources for locating, accessing, and selecting such datasets are scattered and variable. Utilizing the uptake of the NSF PAR system for research data may facilitate increased access to datasets for research and education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931512","Elements: RADICAL-Cybertools: Middleware Building Blocks for NSF's Cyberinfrastructure Ecosystem.","OAC","Software Institutes","01/01/2020","07/22/2019","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Amy Walton","12/31/2022","$599,981.00","","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8004","075Z, 077Z, 7923","$0.00","This project builds upon a previously funded middleware development effort by the Research in Advanced Distributed Cyberinfrastructure and Applications Laboratory (RADICAL) at Rutgers University; the previously developed middleware building blocks were known as RADICAL-Cybertools (RCT).  In the current project, the team pursues a targeted set of developments, driven by the need to  scale the number of software components, user, and supported platforms; and improve performance, engineering processes, and sustainability.  The resulting capabilities will serve scientific applications in multiple domains, including software engineering, chemical physics, materials science, health science, climate science, drug discovery and particle physics.<br/><br/>This project builds upon a prior prototype investment, which developed a pilot system for leadership-class HPC machines, and a Python implementation of SAGA, a distributed computing standard.  The current effort is organized around three activities:<br/> - Extending RCT functionality to reliably support a range of novel applications at scale (examples include tightly coupling traditional HPC simulations with machine-learning methods);<br/> - Enhancing RCT to be ready to support new NSF systems, such as the Frontera supercomputing system and other new systems;<br/> - Prototyping a new component: a campaign manager for computational resource management. <br/>Data-driven approaches will be used to improve software development, engineering, and life-cycle management, and to enhance the long-term sustainability of RCT and the supported communities.  The project includes use cases that are representative examples of the growing community that RCT engages and supports, such as the ATLAS high-energy physics project and the QCArchive project enabling large-scale force-field construction and physical property prediction.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835267","Elements: Software: NSCI: A Quantum Electromagnetics Simulation Toolbox (QuEST) for Active Heterogeneous Media by Design","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/15/2018","09/06/2018","Carlo Piermarocchi","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2022","$563,340.00","Shanker Balasubramaniam","carlo@pa.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1712, 8004","026Z, 053Z, 077Z, 7923, 7926, 8004, 8990, 9216, 9263","$0.00","Designing novel optical materials with enhanced properties would impact many areas of science and technology, leading to new lasers, better components for photonics, and to a deeper understanding of how light interacts with matter. This project will develop software that simulates how light would propagate in yet to be made complex optical materials. The final product will be a software toolbox that computes the dynamics of each individual light emitter in the materials rather than calculating an average macroscopic field. This toolbox will permit the engineering and optimization of optical properties by combining heterogeneous components at the nanoscale. The software will be disseminated widely to enable scientists worldwide to conduct research on this area and will provide a blueprint for broader applications to magnetic materials and ultrasound acoustics. Two graduate students will be engaged in this research and will be trained in interdisciplinary topics encompassing fundamental physics, mathematics, materials science, and software engineering.<br/><br/>Three innovative modules will be implemented and tested in the software toolbox: (i) A module based on Time Domain Accelerated Integrated Methods. These methods rely on the separation into near field and far field terms in the interaction between optically active centers and introduce a hierarchical structure that can be computationally exploited. This module will dramatically reduce computational costs, leading to simulations of realistic systems with millions of optical emitters. (ii) A stochastic optimization module that maximizes materials functionalities based on geometrical and compositional distribution of the emitters in the medium. This optimization module will simulate in parallel several virtual samples and will guide the computational effort towards optimal materials. (iii) A rationalized representation of electromagnetic field localization based on the novel mathematical concept of landscape functions, which effectively reduces the eigenvalue problem associated to localization into a static boundary condition problem. This computationally efficient approach provides approximated eigenvalues and quickly identifies the sub-regions of the system that support electromagnetic field localization.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835834","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","08/09/2018","Kenton McHenry","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","08/31/2023","$3,752,045.00","Klara Nahrstedt, Praveen Kumar","kmchenry@ncsa.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924112","CyberTraining: Implementation: Small: Collaborative Research: Easy-Med: Interdisciplinary Training in Security, Privacy-Assured Internet of Medical Things","OAC","CyberTraining - Training-based, CYBERCORPS: SCHLAR FOR SER","09/01/2019","07/15/2019","Saraju Mohanty","TX","University of North Texas","Standard Grant","Alan Sussman","08/31/2022","$247,373.00","Elias Kougianos, Elias Mpofu","saraju.mohanty@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","044Y, 1668","026Z","$0.00","The combination of a network of physical devices embedded with electronics, Internet connectivity, and other hardware, such as sensors, that can communicate and interact with others over the Internet is known as the Internet of Things.  One familiar application of this technology is the ""smart home"" in which people monitor and control their lights, thermostats and security systems remotely using smart phones or smart speakers. An Internet of Things-based framework for the healthcare industry is called the Internet of Medical Things.  This technology connects patients to their physicians and supports the transfer of medical data over the Internet.  Concerns about the privacy of data transmitted over the Internet and network security are challenges facing all applications of the Internet of Things concept, but they can be exacerbated by the knowledge gap between the designers of the frameworks and the end users in the medical field.  As the healthcare industry grows to meet the needs of an aging population, the workforce that designs Internet of Medical Things devices and the networks that connect them must be ready to address these privacy and security concerns. The project is addressing this gap, and thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>Easy-Med is a multi-disciplinary training program designed to improve core literacy of cyber infrastructure for students at the undergraduate level in northeast Texas. The six-week-long mentored program provides immersive training to increase the students' ability to develop and use secure, privacy-assured sensing healthcare frameworks.  A different training module is provided each week and each day of the module includes four hours of lecture and three hours of hands-on lab exercises.  Training modules introduce the students to the different aspects involved in designing devices and networks for the Internet of Medical Things.  The six training modules, components of which are also provided online, include: 1) types of biosensors and Internet of Medical Things components, 2) system-level modeling of Internet of Medical Things networks, 3) signal and data analytics used in healthcare, 4) security and privacy assurance in Internet of Medical Things technology, 5) applications of biosensors in the healthcare industry, and 6) use of and ethics involved in using the Internet of Medical Things in the community setting.  Students who participate in Easy-Med during the summer are encouraged to further their knowledge and provide outreach about the program by participating in a Build-a-Thon activity during the following fall semester and a research symposium in the following spring semester.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924094","CyberTraining: Pilot: Institutional Cyberinfrastructure Training Center in a Box","OAC","CyberTraining - Training-based","08/01/2019","06/21/2021","Mark Meysenburg","NE","DOANE UNIVERSITY","Standard Grant","Alan Sussman","07/31/2022","$299,997.00","Mike Carpenter","mark.meysenburg@doane.edu","1014 Boswell Avenue","Crete","NE","683332426","4028266765","CSE","044Y","","$0.00","Several challenges impact CyberInfrastructure (CI) workforce development that go beyond significant increases in data generation and the need for interdisciplinary research.  There continues to be a general CI systems awareness problem which limits the number of students entering college to learn about CI systems or moving into computational fields later. The project establishes a ""coding center in a box,"" adoptable by other institutions, to support their own CI Users and enable the establishment of centers capable of providing student-led peer consultation and instruction to undergraduate students interested in programming, computational and data-driven science, and advanced CI tools for use in teaching and research. The project leverages Doane University's Center for Computing in the Liberal Arts (CCLA), an entity modeled after collegiate writing centers. Doane's CCLA targets CI Users, as well as students and faculty unfamiliar with or unaware of CI tools and technologies, with the goal of increasing technology adoption, specifically CI tools, in computational and data-driven research projects. Students and faculty can come for peer review and training on how to incorporate coding and other advanced CI tools into their own research and projects. A workshop on advanced CI techniques (high-performance and high-throughput computing on large datasets), targeted at undergraduate students and faculty, enables institutions to quickly bring their community up to speed on these techniques. Classroom modules, across a variety of disciplines, allow advanced CI techniques to be easily incorporated into existing classes, broadening the reach of CI knowledge.  The Doane University pilot project thus serves the national interest, as stated by NSF's mission - to promote the progress of science and to serve the national health, prosperity, and welfare. <br/><br/>This pilot project has the following specific objectives: (1) Build a campus culture for computing using a comprehensive marketing strategy; (2) Recruit, train, and manage a cohort of undergraduate students to serve as Peer Consultants and develop training materials for preparing Peer Consultants; (3) Create a comprehensive package of materials related to the implementation of a Coding Center; (4) Develop and implement a workshop in advanced CI techniques targeted at undergraduate students and faculty; (5) Develop five classroom modules that feature advanced CI techniques; and (6) compile and maintain a curated repository of large, open data sets available in the cloud that undergraduate students and faculty can access and study. The project tests whether a coding center model, in addition to CI course modules and a CI workshop focused on parallel computation using large data sets, can increase the adoption of CI techniques in underserved CI User communities, namely, undergraduate students majoring in natural science, computer science, mathematics, and engineering, and teaching faculty in those fields. The project also determines if these interventions increase computing self-efficacy and dispositions towards pursuing a career involving computation for undergraduate CI Users. The CCIMS project offers several avenues by which other institutions can support CI Users and advance computational and data-driven science and the use of advanced CI tools in education and research. The CCIMS project increases the number of undergraduate students engaged with CI, initially at Doane University, and later at outside institutions. The CCIMS project results in increased research productivity and competitiveness of students and faculty, and enhanced collaboration with other academic partners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931439","Collaborative Proposal: Frameworks: Project Tapis: Next Generation Software for Distributed Research","OAC","Software Institutes","09/01/2019","07/30/2019","Joseph Stubbs","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","08/31/2024","$2,985,754.00","Maytal Dahan","jstubbs@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","The goal of a robust cyberinfrastructure (CI) ecosystem is to become a catalyst for discovery and innovation by fostering the development of software frameworks as sustainable production-quality services. Modern science and engineering research increasingly span multiple, geographically distributed data centers and leverage instruments, experimental facilities and a network of national and regional CI. The Tapis framework will enable scientists to accomplish computational and data intensive research in a secure, scalable, and reproducible way allowing scientists to focus on their research instead of the technology needed to accomplish it.  Tapis will allow easier implementation, sharing and re-use of complex computational applications, workflows, and infrastructure and enable analysis previously too challenging for researchers. The framework will maximize application portability, allowing flexible scheduling of geographically distributed computational workloads, offer a web-based science-as-a-service to enable multi-facility, decentralized deployments, and provide production-grade support for sensors and streaming data.  Tapis will impact multiple science domains, geographic and underrepresented communities with the potential to tackle the world's most important scientific problems spanning astronomy, climate science, medicine, natural hazards, and sustainability science.  Education and outreach will include sponsored workshops, hackathons and training materials covering the platform and providing examples to encourage widespread adoption for users across a variety of technical skills and targeting the next generation of young researchers and professionals through immersive workshops and professional development opportunities.<br/><br/>Tapis, will be a new platform for distributed computational experiments that leverages NSF's investments in the Agave, Abaco and CHORDS projects. The Tapis software framework will 1) provide production-grade support for sensors and streaming data, 2) maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers, and 3) provide science-as-a-service HTTP-based RESTful APIs to enable multi-facility, decentralized deployments that are both secure and scalable. Working alongside a diverse set of domain researchers to drive real-world use cases, Tapis will be the underlying cyberinfrastructure for computational workflows and science gateways. Tapis will leverage containers to maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers.  The project will achieve this flexibility by introducing execution system capabilities and application requirements throughout the framework. The Jobs service will be run in a distributed manner to take advantage of data locality and, optionally, to schedule jobs on underutilized systems. Tapis will deploy in centralized or distributed configurations using a microservices architecture that includes a novel, decentralized security and authorization kernel. This kernel can be deployed on-premises to retain local control over confidential data. Custom microservices can be plugged into the security kernel to provide new capabilities, resulting in a cyberinfrastructure ecosystem for distributed computing. To effectively execute Tapis, teams from the Texas Advanced Computing Center (TACC), the University of Texas at Austin (UT), and the University of Hawaii (UH) will leverage a long-standing collaboration to support investigator-driven, geographically distributed, data-intensive research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934745","The Learner Data Institute: Harnessing The Data Revolution To Make The Learning Ecosystem More Effective, Efficient, and Engaging","OAC","Discovery Research K-12","09/01/2019","06/26/2020","Vasile Rus","TN","University of Memphis","Continuing Grant","Finbarr Sloane","08/31/2022","$2,584,309.00","Philip Pavlik Jr., Stephen Fancsali, Dale Bowman, Deepak Venugopal","vrus@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","099y, 7645","062Z, 7645, 9150","$0.00","The project will lay the foundation for a Learner Data Institute, to further understanding of how people learn, how to improve adaptive instructional systems, and how to create learning systems that are more effective, efficient, engaging, and affordable.  An interdisciplinary team of people from academia, industry, and government will work together to improve the effectiveness of the learning ecosystem for the benefit of its hundreds of thousands of students, prepare teachers for the future learning ecosystem, and contribute to accelerating discovery and transforming the education ecosystem through the use of big data and cloud computing. The project will impact a number of communities including learning sciences, data science, artificial intelligence in education, assessment, educational data mining, and machine learning. The outcomes will be widely disseminated through training materials, workshops, tutorials, and a course for researchers and practitioners. <br/><br/>The two-year conceptualization phase of the Learner Data Institute will focus on building a strong community of researchers, define research priorities, and develop interdisciplinary prototype solutions that address critical student learning, cyber-learning, and learning engineering challenges. Based on modern theories of learning and recent advances in educational technologies, artificial intelligence, sensing technologies, signal processing, and data science, the team will explore new frontiers in multi-faceted (cognitive, motivational, emotional, etc.) learner data collection, analysis, and visualization in order to understand and possibly transform how learners learn with technology.  The multi-disciplinary team will address core research questions such as: (1) how to transform a widely distributed group of interdisciplinary researchers, developers, and practitioners into a community of practice that can fully exploit the data revolution for the benefit of the learners and educational stakeholders; (2) how adaptive instructional systems (AISs) and data science can be used as a research vehicle to further understanding of how learners learn; (3) how the human-technology partnership with data and data science can be used to improve learners? and teachers? ability to employ technology in ways that facilitate learning and improve the effectiveness, scalability, and affordability of AISs in order to maximize the potential of learning ecologies of the future; and (5) more generally, how to extend the frontiers of data science to include: new methods of data collection and design; more interpretable, knowledge-rich machine learning methods (e.g., by combining Deep Learning with Markov Logic); scalable new inference and learning algorithms symmetries and joint dependencies in the data; and methods for identifying causal mechanisms from unstructured, semi-structured, and structured data. While the project will address core educational tasks in the context of online and blended learning environments, the proposed data science methods and models are generally applicable to other instructional contexts as well as other science and engineering areas. All models, software, processes, and data used in the project will be documented and disseminated for everyone to use and build on the outcomes of the project.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931487","Elements: Models and tools for on-line design and simulations for DNA and RNA nanotechnology","OAC","GVF - Global Venture Fund, Cellular & Biochem Engineering, Special Initiatives, Software Institutes","10/01/2019","06/29/2020","Petr Sulc","AZ","Arizona State University","Standard Grant","Alan Sussman","09/30/2022","$444,574.00","Hao Yan, Nicholas Stephanopoulos","psulc@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","054Y, 1491, 1642, 8004","026Z, 077Z, 5920, 7923, 8004","$0.00","DNA and RNA nanotechnology are rapidly developing fields that use DNA and RNA molecules as basic building blocks for constructing self-assembled nanoscale structures and devices. Promising applications include novel materials for diagnostics, drug delivery, nanophotonics, biophysical studies, and protein structural biology, as well as devices to perform molecular computation. Development of these structures is currently done with a trial-and-error approach, which is costly and time consuming because each new design has to be experimentally tested and gradually optimized until the desired structure is achieved. Computer simulations can provide insight into the assembly and function of these systems and greatly simplify and accelerate the design process, as well as guide understanding of the nanostructure function. However, computer simulations require molecular simulation expertise and high performance computing infrastructures that are not readily accessible to experimental groups. In this project, we will develop a new web server to provide users with online automatized tools that can be used to design, simulate, and analyze the properties of DNA and RNA nanostructures. The web server will also contain a repository of previously reported nanostructures so that researchers can easily access and use existing designs and adapt them for their use. We will also develop new models for hybrid DNA/RNA and protein-DNA/RNA nanostructures, thereby extending the ability of computational design and verification to larger and more complex nanostructures. This project will benefit the public by creating a highly efficient integrated platform to store, edit, design and computationally analyze nanostructures, thus providing a common resource to groups working in nanotechnology while simultaneously expanding access to these nanostructures to researchers in other fields. The net result will be to speed up and integrate the development of nanotechnology, simplify the design process, and facilitate the extension of biomolecular nanostructures to practical applications.  The tools will also be impactful for teaching the simulation methods and for outreach activities, where students will be taught the principles of simulation and self-assembly by using the tools to design their own nanostructures.<br/> <br/>Despite significant progress in the development of experimental methods that allow for assembly and characterization of the DNA and RNA nanostructures, the lack of easy-to-use software tools for design of nanostructures still remain a major bottleneck for wider adoption of the DNA and RNA nanotechnology in other related fields. Novel methods for structure design and verification are needed for the field to reach its full potential. Furthermore, the nascent field of hybrid DNA-protein and RNA-protein nanotechnology currently lacks efficient coarse-grained tools that would allow for the simulation. This proposal will consist of 1) Creation of a web-based platform for the interactive design of DNA/RNA nanostructures, along with a publicly accessible webserver for simulations and analysis of nucleic acid nanotechnology; 2)  Creation of a public online repository of successfully assembled and verified DNA and RNA nanostructures from the field, where researchers will be able to share and edit published designs, allowing easy sharing and extensions of  nanostructures; 3) Extension of our previously developed models of DNA and RNA to include coarse-grained representation of proteins and DNA-RNA hybrids, thus enabling the design of hybrid protein-DNA/RNA nanomaterials. The project will require development of new methods for nanostructure visualizations and analysis coupled with interactive high performance simulations on GPU cards, and parametrization of new models on new experimental data of hybrid nanostructures. The proposed research will have significant societal and educational impact. The user-friendly design tool will be easily incorporated into both the graduate and undergraduate curriculum. We will also incorporate the tools also into our outreach activities among high school students and teacher. The online design tool will be used for crowd-sourced science for web-based nanostructure design competitions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923983","Collaborative Research: CyberTraining: Implementation: Small: Multi-disciplinary Training of Learning, Optimization and Communications for Next-Generation Power Engineers","OAC","CyberTraining - Training-based","09/15/2019","07/01/2019","Dongliang Duan","WY","University of Wyoming","Standard Grant","Alan Sussman","08/31/2023","$199,924.00","","dduan@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","044Y","9150","$0.00","With the increasing adoption of interconnected power/micro grid infrastructures, today's power engineering research professionals require broader knowledge and a more diverse skillset. This project provides undergraduate and graduate students in the Northern Plains region with access and opportunity to learn using state-of-the-art smart grid cyberinfrastructure. The Northern Plains region sees increasing use and potential in renewable energy, in addition to energy exports to other areas, solidifying the importance of advanced training in power engineering. The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense. The resulting curriculum and instructional materials integrate advanced skills from multiple areas into power engineering infrastructure education. Students practice the multi-disciplinary skillsets needed for the power industry using a unique, remotely connected smart grid cyberinfrastructure. They extend their academic and research portfolios and strengthen their career competitiveness as smart grid cyberinfrastructure professionals and cyberinfrastructure users for regional and national levels. The cyber training model leverages resources for underrepresented minority schools and schools with limited research cyberinfrastructure to participate, either remotely or on-site. The mobile microgrid laboratory demonstrations introduce K-12 teachers and students to Science, Technology, Engineering and Mathematics majors.<br/><br/>This project establishes a new, remotely-connected smart grid cyberinfrastructure platform between the collaborative institutes using a real-time digital simulator, and sharing software licenses and hardware resources. This project promotes the application of advanced cyberinfrastructure techniques in power system monitoring, planning, operation and control, and prepares the next-generation power engineers to face challenges in modern power systems. This remotely connected power cyberinfrastructure provides an ideal platform for interdisciplinary research and education of computational intelligence, machine learning, control, communications, and data analytics in the smart grid area. The project team collects heterogeneous smart grid measurement data from this new cyberinfrastructure to conduct real-time learning, event-detection and data integrity, online optimization and multi-level decision-making process of intelligent systems. The project team integrates results into existing undergraduate courses and expand graduate courses from a frontier interdisciplinary viewpoint. The project team also creates replicable project templates/demos with designated benchmark data so other schools can easily adopt this new educational model with or without specific resources. Feedback from an independent evaluator, institutional stakeholders, national laboratory scientists and local industry partners supports periodic improvement of the educational model and materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835825","Collaborative Research: Elements: Software NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Jed Brown","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","08/31/2022","$293,441.00","","Jed.Brown@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales.  The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments.  This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses.  The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis.  This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability.  The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization.  The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839909","CICI: SSC: SciTrust: Enhancing Security for Modern Software Programming Cyberinfrastructure","OAC","Cybersecurity Innovation","10/01/2018","08/24/2018","Yanfang Ye","WV","West Virginia University Research Corporation","Standard Grant","Robert Beverly","10/31/2019","$649,156.00","Brian Woerner, Xin Li","yye7@nd.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","8027","9150","$0.00","Software plays a vital role supporting scientific communities. Modern software programming cyberinfrastructure (CI), consisting of online discussion platforms (such as Stack Overflow) and social coding repositories (such as Github), offers an open-source and collaborative environment for distributed scientific communities to expedite the process of software development. Within the ecosystem, researchers and developers can reuse code snippets and libraries, or adapt existing ready-to-use software to solve their own problems. Despite the apparent benefits of this new social coding paradigm, its potential security-related risks have been largely overlooked; insecure or malicious codes could be easily embedded and distributed, which could severely damage the scientific credibility of CI. Therefore, there is an urgent need for developing scalable techniques and tools to automatically detect these open-source insecure or malicious codes. To address this issue, this proposed project seeks to explore innovative links between Artificial Intelligence (AI) and cybersecurity to enhance the security of modern software programming CI. <br/><br/>The key components of the proposed research are three-fold: (1) a novel AI-based solution (iTrustSO) utilizing social coding properties is developed to automatically identify suspicious insecure code snippets on Stack Overflow; (2)  a cross-platform model is constructed to represent the complex interplay between GitHub and Stack Overflow; deep learning techniques are then utilized to build a predictive model (iTrustGH) for automatic detection of malicious codes on GitHub; and (3) a user-friendly tool (SciTrust) is developed to enhance code security for software development. The broader impacts of this work include benefits to scientific communities and the whole society by promoting the efficiency of cyber-enabled software development without sacrificing the security. The establishment of a Cybersecurity Lab through this project enhances the education and workforce training in cybersecurity. The project integrates research with education through curriculum development and student mentoring activities for the newly-established cybersecurity degree program. It is also expected to increase the participation of underrepresented groups including minority and women.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1910213","OAC Core: Small: Sust-CI: A Machine Learning based Approach to Make Advanced Cyberinfrastructure Applications More Efficient and Sustainable","OAC","OAC-Advanced Cyberinfrast Core","05/01/2019","04/22/2019","Janardhan Rao Doppa","WA","Washington State University","Standard Grant","Seung-Jong Park","04/30/2022","$499,998.00","Anantharaman Kalyanaraman","jana@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","090Y","026Z, 9179","$0.00","Many high-end engineering and scientific applications routinely employ advanced cyber infrastructure (CI). CI is formed of a combination of high performance computing (HPC) systems, software, application developers, and application users. Among these high-end applications, a growing number require repeated runs on HPC systems that are not designed optimally for their executions. These challenges are further exacerbated by the continuously changing hardware landscape. Faced with these challenges how is a CI application developer expected to develop and deploy applications in an efficient and sustainable manner? This is the central research question that this project seeks to address. The overarching goal is to develop a systematic and structured way to explore design spaces of CI configurations using machine learning techniques, and to demonstrate value in application and discovery potential through real-world applications. Other project activities integrate and leverage upon the research outcomes of this project, while preparing the next generation scientific workforce. The project is also leading to the development of curricular modules in parallel algorithms/applications and machine learning, and conference tutorials for broader outreach. The project will lead to the training of two PhD students in performing interdisciplinary research.<br/><br/>This project lays the foundations for a novel computational framework referred as Sust-CI that enables the developers to design and optimize cyber infrastructures for efficiency. This framework synergistically combines algorithmic abstractions, programming tools, and machine learning techniques to enable adaptive cyber infrastructures. This approach will automatically learn policies to make design decisions to optimize an objective specified by the developer (e.g., performance) in a data-driven manner. The project is leading to the development of sample-efficient machine learning algorithms for CI design space exploration and optimization. The key idea is to provide advanced CI applications a new capability to derive knowledge by exploring different execution traces (computational behavior) on the given training problem instances. The research will lead to a first-of-its-kind design space exploration framework to enable a sustainable use of CI resources toward leadership applications in science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2140068","Microstructure Research Data Workshop at the South Big Data Innovation Hub","OAC","NSF Public Access Initiative","09/15/2021","08/30/2021","Eva Campo","MD","University of Maryland, College Park","Standard Grant","Martin Halbert","02/28/2022","$50,000.00","","ecampo@umd.edu","3112 LEE BLDG 7809 Regents Drive","College Park","MD","207425141","3014056269","CSE","7414","7556","$0.00","This workshop will bring together experts to advance public accessibility and interoperation standards for microstructural material science datasets, a category of data that has broad utility for many areas of materials science research.  The workshop builds on previous workshops, which succeeded in identifying targeted gaps in microstructure descriptors commonly used in manufacturing to correlate synthesis and macroscopic properties that determine structural performance.  This workshop aims at identifying metadata needed to enable interoperability of distributed and federated microstructure data worldwide.  The identification and adoption of these metadata standards will have widespread practical impacts in manufacturing, and be pursued through community engagement and consensus building.  The results of this workshop will have the potential to dramatically advance manufacturing industries; efficiently developing synthesis strategies towards targeted structural performance, that could include resilience to extreme environments such as nuclear reactors and wind farms, to extreme weather conditions (coastal erosion), to fatigue in bridges, and to earthquake damage.<br/><br/>This workshop will focus on FAIR principles-oriented specifications for data management issues that were deemed crucial to the data science enterprise in previous gatherings, including: 1) metadata curation, 2) data ownership, provenance and sharing, 3) ontologies and schema, etc.  The workshop will include a number of tracks: Data Science Education and Workforce, Manufacturing, Team Science, Smart Cities, Data Sharing, and Health Disparities. In addition, the materials track will reach out to the other tracks and will: 1) connect with experts in resilient cities and ask what properties in cement, steel, etc. they are concerned with and incorporate those in the case studies, 2) connect with experts in Team Science for advice on how to efficiently execute the actionable items identified in the workshop, and 3) connect with experts in Workforce development for help in crafting a training program for data technicians in this realm (e.g. how much data science and how much materials science would a technician, researcher, etc. need to know to work in this space).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106059","Collaborative Research: OAC Core: Simulation-driven runtime resource management for distributed workflow applications","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","08/30/2021","Henri Casanova","HI","University of Hawaii","Standard Grant","Alan Sussman","09/30/2024","$279,988.00","","henric@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","090Y","026Z, 7923, 9150","$0.00","Many scientific breakthroughs in domains such as health, climate modeling, particle physics, seismology, etc., can only be achieved by performing complex processing of vast amounts of data.  This processing is automated by software systems that use the compute, storage, and network hardware provided by the cyberinfrastructure.  In addition to automation, a key objective of these systems is the efficient use of the resources as measured by cost and energy usage, while making the processing as fast as possible or as needed. To this end, these systems must make decisions regarding which resources should be used to do what and when.  Many such systems are used in production today and make such decisions. Yet making good, let alone best, decisions is still an open research challenge. Theoretical research has proposed solutions that are difficult to put into practice, and practical solutions are known to not make good decisions, or at least not consistently so.  However, both theory and practice follow the same basic philosophy: make decisions by reasoning about known information on what needs to be computed and on what hardware resources are available. This philosophy has shown its limits, so this project adopts a radically different approach.  The key idea is to repeatedly execute fast, computationally inexpensive simulations of the application execution in order to evaluate large sets of potential resource management decisions and automatically select the most desirable ones. The benefits of this approach will be demonstrated for several software systems used to support scientific applications that are critical for the development and sustainability of society.<br/><br/>Software systems are used to run scientific applications on advanced cyberinfrastructure.  These systems automate application execution, and make resource management decision along several axes including selecting and provisioning (virtualized) hardware, picking application configuration options, and scheduling application activities in time and space. Their objective is to optimize both application performance and also a set of resource usage efficiency metrics that include monetary and energy costs. Consequently, the resource management decision space is enormous, and making good decisions is a steep challenge that has been the subject of countless efforts, both from theoreticians and practitioners.  However, the challenge is far from being solved: theoreticians produce solutions that are rarely used by practitioners, and conversely practitioners implement solutions that may be highly sub-optimal because they not informed by theory. This project resolves this disconnect by obviating the need for developing effective resource management strategies.  The key idea is to use online simulations to search the resource management decision space rapidly at runtime. Large numbers of fast simulations of the application's execution are executed throughout that very execution, so as to evaluate many potential resource management options and automatically select desirable ones.  This approach thus shifts the overall problem from the design of complex resource management algorithms to the enumeration of many resource management decisions. The transformation of resource management practice in cyberinfrastructure systems not only renders the resource management problem tractable but also unlocks previously out-of-reach resource management decisions.  The benefits of this transformation will be demonstrated for a critical class of production systems and applications, specifically Workflow Management Systems and the scientific applications they support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103627","Elements: Open-source tools for block polymer phase behavior","OAC","Proc Sys, Reac Eng & Mol Therm, DMR SHORT TERM SUPPORT, Software Institutes","09/01/2021","08/30/2021","Kevin Dorfman","MN","University of Minnesota-Twin Cities","Standard Grant","Alan Sussman","08/31/2024","$506,505.00","David Morse","dorfman@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1403, 1712, 8004","054Z, 077Z, 7923, 8004, 9216, 9263","$0.00","Block polymers are molecules formed by bonding one or more different polymer molecules at their ends. When a liquid of block polymers is cooled, it spontaneously forms an ordered structure with nanoscale domain sizes and a structure that is dictated by the chemistry of the block polymer. Understanding the selection of different ordered states has both fundamental and practical implications. On the fundamental side, block polymers provide an important model system for examining self-assembly and the formation of crystalline order in soft matter. At a practical level, the ability to construct materials with a controllable arrangement of components with dramatically different properties (e.g., glassy spheres embedded in a rubbery matrix) enables novel applications. Computation plays an important role for both fundamental studies and these technological applications by providing guidance towards the selection of materials to analyze and a framework for interpreting the results. This project will produce an open-source software package that enables block polymer researchers to easily use state-of-the-art computational tools in their own research. The long-term goal of the project is to achieve, within the context of the block polymer theory and experimental community, a widespread use of field-based simulation tools that naturally complement particle-based molecular dynamics simulation methods for studying block polymer properties. This project also provides graduate students and undergraduates with the opportunities to work on state-of-the-art simulation methods and training in high-performance scientific computing.<br/><br/>The proposed open-source software package will enable self-consistent field theory (SCFT) and field theoretic simulations (FTS) under a common framework implemented in C++. The package will include SCFT tools that build on the successful Polymer Self Consistent Field (PSCF) software package for periodic systems to provide the full suite of functionality required for advanced polymer physics research. This new package, entirely rewritten in a new language and with a more flexible design, will enable accelerated calculations on either GPUs or multicore CPUs, efficient treatment of problems with special symmetries, and treatment of complex boundaries such as polymer-grafted nanoparticles and patterned surfaces. A separate set of FTS programs will allow the user to implement either complex Langevin sampling of the fully fluctuating model or a more efficient form of stochastic simulation that involves an approximate treatment of non-specific steric interactions that maintain a uniform density. Input/output tools will also be developed to lower the barrier to usage of the software. Dissemination of the work will include a project website, which will provide background for potential users and links to documentation, and an online tutorial workshop that will introduce the software and its capabilities to the broader community. <br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research and the Division of Chemistry in the Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental and Transport Systems in the Directorate for Engineering also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103511","Collaborative Research: OAC Core: Improving Utilization of High-Performance Computing Systems via Intelligent Co-scheduling","OAC","OAC-Advanced Cyberinfrast Core","09/01/2021","08/30/2021","David Lowenthal","AZ","University of Arizona","Standard Grant","Alan Sussman","08/31/2024","$250,298.00","","dkl@cs.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","090Y","026Z, 7923","$0.00","This project is aimed at increasing efficiency of high-performance computing systems by scheduling multiple jobs on the same set of nodes in a system, generally called co-scheduling. This is a break from current practice in which nodes are dedicated to one job at a time, which results in predictable execution time but inefficient use of system resources. To make this practical, the project will develop analyses to determine how to carry out co-scheduling such that overall system efficiency is improved while the performance impact on individual applications is minimized. In particular, the goal is to co-schedule jobs that can co-exist without contending for similar resources on the nodes.  The work done in this project will help achieve better efficiency on high-performance systems, which will impact application domains such as climate/weather, renewable energy, and national security. The work will be implemented and validated on systems at Lawrence Livermore and Sandia National Laboratories and then transitioned into software that will be used at these national laboratories. The project will also have an impact on education by integrating the techniques in this research into courses covering parallel and distributed computing at the PIs' institutions. In addition, the project will take place at two Hispanic-serving institutions, and the PIs have a history of advising under-represented students; the project will broaden participation in computing by recruiting Hispanic undergraduates to work on the project and sending them to national laboratories for internships.<br/><br/>The long-standing abstraction at high-end computing facilities is one of a submitted job being allocated a set of dedicated nodes. However, this makes systems much less efficient, as there are more per-node resources that will often be used inefficiently. In addition, the demand for high-end systems is increasing and dedicating nodes to jobs can increase job turnaround time and decrease overall system throughput.  One way to address this problem is for supercomputer centers to break from the current common practice of assigning each job a private, isolated portion of a supercomputer.  The intellectual merit of the project is three-fold. First, novel profile analyses will be developed that will reveal the effects on jobs due to sharing nodes. Second, novel statistical projection techniques will be developed that predict scaling behavior of jobs that are utilizing shared nodes. Third, new job-level scheduling techniques will be designed that use the interference analysis and projections to choose a set of shared nodes that will lead to good job turnaround time and maximize system throughput. The broader impact of the project is multifold.  This project will help achieve better efficiency on high-performance systems, which will benefit a broad range of applications that includes climate/weather prediction, nuclear energy, and national security.  Through a long-standing collaboration with both Lawrence Livermore and Sandia National Laboratories, the PIs will implement and validate the techniques on LLNL and SNL systems as well as transition the techniques into future resource managers at the national laboratories. In addition, both PIs will broaden participation in computing by recruiting Hispanic undergraduates to work on the project and sending them to national labs for internships.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849970","CRII: OAC: RUI: Improving Software Deployments to Serverless Computing Environments","OAC","CRII CISE Research Initiation","03/15/2019","07/02/2021","Wes Lloyd","WA","University of Washington","Standard Grant","Alan Sussman","02/28/2022","$229,739.00","","wlloyd@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","026Y","019Z, 8228, 9179, 9229","$0.00","A form of cloud computing, called serverless computing, has recently emerged as a new approach to abstract the management and configuration of cloud computing servers to deliver compute resources to end users, while promising performance improvements, around-the-clock availability, and energy savings to enable a better return on investment from computer hardware.  The benefits of serverless computing are offset, however, by new software development challenges arising from mitigation of resource constraints employed by cloud providers to share physical servers among many users, and by the complexity of understanding performance and cost implications of software designed for serverless platforms.  This project serves the national interest, as stated by NSF's mission: to promote the progress of science, and to promote national prosperity and welfare by developing a reusable framework enabling new data driven analytics and evaluation techniques that characterize the quality of software designed for serverless computing platforms.  These advancements serve to extend the applicability of serverless computing to a broader range of use cases enabling the next generation of cloud software to realize performance improvements and cost savings. This project additionally leverages the simplicity of serverless computing to develop an online hour-of-code tutorial to introduce cloud computing, client/server, and distributed systems concepts to high school students for dissemination.<br/><br/>This research develops the Serverless Application Analytics Framework (SAAF), a novel framework that supports the characterization of software deployed to serverless computing platforms enabling the collection of resource utilization (CPU, memory, disk/network I/O), infrastructure (type, state, and distribution), load balancing, and performance metrics.  SAAF enables an improved understanding of several factors that impact performance.  This research develops predictive models using statistical and machine learning approaches to automate the characterization of performance and hosting cost of serverless software leveraging metrics from SAAF, and software quality metrics from static analysis tools as features.  SAAF enables identification of best practices over serverless architectures (monolithic, composable, and RESTful APIs), software composition patterns (monolithic, fine grained, library composition, and switchboard), and workflow instrumentation (remote client, vendor service, microservice controller, and asynchronous call chains).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849821","CRII: OAC: Real-time Computational Modeling of Crop Phenological Progress towards Scalable Satellite Precision Farming","OAC","CRII CISE Research Initiation","03/15/2019","02/14/2019","Chunyuan Diao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alan Sussman","02/28/2022","$174,988.00","","chunyuan@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","026Y","8228","$0.00","Precision agriculture aims to leverage advanced data-intensive technology to maximize agricultural productivity and to reduce environmental footprints. With considerable technological advancements, it is a pivotal time to harness the intensive data collected in space and time to benchmark precision agriculture systems worldwide. The recent launch of groups of satellites designed to work together -- known as ""satellite constellations"" -- opens up unprecedented opportunities to revolutionize precision agriculture, through monitoring the crop phenological progress at fine spatial and temporal scales. However, the gigantic amount of data brings significant challenges to conventional remote sensing software and tools. The overarching goal of this project is to prototype advanced remote sensing cyberinfrastructure in support of both data- and compute-intensive satellite-based precision agriculture systems. This advanced cyberinfrastructure is applicable to a diverse range of agricultural systems, especially for resource poor and vulnerable smallholder farming systems. With its potential to improve global farming practices, the cyberinfrastructure helps optimize the trajectory of agricultural development to meet future crop demands as well as lower environmental impacts. The integrated educational and training activities of the project offer unique learning opportunities to students of various academic levels and backgrounds, and enhance the broader engagement of diverse scientific communities, especially minority and underrepresented groups. Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The advanced remote sensing cyberinfrastructure focuses on the development of an innovative real-time phenological computational (RTPC) model and a high-performance system to harness massive parallelism in modeling crop phenological progress towards scalable satellite-based precision farming. The RTPC model integrates dynamic complex networks with time series remote sensing, and is unique to predict the real-time crop phenological progress at both fine spatial and temporal scales. The high-performance system enhances the parallelism of the RTPC using a hybrid computation model, including a node-level computation model and a system-wide data distribution model. The node-level computation model takes advantage of multi-core architecture of computing nodes to parallelize the compute-intensive RTPC in predicting dynamic network characteristics of crop phenology. The system-wide data distribution model devises a novel Space-and-Time parallel decomposition strategy in distributing massive remote sensing time series data to reduce memory requirements and to achieve high scalability. An open-source toolkit is designed to facilitate the open development and adoption of the remote sensing cyberinfrastructure across a broad range of disciplines. Through leveraging the power of high performance computing and this hybrid computation model, the cyberinfrastructure can analyze PB-level remotely sensed data in a highly scalable manner to conduct real-time monitoring of earth system dynamics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849559","CRII: OAC: A Framework for Parallel Data-Intensive Computing on Emerging Architectures and Astroinformatics Applications","OAC","CRII CISE Research Initiation","03/15/2019","02/14/2019","Michael Gowanlock","AZ","Northern Arizona University","Standard Grant","Alan Sussman","02/28/2022","$174,975.00","","michael.gowanlock@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","026Y","062Z, 8228","$0.00","The amount of data that needs to be analyzed by the scientific community is increasing due to growing volumes of data collected by current and future instruments and sensors. Consequently, scientific data analysis requires a significant amount of time. To decrease the amount of time needed to analyze data, methods need to utilize more computational resources, such as more processors in a computer. While the central processing unit (CPU) in a modern computer has traditionally been used to carry out data analysis, the past decade has seen an increase in using graphics processing units (GPUs) for data analysis. The modern graphics processing unit contains thousands of processors that can be used to execute a program faster than on the CPU. However, many algorithms for data analysis do not use the GPU to its full potential within the context of the broader computer system. This project advances a framework for understanding the performance of GPUs as applied to data analysis applications. The major goal of the project is to bridge the gap between algorithms that only use the GPU, and fully integrated algorithms that exploit the strengths of both the CPU and GPU. The ensemble of algorithms that are explored in the project support the needs of the astronomy community and researchers in other scientific areas that require efficient data analysis methods. The project aims to realize a new era in CPU/GPU computing that impacts both computer science and other scientific fields.  An outcome of the project is the development of materials for educators teaching at the intersection of data analysis and parallel computing. This project includes mentoring K-12, undergraduate, and graduate students.  Consequently, the project serves the national interest, as stated by NSF's mission, by promoting the progress of science, and to advance the national health and prosperity. <br/><br/>New cyberinfrastructure, such as data analysis algorithms for emerging heterogeneous architectures, are needed to address cutting-edge scientific problems. Data analysis building blocks and algorithms have many data-dependent performance bottlenecks. New architectures have the potential to alleviate some of these key bottlenecks. However, the majority of GPU research minimally involves the CPU/host, and performs most of the computation on the GPU. This is a missed opportunity to more closely integrate both data and task parallelism between the CPU and GPU to simultaneously exploit concurrency across both architectures. This project examines a selection of key algorithms in the database, machine learning, data mining, and parallel computing communities. Using these algorithms, this project explores the continuum between GPU-only and mixed hybrid parallelism (data and task parallelism between the CPU and GPU) to identify key bottlenecks that can be reduced by exploiting underutilized resources. The selected algorithms are fundamental to scientific data processing workflows, and can advance time-domain astronomy cyberinfrastructure.  The project integrates data-intensive computing insights into courses at the undergraduate and graduate levels, and pedagogical modules are developed to be used by instructors for teaching concepts of mixed (data and task) parallelism across the CPU and GPU. This project includes mentoring students at the undergraduate, graduate, and K-12 levels, including outreach at science festivals to encourage participation and interest in science, technology, engineering, and mathematics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118193","CyberTraining: Pilot: A Professional Development and Certification Program for Cyberinfrastructure Facilitators","OAC","CyberTraining - Training-based","09/01/2021","08/31/2021","Henry Neeman","OK","University of Oklahoma Norman Campus","Standard Grant","Alan Sussman","08/31/2023","$299,993.00","Dirk Colbry, Dana Brunson","hneeman@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","044Y","9102, 9150","$0.00","This CyberTraining pilot project is initiating the Certified Cyberinfrastructure Facilitator Training and Development (CCIFTD) program, a first-of-its-kind, non-matriculated certification of professional development for Cyberinfrastructure Facilitators. The Cyberinfrastructure (CI) workforce suffers from a critical deficit of CI Facilitators, who work directly with researchers to advance the computing-intensive/data-intensive aspects of their research. CCIFTD's purpose is to attest to proficiency in core skills needed for facilitating computing/data-intensive research, across all Science, Technology, Engineering and Mathematics (STEM) disciplines. The focus is on crucial professional/interpersonal (soft) skills, and complementary research computing technical topics. CCIFTD will establish both (A) the set of skills and (B) a means for determining whether a CI Facilitator has these skills and thus merits certification. This project promotes the progress of science by accelerating STEM research, in particular by training and certifying a national cohort of research computing professionals who work directly with STEM researchers to advance the computing/data-intensive aspects of their research. Such research is becoming increasingly important across all STEM disciplines, while at the same time large scale and advanced computing is becoming more challenging to learn. Financial efficiency is achieved via a ""train the trainers"" approach: every Facilitator trained by CCIFTD is anticipated to work with dozens or hundreds of researchers over their career.<br/><br/>The project has a well-defined methodology: 1. Determine the skills that are most valuable for CI Facilitation, by surveying (i) experienced CI Facilitators, (ii) CI organization leaders such as supercomputing center directors, and (iii) STEM researchers who use CI. 2. Develop for each such skill a training mechanism, an examination instrument, and its scoring rubric, via pilot testing at workshops and via online resources. 3. Construct certification pathways, namely subsets of badges that, collectively, merit certification. 4. Test badging methods. 5. Evaluate the success of the CCIFTD program both during this pilot project and at the end, to improve CCIFTD as it progresses and to determine how successful it has been. The project objectives include: 1. From the perspective  of CI Facilitators: (a) Provide both training and proof of mastery in skills that are mission critical to CI Facilitation; and (b) Cultivate and expand the CI Facilitators community. 2. From the perspective of CI organizations: Increase uptake of CI Facilitation services, and therefore of CI. 3. From the perspective of STEM researchers: Increase computing/data-intensive STEM research productivity, by applying CI expertise that many STEM researchers lack.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107511","Collaborative Research: OAC Core: ScaDL: New Approaches to Scaling Deep Learning for Science Applications on Supercomputers","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","08/31/2021","Ian Foster","IL","University of Chicago","Standard Grant","Alan Sussman","09/30/2024","$271,577.00","","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","090Y","026Z, 079Z, 7923","$0.00","Today's deep learning (DL) revolution is enabled by efficient deep neural network (DNN) training methods that capture important patterns within large quantities of data in compact, easily usable DNN models. DL methods are applied routinely to tasks like natural language translation and image labeling--and, in science and engineering, to problems as diverse as drug design, environmental monitoring, and fusion energy. Yet as data sizes increase and DL methods grow in sophistication, the time required to train new models often emerges as a major challenge. The Scalable Deep Learning (ScaDL) project will address this challenge by making it possible to use specialized high-performance computing (HPC) systems to train bigger models more rapidly. Efficient use of the thousands of powerful processors in modern HPC systems for DNN training has previously been stymied by communication costs that grow rapidly with the number of processors used. ScaDL will overcome this obstacle by developing new DNN training methods that reduce communication requirements by performing additional computation, by validating the effectiveness of these new methods in a range of scientific applications that use DL in different ways, and by integrating the new methods into scalable DL software for use by domain scientists, computer scientists, and engineers supporting DL application in HPC centers. By permitting the use of powerful HPC systems to train DNN models thousands of times faster than on a single computer, ScaDL will enable advances in many areas of science and engineering. The project will also contribute to educational outcomes by engaging PhD students in project goals, by using ScaDL tools in a new DL systems engineering class at the University of Chicago, and by enlisting participants in summer schools at the Texas Advanced Computing Center (TACC) and U. Chicago, both of which target recruitment of students from underserved communities at the graduate, undergraduate, and high-school levels, to apply the tools to scientific problems. ScaDL's focus on science applications and education aligns the project with NSF's mission of promoting the progress of science.<br/><br/>The ScaDL project contributes to science in two ways. First, it explores new techniques for enhancing the speed and scalability of commonly used optimization methods without losing model performance, by: 1) exploiting scalable algorithms for second-order information approximation; 2) developing methods for adapting to different computer hardware by tuning computation and communication to maximize training speed; 3) exploring compression techniques to reduce communication overheads; 4) using well-known benchmark applications to evaluate the convergence of ScaDL; and 5) applying its new algorithms and systems to science applications.  Second, it will release an open-source implementation of the proposed algorithms and system. The implementation will be available on a variety of hardware platforms and capable of choosing the ratio of computation and communication required to make efficient use of the computation and communication hardware on a particular HPC system. The resulting algorithms and system will help disseminate ScaDL research results to a wide spectrum of research domains and users, and promote the adoption of the new methods in practical settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104158","OAC Core: A Machine Learning Assisted Visual Analytics Approach for Understanding Flow Surfaces","OAC","OAC-Advanced Cyberinfrast Core","01/01/2022","08/31/2021","Chaoli Wang","IN","University of Notre Dame","Standard Grant","Alan Sussman","12/31/2024","$499,907.00","Danny Chen, Jian-Xun Wang","chaoli.wang@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","090Y","079Z, 7923","$0.00","Among the popular integration-based techniques for fluid flow visualization, line-based techniques have made significant advances over the years, providing a sharp contrast to surface-based techniques. Surface-based flow visualization is often regarded as one of the most challenging problems in scientific visualization. Flow surfaces can provide better illustrative capabilities and much-improved visualization compared to flow lines. However, existing surface-based flow visualization methods face substantial challenges related to surface selection, visualization, and analytics, calling for creative ideas and enabling solutions. The overarching goal of this project is to develop a machine learning-assisted visual analytics approach for understanding three-dimensional complex flow surfaces. The project contributes to the state-of-the-art flow visualization by providing end-to-end solutions that introduce an innovative deep learning approach to surface feature learning, investigate a new template-based visual interface for flow surface exploration, and explore occlusion reduction and relationship interrogation via visual transformation. The team will work with domain experts and apply the proposed solutions to study cardiovascular flow, aligning with the NSF's mission to promote the progress of science and to advance the national health, prosperity, and welfare.<br/><br/>Underlying the proposed work is a novel data-driven and user-centric approach for surface selection and exploration. The project will develop a new framework that supports (1) selection of representative surfaces through feature learning, projection, and clustering powered by graph neural networks, (2) exploration of surface patterns via a principled vocabulary-based method for shape-invariant partial flow surface querying and matching, and (3) comparative analytics of flow surfaces for studying seeding sensitivity or variability via a river-like visual metaphor. Research results will be evaluated through comparison against existing methods, assessment by domain experts, and a formal user study. Furthermore, for cardiovascular flows the project will investigate the hemodynamics of aortic dissection and intracranial aneurysm for solution validation. The project outcomes include (1) source code that implicitly learns surface features using graph neural networks, (2) a surface shape database based on the universal alphabet, and (3) an educational tool for teaching and learning flow surfaces. The general approach will impact other fields of study, including machine learning for visualization, shape analysis, visual analytics, and visualization in education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849273","NSF Student Travel Grant for 2018 Forum on Infrastructural Resilience to Low-Level Seismicity in Oklahoma","OAC","Information Technology Researc, Data Cyberinfrastructure","10/01/2018","09/13/2018","Priyank Jaiswal","OK","Oklahoma State University","Standard Grant","Amy Walton","09/30/2021","$19,600.00","","priyank.jaiswal@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","1640, 7726","062Z, 7556, 9150","$0.00","This award provides travel support to students from across the country to attend the 2018 Forum on Infrastructural Resilience to Low-level Seismicity, to be held in Stillwater, Oklahoma.  Undergraduate and graduate students in data science, geoscience and engineering will be able to take an active role in understanding the interdisciplinary problems and potential solutions.<br/><br/>The forum includes an opportunity for hands-on interaction with the latest data and sensing capabilities.  Dr. Priyank Jaiswal recently received a Cyberinfrastructure for Sustained Innovation (CSSI) award (#1835371): Oklahoma State University and UCSD are collaborating on the comparative characteristics of earthquakes induced by wastewater injection.  The project demonstrates an approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828567","MRI: Acquisition of a Regional Resource for Long-Term Archiving of Large Scale Research Data Collections","OAC","Major Research Instrumentation, Information Technology Researc, Data Cyberinfrastructure, LARS SPECIAL PROGRAMS","09/01/2018","08/23/2018","Henry Neeman","OK","University of Oklahoma Norman Campus","Standard Grant","Alejandro Suarez","08/31/2022","$967,755.00","Horst Severini, Amy McGovern, Kendra Dresback, Laura Bartley","hneeman@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","1189, 1640, 7726, 7790","062Z, 1189, 9150","$0.00","The project will support University of Oklahoma (OU)'s acquisition, deployment and maintenance of a large-scale storage instrument -- the OU & Regional Research Store (OURRstore). This instrument will enable faculty, staff, postdocs, graduate students and undergraduates to pursue data-intensive research (by building large and growing data collections), and to share and publish these datasets (which they can make discoverable and searchable). Science, Technology, Engineering and Mathematics (STEM) research is increasingly data-intensive, with massive growth of research data collections. Yet a substantial fraction of universities and colleges have been underprepared not only for the volume, velocity and variety of data, but especially for long term stewardship of rapidly growing data collections. In addition, many STEM research projects require substantial storage during their experiments, so much so that holding it all on disk is too expensive to be practical. This challenge is quickly becoming more acute, because disk prices are now improving much more slowly than in the past.<br/><br/>OURRstore is a large-scale storage instrument, consisting of a substantial tape library, as well as crucial support subsystems such as servers, disk, network components and software. OURRstore allows the massive growth of storage capacity needed to address the requirements of the large and broad research community that it serves. Via an innovative, low cost business model, OURRstore offers a cost-effective data storage and access and solution. The research topics supported by the instrument include: weather forecasting, weather radar and weather data mining, including for severe storms such as tornadoes and hurricanes; earthquake triggers and seismology; data and modeling for agriculture, forestry, water and earth ecosystems; coastal simulation; molecular systems that control growth and development of vegetation; microbial contamination of infrastructure; effects of repeated stress on organisms; RNA processing in mitochondria; nanotechnology; visual neuroscience; physiological adaptation to extreme environments; astrophysical objects and stellar atmospheres; malware cybersecurity; influence of microblogs on social media. OURRstore maximizes its impact across the US in the following ways. First, it serves as a national model for affordable, large scale, long term, multi-institutional storage. Second, the OURRstore community includes a substantial number of people from underrepresented populations: over 200 research team members are one or more of the following: women, African Americans, Hispanics, Native Americans, disabled, and US veterans. Third, the instrument will enhance OU's current ""Supercomputing in Plain English"" webinar series, a popular outreach program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743178","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","08/31/2021","Kenneth Jansen","CO","University of Colorado at Boulder","Continuing Grant","Seung-Jong Park","08/31/2022","$321,838.00","Alireza Doostan, John Farnsworth, John Evans, Jed Brown","jansenke@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835410","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, Data Cyberinfrastructure","01/01/2019","08/27/2018","Sarah Benson-Amram","WY","University of Wyoming","Standard Grant","Amy Walton","12/31/2021","$85,284.00","Jeff Clune","sbensona@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2129051","Data CI Pilot: VariMat Streaming Polystore Integration of Varied Experimental Materials Data","OAC","DMR SHORT TERM SUPPORT, CESER-Cyberinfrastructure for","10/01/2021","09/03/2021","David Elbert","MD","Johns Hopkins University","Standard Grant","Bogdan Mihaila","09/30/2023","$1,316,305.00","Amarnath Gupta, Ilkay Altintas, Stephen Wilson, Tyrel McQueen","elbert@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1712, 7684","054Z, 094Z, 7203, 9102","$0.00","VariMat is a pilot project to integrate experimental data sources with cyberinfrastructure components and bridge the data variety gap in experimental materials research data. The central goal of Materials Science and Engineering is to discover and deploy innovative materials to serve society, but the process is complex and frequently slow. Recent work to accelerate such materials discovery is focused by the Materials Genome Initiative (MGI), which recognizes the critical need for new materials in fields as diverse as energy, transportation, and national security.  The MGI centers on harnessing the data revolution and fueling data-intensive methods including artificial intelligence and machine learning.  To reach the potential of the MGI, however, there is pressing need for robust, high-performance data cyberinfrastructure (CI) that facilitates machine-actionable data and better implementation of FAIR data principles suited to the materials domain.  Among remaining CI gaps, none is more important than the need to integrate experimental data in ways that make it more operable and consonant to the research community needs.  This complex gap is compounded by the transdisciplinary nature of materials science and engineering where most projects depend on experimental data collected by highly varied techniques, in distributed labs, and by multiple investigators. The variety and volume of this data layers onto the dispersed nature of materials research to create a data variety gap that impedes both rapid use and valuable reuse of data as required by many data-hungry machine learning methods. The VariMat Data CI is designed to break these barriers with a pilot instantiation in the subdomain of quantum materials and maximize experimental data value across its whole lifecycle.  The project links teams from the UCSB Quantum Foundry and PARADIM, a Materials Innovation Platform (MIP) ? two of the NSF's premier investments in the Quantum Leap. This linkage integrates strong science drivers with infrastructure development while adopting a strategic focus on influential centers of high-quality, high-volume data production that are conduits to user training and workflows.  VariMat will provide investigators with integrated and timely access to the breadth of experimental data needed to enable new discovery pathways and drive novel-materials development that relies on controlled, replicable synthesis; structural and compositional characterization; property determination; and connectability to theory and modeling studies.<br/><br/>The proposed research will establish a new paradigm for an integrated data infrastructure leveraging a streaming layer for real-time ingest to a polystore.  VariMat uses an automated streaming layer to link instrumental data to a polystore of heterogeneous data management systems that optimize storage, query, and access for disparate data types.  The polystore encompasses multiple data models while unifying the query process for users. The VariMat polystore creates a new option for unified management and query of disparate experimental Big Data created across distributed facilities and to expand FAIR data compliance in the materials domain. VariMat implements an innovative stream processing Data Ingress Module for analytics driven ingest of experimental data. Together, the streaming and polystore layers serve a user-oriented web portal that combines advanced search with data analysis, visualization, and compute resources. Such integration will be facilitated by a unified semantic standard specific to the instantiation and that spans the project. VariMat will leverage the PARADIM data model describing synthesis and characterization with a directed acyclic graph (DAG) allowing traversal of the materials entire history.  VariMat's ""loosely coupled"" architecture provides operational and managerial independence of subsystems well suited for geographically distributed systems with on-going evolution in components as is typical in mid-scale or larger materials science research.  While the infrastructure fills a critical, community identified gap, VariMat components will be readily deployable and have broad applicability in other scientific fields dependent on distributed, operationally independent instrumental laboratories.  Automated deployment and open source components will facilitate ready instantiation in new domains. To maximize impact, concepts and tools developed will be disseminated through freely available, open source codes, online tutorials and data sets, and trainings that leverage existing schools and workshops at the Quantum Foundry and PARADIM.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829674","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Zhipeng Cai","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Alan Sussman","08/31/2022","$150,000.00","Yingshu Li","zcai@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829799","CyberTraining: CIC: The Texas A&M University Computational Materials Science Summer School (CMS3)","OAC","CyberTraining - Training-based","09/01/2018","07/09/2018","Ahmed-Amine Benzerga","TX","Texas A&M Engineering Experiment Station","Standard Grant","Alan Sussman","08/31/2022","$499,822.00","Lisa Perez, Honggao Liu, Raymundo Arroyave","benzerga@aero.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","Advances in both hardware and software infrastructure are quickly making it possible to carry out realistic simulations of materials and materials phenomena that can lead to a better understanding of their behavior. Simulations constitute one of the key ingredients necessary to realize the Materials Genome Initiative (MGI), which calls for the reduction in time and resources necessary to develop the materials needed to enable potentially transformative technologies. To push the thriving field of Computational Materials Science (CMS) forward it is necessary to provide the next generation research workforce with a broad exposure to as many computational techniques as possible.  This project brings together researchers in materials science and engineering, and in advanced cyber-infrastructure (CI) to establish the Computational Materials Science Summer School (CMS3) that aims to train graduate students and junior scientists and engineers in some of the most advanced and widely used computational materials science simulation tools. The training provided by CMS3 will equip participating students with the knowledge and skills necessary to push the frontiers of simulation-enabled materials research. Furthermore, CMS3 will contribute to MGI's mission of maintaining the Nation's overall economic competitiveness and security by training the scientists and engineers that will discover and develop the materials that will make technologies of national importance possible.<br/><br/>This award leverages existing cyber-infrastructure to expand participation, including from underrepresented groups, through local and remote training. Expert instructors are selected from academic, domestic and international institutions, and national laboratories to cover three modules: continuum micromechanics, mesoscopic simulation and atomistic modeling, along with an overarching theme of data science. The school targets 20 on-site and up to 80 remote graduate and post-doctoral researchers, including industry participants. The activities of CMS3 are to (i) establish a CMS network among national laboratories and academic institutions to leverage the expertise of the CMS community at large in developing the next-generation workforce in materials research across multiple scales; (ii) develop a CMS curriculum, commensurate with a summer school format, that involves utilizing and supporting advanced CI for effective scale-up of a series of practica; (iii) organize shorter seed programs in specialized areas, such as dislocation dynamics, phase-field modeling and data-enabled materials science; (iv) integrate the theoretical foundations and practical training into the graduate curriculum and continuing education at Texas A&M University and elsewhere; and (v) introduce immersive visualization through virtual and augmented reality tools to help students with different backgrounds and learning styles interpret complex material data. Course material and software codes associated with this project not already disseminated by the original developers are maintained in a GitHub repository. This ensures that all released materials and packages are preserved to maintain historical context and broad access.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829707","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Sudhir Malik","PR","University of Puerto Rico Mayaguez","Standard Grant","Alan Sussman","07/31/2022","$124,342.00","","sudhir.malik@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","044Y","026Z, 062Z, 7361, 9102, 9150","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829740","CyberTraining: CIU: The LSST Data Science Fellowship Program","OAC","CyberTraining - Training-based, SPECIAL PROGRAMS IN ASTRONOMY","08/01/2018","06/29/2018","Adam Miller","IL","Northwestern University","Standard Grant","Alan Sussman","07/31/2022","$499,251.00","","amiller@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","044Y, 1219","026Z, 062Z, 1207, 7361, 9179","$0.00","This National Science Foundation (NSF) Training-based Workforce Development for Advanced Cyberinfrastructure award supplements graduate education in astronomy by providing in-depth training in the skills necessary to make scientific discoveries using big data. Ongoing and future surveys, such as the NSF's flagship optical telescope project, the Large Synoptic Survey Telescope (LSST), are producing data at an unprecedented rate. The sheer size of these data sets requires new working practices: sophisticated computational software and data mining procedures are necessary to fully exploit the rich information present in the data. However, these skills are not typically a core component of the astronomy and astrophysics graduate curriculum. The LSST Data Science Fellowship Program (DSFP) supplements traditional educational programs by training students in a variety of data science methods to work with and ultimately analyze big data. DSFP students are selected from a wide variety of universities using an innovative admissions procedure that increases the participation of students from underrepresented groups. Furthermore, DSFP students are trained in science communication and receive a certification in teaching data science so they can tutor peers and lead training workshops in the material learned as part of the program. The project serves the national interest, as stated by the National Science Foundation's mission: to promote the progress of science, by training the next generation of astronomers to have the computing skills necessary to derive scientific insights from the largest telescopic surveys that have ever been conducted.<br/><br/>DSFP students attend six week-long sessions over the course of two years as part of their program training. Each session is hosted by a different institution and designed to focus on a single topic including: the basics of managing and building code, statistics, machine learning, scalable programming, data management, image processing, visualization, and science communication. This curriculum empowers trainees to ask broader questions of their data, prepares them for the technical challenges associated with LSST, and exposes them to the tools and methods necessary to advance fundamental science research. Student participants spread the adoption of data science tools, methods, and resources via the aforementioned teaching workshops, fostering new pathways to discovery in the broader research community. Students must work in collaborative groups, which in conjunction with their science communication training, enhances their leadership and mentoring skills. To reach a broad audience, all materials developed as part of the program are made available to the public, and a guide to convert the material into a semester-long course at the undergraduate or graduate level is provided. This program prepares students for success in a wide range of careers, providing education in data science methodologies, domain-specific<br/>considerations, and professional skill development in research, teaching, and communication.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827177","CC* Integration: End-to-End Performance and Security Driven Federated Data-Intensive Workflow Management","OAC","CISE Research Resources","10/01/2018","07/22/2020","Prasad Calyam","MO","University of Missouri-Columbia","Standard Grant","Deepankar Medhi","09/30/2021","$500,000.00","Timothy Middelkoop, Trupti Joshi, Isa Jahnke, Saptarshi Debroy","calyamp@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","2890","","$0.00","Data-intensive science applications in research fields such as bioinformatics, chemistry,<br/>and material science are increasingly becoming multi-domain in nature. To augment local<br/>campus CyberInfrastructure (CI) resources, these applications rely on multi-institutional resources<br/>that are remotely accessible (e.g., scientific instruments, supercomputers, public clouds).<br/>Provisioning of such federated CI resources has been traditionally based on applications'<br/>performance and quality of service (QoS) requirements. This project aims to augment<br/>traditional resource provisioning schemes through novel schemes for<br/>formalizing end-to-end security requirements to align security posture<br/>across multi-domain resources with heterogeneous policies.<br/><br/>This project addresses the end-to-end multi-domain security design for<br/>scientific applications by defining and formalizing security specifications along an application's workflow lifecycle stages.<br/>The research work will advance the current knowledge for a CI engineer in the following areas:<br/>(i) how to intelligently perform resource allocations among private and public cloud locations;<br/>(ii) by streamlining end-to-end security posture across domains that are constructed via dynamic network services; and<br/>(iii) how to ""bring your own compute"" programs in large facilities to reduce turnaround times in a secured and policy-compliant manner.<br/>The resulting security formalization and alignment schemes will be implemented as a security middleware coupled within a unified resource broker framework that: (a) operationally integrates various software tools and systems such as perfSONAR, OpenStack, iRODS and Shibboleth; and (b) supports prototypes of web-portals and actual users (e.g., researchers and educators) within usability evaluation and validation experiments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833170","Ph.D. Forum at the International Conference on Parallel Processing (ICPP)","OAC","Information Technology Researc, EDUCATION AND WORKFORCE","06/01/2018","05/08/2018","Allen Malony","OR","University of Oregon Eugene","Standard Grant","Alan Sussman","05/31/2019","$20,600.00","","malony@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","1640, 7361","7556, 9102","$0.00","Now in its 47th year, the International Conference on Parallel Processing (ICPP) is one of the longest continuously running conferences in parallel processing, and it is a rewarding event for Ph.D. graduate students to connect with and learn from the parallel computing community.  The Ph.D. Forum at ICPP promotes the progress of science as stated by NSF's mission by creating a rich educational and learning experience where Ph.D. graduate students (i) present and discuss their research in parallel computing with conference attendees, (ii) expand their understanding of the parallel computing by attending technical sessions, (iii) explore potential research projects with new colleagues, and (iv) receive guidance from leaders in the field.  By assisting in the training, development, and mentorship of the next-generation of parallel computing researchers, the ICPP Ph.D. Forum is contributing to the NSF's mission for advancements in science, support for education and diversity, and broader benefits to society.<br/>    <br/>This award will support the travel of up to 24 eligible students from US-based institutions to participate in the student program at ICPP 2018 and to organize a PhD Forum. There are two main objectives of the Ph.D. Forum at ICPP.  First, it provides Ph.D. students with intellectual opportunities to discuss their research work in a constructive and supportive environment where leading parallel computing topics are presented and debated.  Second, it provides mentoring resources to help graduate students organize their academic research work and plan for their future careers.  The Ph.D. Forum makes it possible for graduate students to (i) present their research during the poster session, (ii) give a 2-3 minute introduction during a Ph.D. mentor luncheon, (iii) attend a special parallel computing career session, (iv) meet and interact with designated faculty and industry mentors during the conference, and (v) distribute their research materials (poster, abstract) and CV to other ICPP participants.  A Ph.D. Forum award acknowledges best overall poster, introduction, and participation.  An associated objective is to lay the groundwork for continuation of the Ph.D. Forum in future years of ICPP.  For this purpose, Ph.D. students provide feedback on their experience at ICPP and thoughts for improvements in the Ph.D. Forum.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761772","Spokes: MEDIUM: MIDWEST: Collaborative: An Integrated Big Data Framework for Water Quality Issues in the Upper Mississippi River Basin","OAC","BD Spokes -Big Data Regional I","08/01/2018","07/29/2018","Philip Gassman","IA","Iowa State University","Standard Grant","Martin Halbert","07/31/2022","$100,000.00","","pwgassma@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","024Y","8083","$0.00","This project will develop a cyberinfrastructure framework to facilitate research on the efficient management of agricultural practices and their impact on water resources in the Upper Mississippi River Basin (UMRB).  Large-scale data acquisition, integration, analysis, and visualization using data-enabled information technologies will accelerate the dissemination of knowledge, experience, and shared resources (e.g., technology, equipment, and people) among communities and partners.  The key element of the project is a new cyber platform, the Upper Mississippi Information System (UMIS), which will provide water quality data within a rich spatio-temporal hydrologic context.  The UMIS directly addresses three of the Grand Challenges for Engineering identified by the National Academy of Engineering: i) provide access to clean drinking water; ii) manage the nitrogen cycle; and iii) engineer the tools of scientific discovery.  The UMIS will immediately begin facilitating data access, integration, and scientific discovery for water quality challenges in the UMRB. <br/><br/>UMIS will offer internet-based open access to water quality information in its meteorological, hydrological, and geographical context, providing almost endless potential benefits for stakeholders.  For example, the experimental design of the UMIS will enable researchers to study spatial scaling, efficiency of various land use and agricultural practices to improve water quality, and the impact of climate change on land management and water quality.  Decision-makers, producers, and extension staff will be able to assess the relative efficacy of local (e.g., best management practices) versus system-level (e.g., state programs) solutions designed to reduce pollution, optimize the use of resources, and evaluate tradeoffs among competing objectives.  For all stakeholders, the UMIS will support partnerships and collaborations, increase dissemination of information about a critical natural resource to empower stakeholders at all levels, and set new standards in the communication of scientific data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750970","CAREER: Computational Optics and Photonics for Deep Imaging of Live Tissue","OAC","CAREER: FACULTY EARLY CAR DEV, EPSCoR Co-Funding","05/01/2018","04/30/2018","Heidy Sierra","PR","University of Puerto Rico Mayaguez","Standard Grant","Alan Sussman","04/30/2023","$498,905.00","","heidy.sierra1@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","1045, 9150","026Z, 1045, 9102, 9150","$0.00","Innovation in non-invasive imaging techniques is part of the effort to provide rapid screening, diagnosis as well as to guide treatment in numerous settings that aspire to offer affordable and efficient healthcare.  Most of the existing high-resolution methods are effective primarily on thin and nearly homogeneous transparent samples or over tissue surface.  In most realistic scenarios, it is important to acquire information at depth within tissue.  High-resolution volumetric imaging approaches may require expensive computational tools for data analysis and complex hardware configurations.  Computational optics grounded on signal processing and image reconstruction concepts offers promising alternatives.  This research contributes to advance the related state-of-the-art in translational cyberinfrastructure and biomedical technology.  Results from this research can improve non-invasive imaging systems for research and patient care while supporting the NSF mission to promote the progress of science and advance the national health.  The development of this project involves multidisciplinary efforts from computer science, bioengineering and electrical engineering as well as educational activities with the participation of students from underrepresented groups.   <br/><br/>This project focuses on providing a framework to support advances on optical imaging techniques that can perform at the needed resolution and speed for various scenarios such as healthcare and biomedical research.  The research plan is geared to creating an advanced cyberinfrastructure with simulation and analysis tools to build a computational optical system for deep imaging of live tissues.  The components of the framework include three-dimensional optical imaging models employing nonlinear scattering theory that integrate tissue optical properties to characterize their effect into the imaging resolution performance.  Additionally, it includes the integration of light-tissue-interaction modeling parameters with compressive sensing concepts and machine learning algorithms for advanced data management.  This project targets realistic challenges in biomedical research, including (i) a gap between complex physics of light propagation in tissues and the design of efficient high-resolution imaging systems, (ii) computational optics and photonics for deep imaging of live tissues, and (iii) integration with reliable and state-of-the-art data analytics and visualization environments.  The simulations and computational optics tools focus on confocal imaging of skin tissue, which is widely used in biomedical research, and is potentially adoptable in the clinic to guide diagnosis of skin conditions.  The education plan addresses three major areas: i) research training and experiences for graduate and undergraduate students, i) course development in topics related with computational optics and data analytics, and iii) outreach to K-12 students and professionals to introduce research issues and opportunities in computational imaging and data analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1956009","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","08/01/2019","10/31/2019","Amit Sheth","SC","University of South Carolina at Columbia","Standard Grant","Wendy Nilsen","08/31/2022","$120,000.00","","amit@sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","024Y","028Z","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities. (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755179","CRII: OAC: Inferring, Attributing, Mitigating and Analyzing the Malicious Orchestration of Internet-scale Exploited IoT Devices: A Network Telescope Approach","OAC","CRII CISE Research Initiation","03/01/2018","02/12/2018","Elias Bou-Harb","FL","Florida Atlantic University","Standard Grant","Alan Sussman","11/30/2019","$175,000.00","","elias.bouharb@utsa.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","026Y","026Z, 8228, 9102","$0.00","Despite the benefits provided by the widespread adoption and deployment of diverse Internet-enabled devices such as phones and smart home components in consumer markets and critical infrastructure - the so called Internet of Things (IoT) devices, security concerns are rising as such devices also introduce new vulnerabilities that could be leveraged by attackers to launch disrupting cyber-attacks.  The objective of this project is to enable exploration of the inherent insecurity of the IoT paradigm by exploring innovative data analytics as applied to raw cyber security data.  Insights gained will allow detection, characterization and attribution of Internet-scale compromised IoT devices, coupled with their malicious activities, in near real-time.  Several technical challenges impede addressing IoT security at large, including, the excessive diversity of IoT devices in addition to their Internet-wide deployment, the lack of IoT-relevant data and the shortage of IoT-specific actionable attack signatures.  In this context, this project serves NSF's mission to promote the progress of science by aiming to generate a first-of-a-kind, large-scale analysis of the magnitude of compromised IoT devices.  The project also promotes cyber security research and training for minorities, given that it will be executed within the boundaries of a designated Hispanic-serving institution.  Moreover, the project will contribute to operational cyber security by developing a real-time capability for storing and sharing IoT-relevant threat information.<br/><br/>The project will draw-upon macroscopic, large-scale passive measurement data collected in real-time from a network telescope to highlight the severity of the insecurity of the IoT paradigm.  Network telescopes, most commonly known as darknets, constitute a set of routable, allocated yet unused IP addresses.  The project will design and develop real-time algorithms that are capable of inferring Internet-scale exploited IoT devices by exploring darknet data.  Furthermore, the project will investigate formal correlation approaches rooted in stochastic data structures between IoT-relevant passive measurements and malware samples to aid in the attribution and thus the remediation objective.  The project will further explore the orchestration behavior of seemingly independent IoT activities, which operate within well-coordinated IoT botnets.  To this end, the project will innovate time series analytics based upon trigonometric interpolation techniques, recursive optimal stochastic estimators, and bitmap matching algorithms to infer such IoT botnets by employing passive measurements.  The project will also (1) develop a unique cyberinfrastructure for IoT cyber threat indexing by automating the proposed algorithms, techniques and methods, (2) generate IoT-specific signatures by employing piecewise hashing techniques, and (3) create access methods based on an API mechanism and a front-end service facilitated by Elasticsearch to allow the sharing of IoT-centric empirical data, threat intelligence and signatures.  <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029261","Collaborative Research: IRNC: Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","09/01/2020","09/07/2021","Ilya Baldin","NC","University of North Carolina at Chapel Hill","Continuing Grant","Kevin Thompson","08/31/2023","$1,226,921.00","Thomas Lehman, Paul Ruth, Ezra Kissel","ibaldin@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7369","","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2001752","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","09/01/2019","10/31/2019","Michael Zentner","CA","University of California-San Diego","Standard Grant","Seung-Jong Park","04/30/2021","$172,474.00","","mzentner@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"2104068","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","Polar Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes, EarthCube","08/01/2021","07/15/2021","Sri Hari Krishn Narayanan","IL","University of Chicago","Standard Grant","Tevfik Kosar","07/31/2025","$800,000.00","Michel Schanen","snarayan@mcs.anl.gov","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","5407, 7726, 8004, 8074","077Z, 1079, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103791","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","PHYSICAL OCEANOGRAPHY, Software Institutes","08/01/2021","07/15/2021","Nora Loose","CO","University of Colorado at Boulder","Standard Grant","Tevfik Kosar","07/31/2025","$166,590.00","","nora.loose@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1610, 8004","077Z, 1079, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118180","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","10/01/2021","09/02/2021","Jindal Shah","OK","Oklahoma State University","Standard Grant","Alan Sussman","09/30/2025","$104,997.00","","jindal.shah@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","044Y","7231, 7301, 9150","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118174","Collaborative Research: CyberTraining: Implementation: Medium: Establishing Sustainable Ecosystem for Computational Molecular Science Training and Education","OAC","CyberTraining - Training-based","10/01/2021","09/02/2021","Michael Shirts","CO","University of Colorado at Boulder","Standard Grant","Alan Sussman","09/30/2025","$104,845.00","","michael.shirts@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","044Y","7231, 7301","$0.00","Computational research in molecular sciences increasingly involves electronic structure theory, advanced sampling algorithms in molecular dynamics/Monte Carlo, and data science and machine learning using increasingly high-end and complex software and hardware resources. The lack of well-curated training materials and hands-on training opportunities significantly inhibits the progress of the next generation of computational molecular science cyberinfrastructure (CI) users. This project will establish an institute focused on serving the advanced cybertraining needs of the communities engaged in computational molecular science and engineering (CMSE). To do so, this project will bring together molecular sciences and engineering experts to address this cybertraining challenge through a core committee, invited instructors, advisory board, and community participants. <br/><br/>This project will establish an Institute for Computational Molecular Science Education, which will be designed to create a sustainable ecosystem for training the next generation of research workforce in molecular simulation CI. This project will include educational modules for training in advanced computational tools while bolstering fundamental understanding of underlying theoretical concepts. The project will encompass summer/winter schools for hands-on training on advanced computational techniques and enhancing peer networking for early-stage researchers, web-based content to support training at a larger scale, and curriculum and instructional materials for undergraduate and graduate courses to support course development in CMSE. This project will train the next generation of professionals in chemical engineering, molecular and materials science, chemistry, and biophysics by providing critical tools in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106147","Collaborative Research: OAC Core: Simulation-driven runtime resource management for distributed workflow applications","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/03/2021","Loic Pottier","CA","University of Southern California","Standard Grant","Alan Sussman","09/30/2024","$220,000.00","","lpottier@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","090Y","026Z, 7923, 9102","$0.00","Many scientific breakthroughs in domains such as health, climate modeling, particle physics, seismology, etc.,  can only be achieved by performing complex processing of vast amounts of data.  This processing is automated by software systems that use the compute, storage, and network hardware provided by the cyberinfrastructure.  In addition to automation, a key objective of these systems is the efficient use of the resources as measured by cost and energy usage, while making the processing as fast as possible or as needed. To this end, these systems must make decisions regarding which resources should be used to do what and when.  Many such systems are used in production today and make such decisions. Yet making good, let alone best, decisions is still an open research challenge. Theoretical research has proposed solutions that are difficult to put into practice, and practical solutions are known to not make good decisions, or at least not consistently so.  However, both theory and practice follow the same basic philosophy: make decisions by reasoning about known information on what needs to be computed and on what hardware resources are available. This philosophy has shown its limits, so this project adopts a radically different approach.  The key idea is to repeatedly execute fast, computationally inexpensive simulations of the application execution in order to evaluate large sets of potential resource management decisions and automatically select the most desirable ones. The benefits of this approach will be demonstrated for several software systems used to support scientific applications that are critical for the development and sustainability of society.<br/><br/>Software systems are used to run scientific applications on advanced cyberinfrastructure.  These systems automate application execution, and make resource management decision along several axes including selecting and provisioning (virtualized) hardware, picking application configuration options, and scheduling application activities in time and space. Their objective is to optimize both application performance and also a set of resource usage efficiency metrics that include monetary and energy costs. Consequently, the resource management decision space is enormous, and making good decisions is a steep challenge that has been the subject of countless efforts, both from theoreticians and practitioners.  However, the challenge is far from being solved: theoreticians produce solutions that are rarely used by practitioners, and conversely practitioners implement solutions that may be highly sub-optimal because they not informed by theory. This project resolves this disconnect by obviating the need for developing effective resource management strategies.  The key idea is to use online simulations to search the resource management decision space rapidly at runtime. Large numbers of fast simulations of the application's execution are executed throughout that very execution, so as to evaluate many potential resource management options and automatically select desirable ones.  This approach thus shifts the overall problem from the design of complex resource management algorithms to the enumeration of many resource management decisions. The transformation of resource management practice in cyberinfrastructure systems not only renders the resource management problem tractable but also unlocks previously out-of-reach resource management decisions.  The benefits of this transformation will be demonstrated for a critical class of production systems and applications, specifically Workflow Management Systems and the scientific applications they support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103937","Elements: Cognitasium - Enabling Data-Driven Discoveries in Natural Hazards Engineering","OAC","Software Institutes, CDS&E","06/01/2021","05/11/2021","Krishna Kumar Soundararajan","TX","University of Texas at Austin","Standard Grant","Tevfik Kosar","05/31/2024","$556,306.00","Niall Gaffney, Ellen Rathje","krishnak@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004, 8084","036E, 037E, 038E, 039E, 040E, 043E, 077Z, 1057, 1576, 7231, 7923, 8004, 8084, 9263","$0.00","Numerical modeling plays a critical role in assessing and mitigating risks posed by natural hazards, such as the risks to coastal communities from hurricanes and the threats to infrastructure from earthquakes. Accurately predicting these hazards requires modeling the multi-scale nature of these problems, covering a range of physical scales from microscopic to kilometer-scale interactions. Traditional approaches often focus on a particular scale and are incapable of predicting the risks accurately. With the advent of community data repositories such as the DesignSafe CyberInfrastructure, there is an as-yet untapped opportunity to effectively use these large datasets to develop new data-driven models to solve multi-scale problems. Furthermore, assessing the risks of natural hazards involves a complex web of interconnected analyses, which leads to difficulties in tracking the various uncertainties driving the final decision. Tracking the modeling workflow can ensure decision processes are informed and transparent and can help decision-makers define their confidence in model results. To support these needs, new methods are required to automate the workflow tracking and help researchers find and effectively utilize the large datasets in community repositories to develop new theories. Cognitasium, an Artificial Intelligence (AI)-powered cyberinfrastructure, addresses these challenges by automatically extracting the hazard analysis workflows, augmenting large community datasets with relevant information for analysis, and enabling AI models to discover new theories from massive datasets. Cognitasium is being developed as an open-source framework and can be easily adapted to a variety of communities. The project uncovers the possibility of discovering new theories by combining field and experimental data with numerical simulations in large community datasets. The automated tracking of workflows improves the research reproducibility and transparency in risk assessment. The project will transform static data repositories into an active community of users and developers working together to develop new theories. With sustainable software practices and open science strategy, it will support a large community of users beyond natural hazard engineering. The software and tools from the project are generalizable to other fields with massive data requirements and the need for multi-scale models and reduced uncertainties (e.g., physics and health sciences). The project incorporates four specific educational objectives: Inspire future scientists through Code@TACC aimed at enabling high-school students to program, mentor and train undergraduate researchers, facilitate the retention of underrepresented minorities through workshops and training offered at the National Society of Black Engineers and underrepresented colleges, and dissemination through documentation, webinars, and summer institutes. <br/><br/>Uncertainties surrounding the modeling process can have important implications for the decision-making process in Natural Hazard Engineering (NHE). Assessing the risks of natural hazards is a complex process involving numerical simulations, integrated field and lab characterization, and uncertainty quantification. The complex web of inter-connected analysis leads to an inability to track workflows and accurately propagate the associated uncertainties, thus impacting the decision-making process. Meanwhile, the emergence of interactive tools such as Jupyter Notebooks has transformed data analysis and exploration. However, the interactive nature of Jupyter has further exasperated the ability to track workflows. Hence, there is an urgent need for automatically extracting workflows to incorporate a data-driven approach to quantify and reduce uncertainty and improve the decision-making process. Cognitasium is a novel machine-learning-powered CI framework for data-driven discoveries in NHE. Cognitasium will become a fundamental component of the NSF-funded DesignSafe CyberInfrastructure offering benefits to a broad community of NHE researchers. The project will: (i)  enable end-to-end integration of uncertainty propagation in agile environments through workflow tracking in parameterized Jupyter notebooks, (ii) build knowledge graphs that integrate experimental and field data with numerical analysis to develop new multi-scale models, and (iii) support scalable machine learning to solve complex multi-scale problems with large datasets. The AI framework will improve natural hazard analysis and mitigation of hurricanes, storm surge, and earthquakes through data-driven discoveries. The data-driven CI framework will be generalizable to other fields with massive data and the need for multi-scale models and reduced uncertainties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931575","Collaborative Proposal: Frameworks: Project Tapis: Next Generation Software for Distributed Research","OAC","Software Institutes","09/01/2019","06/11/2021","Gwen Jacobs","HI","University of Hawaii","Standard Grant","Seung-Jong Park","08/31/2024","$1,013,212.00","Sean Cleveland","gwenj@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","8004","026Z, 077Z, 7925, 8004, 9102, 9150, 9251","$0.00","The goal of a robust cyberinfrastructure (CI) ecosystem is to become a catalyst for discovery and innovation by fostering the development of software frameworks as sustainable production-quality services. Modern science and engineering research increasingly span multiple, geographically distributed data centers and leverage instruments, experimental facilities and a network of national and regional CI. The Tapis framework will enable scientists to accomplish computational and data intensive research in a secure, scalable, and reproducible way allowing scientists to focus on their research instead of the technology needed to accomplish it.  Tapis will allow easier implementation, sharing and re-use of complex computational applications, workflows, and infrastructure and enable analysis previously too challenging for researchers. The framework will maximize application portability, allowing flexible scheduling of geographically distributed computational workloads, offer a web-based science-as-a-service to enable multi-facility, decentralized deployments, and provide production-grade support for sensors and streaming data.  Tapis will impact multiple science domains, geographic and underrepresented communities with the potential to tackle the world's most important scientific problems spanning astronomy, climate science, medicine, natural hazards, and sustainability science.  Education and outreach will include sponsored workshops, hackathons and training materials covering the platform and providing examples to encourage widespread adoption for users across a variety of technical skills and targeting the next generation of young researchers and professionals through immersive workshops and professional development opportunities.<br/><br/>Tapis, will be a new platform for distributed computational experiments that leverages NSF's investments in the Agave, Abaco and CHORDS projects. The Tapis software framework will 1) provide production-grade support for sensors and streaming data, 2) maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers, and 3) provide science-as-a-service HTTP-based RESTful APIs to enable multi-facility, decentralized deployments that are both secure and scalable. Working alongside a diverse set of domain researchers to drive real-world use cases, Tapis will be the underlying cyberinfrastructure for computational workflows and science gateways. Tapis will leverage containers to maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers.  The project will achieve this flexibility by introducing execution system capabilities and application requirements throughout the framework. The Jobs service will be run in a distributed manner to take advantage of data locality and, optionally, to schedule jobs on underutilized systems. Tapis will deploy in centralized or distributed configurations using a microservices architecture that includes a novel, decentralized security and authorization kernel. This kernel can be deployed on-premises to retain local control over confidential data. Custom microservices can be plugged into the security kernel to provide new capabilities, resulting in a cyberinfrastructure ecosystem for distributed computing. To effectively execute Tapis, teams from the Texas Advanced Computing Center (TACC), the University of Texas at Austin (UT), and the University of Hawaii (UH) will leverage a long-standing collaboration to support investigator-driven, geographically distributed, data-intensive research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103963","CSSI Elements: EWMS - Event Workflow Management Service","OAC","IceCube Research Support, WoU-Windows on the Universe: T, ANT Astrophys & Geospace Sci, Polar Cyberinfrastructure, PHYSICS AT THE INFO FRONTIER, Software Institutes","07/01/2021","05/06/2021","Benedikt Riedel","WI","University of Wisconsin-Madison","Standard Grant","Amy Walton","06/30/2024","$596,051.00","Brian Bockelman, Miron Livny","briedel@icecube.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","011Y, 107Y, 5115, 5407, 7553, 8004","069Z, 077Z, 4444, 7569, 7923","$0.00","This award will begin developing an Observation Management System Framework which will help alleviating multiple types of astrophysical data and respective workflow management - a burden shouldered now by human researchers who process large numbers of independent pieces of data (?events?). The core element of this framework - Event Workflow Management System (EWMS) - is a workload manager designed for processing events (simulated readouts from a particle physics detector, recorded data points, images, etc.) with complex workflows. EWMS could become a central core for applying novel computer science methods to improve scientific data processing with an initial focus on Multi-Messenger Astrophysics (MMA) - it will transform how ?event?-based computational problems are tackled on the national CI ecosystem through a set of reusable services. This paradigm is applicable across a wide range of science domains, including astrophysics, astronomy, physics, biology, and other disciplines that deal with Big Data flows.<br/> <br/>The EWMS will benefit three of NSF?s 10 big ideas ? ?Growing Convergence Research,? ?Harnessing the Data Revolution,? and ?Windows on the Universe"", bringing together data from the particle physics-based detectors (IceCube Neutrino Observatory, High Altitude Water Cherenkov Observatory, Cherenkov Telescope Array), traditional astronomical observatories (large telescopes), and gravitational wave observatories (LIGO, VIRGO, KAGRA). While the data types of each of these experiments or observatories are dramatically different, they all record data in the independent spatio-temporal increments, observations, triggers, events, etc.  that are processed and stored separately. The EWMS is applicable to nearly all current workflows in MMA experiments, and by combining these most precise observations via EWMS, scientists will be able to observe the Universe in fundamentally new ways and learn more about its history than any one of these messengers can provide in isolation. Thus, this award addresses and advances the science objectives and goals of the NSF's ""Windows on the Universe: The Era of Multi-Messenger Astrophysics"" program.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe Program, the Physics at the Information Frontier Program, the IceCube Science Program, the Antarctic Astrophysics and Geospace Sciences Program, and the Office of Polar Programs CyberInfrastructure Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104831","Collaborative Research: CDS&E: Multifidelity Uncertainty Quantification Through Model Ensembles and Repositories","OAC","CDS&E","09/01/2021","05/21/2021","Daniele Schiavazzi","IN","University of Notre Dame","Standard Grant","Tevfik Kosar","08/31/2024","$254,860.00","","dschiavazzi@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8084","026Z, 8084","$0.00","Numerical simulations are increasingly used in clinical research and practice for diagnosis and treatment planning in cardiovascular disease, creating new demand for reliable simulation and analysis tools. Quantification of uncertainty in these simulations is crucial to increased clinical adoption but has previously been largely disregarded due to its excessive computational cost and complexity. To address these challenges, the project leverages a new class of multi-fidelity Monte Carlo estimators for direct and inverse problems, designed to mitigate computational complexity through the solution of a large number of inexpensive low-fidelity surrogates. It demonstrates the proposed approach in full-scale clinical problems including multiple uncertainty sources at a reasonable computational budget. The project?s main objective is to create an end-to-end advanced cyberinfrastructure ecosystem for uncertainty quantification (direct problem) and parameter estimation (inverse problem) in cardiovascular models incorporating realistic sources of uncertainty, able to leverage arbitrary low-fidelity models through advanced Monte Carlo estimators, while drastically reducing computational cost and complexity. The project?s interdisciplinary team is synergizing computational modeling, cardiovascular physiology, UQ and open-source software towards making UQ tractable in full-scale 3D cardiovascular simulations, leveraging multi-fidelity estimators for the solution of both direct and inverse problems. The project will produce seamless cyberinfrastructure linking two well-regarded open-source packages, Dakota and SimVascular, with sizable user communities.<br/><br/>The project is creating new cyberinfrastructure ecosystems for large-scale UQ tasks. Dissemination to industry/academia is performed through SimVascular, a leading open-source platform for cardiovascular modeling. It will leverage SimVascular and the proposed multi-fidelity estimators to create hands-on teaching material for graduate and undergraduate courses. Although the project focuses on cardiovascular modeling, its results are directly applicable to other engineering problems. The PIs will organize minisymposia and workshops at national conferences. They will lead outreach activities to local K-12 schools to attract girls and underrepresented minority (URM) students to STEM. The PIs will mentor URM summer students through the SURF program and women students through the Women in Mathematics, Scientific Computing and Engineering (WiMSCE) group at Stanford.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104032","Elements: A Deep Neural Network-based Drone (UAS) Sensing System for 3D Crop Structure Assessment","OAC","Capacity: Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes","06/01/2021","05/21/2021","Guoyu Lu","NY","Rochester Institute of Tech","Standard Grant","Amy Walton","05/31/2024","$583,703.00","Jan van Aardt","luguoyu@cis.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","168Y, 7726, 8004","077Z, 1165, 7923","$0.00","This project develops a 3D reconstruction sensing system that can be installed on unmanned aerial systems (UAS), to be used by agricultural researchers, growers, and service providers to assess crop growth.  Applying Artificial Intelligence (AI) technology for large scale agriculture reconstruction applications, the sensing system would be able to estimate crop structure for a large coverage area at a much lower cost than current standards that rely on light detection and ranging (LiDAR). <br/>    <br/>The project would develop and refine a deep neural network-based 3D assessment workflow, based solely on a low cost and lightweight 2D LiDAR and color camera configuration.  Researchers, growers, and service providers would be able to extract detailed crop structure and forecast yields, based on a 3D time series of crop growth.  The technology would provide a less expensive alternative to the current 3D LiDAR sensor approach, and the sensing system could also be applied to related areas such as high-throughput phenotyping and variation estimation of general terrestrial vegetation.  Outreach and extension activities are included, to deliver research outcomes to the stakeholders, including agricultural researchers, growers and service providers.   PhD students, undergraduates, and high school students will be trained through this project, including a summer activity training high school students through the Rochester Institute of Technology Imaging Science High School Summer Intern Program.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103832","Collaborative Research: ELEMENTS: Tuning-free Anomaly Detection Service","OAC","Data Cyberinfrastructure, Software Institutes","05/01/2021","05/21/2021","Elke Rundensteiner","MA","Worcester Polytechnic Institute","Standard Grant","Amy Walton","04/30/2024","$259,651.00","","rundenst@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7726, 8004","077Z, 7923","$0.00","Finding and understanding anomalous behavior in data is important in many applications. A large number of anomaly detection algorithms exist, and it can be difficult to determine which algorithm is best suited to a particular domain.  And once an algorithm is selected, users must tune many parameters manually to get the algorithm to perform well; this requires in-depth knowledge of the machine learning process and an understanding of the trade-offs among different algorithms to select the best performing approach.  To address these difficulties, this team develops a package that can test a range of unsupervised anomaly detection techniques on a dataset, explore options to identify best-fit, and classify anomalies with higher accuracy than manual tuning.<br/><br/>The project will automatically test a range of unsupervised anomaly techniques on a data set, extract knowledge from the combined detection results to reliably distinguish between anomalies and normal data, and use this knowledge as labels to train an anomaly classifier; the goal is to classify anomalies with an accuracy higher than what is achievable by thorough manual tuning. The approach can be applied across of a range of data types and domains. The resulting cyberinfrastructure provides tuning-free anomaly detection capabilities while making it easy to incorporate domain-specific requirements. It enables scientists and engineers having little experience with anomaly detection techniques to steer the anomaly detection process with domain expertise.  Evaluation of the unsupervised anomaly detection package will use data sets and partnerships with collaborators from the Massachusetts General Hospital/Harvard Medical School, Cyber Security research, and Signify (formerly Philips Lighting) to ensure that the utility and usability of the package is verified throughout the development process. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103563","CDS&E: Fast Search of Growing High-Dimensional Big Data to Enable Accurate Semiclassical Molecular Dynamics Studies of Large Molecular Systems","OAC","CDS&E","06/01/2021","05/21/2021","Yu Zhuang","TX","Texas Tech University","Standard Grant","Tevfik Kosar","05/31/2024","$278,348.00","","yu.zhuang@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","8084","026Z, 8084","$0.00","Quantum effects are inherent factors for material properties and chemical processes. By capturing quantum effects with good quantitative accuracy, ab initio semiclassical molecular dynamics simulation is a generally applicable investigation tool for a broad range of chemical and material science studies, including studies on pollutant effects on lung health, enzyme catalysis, ozone depletion, space craft surface coating, solar cells, and a lot more studies that promise to advance national health and pharmaceutical sciences, material design investigations for national defense, energy and environmental protection researches, etc. But the computation cost of semiclassical dynamics simulations is enormously high, making semiclassical dynamics highly challenging, and even infeasible in many cases, for large molecular systems. This project proposed methods for reducing computation cost while maintaining simulation accuracy, which will expand the reach of semiclassical dynamics study to a broader range of studies of national and scientific importance. <br/><br/>Ab initio semiclassical molecular dynamics simulation has enormous computation cost in calculating ab initio Hessians from quantum mechanical electronic structure theories. Hessian modeling using training data in the closest time distances from a set of saved ab initio data has been successful in reducing the cost of Hessian calculations while maintaining simulation accuracy. It was observed that opportunities exist for further reduction of computation cost by using training data in the closest spatial distances, which offers more chances for Hessian modeling to replace ab initio Hessian. Due to the frequent incoming of new ab initio data, the ab initio data set is constantly growing.  To search frequently updated growing datasets, a challenge is that the algorithms not only need to achieve high search efficiency but also have to be efficient for re-organizing the dataset with frequent insertions of new data. Existing searching algorithms are good in search efficiency but not so good in data-organizing efficiency since they were designed for static or infrequently updated datasets. This project develops search algorithms that will be the first to leverage the growing process of datasets to deliver high efficiency in both searching and data organizing. Hessian modeling using training data of closest spatial distance returned by the new search algorithms has the potential for further reduction of computation cost, promising to speed up dynamics simulations and enable simulations of larger molecular systems and/or the use of higher-accuracy electronic structure theories to capture better details of the molecular systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031845","Frontera Travel Grant: Unraveling Hadron Mass and Quark Structure with COMPASS and COMPASS++/AMBER","OAC","Leadership-Class Computing","09/01/2020","05/12/2020","Caroline Riedl","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2022","$9,300.00","","criedl@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103494","Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED)","OAC","Software Institutes","09/01/2021","08/24/2021","Yinzhi Wang","TX","University of Texas at Austin","Standard Grant","Tevfik Kosar","08/31/2025","$646,826.00","","iwang@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","077Z, 7925, 8004","$0.00","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. <br/><br/>The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106991","Collaborative Research: OAC Core: Stochastic Simulation Platform for Assessing Safety Performance of Autonomous Vehicles in Winter Seasons","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/01/2021","Xianfeng Yang","UT","University of Utah","Standard Grant","Alan Sussman","09/30/2024","$299,913.00","","x.yang@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","090Y","079Z, 7923","$0.00","The safety of an autonomous vehicle (AV) highly depends on the generalization capability of its automation systems (e.g., perception and decision-making) when being deployed in diverse physical environments. Although the current commercialization of AVs has been shown to improve traffic safety, AV safety performance under adverse driving conditions in winter seasons still lacks comprehensive evaluation. To bridge the research gap, this project aims to develop a stochastic simulation platform, which examines the efficiency, reliability, and safety of AVs, to prevent costly mistakes in widespread field implementations. The research methods use a foundation of machine learning and physics principles to formulate an integrated and hybrid approach to model stochastic vehicle behaviors in traffic streams. Potential AV safety risks under adverse driving conditions will be assessed with dynamic modeling of vehicle behavior. The project will produce an open-source and cloud-based simulation platform that allows public access to test vehicle automation systems. The simulation models can be improved over time through the use of an online machine learning architecture. The research activities will be closely integrated with a set of education and outreach activities that include (i) incorporating advanced computational techniques into the curriculum, (ii) sparking the interests of younger generations in science and engineering by local K-12 outreach efforts and summer camps, and (iii) broadening the participation of underrepresented student groups in computing through the artificial intelligence club at San Diego State University, a Hispanic serving institution.  <br/><br/>This multidisciplinary research project aims at contributing improved algorithms in simulation and fundamental knowledge in computing to building an advanced cyberinfrastructure toolkit. The project focuses on producing a stochastic simulation platform that can evaluate the capabilities of AVs' automated driving systems. The motivation is to produce a reliable tool that can model stochastic vehicle behaviors, study vehicle dynamics, and predict potential AV safety risks under adverse driving conditions in winter. To this end, the project will first leverage the physics principles of a microscopic traffic model to regularize the machine learning process for simulating vehicle interactions. Second, both multi-vehicle and single-vehicle crash probabilities in mixed traffic will be predicted by integrating the traffic simulation model with a new vehicle dynamics model. The stochastic vehicle motions will then be studied to assess AV safety performance on icy/snowy pavement. Third, the models will be integrated into an open-source software package with comprehensive documentation and multiple application cases. The expected deliverable will be a public cloud-based platform that is easy to access and is capable of incorporating new data streams for model improvement. After validating the models with field data, the project will connect the simulations with existing automated driving systems for testing. The project can have broad impacts on other science and engineering fields, such as physics-supported artificial intelligence, smart and autonomous systems, and other research domains that depend on simulated data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106965","Collaborative Research: OAC Core: Stochastic Simulation Platform for Assessing Safety Performance of Autonomous Vehicles in Winter Seasons","OAC","OAC-Advanced Cyberinfrast Core","10/01/2021","09/01/2021","Xiaobai Liu","CA","San Diego State University Foundation","Standard Grant","Alan Sussman","09/30/2024","$199,691.00","","xiaobai.liu@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","090Y","079Z, 7923","$0.00","The safety of an autonomous vehicle (AV) highly depends on the generalization capability of its automation systems (e.g., perception and decision-making) when being deployed in diverse physical environments. Although the current commercialization of AVs has been shown to improve traffic safety, AV safety performance under adverse driving conditions in winter seasons still lacks comprehensive evaluation. To bridge the research gap, this project aims to develop a stochastic simulation platform, which examines the efficiency, reliability, and safety of AVs, to prevent costly mistakes in widespread field implementations. The research methods use a foundation of machine learning and physics principles to formulate an integrated and hybrid approach to model stochastic vehicle behaviors in traffic streams. Potential AV safety risks under adverse driving conditions will be assessed with dynamic modeling of vehicle behavior. The project will produce an open-source and cloud-based simulation platform that allows public access to test vehicle automation systems. The simulation models can be improved over time through the use of an online machine learning architecture. The research activities will be closely integrated with a set of education and outreach activities that include (i) incorporating advanced computational techniques into the curriculum, (ii) sparking the interests of younger generations in science and engineering by local K-12 outreach efforts and summer camps, and (iii) broadening the participation of underrepresented student groups in computing through the artificial intelligence club at San Diego State University, a Hispanic serving institution.  <br/><br/>This multidisciplinary research project aims at contributing improved algorithms in simulation and fundamental knowledge in computing to building an advanced cyberinfrastructure toolkit. The project focuses on producing a stochastic simulation platform that can evaluate the capabilities of AVs' automated driving systems. The motivation is to produce a reliable tool that can model stochastic vehicle behaviors, study vehicle dynamics, and predict potential AV safety risks under adverse driving conditions in winter. To this end, the project will first leverage the physics principles of a microscopic traffic model to regularize the machine learning process for simulating vehicle interactions. Second, both multi-vehicle and single-vehicle crash probabilities in mixed traffic will be predicted by integrating the traffic simulation model with a new vehicle dynamics model. The stochastic vehicle motions will then be studied to assess AV safety performance on icy/snowy pavement. Third, the models will be integrated into an open-source software package with comprehensive documentation and multiple application cases. The expected deliverable will be a public cloud-based platform that is easy to access and is capable of incorporating new data streams for model improvement. After validating the models with field data, the project will connect the simulations with existing automated driving systems for testing. The project can have broad impacts on other science and engineering fields, such as physics-supported artificial intelligence, smart and autonomous systems, and other research domains that depend on simulated data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031997","Frontera Travel Grant: Multi-Scale Modeling of Accretion and Jets in Active Galactic Nuclei","OAC","Leadership-Class Computing","09/01/2020","06/05/2020","Alexander Chekhovskoy","IL","Northwestern University","Standard Grant","Edward Walker","08/31/2022","$8,704.00","","atchekho@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005597","Category II: Unlocking Interactive AI Development for Rapidly Evolving Research","OAC","Innovative HPC","06/01/2020","09/01/2021","Paola Buitrago","PA","Carnegie-Mellon University","Cooperative Agreement","Robert Chadduck","05/31/2025","$11,250,000.00","Sergiu Sanielevici, Nicholas Nystrom, Shawn Brown","paola@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7619","7619","$0.00","High-accuracy artificial intelligence (AI) has repeatedly delivered great benefit across science and engineering. AI enables information extraction and analysis of large datasets and the creation of high-fidelity models that can augment or replace more computationally expensive calculations in traditional simulation codes. In this way, AI can accelerate time-to-science by orders of magnitude. To reap these benefits, highly complex AI models must first be trained. Training is a computation-intensive process that takes days, weeks, or even months, requires optimization of both the network architecture and its hyperparameters, and limits the scope and complexity of challenges addressed. What if training time could be reduced to the point of being interactive, taking only minutes or hours even in the more extreme cases? The result would be transformative: scientists and engineers could rapidly develop and refine their ideas, enabling them to achieve high-impact solutions to the most pressing and complex issues.<br/><br/>To help advance knowledge by enabling unprecedented AI speed and scalability, the Pittsburgh Supercomputing Center (PSC), a joint research center of Carnegie Mellon University and the University of Pittsburgh, in partnership with Cerebras Systems and Hewlett Packard Enterprise (HPE), will deploy Neocortex, an innovative computing resource that will accelerate scientific discovery by vastly shortening the time required for deep learning training/inference, foster greater integration of deep AI models with scientific workflows, and provide revolutionary innovative hardware for the development of more efficient algorithms for artificial intelligence and graph analytics. Neocortex will advance knowledge by accelerating scientific research, enabling development of more accurate models and use of larger training data, scaling model parallelism to unprecedented levels, focusing on human productivity by simplifying tuning and hyperparameter optimization, and providing a revolutionary hardware platform for the exploration of new frontiers.<br/><br/>Neocortex will introduce the most powerful AI processor to the NSF cyberinfrastructure ecosystem and will democratize access to game-changing compute power, otherwise only available to tech giants, for students, postdocs, faculty, and others, who require faster training turnaround to analyze data and integrate AI with simulation. It will provide a unique opportunity to explore the potential of a groundbreaking new AI hardware architecture, tapping into the revolutionary AI processor technology of the Cerebras CS-1 AI platform and the large in-memory scale up capabilities of HPE Superdome Flex to unlock new insights and accelerate time to discovery. The Neocortex project will additionally focus on building a strong community around these revolutionary capabilities, including collaborations with other leading national institutions and emphasizing inclusion and diversity. It will build STEM talent through training and internships, develop the U.S. workforce and national competitiveness through industrial outreach, and foster international collaborations. Public outreach and XSEDE campus champion and domain champion activities will help engage a wider audience.<br/><br/>The novel Neocortex architecture will couple two exceptionally powerful Cerebras CS-1 AI servers with an exceptionally large shared memory HPE Superdome Flex HPC server to achieve unprecedented AI scalability with excellent system balance. Each Cerebras CS-1 is powered by one Cerebras Wafer Scale Engine (WSE) processor, a revolutionary high-performance processor designed specifically to accelerate deep learning training and inferencing. The Cerebras WSE is the largest chip ever built, containing 400,000 AI-optimized cores implemented on a 46,225 square millimeter wafer with 1.2 trillion transistors. An on-chip fabric provides 100Pb/s of bandwidth through a fully configurable 2D mesh with no software overhead. The Cerebras WSE includes 18GB of SRAM accessible within a single clock cycle at 9PB/s bandwidth. The Cerebras WSE is uniquely engineered to enable efficient sparse computation, wasting neither time nor power multiplying the many zeroes that occur in deep networks. The Cerebras CS-1 software can be programmed with common ML frameworks such as TensorFlow and PyTorch, which for computational efficiency are mapped onto an optimized graph representation and a set of model-specific computation kernels. It also supports native code development. Support for the most popular deep learning frameworks and automatic, transparent acceleration will provide researchers with exceptional ease of use.<br/><br/>The HPE Superdome Flex HPC server of Neocortex will be an extremely powerful, user-friendly front end for the Cerebras CS-1 servers. This will enable flexible pre- and post-processing of data flowing in and out of the attached WSEs, preventing bottlenecks to taking full advantage of the WSE capability, and implementing advanced deep learning functions such as augmentation, hyper-parameter and model optimization, and ensemble learning. The Superdome Flex will be robustly provisioned with 24TB of RAM, 204.8TB of high-performance NVMe flash storage, 32 Intel Xeon CPUs, and 24 100GbE network interface cards to create the greatest flexibility for scaling applications across multiple CS-1 systems.  Internally, the HPE Superdome Flex is interconnected by a custom memory fabric ASIC for cache-coherent hardware shared memory sustaining 850GB/s of interconnect bandwidth. Its large and fast memory and high compute performance will enable training on very large datasets with exceptional ease, avoiding the laborious task of splitting and trying to load-balance datasets across worker nodes.<br/><br/>Each Cerebras CS-1 has 1.2Tbps I/O, and will connect to the HPE Superdome Flex via twelve, standard 100GbE links. This configuration will deliver the greatest possible performance and flexibility, including, via PSC-Cerebras and PSC-HPE research partnerships, exploration of scaling training to multiple CS-1 systems.<br/><br/>Neocortex will be federated via 16 InfiniBand HDR100 connections (an aggregate 1.6Tbps) with Bridges-2, an NSF-supported capacity resource. This federation will yield great benefits to the user community including access to the Bridges-2 filesystem to manage persistent data; general-purpose computing for data preprocessing and traditional machine learning; interoperation with data-intensive projects using Bridges-2; and high-bandwidth external network connectivity to other XSEDE Service Providers, campus, labs, and clouds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940270","Collaborative Research: Integrating Physics and Generative Machine Learning Models for Inverse Materials Design","OAC","HDR-Harnessing the Data Revolu","10/01/2019","07/28/2020","Fuchang Gao","ID","Regents of the University of Idaho","Continuing Grant","Daryl Hess","09/30/2022","$312,016.00","","fuchang@uidaho.edu","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","099Y","054Z, 062Z, 8396, 8399, 9150","$0.00","This project is aimed to address a grand challenge in data-intensive materials science and engineering to find better materials with desired properties, often with the goal to enhance performance in specific applications. This project addresses this grand challenge with a specific focus on finding metal organic framework (MOF) materials that are used to separate gas mixtures and finding better battery materials for energy storage. The PIs will combine theoretical methods from statistical mechanics and condensed-matter physics, and physics-based models, to generate information-rich materials data which is integrated with generative machine learning (ML) algorithms to search a complex chemical design space efficiently and to train deep learning models for fast screening of materials properties. This project will be carried out by a multidisciplinary collaboration involving researchers from physics, materials science and engineering, computer science, and mathematics. The resulting multidisciplinary environment fosters training the next generation data savvy scientists who will engage in collaborative multidisciplinary research.  <br/><br/>Existing approaches for computational design of metal organic frameworks (MOF) and solid-state electrolyte materials are largely based on screening of known materials or enumerative search of hypothetical materials. This project develops a new approach that integrates first principles calculations, experimental data and abundant data generated by physics-based models to train generalized antagonistic network (GAN) models for efficient search of the materials design space, and to train deep convolutional neural network (DCNN) models for fast and accurate screening of properties of the GAN-generated candidate materials. Additionally, graph-based GAN models will be used for MOF topology exploration and can be applied to other nanomaterials designs. More specifically, the investigators will: 1) develop and exploit physics-based models for fast calculation of properties such as diffusivity, ion conductivity, and mechanical stability; 2) develop generative adversarial network (GAN) models with built-in physics rules for efficient exploration of the chemical design space for both MOF materials and solid electrolytes; 3) use persistence homology and Bravais lattice sequence representations of MOF materials and solid electrolytes, respectively, to build Deep Convolutional Neural Network (DCNN) models for fast and accurate prediction of the physical properties of generated materials; 4)  apply high-level quantum-mechanical calculations for verification of discovered materials. Accomplishments from this project will lead to accelerated discovery of novel nanostructured materials for gas separation and energy storage, materials for lithium-ion batteries, novel data-driven scheme for materials design, and theoretical methods enabling implementation of advanced data science techniques. The highly interdisciplinary collaboration will offer students unique opportunities to interact with a variety of disciplines, and training the next-generation scientists with the mindset for multidiscipline collaborations. Educational and outreach activities will be developed and undertaken in conjunction with the proposed research activities.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017072","Collaborative Research: CyberTraining: Implementation: Medium: The Informatics Skunkworks Program for Undergraduate Research at the Interface of Data Science and Materials Science","OAC","CyberTraining - Training-based, DMR SHORT TERM SUPPORT","09/01/2020","08/18/2021","Dane Morgan","WI","University of Wisconsin-Madison","Standard Grant","Alan Sussman","08/31/2024","$844,570.00","Anne Gillian-Daniel, Wendy Crone","ddmorgan@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","044Y, 1712","054Z, 062Z, 095Z, 1711, 9178","$0.00","The project will develop a sustainable and scalable approach to train a workforce skilled in research and application of machine learning (ML) in materials informatics. ML in materials informatics is rapidly transforming materials science and engineering (MS&E) by an unprecedented ability to extend materials databases, improve materials simulation, mine texts, automate materials research and development, and accelerate materials design. As pointed out by multiple recent studies, including from the National Academies and the Minerals, Metals & Materials Society (TMS), it is essential to train a next generation workforce in ML for materials informatics to realize its enormous potential for improving the human condition through advanced materials. Unfortunately, ML in materials informatics is almost completely absent from today's materials curricula at the undergraduate level. However, the power of informatics tools combined with their rapid evolution and relative novelty in MS&E creates an opportunity for engaging undergraduates (UGs) with active learning through impactful research. With this motivation, the project will create new infrastructure and an ecosystem for the engagement and training of UGs across the U.S. in research using applied ML in materials informatics, called the Informatics Skunkworks. The Skunkworks consists of mentor/UG teams performing research in materials informatics. The project provides the teams with new resources, consisting of curricula, software, and research problems, and with a community of practice to support research and to work effectively and collaboratively. The low cost and accessibility of ML and materials informatics creates an opportunity for Skunkworks to engage mentors and students with limited research resources, particularly at institutions serving underrepresented groups. The Skunkworks is a sustainable and scalable approach that can fulfill this unmet need by training a diverse workforce skilled in research and application of ML for materials informatics. <br/>   <br/>The project will provide freely available (a) curriculum to train UGs in relevant ML, materials informatics and research professional development, (b) software tools that augment existing ML packages to be UG accessible, and (c) authentic and appropriate-level research problems. The proposed work will also develop a community of practice to enable a network of productive mentor/UG research teams to effectively and collaboratively use the curricular, software and other resources developed by the project to support transforming the future workforce. The intellectual merit of the proposed work is to (a) develop scalable resources to increase UG experience and learning in research at the boundary of data science and materials science and engineering, (b) grow a community of mentors and UG researchers engaged in materials informatics research, and (c) increase the utilization of data science tools for solving critical problems in MS&E and related fields through workforce development and materials informatics training. The broader impact of the proposed work is to (a) freely disseminate enabling curricula and tools for materials informatics, (b) train staff, mentors and UGs in broadly applicable research and professional skills, and (c) develop a diverse community of practice for materials informatics researchers. The project will enable the development of a new workforce capable of advanced materials informatics, especially for underrepresented groups, by supporting primarily UG institutions and community colleges that often have limited research resources. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104106","Collaborative Research: Elements: EXHUME: Extraction for High-Order Unfitted Finite Element Methods","OAC","Software Institutes","06/01/2021","05/11/2021","John Evans","CO","University of Colorado at Boulder","Standard Grant","Tevfik Kosar","05/31/2024","$300,331.00","Kurt Maute","john.a.evans@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8004","077Z, 7923, 8004","$0.00","Unfitted finite element methods allow for the simulation of physical systems that are difficult if not impossible to simulate using classical finite element methods requiring body-fitted meshes. For instance, unfitted finite element methods can be directly applied to the simulation of physical systems exhibiting a change of domain topology, such as the movement of blood cells through a human capillary or the flow of blood past the heart valves between the four main chambers of the human heart. Unfitted finite element methods also streamline the construction of computational design optimization technologies that optimize the geometry and material layout of an engineered system based on prescribed performance metrics. However, the computer implementation of an unfitted finite element method remains a challenging and time-consuming task even for domain experts. The overarching objective of this project is to construct a novel software library, EXHUME (EXtraction for High-order Unfitted finite element MEthods), to enable the use of classical finite element codes for unfitted finite element analysis. EXHUME will empower a large community of scientists and engineers to employ unfitted finite element methods in their own work, allowing them to carry out biomedical, materials science, and geophysical simulations that have been too expensive or too unstable to realize using classical finite element methods. EXHUME will also improve the fidelity of design optimizations being performed in academia, national laboratories, and industry on a near daily basis.<br/><br/>Unfitted finite element methods simplify the finite element solution of PDEs (Partial Differential Equations) on complex and/or deforming domain geometries by relaxing the requirement that the finite element approximation space be defined on a body-fitted mesh whose elements satisfy restrictive shape and connectivity constraints. Early unfitted finite element methods exhibited low-order convergence rates, but recent progress has led to high-order methods. The key ingredient to success of a high-order unfitted finite element method is accurate numerical integration over cut cells (i.e, unfitted elements cut by domain boundaries). EXHUME uses the concept of extraction to express numerical integration over cut cells in terms of basic operations already implemented in typical finite element codes, an integration mesh, and extraction operators expressing unfitted finite element basis functions in terms of canonical shape functions. EXHUME generates integration meshes and extraction operators outside of the confines of a particular finite element code so it may be paired with existing codes with little implementation effort. A key goal of the project is demonstration of EXHUME by connecting it to existing research codes and the popular FEniCS toolchain for finite element analysis. An effort parallel to software development explores accuracy versus efficiency trade-offs associated with 1) approximations made during extraction and 2) novel numerical quadrature schemes for cut cells. The breadth of EXHUME's technical impact is ensured by several factors: 1) the ubiquity of PDEs across nearly all disciplines of science and engineering, 2) the library's interoperability with existing finite element codes, and 3) the generic nature of the EXHUME+FEniCS demonstrative example, which can be applied to arbitrary systems of PDEs. By simplifying the setup of PDE-based computational models, EXHUME+FEniCS enables classroom demonstrations simulating complicated physical scenarios without letting the technical details of numerical methods distract from the scientific principles being taught.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004973","Collaborative Research: Frameworks: Community-Based Weather and Climate Simulation With a Global Storm-Resolving Model","OAC","Software Institutes, EarthCube","08/01/2020","08/23/2021","Andrew Gettelman","CO","University Corporation For Atmospheric Res","Standard Grant","Alan Sussman","07/31/2025","$1,993,269.00","Thomas Hauser, Richard Loft, William Skamarock","andrew@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","8004, 8074","026Z, 077Z, 4444, 7925, 8004","$0.00","Global Earth System Models (ESMs) use mathematical equations to simulate both weather and climate. ESMs include the dynamics of the atmosphere, oceans, land surface, ice, and vegetation. They can be used to make predictions of use to the public and policymakers. Today?s ESMs use coarse grids with cells about 100 km wide. Important weather systems like thunderstorms are too small to be simulated with such grids. One way to improve ESMs is to use finer grids that can directly simulate thunderstorms, but such models can only be run on very powerful computers. This project, called EarthWorks, will create an ESM capable of resolving storms by taking advantage of recent developments in high performance computing. EarthWorks will also use artificial intelligence to improve and speed up the model, and state-of-the-art methods to limit the amount of data produced as the model runs. The EarthWorks ESM will be built by spinning off and modifying a copy of the most recent version of the widely used Community Earth System Model. The modified model will represent the atmosphere, the oceans, and the land surface on a single very high-resolution grid, with grid cells about 4 km wide. It will have improved forecast skill, and produce more realistic simulations of past, present, and future climates. The project will make the model and its output openly available for use by all scientists.<br/><br/>The open-source Community Earth System Model (CESM) is both developed and applied to scientific problems by a large community of researchers. It is critical infrastructure for the U.S. climate research community. In the atmosphere and ocean components of the CESM, the adiabatic terms of the partial differential equations that express conservation of mass, momentum, and thermodynamic energy are solved numerically using what is called a dynamical core. Atmosphere and ocean models also include parametric representations, called parameterizations, that are designed to include the effects of storm and cloud processes that occur on scales too small to be represented on the model's grid. Despite decades of work by many scientists, today's parameterizations are still problematic and limit the utility of ESMs for many applications of societal relevance. Fortunately, recent advances in computer power have made it possible to parameterize less, by using grid spacings on the order of a few kilometers over the entire globe. These ""global storm-resolving models"" (GSRMs) can only be run on today's fastest computers. GSRMs are under very active development at a dozen or so modeling centers around the world. Unfortunately,  however, the current formulation of the CESM prevents it from being run as a GSRM. This project, called EarthWorks, will create a new, openly available GSRM by spinning off and intensively modifying a copy of the CESM. To accomplish this goal, the researchers will use recently developed and closely related dynamical cores for the atmosphere and ocean. All components of the model will use the same very high-resolution grid. This high resolution will make it possible to eliminate the particularly troublesome parameterization of deep cumulus convection (i.e., thunderstorms), and thereby reduce systematic biases that plague current ESMs. Earthworks will exploit the pre-exascale and exascale technologies now being brought to market by high performance computing vendors. The new exascale ESM will run the most computationally intensive components on powerful graphics processor units (GPUs), and exploit node-level task parallelism to execute the rest of the model asynchronously. The component model codes are close to completion and are currently being tested on GPUs. EarthWorks will use a simplified component-coupling approach, incorporate machine learning where feasible, and leverage lossy compression techniques and parallel I/O tools to deal with the enormous data volumes that will be generated as the model runs. The completed model will be simple, powerful, and well documented. The project will apply it to pressing scientific problems in both numerical weather prediction and climate simulation. The model and its input datasets will be made openly available to the broad research community, via GitHub.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104076","Collaborative Research: Framework Implementations: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network Dynamics","OAC","Software Institutes","09/01/2021","05/11/2021","Sanjukta Bhowmick","TX","University of North Texas","Standard Grant","Tevfik Kosar","08/31/2025","$449,410.00","","Sanjukta.Bhowmick@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","8004","077Z, 7925, 7942, 8004","$0.00","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.<br/><br/>This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104007","Elements: Crowdsourced Materials Data Engine for Unpublished XRD Results","OAC","DMR SHORT TERM SUPPORT, Software Institutes, CDS&E","08/01/2021","05/11/2021","Yinghui Wu","OH","Case Western Reserve University","Standard Grant","Tevfik Kosar","07/31/2024","$554,395.00","Alp Sehirlioglu","yinghuiwu.ed@gmail.com","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","1712, 8004, 8084","022E, 023E, 024E, 054Z, 077Z, 094Z, 095Z, 1633, 7923, 8004, 8021, 8025, 9263","$0.00","Although data-driven analysis has been heralded as a new paradigm in fundamental material science such as X-ray Diffraction (XRD) analysis, high-value material datasets are often not made public and are underutilized. This project designs and develops CRUX, a crowdsourced data infrastructure and services to curate, discover, share, and recommend unpublished XRD data and analytical results. CRUX promotes underutilized high-quality material science data by allowing the sharing and exploration of unpublished data with state-of-the-art crowdsourcing, knowledge harvesting, and machine learning techniques. CRUX provides a crowdsourced knowledge base to allow scientists and the general public to share and access unpublished data resources. It also provides (a) a novel search engine that supports simple keyword search, can provide relevant data resources when the exact keyword matching does not exist, and self-evolves to improve the search quality, and (b) a ""data feed"" service to allow users to easily receive and track updates of specific data resources of interest. The developed infrastructure and tools enable an open, collaborative, and sustainable platform that can facilitate exchanging of unpublished XRD data and discoveries, unlock new research problems (e.g., predictive analysis of materials compositions with multi-phase data), and inspire the novel design of machine learning pipelines (e.g., deep neural networks) for data-driven materials science. CRUX will make materials data resources available and shareable for a broad community including materials scientists, data analysts, software developers, and the general public, and thus promote long-term collaborative research, software development, and education. <br/><br/>The developed CRUX system enables (1) coherent representation of materials data, metadata, and knowledge in terms of a three-tier knowledge graph model; (2) scalable XRD metadata curation and information extraction techniques to promote high-value unpublished XRD data sources for data-driven materials research; (3) adaptive, self-improving search and recommendation techniques to recommend relevant datasets upon user requests and feedback, with sustainability beyond the time of the project; and (4) interactive and exploratory search techniques to explain and recommend the relevant datasets beyond the scope of initial queries. CRUX will be evaluated with established human-in-the-loop knowledge bases and active machine learning algorithms by cornerstone materials research such as the discovery of new high-temperature ferroelectrics. The research community will be able to share XRD data resources (analytical results, machine learning models, processing data) via ""one-click"" upload, search for high-quality data resources, and (re)discover new resources for machine learning pipelines. CRUX enables several components to advance data-driven materials research, including a materials knowledge graph model,  automatic data integration, and exploratory query engine that support ""Why"" and ""What-if"" analysis for XRD analysis. Developed solutions will benefit data-driven material science in general. For example, researchers can make use of unpublished two-phase data to predict new materials compositions, identify solubility limits through parameterization by machine learning tools, and refine machine learning models with more sophisticated techniques such as deep neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105044","CRII: OAC: Scalability of Deep-Learning Methods on HPC Systems: An I/O-centric Approach","OAC","CRII CISE Research Initiation","06/01/2021","05/06/2021","Loic Pottier","CA","University of Southern California","Standard Grant","Alan Sussman","05/31/2023","$175,000.00","","lpottier@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","026Y","026Z, 075Z, 079Z, 8228","$0.00","Machine learning (ML) algorithms have became key techniques in many scientific domains over the last few years. Thanks to the recent democratization of graphics processing units (GPUs), machine learning is mostly fueled by deep learning (DL) techniques that require extensive computational capabilities and vast volumes of data. Training large deep neural networks (DNN) is compute-intensive. However, thanks to GPUs, the cost of the compute-intensive components of the training has been reduced, while the relative cost of memory accesses has increased following the huge increase in the size of inputs datasets and in the complexity of the ML models. Due to their increasing requirements in terms of computational and memory capabilities, DNNs are now trained on distributed systems and have recently gained attention from the high-performance computing (HPC) community. A key challenge on HPC systems at extreme scale is the communication bottleneck, as communication is much slower than the required computations and also accounts for high energy consumption on large-scale machines. A lack of a comprehensive understanding of the different trade-offs, costs, and impacts induced by ML algorithms may severely impair science discoveries and AI breakthroughs in the near future. This project aims to address this problem by developing accurate performance models that can capture the complexity of training a DNN at scale in terms of I/O (communication) and, based on these models, producing efficient scheduling heuristics to reduce communication when training DNNs on HPC machines. Reducing data exchanges during the training phase decreases the execution time of this costly process and is likely to also reduce its energy consumption. The training of DNNs is becoming essential for many scientific domains, so optimizing the execution of this key component will help NSF fulfill its mission to advance and promote the progress of science. The proposed research will provide researchers with performance models that are key to supporting the development of novel middleware systems for large-scale ML on HPC platforms. Educational and outreach activities will include the development of pedagogic modules that will teach students key concepts of distributed computing and training of large neural networks and enable students to participate in workshops and conferences that serve the community. <br/><br/>Training large neural networks on distributed HPC systems is challenging. DNN training involves complex communications patterns with some randomness due to the optimization method used to solve the network, which most of the time is stochastic gradient descent (SGD). Most distributed ML has been designed to run on cloud infrastructures, however HPC machines exhibit different characteristics in terms of hardware with fast interconnect networks and advanced communications capabilities, such as remote direct memory access (RDMA) and, in terms of software with the usage of the message passing interface (MPI) and OpenMP parallel programming models. This project will design performance models taking into account HPC characteristics that  will give useful insights into the behavior of DNN training at scale, for example, how the data communication volume evolves with the DNN batch size or how to leverage HPC multi-layered storage, such as burst buffers, to improve DNN training performance.  This project is organized around three research thrusts: (i) estimation of data movement costs when training DNNs on HPC machines (ii) augmenting performance models with energy metrics and (iii) developing bi-objective heuristics minimizing communication and energy while still providing training accuracy guarantees. In order to address these three research thrusts, this project  will adopt a simulation-driven approach. The first step will be to characterize the I/O behaviors of DNNs when trained on HPC machines. Based on the analysis of the collected data, several performance models and scheduling heuristics will be designed. Then, a simulator of the HPC machine will be developed using the NSF-funded WRENCH project. This simulator will be calibrated with the data collected during the characterization phase. Finally, the performance models and the scheduling heuristics will be evaluated using the calibrated simulator. The project will also leverage the simulator to continuously improve the performance models and heuristics. This project will provide scientists with models to better understand performance trade-offs arising when training large-scale neural networks on complex distributed systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105012","CRII: OAC: A Computational Framework for Studying Transport Phenomena in Complex Networks: From Biological Towards Sustainable and Resilient Engineering Networks","OAC","CRII CISE Research Initiation","06/01/2021","05/06/2021","Nariman Mahabadi","OH","University of Akron","Standard Grant","Alan Sussman","05/31/2023","$174,381.00","","nmahabadi@uakron.edu","302 Buchtel Common","Akron","OH","443250001","3309722760","CSE","026Y","8228","$0.00","Transport networks are found everywhere in living systems, from the veins in the leaves of plants to the networks in our bodies: the respiratory system that handles the flow of air, the circulatory system that carries nutrients through blood circulation, and the networks of nerves and brain cells that transport electrical impulses. Because biological networks are necessary for transporting essential resources (blood, oxygen, water, and nutrients), they are critical for health and survival. Therefore, there is a clear need to explore the fundamental principles of these networks to better predict how they will behave under unexpected conditions in order to prevent potential failures. Exploring the underlying mechanisms of biological flow will not only advance the current state of knowledge of biological transport networks but can provide exceptional opportunities to solve complex problems in discrete calculus, graph theory, and optimization. This would, in turn, lead to improvements in engineering transport networks that are critical for maintaining human life in ways that will make them more durable and operate with greater efficiency, from networks that are large in scale, such as traffic systems, irrigation and water delivery systems, and power grids to networks that are small in scale such as fuel cells, solar cells, or artificial organs. The computational framework developed in this project will advance our knowledge of biological vascular networks, which can lead to optimized bioinspired solutions for many engineering transport networks such as water distribution and drainage networks to the treatment of cardiovascular diseases and more efficient drug delivery through the use of nanoparticles. Since the developed models for leaf venation networks in this project are critical to plant performance, the results can also enhance productivity of ecosystems and will have applications in agriculture. As such, this research project aligns with NSF?s mission to promote the progress of science and to advance national health, prosperity and welfare. This work incorporates multidisciplinary research collaborations that will make a significant contribution to education, outreach, and diversity by engaging undergraduate students, including underrepresented students, in research and incorporation of biology and engineering in outreach programs for K through 12 students.z<br/><br/>Over billions of years of natural selection, nature has evolved complex topologies to solve a wide range of problems. A conspicuous class of such topologies are the ramified heterogeneous structures in numerous biological systems that transport resources, such as leaf venation networks, the root and axis system of plants, and the cardiovascular system of animals and humans. The evolution and function of such branched structures is not only critical for an organism?s survival and fitness but has also inspired scientists and engineers to improve the performance of many engineering flow networks such as fuel cells, solar cells and synthetic organs. The overall aim of this project is to 1) develop a robust and efficient computational framework to study transport phenomena in complex biological networks, 2) apply the framework to study the rules of nature that optimize mass and heat transfer in biological networks, and 3) assess the feasibility of bioinspired principles to design sustainable and resilient engineering networks. The proposed framework will enable the development of transformative models that not only advance our knowledge about underlying biophysical phenomena in highly heterogeneous biological networks but also provide new opportunities to apply bioinspired solutions for many engineering applications. The proposed research will result in a highly efficient and robust framework that enables (1) a fundamental understanding of the performance of complex biological transport networks and their biophysical characteristics and functions which are essential for survival, (2) an understanding of multiphysics coupled transport phenomena in highly heterogeneous biological networks, (3) an assessment of the resilience of networks to damage and varying fluxes and an understanding the underlying mechanisms and principles used by biological systems for optimization of cost and performance, and (4) assessment of the feasibility and scalability of the biological mechanisms as bioinspired solutions for practical engineering problems that range from micro to macro in scale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103509","Elements: AMR-H: Adaptive multi-resolution high-order solver for multiphase compressible flows on heterogeneous platforms","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes","09/01/2021","05/06/2021","Sanjiva Lele","CA","Stanford University","Standard Grant","Tevfik Kosar","08/31/2024","$600,000.00","Alexander Aiken","lele@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","1443, 1642, 8004","077Z, 1443, 7923, 8004","$0.00","In the past decades, computational research has enabled high-fidelity simulations of complex fluid dynamics problems. However, the new generation of high performance computing architectures present significant challenges in portability and, more importantly, parallel performance of complex science application software. This work develops a multi-purpose computational fluid dynamics solver for high-fidelity high-order adaptive resolution simulations. It leverages the state-of-the-art high performance programming models, Legion and Kokkos, which guarantees the performance portability on various existing and upcoming high performance computing platforms. Additionally, it integrates the commonly used numerical and physical modules, with an easy-to-use programming interface for users. As a significant benefit, the researchers can maintain their focus on physical modeling and not require a deep understanding of code design for new high-performance hardware. The educational and community outreach elements of the project will develop a growing community of computational scientists and engineers who are educated to exploit the power of task-level parallelism and enable a new era in high-fidelity computational science.<br/><br/>This project develops a general computational framework combining high-order, high accuracy, solution-adaptive discretizations of partial differential equations (with emphasis on flows of non-ideal fluids) tailored to the physics they represent. The discretization is optimized for high resolving efficiency, allows optimal use of the computational degrees of freedom and high utilization of the computer resources due to its high arithmetic intensity and data locality. Adaptive mesh refinement in combination with high-order multi-resolution compact scheme allows for easy pre-processing and meshing for complex-geometry problems. Co-designing the numerical framework with new developments in the Legion framework would allow for automated, optimized runtime scheduling of tasks involving computational kernels and data movement across memory hierarchies. This, combined with efficient leveraging of Kokkos, would free the computational scientist/engineer from hardware specific programming models and allow exascale computations on heterogeneous computers. It will enable first of its kind simulations of compressible multiphase flow phenomena in turbulent flow regimes for retrograde fluids on exascale platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103845","Collaborative Research: Elements: SENSORY: Software Ecosystem for kNowledge diScOveRY - a data-driven framework for soil moisture applications","OAC","Hydrologic Sciences, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","06/01/2021","05/05/2021","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","Amy Walton","05/31/2024","$349,998.00","","taufer@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1579, 7222, 8004, 8074","077Z, 7923","$0.00","Tools for gathering soil moisture data (such as in situ soil sensors and satellites) have differing capabilities.  In situ soil moisture data has fine-grained spatial and high temporal resolution, but is only available in limited areas; satellite data is available globally, but is more coarse in resolution. Existing software tools for studying the dynamic characteristics of soil moisture data are limited in their ability to model soil moisture at multiple spatial and temporal scales, and these limitations hamper scientists? ability to address urgent practical problems such as wildfire management and food and water security. Accurate gathering and effective modeling of soil moisture data are essential to address pressing environmental challenges. This interdisciplinary project designs, builds, and shares a data-driven software ecosystem for soil moisture applications. This software ecosystem models and predicts soil moisture at scales suitable to support studies in forestry, precision agriculture, and earth surface hydrology.<br/><br/>This project connects multi-disciplinary advances across the scientific community (such as generating datasets at scale and supporting cloud-based cyberinfrastructures) to develop a data-driven software ecosystem for analyzing, visualizing, and extracting knowledge from the growing data collections (from fine-grained, in situ soil sensor information to coarse-grained, global satellite measurements) and releasing this knowledge to applications in environmental sciences.  Specifically, this project (a) develops scalable methodologies to integrate and analyze soil moisture data at multiple spatial and temporal scales; (b) implements a data-driven software ecosystem to access complex information and provide basic and applied knowledge to inform researchers and stakeholders interested in soil moisture dynamics (scientists, educators,  government agencies, policy makers); and (c) builds cyberinfrastructures to support discovery on cloud platforms, lowering resource barriers to improve accessibility and interoperability.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104002","Elements: Developing an integrated modeling platform for tectonics, earthquake cycles and surface processes","OAC","Tectonics, Geophysics, XC-Crosscutting Activities Pro, Geomorphology & Land-use Dynam, Software Institutes, EarthCube","05/01/2021","04/26/2021","Eunseo Choi","TN","University of Memphis","Standard Grant","Amy Walton","04/30/2024","$443,830.00","","echoi2@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","1572, 1574, 7222, 7458, 8004, 8074","077Z, 7923","$0.00","This project develops user-friendly and sustainable code that can simulate lithospheric deformations coupled with landscape evolution and earthquake cycles. Natural hazards such as earthquakes, flooding, landslides, and volcanoes sit at the intersection of geologic and human time scales; the ability to simulate interactions among diverse geologic processes provides a baseline to study how global changes are perturbing these processes on human time scales.  The goal of the project is to view earthquake cycles and landscape evolution as a system coupled with long-term lithospheric deformation. Multiple research communities including geomorphology, structural geology and earthquake physics are expected to benefit from the new modeling capability this project enables. <br/><br/>The product software extends existing open-source code, DES3D (Dynamic Earth Solver in 3D). With internal enhancements and scalable performance enabled by parallel computing technology, the software functions as a reliable integrated modeling platform that can explore interplay between tectonics, surface processes and earthquake cycles. The project includes measures to improve user experience with the product code; Web-based input file generation, useful documentation and tutorials readable and executable online will make the product code accessible and easy to use. To facilitate sustained development and maintenance of the product code, modern software engineering practices are adopted.  Container-based technology is built into the product code to create reproducible packages of a model without user intervention. To ensure that the product code will be a valuable addition to the existing cyberinfrastructure, this project will actively leverage expertise from collaborators who have conducted related research. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Geophysics Program, the Tectonics Program, the Geomorphology and Land-Use Dynamics Program of the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103945","Framework: Awkward Arrays - Accelerating scientific data analysis on irregularly shaped data","OAC","Software Institutes","09/01/2021","04/26/2021","Jim Pivarski","NJ","Princeton University","Standard Grant","Tevfik Kosar","08/31/2024","$1,067,480.00","","pivarski@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8004","077Z, 7925, 8004","$0.00","Nowadays, scientists need to be programmers as well as experts in their fields. Any software that simplifies the computational gymnastics of analyzing data is welcomed, as it allows scientists to focus more of their attention on what they want to compute, rather than how, but there is usually a tension between ease of use and computational speed. Faster computation means more data analyzed, but error-free code matters most. Awkward Array is a software library that performs calculations on data that do not fit into neat rows. For instance, a fleet of undersea probes may each measure ocean temperatures at a different number of depths, but data analysis tools like Excel, Pandas, and SQL require measurements to be arranged in rectangular tables with the same number of columns in each row. The problem is more acute when variable numbers of measurements are nested within variable numbers of entities. Traditionally, scientists have either used slow scripting languages or unforgiving ""bare metal"" languages to perform calculations on these non-tabular datasets. Awkward Array generalizes array concepts so that the easy and fast expressions that once applied only to rectangular tables now apply to irregularly shaped data, allowing bare metal speed in a high-level scripting language. This project broadens the applicability of Awkward Array beyond the problem domain it was originally intended for, particle physics, to a wide variety of scientific fields, including oceanography, astronomy, genetics, chemistry, and health care. It also integrates the Awkward Array library with popular data science and machine learning tools and extends the implementation to GPUs for extremely fast processing of the same array idioms.<br/><br/>Scientists use NumPy-based tools, such as Pandas, to analyze large and regularly shaped data efficiently. Packing tables of numbers into contiguous arrays allows operations to be precompiled and fast; and expressing operations as concise commands with implicit loops makes them easier to read and quicker to type during data exploration. However, scientific data often has a complexity that does not fit well into tabular format. This forces scientists to write programs with explicit loops, which are slow in Python. But what about analysis of large and irregularly shaped data? Awkward Array is a generalization of NumPy; it is a Python library defining arrays of objects with arbitrary types and a suite of generic operations on them. Like NumPy arrays, Awkward Arrays are packed into contiguous buffers for efficiency, but unlike NumPy arrays, they can include variable-length lists, nested fields, missing values, and mixed types. Awkward Arrays use 10 times less memory than equivalent Python objects and are up to 100 times faster in computations. This project generalizes Awkward Array as a foundational library for science. It is a cross-disciplinary effort, extending Awkward Array to efficiently solve challenges faced by scientists in a variety of fields and data science in industry. The project members work with scientific collaborators to solve specific problems, adding features to Awkward Array if necessary, and industry collaborators at Anaconda and Nvidia update Python?s standard tools to recognize these new array types for CPU and GPU workloads. It is also an educational project, teaching new approaches to complex problems using array idioms, both to practicing scientists and to students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103682","Elements: Scaling MetPy to Big Data Workflows in  Meteorology and Climate Science","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","05/01/2021","04/26/2021","Ryan May","CO","University Corporation For Atmospheric Res","Standard Grant","Amy Walton","04/30/2024","$598,024.00","Michael Camron, Kevin Goebbert","rmay@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","1525, 8004, 8074","077Z, 4444, 7923","$0.00","MetPy is a Python-based software package for atmospheric science; it provides modern, well-tested, domain-specific software tools for reading data formats, performing calculations, and creating visualizations. MetPy builds upon an extensive set of community-developed scientific Python tools, and leverages technologies such as continuous integration, automated documentation generation, screencasts, and Jupyter notebook tutorials.  This project advances MetPy to address some current limitations in supported data formats, scalability, and run-time performance; it makes MetPy more suitable for working on much larger datasets, frequently encountered in climate science and ensemble-based modeling studies. Addressing these areas allows MetPy to continue to be a powerful tool for Python users in the atmospheric sciences for both small and large datasets. When equipped with modern and well-engineered tools, researchers will be able to better utilize the large amounts of data available in a more time-efficient way.<br/><br/>This project has three main goals: enabling efficient access to big and small datasets, enabling access to cloud-based datasets, and creating training resources for using MetPy with big data.  In tackling these challenges, the project is creating a benchmark suite for MetPy, to quantify the current performance of MetPy in important workflows as well as the performance improvements that occur as a result of further development. Using performance profiling tools, bottlenecks in MetPy are identified and optimized using Python performance tools like Cython and Numba.  MetPy is also being refactored to work better with the Dask library, which provides facilities for distributed computing in Python and would allow MetPy to work more effectively with large datasets.  The World Meteorological Organization (WMO) has made GRIB (GRIdded Binary) its standard format for gridded model output, and BUFR (Binary Universal Form for the Representation of meteorological data) the standard format to encode meteorological observational data. MetPy enhancements developed in this project will enable reading of additional datasets used in large cloud-based data holdings, such as GRIB and BUFR.  All of this work will be featured in additional, freely available training materials in MetPy?s online documentation.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physical and Dynamic Meteorology Program and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104377","CDSE: Collaborative: Cyber Infrastructure to Enable Computer Vision Applications at the Edge Using Automated Contextual Analysis","OAC","CDS&E","09/01/2021","08/27/2021","Vipin Chaudhary","OH","Case Western Reserve University","Standard Grant","Tevfik Kosar","08/31/2024","$119,874.00","","vipin@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","8084","026Z, 1504, 8084","$0.00","Digital cameras are deployed as network edge devices, gathering visual data for such tasks as autonomous driving, traffic analysis, and wildlife observation. Analyzing the vast amount of visual data is a challenge. Existing computer vision methods require fast computers that are beyond the computational capabilities of many edge devices. This project aims to improve the efficiency of computer vision methods so that they can run on battery-powered edge devices. Based on the visual data and complementary metadata (e.g., geographical location, local time), the project first extracts contextual information (such as a city street is expected to be busy at rush hour). The contextual information can help assist determine whether analysis results are correct. For example, a wild animal is not expected on a city street. Moreover, contextual information can improve efficiency.  Only certain pixels need to be analyzed (pixels on the road are useful for detecting cars, while pixels in the sky are not) and this can significantly reduce the amount of computation, thus enabling analysis on edge devices. This project constructs a cyberinfrastructure for three services: (1) understand contextual information to reduce the search space of analysis methods, (2) reduce computation by considering only necessary pixels, and (3) automate evaluation of analysis results based on the contextual information without human effort.<br/><br/>Understanding contextual information is achieved by using background segmentation, GPS-location-dependent logic, and image depth maps.  Background analysis leverages semantic segmentation and analysis over time to identify the background pixels and then generate inference rules via a background-implies-foreground relationship. If a pixel is consistently marked by the same semantic label across a long period of time, this pixel is classified as a background pixel. The background information can infer certain types of foreground objects. For example, if the background is city streets, the foreground objects can be vehicles or pedestrians; if a bison is detected, this is likely a mistake. This project processes only the foreground pixels by adding masks to the neural network layers. Masking convolution can substantially reduce the amount of computation with no loss of accuracy and no additional training is needed. Meanwhile, hierarchical neural networks can skip sections of a model based on context. For example, pixels in the sky only need to be processed by the hierarchy nodes that classify airplanes. The project provides an online service that can accept input data and analysis programs for automatic evaluation of the programs, without human created labels. The evaluation is based on the correlations of background and foreground objects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003887","RUI: Collaborative Research: CDS&E: A Modular Multilayer Framework for Real-Time Hyperspectral Image Segmentation","OAC","CDS&E","08/01/2020","07/23/2020","Luis Cueva Parra","GA","University of North Georgia","Standard Grant","Tevfik Kosar","07/31/2023","$120,588.00","","luis.cuevaparra@ung.edu","82 College Circle","Dahlonega","GA","305971001","7068672064","CSE","8084","026Z, 8084","$0.00","The analysis of images has been used by the scientific community to solve challenging problems and to get insight into diverse natural, social, and technical phenomena. Different types of images have been employed in various areas of study. One example is the hyperspectral images, which have higher resolution when compared to conventional camera images. Analyzing such images has its challenges. For instance, it is computationally demanding, and traditional methods have some limitations. This project provides an efficient solution to analyze such images, by exploiting high-performance computing tools and machine learning techniques. The resulting methods are applied to image-based atmospheric cloud detection.<br/><br/>The project develops a real time, multi-layer, and modular segmentation framework for hyperspectral images. The developed framework automatically identifies various regions within a hyperspectral image by classifying each pixel of the image and associating them to class segments. The developed system is multi-layer, where each layer?s responsibility is to perform an operation on its input, generate region classification data, and pass the resultant output to the next layer. Importantly, each layer analyzes its input from distinct viewpoints, utilizing spectral and spatial data, resulting in a multi-layer framework where the layers complement each other. Also, this project aims to provide an optimized high-performance (speed-up and accuracy) computational tool for real-time hyperspectral image analysis. This is achieved by adapting the algorithms used in the different parts of the model for parallel processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103799","Collaborative Research: Elements: A Self-tuning Anomaly Detection Service","OAC","Data Cyberinfrastructure, Software Institutes","05/01/2021","05/21/2021","Samuel Madden","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","04/30/2024","$340,000.00","","madden@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7726, 8004","077Z, 7923","$0.00","Finding and understanding anomalous behavior in data is important in many applications. A large number of anomaly detection algorithms exist, and it can be difficult to determine which algorithm is best suited to a particular domain.  And once an algorithm is selected, users must tune many parameters manually to get the algorithm to perform well; this requires in-depth knowledge of the machine learning process and an understanding of the trade-offs among different algorithms to select the best performing approach.  To address these difficulties, this team develops a package that can test a range of unsupervised anomaly detection techniques on a dataset, explore options to identify best-fit, and classify anomalies with higher accuracy than manual tuning.<br/><br/>The project will automatically test a range of unsupervised anomaly techniques on a data set, extract knowledge from the combined detection results to reliably distinguish between anomalies and normal data, and use this knowledge as labels to train an anomaly classifier; the goal is to classify anomalies with an accuracy higher than what is achievable by thorough manual tuning. The approach can be applied across of a range of data types and domains. The resulting cyberinfrastructure provides tuning-free anomaly detection capabilities while making it easy to incorporate domain-specific requirements. It enables scientists and engineers having little experience with anomaly detection techniques to steer the anomaly detection process with domain expertise.  Evaluation of the unsupervised anomaly detection package will use data sets and partnerships with collaborators from the Massachusetts General Hospital/Harvard Medical School, Cyber Security research, and Signify (formerly Philips Lighting) to ensure that the utility and usability of the package is verified throughout the development process. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003709","CDS&E: Collaborative Research: HyLoC: Objective-driven Adaptive Hybrid Lossy Compression Framework for Extreme-Scale Scientific Applications","OAC","CDS&E","08/01/2020","07/22/2020","Sheng Di","IL","University of Chicago","Standard Grant","Tevfik Kosar","07/31/2023","$256,761.00","","sdi@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8084","026Z, 8084, 9150","$0.00","Today's extreme-scale scientific simulations and instruments are producing huge amounts of data that cannot be transmitted or stored effectively. Lossy compression, a data compression approach leading to certain data distortion, has been considered as a promising solution, because it can significantly reduce the data size while maintaining high data fidelity. However, the existing lossy compression methods may not always work effectively on all datasets used in specific applications because of their distinct and diverse characteristics. Moreover, the user objectives in compression quality and performance may vary with applications, datasets or circumstances. This project aims to develop a hybrid lossy compression framework to automatically construct the best-fit compression for diverse user objectives in data-intensive scientific research. Educational and engagement activities are provided to develop new curriculum related to scientific data compression and promote research collaborations with national laboratories.<br/><br/>Designing an efficient, adaptive, hybrid framework that can always choose the best-fit compression strategy is nontrivial, since existing state-of-the-art lossy compression methods are developed with distinct principles. The project has a three-stage research plan. First, the project decouples the state-of-the-art error-bounded lossy compression approaches into multiple stages and effectively models the working efficiency (e.g., compression ratio, error, speed) of particular approaches in each stage. Second, the project develops a loosely-coupled framework to aggregate the decoupled compression stages together and also explores as many compression pipelines composed of different stages as possible, to optimize the classic compression efficiency, including compression quality and performance. Third, the project optimizes the synthetic data-movement performance regarding the external devices and resources, such as I/O performance. The team evaluates the proposed framework on multiple extreme-scale scientific applications, including cosmological simulations, light source instrument data analytics, quantum circuit simulations, and climate simulations. The project may create technologies that can increase the storage availability and improve the performance for extreme-scale scientific applications, opening opportunities for new discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2114582","Collaborative Research: Frameworks: The Einstein Toolkit Ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","WoU-Windows on the Universe: T, Software Institutes","10/01/2020","02/02/2021","Pablo Laguna","TX","University of Texas at Austin","Standard Grant","Amy Walton","08/31/2024","$414,003.00","","pablo.laguna@physics.gatech.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","107Y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2046708","CAREER: Systematic Mitigation of Deep Learning Adversaries in Medical Imaging","OAC","CAREER: FACULTY EARLY CAR DEV","07/15/2021","07/09/2021","Pingkun Yan","NY","Rensselaer Polytechnic Institute","Continuing Grant","Alan Sussman","06/30/2026","$318,853.00","","yanp2@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1045","075Z, 079Z, 1045","$0.00","With the enormous amounts of data being acquired by large-scale healthcare systems, computational data analysis has become an essential component in healthcare applications to process and extract information. Deep learning, a sub-category of artificial intelligence (AI), has established itself as a paradigm-shifting technology for data analytics due to its powerful ability to extract high-level data representations. However, deep learning is known to be vulnerable to adversaries, which cause algorithms to yield dramatically different results by making very small alterations to input data samples. Adversaries are particularly hazardous in medical imaging applications where an altered image may lead an AI algorithm to cause medical errors. Thus, there is an urgent need to innovate and build robust healthcare cyberinfrastructure to guard against deep learning adversaries. This project develops novel AI techniques to tackle the unprecedented challenges of adversaries in medical imaging applications from a systematic standpoint. It brings awareness to potential issues when implementing AI in healthcare and develops new tools to mitigate these issues. This research will bolster confidence in adopting AI to improve healthcare efficiency and will also attract and train the next generation of AI researchers and engineers.<br/><br/>This project aims to develop innovative AI techniques to systematically mitigate deep learning adversaries in medical imaging applications. This project is timely as deep learning is already widely used in image reconstruction, quality enhancement, computer-aided diagnosis, and image-guided intervention and surgery. Several challenges, including detection and rectification of adversaries as well as robust algorithm training across data domains, must be resolved before achieving robust medical imaging applications. Existing methods are concerned with only the deep learning algorithms themselves and try to build universal blind robustness against arbitrary adversaries, which overlooks upstream data characteristics and downstream task specifics. This research adopts a holistic approach and is organized around a series of integrated subtopics, including detecting individual adversarial images, differentiating adversarial images from different sources, rectifying adversarial images, determining the transferability of robustness across data domains, and quantifying output uncertainties. The research will provide new insights, accurate yet robust AI techniques, and novel strategies to improve the robustness of medical imaging applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018912","CC*: Integration-Large:  POWWOW: Software-Defined Infrastructure for Wireless, Edge Cybersecurity Testbeds","OAC","CISE Research Resources","10/01/2020","06/30/2020","Anish Arora","OH","Ohio State University","Standard Grant","Deepankar Medhi","09/30/2022","$749,968.00","Rajiv Ramnath, Kannan Srinivasan","anish@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","2890","","$0.00","The project, POWWOW, is a test-bed for large-scale, real-world experimentation in sensing, edge computing, networking, cyber-security, privacy and ethics. POWWOW will extend the large campus network that is in regular use at The Ohio State University (OSU). By embedding novel software-defined sensing, computing and networking capabilities at the edge and the core of this network, researchers will be able to study, in a carefully controlled manner, real-world communications and activities on campus, in order to develop new technology for not only sensing and networking but in particular technology and policies for cyber-security, privacy and ethical use of data.<br/><br/>The POWWOW research cyberinfrastructure is unique because it integrates with OSU's large-scale production networks to support ?living lab? studies in smart sensing. Its focus on real-world sensing and device operations distinguishes POWWOW from other wireless networking testbeds that focus on communication and networking. Researchers and educators will add their own devices and systems, including diverse sensors, wireless sensor networks, software defined radios, controllers, actuators, and vehicles all of which can be dynamically instrumented. Researchers may then, privately and in real time, process data from these devices, share the data, and communicate securely with authenticated users and their devices leading to the discovery of new methods for sensing the acoustic and the radio frequency environment to get private information as well as new methods for concealing information from being sensed. The project team is uniquely composed of researchers across OSU several institutes as well as cyberinfrastructure engineers from the OSU's Office of the CIO that will translate POWWOW into sustained, production use for wide-ranging projects in research and education. Lessons learned from POWWOW will enable systems like it to be replicated in other research environments, thus greatly scaling research.<br/><br/>POWWOW will accelerate the ability of research communities across the nation to perform new types of data-driven research and education. Because it connects to state level resources, such as the Ohio Cyber Range, it may be leveraged at the national level through initiatives such as Fabric-Net, to stimulate the development of an ethical basis for designing privacy preserving services in a world of ubiquitous sensing where explicit consent for every service is infeasible. Project activities will engage students via academic programming and R&D opportunities, ranging from scrubbing and curating data to hackathon-type projects that explore innovative uses of data, and developing, deploying and supporting POWWOW capabilities. This project will contribute to broadening participation in computing by prioritizing research users and projects where engagement of undergraduates from under-represented groups by leveraging established pathways at OSU such as the GEM Consortium and the Ohio LSAMP Alliance.<br/><br/>POWWOW will be hosted on campus at http://powwow.osu.edu. This site will disseminate the artifacts resulting from the project (such as publications, software, system designs and datasets), along with the policies and guidelines for accessing and using POWWOW. The site will be maintained for the duration of this project, and potentially well beyond it, as POWWOW is adopted by researchers and becomes widely used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2046387","CAREER: Scalable Computational Seismology for All","OAC","CAREER: FACULTY EARLY CAR DEV","07/01/2021","06/28/2021","Eileen Martin","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Alan Sussman","06/30/2026","$398,024.00","","eileenrmartin@mines.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1045","026Z, 1045, 9102","$0.00","Computational seismology methods analyze data that measure vibrations of the Earth. These methods allow scientists to understand earthquake hazards, to measure stability of the ground underneath structures, to monitor groundwater systems, to study changes in threatened Earth systems such as glaciers and permafrost, to safely and efficiently explore natural resources underground, and to monitor civil infrastructure health, among other applications. Seismology has undergone a radical shift recently; new sensor technologies have made data collection much easier, enabling hundreds to thousands of times larger datasets that can be used for detailed studies of larger regions for long periods of time. Most scientists cannot use these data because: (1) data are only shared internally among groups that have new sensors, (2) public seismology data storage facilities cannot support such large data quantities, and (3) most geoscientists do not have the computational resources to analyze the data. Because of these three issues, there is an inequitable research environment, much data remains unexplored, and important geoscience discoveries cannot occur. While there are ongoing efforts to address the first issue, without major cyberinfrastructure advances addressing the second and third issues newly acquired data is unlikely to be fully analyzed. This project aims to create new computational algorithms, software and models of open data sharing to ensure that any geoscientist can glean valuable insights from large-scale seismology data. The education and outreach program will create opportunities for more people to participate in mathematical modeling and large-scale data analysis for science and engineering applications. The project PI will develop and strengthen existing efforts to support diverse and inclusive research and learning environments. She will continue to develop a program to introduce women undergraduates to mathematics research, growing it to be a sustainable multi-faculty course serving more students from underrepresented groups. The project will increase the impact of the annual data science conference led by the  PI. The conference features research by women data scientists and tutorials on modern data science techniques, and connects the interdisciplinary data science community on a rural campus.<br/><br/>The project will derive and analyze new geoscience algorithms, develop community software and explore models of open data distribution. The project goal is to ensure that any seismologist can gain valuable geophysical insights from extreme-scale seismic data in the field, at institutions with limited computing resources, and on modern high performance computing (HPC) systems. Expertise in large-scale seismic sensing, mathematics, high-throughput computational science, and algorithm design are necessary to achieve these advances. The project proposes a new model for public seismology data archives that allows for the storage of lossy-compressed data and data products, thus creating a new capacity to host ultra-high-density and large-scale seismic data, without displacing existing systems for high-quality seismometer data. To address large-scale data analysis, the PI has previously created several scalable algorithms, and theoretical analyses suggest that a complete suite of scalable, parallelizable algorithms for multiple types of passive seismic data processing can be developed. Many of the algorithms operate directly on compressed information without reconstructing the original data, which reduces costly data movement. The project will develop fast serial and parallel software algorithm implementations, and investigate the use of accelerator hardware for high computational efficiency. For each algorithm the project will theoretically derive and  computationally verify trends governing tradeoffs between computational efficiency, memory footprint, and end-to-end accuracy specific to the geophysical analyses. The algorithms will incorporate error bounds for realistic non-idealized data and will be included in predictive software for geoscientists to make informed decisions prior to requests for compressed data or data products. The new methods will be tested by applying them to cutting-edge passive seismic data at the scale of tens to hundreds of terabytes. The data will enable seismology analyses in for urban  hydrology and geotechnical engineering, and also analyses to aid in understanding glacier movements to improve climate models via improved boundary conditions and mechanistic understanding.  In addition to earthquake seismology, hydrology, geotechnical engineering, and cryosphere studies, the methods developed can be applied to many high-throughput computational science problems utilizing sparse or compressed representations (e.g., structural health monitoring, imaging science, solar physics, radioastronomy, wireless communications, industrial facility monitoring). To increase adoption of new methods by geoscientists, the project will develop tutorials, promote scientific community collaboration, and organize research workshops.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2128759","Strategic Communications for Advanced Cyberinfrastructure (SCAC)","OAC","CYBERINFRASTRUCTURE","09/01/2021","08/31/2021","Aaron Dubrow","TX","University of Texas at Austin","Standard Grant","Thomas Gulbransen","05/31/2022","$99,975.00","","aarondubrow@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7231","7231","$0.00","The societal value of cyberinfrastructure (CI) impacts on science and engineering research are not as evident or understandable to the public as they should be. Targeted communications can strengthen the understanding of the connection between publicly supported CI resources and U.S. competitiveness, health, and security. Strategic communication satisfies the long-term goals of an organization by facilitating advanced planning and implementation, developing consistent messaging, and enabling successful information sharing. It involves clarifying messages, managing information flow, and cultivating an organization?s image over a long-term horizon, and serves to enhance the strategic positioning of an institution. <br/><br/>The principal investigator (PI) proposes a two-part strategic communications research project with objectives to: <br/>1. Identify the key goals, objectives, audiences, and messages, by which to elevate the awareness and understanding of the value of NSF-funded CI for the U.S; and <br/>2. Develop a framework for strategic communication that can be applied by NSF?s Office of Advanced Cyberinfrastructure (OAC), its funded projects, principal investigators, and partners, to build support for CI investment nationwide. <br/><br/>The research will gather CI successes of NSF and its awardees, as well as broaden conversations about CI in government, the media, and society, and seek to find means by which to best advance the goals of NSF and OAC. Strategic Communications for Advanced Cyberinfrastructure (SCAC) has the potential to advance methods for capture of knowledge and visions, for engaging CI leaders, practitioners, and stakeholders, and for synthesizing these perspectives into digestible forms for community use. Project impacts will include broadening the corpus of research on scientific communications and strategy and serve as a reference point for organizations and academic institutions seeking to contribute to the advancement of CI in the U.S. The effort will directly engage underrepresented minority-serving institutions to tailor communications across a more diverse community of stakeholders and potential participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007661","OAC Core: Small: Open-Source Robust 4D Reconstruction Framework for Real-Time Dynamic Human Capture","OAC","OAC-Advanced Cyberinfrast Core","09/01/2020","05/19/2021","Xiaohu Guo","TX","University of Texas at Dallas","Standard Grant","Alan Sussman","08/31/2023","$515,997.00","","xguo@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","090Y","7923, 9251","$0.00","The upcoming deployment of 5G technology makes it feasible to communicate with extremely low latency the vast amounts of data needed for Augmented Reality (AR) and Mixed Reality (MR), which enables transformative applications such as 3D tele-immersive (or 3D facetime) communication, mobile AR apps using captured 4D human contents, AI assistants of lifelike and personalized avatars, human-aware robots that can serve or work with humans, tele-rehabilitation to connect physical therapists with wounded patients far from treatment facilities, etc. All these examples of AR and MR applications require the development of real-time 4D (space and time) capture and reconstruction of dynamic scenes involving human bodies, faces, body add-ons (like clothes), and their surrounding environments. Despite prior research on real-time 4D reconstruction, there is still no open-source and robust reconstruction system that can model topological changes of the dynamic scenes and track the moving surfaces with accuracy and robustness. This project is designed to bridge such gaps, to develop an open-source and robust 4D reconstruction framework that can benefit researchers and developers in broader scientific communities as well as industry alliances, including 5G medical standards, AR and MR game engines, lifelike AI assistants, human-aware robotics, tele-rehabilitation, etc. This project also provides curriculum development and educational activities for graduate, undergraduate, and K-12 students through summer camps. <br/> <br/>The research proposed for this project centers around the elegant modeling of topological changes in dynamic scenes and the robust tracking of moving surfaces, towards the ultimate goal of developing an open-source robust 4D reconstruction framework for real-time capture of dynamic human scenes. To solve the challenges of topological changes, the volumetric fusion framework and its data structures will be fundamentally redesigned, by introducing Non-manifold Volumetric Grids into both Truncated Signed Distance Field (TSDF) and Embedded Deformation Graph (EDG) representations, allowing both the volumetric cells to replicate themselves and the edges to be broken. Such a novel topology-change-aware framework will allow the reconstructed mesh geometry to update its connectivity on-the-fly, along with a flexible deformation graph updating its connectivity between nodes throughout the 4D capture process. To solve the robust surface tracking problem in 4D dynamic human capture, a Parameterized Animatable Volumetric Model (PAVM) is proposed to combine the benefits of both the parametric human body model and the volumetric TSDF. The TSDF volumetric grids are built on top of the parameterized human body surfaces, so that they can be used to represent the add-ons (e.g. clothes) to the human body. The regular volumetric structure of our PAVM makes it easy to integrate into deep neural networks for robust surface tracking, as well as providing semantic modeling capability. The technical feasibility of the 4D reconstruction framework will be validated by the development of a Mobile 3D Facetime testbed, which will allow people in remote places to interact with each other in a natural AR fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118311","CyberTraining: Implementation: Small: Cybertraining on P4 Programmable Devices using an Online Scalable Platform with Physical and Virtual Switches and Real Protocol Stacks","OAC","CyberTraining - Training-based","10/01/2021","08/31/2021","Jorge Crichigno","SC","University of South Carolina at Columbia","Standard Grant","Alan Sussman","09/30/2025","$499,540.00","Neset Hikmet","jcrichigno@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","044Y","9102, 9150","$0.00","Traditionally, the data plane of network devices has been designed with fixed functions to forward data packets, using a small set of communication protocols. This closed-design paradigm has limited the capability of switches to costly proprietary implementations that are hard-coded by vendors. Recently, data plane programmability has attracted significant attention, permitting the owners of communication networks to use switches with customized processing functions. While large companies are now using programmable platforms, campus networks and small- and medium-sized enterprises have yet to fully benefit from the advantages of P4, the de-facto standard for programming the data plane. A key barrier preventing faster adoption of P4 is the availability of engaging training material for cyberinfrastructure (CI) professionals that focuses on the operation and management of P4 systems. This project addresses the gap by developing hands-on virtual labs that run on a platform for online instruction, referred to as the academic cloud. The project will lower the entry barrier to innovation through P4 technology, which will enable CI professionals to reduce the time to design, test, and adopt new communication protocols; devise new customized applications; understand the behavior of data packets as they travel across networks; develop more effective defenses against cybersecurity attacks; and improve the performance of applications used in essential areas such as cybersecurity, Internet of Things (IoT), congestion control, and others.<br/><br/>The first goal of the project is to facilitate the adoption of programmable P4 devices by CI professionals and by network owners in general, by developing virtual labs. The second goal is to promote the integration of P4 and virtual labs into academic degree programs at the associate, bachelor, and graduate levels. Equipment used in virtual labs consists of production-grade devices such as software switches (e.g., Open vSwitch, PISCES), hardware switches based on state-of-the-art Tofino chips, and open-source operating systems and controllers (e.g., Open Network Linux, Open Network Operating System). For virtual labs using physical devices, the equipment pods incorporate P4 programmable hardware switches that are attached to the cloud and are managed via remote-access capability. Virtual labs provide both functional and traffic realism, as they use the same equipment as in real deployments and generate interactive network traffic. They emulate communications across local area networks (LANs), wide area networks (WANs), campus networks, data centers, and high-performance systems. The project will organize workshops to create awareness of this new technology and virtual labs resources, and to train CI professionals on P4. Workshops are co-organized and broadly disseminated through collaborators that play a critical role in enhancing and securing the national cyberinfrastructure: ESnet, the high-performance network that carries science traffic for the U.S. Department of Energy, including the National Laboratory system; and Internet2 and Front Range GigaPOP, two Research and Education Networks (RENs) that operate national and regional communication backbones. Finally, in coordination with the Western Academy Support and Training Center, one of the main technical training centers in the U.S. for two- and four-year instruction, and the Network Development Group, a company in virtualized training, the project will train IT instructors interested in the P4 technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118250","CyberTraining: Pilot: An Artificial Intelligence Bootcamp for Cyberinfrastructure Professionals","OAC","CyberTraining - Training-based","09/01/2021","08/31/2021","Karen Tomko","OH","Ohio State University","Standard Grant","Alan Sussman","08/31/2023","$299,858.00","Dhabaleswar Panda, Raghu Machiraju, Eric Fosler-Lussier, Katharine Cahill","ktomko@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","044Y","9102","$0.00","Artificial Intelligence (AI) is used in many aspects of modern life such as language translation and image analysis. In addition to consumer and business applications, researchers are increasingly using AI techniques in their scientific processes. The growth in AI is heavily dependent on new Deep Learning (DL) and Machine Learning (ML) schemes. As datasets and DL and ML models become more complex the computing requirements for AI increase and researchers turn to high performance computing (HPC) facilities to meet these needs. This is leading to a critical need for a Cyberinfrastructure (CI) workforce that supports HPC systems with expertise in AI techniques and underlying technology. This project will pilot an AI bootcamp for CI professionals that is targeted based on the professional's job requirements. After attending the bootcamp CI professionals will be better equipped to provide computing and data services to AI research users. This in turn will broaden adoption and effective use of advanced CI by researchers in a wide range of disciplines and will have an impact on science and corresponding benefits to society from their successes. The training materials developed during this project will be openly shared with the CI community so that others can use and adapt the materials for similar training activities.<br/><br/>This project is novel in taking a holistic approach to addressing the AI expertise gap for CI professionals. The project will develop an AI Bootcamp for CI professionals with the overarching goal of increasing the confidence and effectiveness of their support of AI researchers. The project leverages the CI professionalization efforts of the Campus Research Computing Consortium (CaRCC) to organize the training outcomes based on four ""facings"" (Strategy/Policy facing, Researcher facing, Software/Data facing, and Systems facing).  The project will identify learning outcomes for each CI facing and organize training tracks customized to specific roles. For this pilot the project is focused on developing a comprehensive training experience for Software/Data facing CI professionals. The AI Bootcamp will be offered virtually over twelve weeks. The instructional materials will be shared openly as notebooks, slide-decks and containers as appropriate so that they can be used for other training offerings. The project team is comprised of CI professionals, experienced in training CI users and providing CI operations, and Computer Science faculty members, experienced in offering courses in Data Analytics, AI and High Performance AI with active AI-based research programs. Drawing on extensive experience and materials in hands-on experiential learning for AI, the project team will create a comprehensive curriculum spanning foundational AI, software frameworks, and high performance computing for AI in a modularized virtual format to minimize barriers to access for the CI professional learner.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118083","CyberTraining: Implementation: Small: Building Future Research Workforce in Trustworthy Artificial Intelligence (AI)","OAC","CyberTraining - Training-based","09/01/2021","08/31/2021","Daniel Takabi","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Alan Sussman","08/31/2025","$499,245.00","Zhipeng Cai","takabi@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y","075Z, 079Z","$0.00","The goal of the project is to train current and future research workforce members in trustworthy artificial intelligence (AI) by developing instructional materials that expose students to various challenges of trustworthy AI systems. The project is focused on the vital national need for well-trained and highly knowledgeable researchers in trustworthy AI who are capable of solving real world problems in complex AI systems and help enable secure and safe adoption of AI systems. The project will have direct and long-term impact in both the public and private sectors by training the research workforce to address trustworthy AI challenges. Georgia State University is a minority-serving institution (MSI), and the project will form a coordination network consisting of research universities, 4-year colleges, historically black colleges and universities (HBCUs), Hispanic-serving institutions (HSIs), and women?s colleges in Metro Atlanta and the broader region. The collaboration will significantly increase the collective impact of the project, benefit numerous students from underrepresented groups and help increase the diversity of the research workforce. <br/><br/>The project team will develop interactive instructional materials including a set of hands-on labs that employ state-of-the-art trustworthy AI techniques to address the various challenges of AI systems. The instructional materials to be developed include modules on adversarial machine learning, evasion attacks and defenses, data poisoning attacks and defenses, privacy attacks and defenses, testing and verification, and fairness, accountability, transparency, and ethics (FATE). The project employs learning science principles, specifically the active learning and inquiry-based learning strategies that result in deeper understanding by students and provide formative feedback to instructors. The instructional materials are based on real-world systems and are designed to systematically cover fundamental principles in trustworthy AI and practical skills. The project also will provide guidelines to help instructors integrate the modules into their curriculum. The hands-on labs will be built based on only open-source software and tools that are free to use for educational purposes and will be distributed via free cloud platforms. The project evaluation includes formative and summative evaluations that use both quantitative and qualitative approaches and will be conducted by an experienced external evaluator with help from the project team. The project will disseminate the developed materials through training workshops for the educational and research communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107089","Collaborative Research: OAC Core: Fast Tools for Complex Event Detection over Bipartite Graph Streams","OAC","OAC-Advanced Cyberinfrast Core","09/01/2021","08/31/2021","Ahmet Erdem Sariyuce","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","08/31/2024","$250,000.00","","erdem@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","090Y","7923","$0.00","The goal of the project is to devise efficient and scalable methods and software infrastructure for detecting complex events in bipartite graph streams. Bipartite graphs are widely used in various domains to model real-world relationships such as credit card transactions, web search and data mining, computational advertising, bioinformatics, and folksonomy. There are significant computational challenges in handling bipartite graph streams including combinatorial explosion. Motifs and complex event detection and counting in bipartite graph streams have many use cases including recommendation systems in online shopping services and personalized music/movie streaming platforms, as well as malicious activity detection in credit card transactions, product rating data (i.e., spam reviews), client/server network interactions, and social security and healthcare systems (e.g., suspicious bankrupt declarations and tax fraud). The project will develop efficient tools and software infrastructure to facilitate research and development in these areas. Through the infrastructure, researchers and practitioners in various disciplines including computer scientists, business researchers and economists, social scientists, and network security engineers will be able to access and share datasets, and to use as well as contribute to the tool repository and documentation.<br/><br/>Complex events in a bipartite graph stream typically cannot be detected by only looking at a single node/edge arrival in isolation. Instead, detection requires monitoring the stream over a period of time. The project has three main tasks: (1) considering motifs as basic complex events or components in a larger complex event, the project will develop dynamic and streaming algorithms for combinatorial and temporal motif detection and counting in bipartite graph streams; (2) developing methods for detecting complex events with a partial time order, as well as dense-subgraph events; (3) devising graph embedding methods for complex events that specify high-order similarity between entities in a bipartite graph stream and for predictive complex events that can find and include the missing edges. The project will contribute to the body of knowledge about graph algorithms and machine learning through streaming and incremental algorithms for structural analysis, motif analysis, and neural embedding of bipartite graphs. The project will draw a parallel between the methods currently applied for batch analysis of static bipartite graphs and incremental and temporal analysis of streaming bipartite graphs. This work will provide foundational methods for the area of complex event detection. The tools and software infrastructure will facilitate researchers and practitioners in computer science, social sciences, finance and business, and network security to conveniently access and share datasets, state-of-the-art methods and tools, documentation, and evaluation results.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106740","Collaborative Research: OAC Core: Fast Tools for Complex Event Detection over Bipartite Graph Streams","OAC","OAC-Advanced Cyberinfrast Core","09/01/2021","08/31/2021","Tingjian Ge","MA","University of Massachusetts Lowell","Standard Grant","Alan Sussman","08/31/2024","$249,965.00","","ge@cs.uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","090Y","7923","$0.00","The goal of the project is to devise efficient and scalable methods and software infrastructure for detecting complex events in bipartite graph streams. Bipartite graphs are widely used in various domains to model real-world relationships such as credit card transactions, web search and data mining, computational advertising, bioinformatics, and folksonomy. There are significant computational challenges in handling bipartite graph streams including combinatorial explosion. Motifs and complex event detection and counting in bipartite graph streams have many use cases including recommendation systems in online shopping services and personalized music/movie streaming platforms, as well as malicious activity detection in credit card transactions, product rating data (i.e., spam reviews), client/server network interactions, and social security and healthcare systems (e.g., suspicious bankrupt declarations and tax fraud). The project will develop efficient tools and software infrastructure to facilitate research and development in these areas. Through the infrastructure, researchers and practitioners in various disciplines including computer scientists, business researchers and economists, social scientists, and network security engineers will be able to access and share datasets, and to use as well as contribute to the tool repository and documentation.<br/><br/>Complex events in a bipartite graph stream typically cannot be detected by only looking at a single node/edge arrival in isolation. Instead, detection requires monitoring the stream over a period of time. The project has three main tasks: (1) considering motifs as basic complex events or components in a larger complex event, the project will develop dynamic and streaming algorithms for combinatorial and temporal motif detection and counting in bipartite graph streams; (2) developing methods for detecting complex events with a partial time order, as well as dense-subgraph events; (3) devising graph embedding methods for complex events that specify high-order similarity between entities in a bipartite graph stream and for predictive complex events that can find and include the missing edges. The project will contribute to the body of knowledge about graph algorithms and machine learning through streaming and incremental algorithms for structural analysis, motif analysis, and neural embedding of bipartite graphs. The project will draw a parallel between the methods currently applied for batch analysis of static bipartite graphs and incremental and temporal analysis of streaming bipartite graphs. This work will provide foundational methods for the area of complex event detection. The tools and software infrastructure will facilitate researchers and practitioners in computer science, social sciences, finance and business, and network security to conveniently access and share datasets, state-of-the-art methods and tools, documentation, and evaluation results.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104023","Collaborative Research: Elements: ROCCI: Integrated Cyberinfrastructure for In Situ Lossy Compression Optimization Based on Post Hoc Analysis Requirements","OAC","Software Institutes","09/15/2021","08/31/2021","Sheng Di","IL","University of Chicago","Standard Grant","Amy Walton","08/31/2024","$319,998.00","Franck Cappello","sdi@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","077Z, 7923, 8004","$0.00","Today?s simulations and advanced instruments are producing vast volumes of data, presenting a major storage and I/O burden for scientists. Error-bounded lossy compressors, which can significantly reduce the data volume while controlling data distortion with a constant error bound, have been developed for years. However, a significant gap still remains in practice. On the one hand, the impact of the compression errors on scientific research is not well understood, so how to set an appropriate error bound for lossy compression is very challenging. On the other hand, how to select the best fit compression technology and run it automatically in scientific application codes is non-trivial because of strengths and weaknesses of different compression techniques and diverse characteristics of applications and datasets. This project aims to develop a Requirement-Oriented Compression Cyber-Infrastructure (ROCCI) for data-intensive domains such as astrophysics and materials science, which can select and run the best fit lossy compressor automatically at runtime, in terms of user's requirement on their post hoc analysis.<br/><br/>The overarching goal of this project is to offer a complete series of automatic functions and services allowing users to transparently run the best fit compressor at runtime during the scientific simulations or data acquisition. This project advances knowledge and understanding with three key thrusts: (1) it builds an efficient layer to interoperate with different lossy compressors and diverse post hoc analysis requirements on data fidelity by leveraging an existing compression adaptor library (LibPressio) and compression assessment library (Z-checker); (2) it develops an efficient engine to determine the best fit compressor with optimized settings based on user?s post-hoc analysis requirements; and (3) it develops a user-friendly infrastructure that integrates compression optimization and execution via the HDF5 dynamic filter mechanism. This project particularly targets cosmology and materials science applications and their specific requirements of using lossy compressors in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2130835","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","HDR-Harnessing the Data Revolu","05/15/2021","05/03/2021","Arindam Banerjee","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Amy Walton","08/31/2022","$241,766.00","","arindamb@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","099Y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2050195","REU Site: The future of discovery: training students to build and apply open source machine learning models and tools","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/15/2021","05/21/2021","Volodymyr Kindratenko","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alan Sussman","03/31/2024","$405,000.00","Daniel Katz","kindrtnk@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1139","075Z, 079Z, 9250","$0.00","Machine learning is a powerful tool that has been successfully applied to a variety of problems that until recently were deemed too difficult or impossible for computers to solve. This REU Site project gives participating students experience in many aspects of machine learning, ranging from developing open source machine learning models and tools to applying them in the real world. The work carried out by the students will lead to research advances in the fields of these projects and the models and tools they develop will be open-source, leading to them being available to other fields where these models can be used to make additional advances. Machine learning  is an emerging field with limitless opportunities to design innovative services and products that will enhance the lives of billions of people, help to address emerging challenges in climate, food, water, energy, transportation, and healthcare, and advance science and engineering discoveries in ways unimaginable today. The project contributes to the development of a highly specialized workforce trained to utilize advanced machine learning methods, and to contribute to open source software. Students from diverse backgrounds and computational/data-oriented disciplines are being trained to apply machine learning and to participate in research where these tools are at the center of scientific discovery, preparing them to apply machine learning methods in other fields and providing them with the foundation and motivation to pursue advanced graduate studies. This project serves NSF's mission by promoting the progress of science and advancing national health, prosperity and welfare. <br/> <br/>The goals of this project are to train undergraduate students, focusing on those from minority serving institutions, in machine learning and open source software, where they will then apply these skills to mentor-guided research projects. This is an on-site summer program at the University of Illinois that brings to campus 10 students per year and is based on matching their preferences and interests to those of a group of mentors, so that each student works with a pair of mentors, one from the project's research area and the other with expertise in machine learning. This program increases the students' knowledge of research and graduate school, and in many cases, stimulates their interest in continuing to graduate school, while in other cases, trains students with skills that enable them to seek data science and data analysis jobs in industry, increasing diversity in these graduate programs and in industry. By their presence in the program as continuing undergraduates, when the students return to their university, they will build a relationship between Illinois and that university, their faculty, and their peers that encourages future students to participate in the program and provides the basis for future joint research projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934529","Collaborative Research: Physics-Based Machine Learning for Sub-Seasonal Climate Forecasting","OAC","","09/01/2019","10/15/2020","Timothy Delsole","VA","George Mason University","Continuing Grant","Amy Walton","08/31/2022","$414,800.00","Benjamin Cash","tdelsole@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","099y","062Z","$0.00","While the past few decades have seen major advances in weather forecasting on time scales of days to about a week, making high quality forecasts of key climate variables such as temperature and precipitation on sub-seasonal time scales, the time range between 2 weeks and 2 months, continues to challenge operational forecasters. Skillful climate forecasts on sub-seasonal time scales would have immense societal value in areas such as agricultural productivity, hydrology and water resource management, transportation and aviation systems, and emergency planning for extreme events such as Atlantic hurricanes and midwestern tornadoes. In spite of the scientific, societal, and financial importance of sub-seasonal climate forecasting, progress on the problem has been limited. The project has initiated a systematic investigation of physics-based machine learning with specific focus on advancing sub-seasonal climate forecasting. In particular, this project is developing novel machine learning (ML) approaches for sub-seasonal forecasting by leveraging both limited observational data as well as vast amounts of dynamical climate model output data. Further, the project is focusing on improving the dynamical climate models themselves based on ML with specific emphasis on learning model parameterizations suitable for accurate sub-seasonal forecasting. The principles, models, and methodology for physics-based machine learning being developed in the project will benefit other scientific domains which rely on dynamical models. The project is establishing a public repository of a benchmark dataset for sub-seasonal forecasting to engage the wider data science community and accelerate progress in this critical area. The project is training a new generation of interdisciplinary scientists who can cross the traditional boundaries between computer science, statistics, and climate science.<br/><br/>The project works with two key sources of data for sub-seasonal forecasting: limited amounts of observational data and vast amounts of output data from dynamical model simulations, which capture physical laws and dynamics based on large coupled systems of partial differential equations (PDEs). The project is investigating the following central question: what is the best way to learn simultaneously from limited observational data and imperfect dynamical models for improving sub-seasonal forecasts? The project is building a framework for physics-based machine that has two inter-linked components: (1) deduction, in which ML models are trained on dynamical model outputs as well as limited observations, and (2) induction, in which ML models are used to improve dynamical models. Across the two components, the project is making fundamental advances in learning representations, functional gradient descent, transfer learning, derivative-free optimization and multi-armed bandits, Monte Carlo tree search, and block coordinate descent. On the climate side, the project is building an idealized dynamical climate model and doing an in depth investigation on learning suitable parameterizations for the dynamical model with ML methods to improve forecast accuracy in the sub-seasonal time scales. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003893","Collaborative Research: Frameworks: The Einstein Toolkit Ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","Software Institutes","07/01/2020","04/01/2020","Pablo Laguna","GA","Georgia Tech Research Corporation","Standard Grant","Amy Walton","02/28/2021","$414,003.00","Deirdre Shoemaker","pablo.laguna@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","107y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2006409","Organizing CSSI PI Meeting - Towards a National Cyberinfrastructure Ecosystem","OAC","Software Institutes","04/01/2020","05/22/2020","Haiying (Helen) Shen","VA","University of Virginia Main Campus","Standard Grant","Amy Walton","03/31/2021","$65,480.00","Upulee Kanewala, Sandra Gesing, Xiaohui Carol Song, Ritu Ritu","hs6ms@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8004","026Z, 7556, 8004","$0.00","This project will host a 2-day workshop in Seattle, WA, which will bring together the community of Cyberinfrastructure for Sustained Scientific Innovation (CSSI) awardees (with the goal of involving at least one principal investigator (PI) from each Elements, Frameworks, Institute Conceptualizations, and Scientific Software Innovation Institutes project, many of which are collaborative awards) from approximately 250 awards. This will be the first CSSI PI meeting. In addition, PIs from the prior connected solicitations (such as Software Infrastructure for Sustained Innovation (SI2) and Data Infrastructure Building Blocks (DIBBS)) will be invited, as well as PIs on NSF awards in which CSSI seed investments were made (Venture funded PIs as well as CSSI Early Concept Grants for Exploratory Research (EAGER) awardees). In addition, the proximity to Society for Industrial and Applied Mathematics (SIAM) Conference on Parallel Processing for Scientific Computing (PP20) will encourage participation by non-PI community to further inform the academic community of CSSI goals and projects. Goals of this workshop include: (1) Serve as a focused forum for PIs to share technical information with each other, community, and NSF Program Officers; (2) Explore innovative topics emerging within software and data infrastructure communities; (3) Discuss emerging best practices across the supported software and data infrastructure projects; (4) Stimulate thinking on new ways of achieving software and data sustainability; (5) Gather the shared experiences in an online web portal. The workshop is expected to host close to 250 CSSI and other awardees, other speakers and panelists. <br/><br/>The proposed workshop will support the exchange of ideas among the current cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and data artifacts and to the problem of their sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary CSSI awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these awardees, program officers, and other researchers in a common forum will help ensure that the cyberinfrastructure developed as part of CSSI projects will be relevant and broadly applicable to most science and engineering domains. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider cyberinfrastructure development community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004766","Elements: Agricultural Cyber-infrastructure support for Field and Grid Modeling, and Runtime Decision-Making","OAC","Software Institutes, EPSCoR Co-Funding","05/15/2020","03/18/2020","Ratnesh Kumar","IA","Iowa State University","Standard Grant","Amy Walton","04/30/2023","$599,998.00","Robert Malone","rkumar@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8004, 9150","077Z, 7923, 9150","$0.00","This project develops science-driven modeling and decision-making algorithms for precision agriculture, with associated cloud-based implementations.  This project would increase the potential for more efficient agricultural systems through the development of more accurate and larger scale models, data collection, and ultimately a software infrastructure that farmers could use for real-time monitoring of their crops.     <br/><br/>The project is derived from scientific questions in sustainable agriculture, sensor networks, decision sciences, data science, and machine learning. The primary innovations are applications of grid-based soil parameters using incomplete data, machine learning-based time series analysis, and optimization problems to estimate the optimal mix of inputs for nutrients and irrigation. These applications are likely to have wide usage among the precision agriculture community.  The cyberinfrastructure, MyGeoHub, is a cloud-based service that users can access through web browsers and leverages an existing infrastructure.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1948066","CRII: OAC: Scalable and Integrated Data Collection Platforms for Connected Vehicle Data","OAC","CRII CISE Research Initiation","08/01/2020","04/24/2020","Jinwoo Jang","FL","Florida Atlantic University","Standard Grant","Alan Sussman","07/31/2022","$174,912.00","","jangj@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","026Y","8228","$0.00","By 2030, nearly 146 million connected vehicles will be in operation in the U.S. Each vehicle will generate 25 gigabytes of data per hour; however, traditional data analytics systems are not capable of handling the high amount of data. This will require the development of novel computational and scientific tools to shape our future connected vehicle cyberinfrastructure. The project aims to create a new data mining engine embedded at the vehicle level to collect and process the streams of data in a highly efficient and scalable way. Harnessing novel data science tools, this project will address important fundamental questions of how to efficiently decide what information to collect, what to filter, and what to process within a vehicle. In turn, having an efficient in-vehicle data collection engine will improve our capabilities to build a high-impact cyberinfrastructure for connected vehicles. This cyberinfrastructure will open up new research capabilities and provide social benefits in the fields of intelligent transportation and smart cities. The data will tell us how to improve traffic congestion, how to create safer streets, and how to better design our cities, resulting in improved mobility, economic improvements, and lives being saved. This project embraces education and workforce development to educate our future data scientists and engineers through the creation of interdisciplinary learning environments, courses, and research-intensive programs. This project will also broadly promote undergraduate research, K-12 educational outreach activities, and STEM education in underrepresented groups.<br/> <br/>This project will investigate novel approaches to understand connected vehicle data, comprised of spatio-temporal trajectories and non-spatial sensor data, and develop scientific tools that can compress, impute, partition, and summarize connected vehicle data, while addressing important data challenges and scalability issues associated with the large scale of the database. First, the data collection platform will minimize data redundancy at a vehicle level through a proximity-based data sampling mechanism. Second, the platform will improve the integrability of connected vehicle data by capturing map-consistent trajectory points. Third, it will link dispersed connected vehicle data with map data by harnessing the special characteristics of new trajectory data formats. This scientific investigation centers on matrix decomposition, optimization, and spectral clustering techniques and includes city-wide vehicle sensor deployment to validate the proposed effort based on real-world and large-scale connected vehicle data. Results of the project will challenge the scientific gap between trajectory data analytics and the matrix decomposition techniques. Finally, this project will advance scientific knowledge in 1) trajectory data compression based on non-spatial sensor data, 2) the matrix decomposition techniques for the location inference of connected vehicle data, 3) the spectral graph partitioning methods for trajectories and non-spatial sensor data, and 4) large-scale trajectory data mining.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1942662","CAREER: Bayesian Inference Networks for Model Ensembles","OAC","CAREER: FACULTY EARLY CAR DEV","06/01/2020","07/02/2021","Daniele Schiavazzi","IN","University of Notre Dame","Continuing Grant","Alan Sussman","05/31/2025","$268,230.00","","dschiavazzi@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1045","026Z, 1045, 9251","$0.00","Integration of cardiovascular models, expert opinion and clinical measurements in a coherent system for probabilistic reasoning represents the next frontier of model-aided diagnostics to inform treatment in personalized medicine. However, several fundamental limitations must be addressed: (1) deterministic models are inadequate to characterize the effect of uncertainty in various properties of the cardiovascular system; (2) cardiovascular models are computationally expensive and therefore a stochastic treatment of these models may be too computationally expensive; (3) predictive models are needed in complex decision workflows to directly answer questions of clinical relevance in justifiable, probabilistic terms. This project proposes key advances in three complementary areas that are essential to make this next generation of model-aided diagnostics a reality: (1) highly scalable computational methods able to efficiently handle multiple instances of cardiovascular models with uncertain parameters; (2) new Monte Carlo estimators that leverage computationally inexpensive low fidelity surrogates; (3) new inference systems based on variables organized in networks, accommodating non-linear hemodynamic models, expert opinion and data. Since cardiovascular disease is the leading causes of death worldwide, this project serves the national interest by advancing the national health, prosperity and welfare, as stated by NSF's mission. The proposed research at the interface of computational mathematics and physiology offers an ideal framework to educate a diverse and globally competitive STEM workforce at the high school, undergraduate and graduate levels. Workshops and mini-symposia will also facilitate the exchange of ideas on stochastic cardiovascular modeling within the scientific community. <br/><br/>Computational models are increasingly being adopted to inform treatment in personalized medicine but innovation in model-based diagnostics is still hindered by three main problems: (1) deterministic simulations, i.e., simulations with certain outputs providing a false sense of confidence; (2) stochastic simulations are typically associated with a dramatic increase in computational cost; (3) current paradigms in uncertainty quantification (UQ) need generalization to provide coherent inference frameworks combining physics-based models, expert opinion, observational data and experiments. Thus, the main goal of this CAREER proposal is to develop the next generation of efficient computational tools to accelerate inference from models and data in computational hemodynamics as well as in a wide range of applications. This goal is achieved through the following objectives: (1) development of efficient ensemble solvers for hemodynamics running on modern CPU/GPU hybrid architectures; (2) research in generalized approximate control variate Monte Carlo estimators to drastically reduce the time required to solve direct and inverse problems in uncertainty analysis; (3) extend Bayesian Networks combining numerical models, expert opinion and data in a coherent inference framework. This research will provide the scientific basis to construct the first model-based inference framework including experimental evidence, expert opinion, and to develop systems directly applicable to the clinical decision making process. Two prototype systems will be finally developed, with a focus on applications to pediatric surgery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004815","Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples","OAC","ICB: Infrastructure Capacity f, Human Networks & Data Sci Res, Software Institutes","08/15/2020","10/15/2020","David Vieglais","KS","University of Kansas Center for Research Inc","Standard Grant","Alan Sussman","07/31/2024","$636,082.00","","vieglais@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457552","7858643441","CSE","085Y, 147Y, 8004","077Z, 1391, 7925, 8004, 9150","$0.00","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.<br/><br/>The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004562","Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples","OAC","ICB: Infrastructure Capacity f, MacroSysBIO & NEON-Enabled Sci, Software Institutes, EarthCube","08/15/2020","02/12/2021","Ramona Walls","AZ","University of Arizona","Standard Grant","Alan Sussman","07/31/2024","$1,072,351.00","","rwalls@iplantcollaborative.org","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","085Y, 7959, 8004, 8074","077Z, 7925, 8004","$0.00","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.<br/><br/>The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104237","CRII:OAC:A Data-Driven Closed-Loop Platform for Optimal Design of Deployable Pin-Jointed Structures","OAC","CRII CISE Research Initiation","09/01/2021","08/30/2021","Sichen Yuan","MI","Lawrence Technological University","Standard Grant","Alan Sussman","08/31/2023","$174,484.00","","syuan@ltu.edu","21000 Ten Mile Road","Southfield","MI","480751051","2482042103","CSE","026Y","8228","$0.00","Deployable pin-jointed (DPJ) structures, due to their being lightweight, foldable, and having high stiffness, have shown high research and development interest in state-of-the-art applications in many fields, such as aerospace and mechanical engineering, civil engineering, robotics and bio-medical materials. A DPJ structure is composed of members in compression (usually bars or struts) and tension (usually cables or tendons), connected by pin joints. In operation, these DPJ structures have various performance requirements, such as maintaining a high level of surface accuracy, and achieving desired stiffness and tunable natural frequencies. Loss of desired performance often yields malfunction or even breakdown of a DPJ structure. However, optimal design of DPJ structures with the desired performance is hard to obtain, due to issues related to constitutive modeling and lack of design tools. This project addresses these problems by creating a data-driven closed-loop platform for optimal design of DPJ structures. A series of data-driven tools, that create the proposed closed-loop platform, will be designed and built: (1) a novel stochastic method for determining an initial equilibrium configuration of a DJP structure will be created; and, (2) a new computational modeling technique for DPJ structures, based on machine learning and advanced nondestructive testing,  will be developed. The project will address an urgent need in structural engineering, and provide a deeper understanding of the design and computational modeling of DPJ structures. The results obtained from this project can help enhance the performance, safety and longevity of a class of structures in various areas, including architectures, spacecraft, military equipment and high-tech devices. The project will complement efforts to build the next-generation advanced cyberinfrastructure ecosystem by developing a series of data-driven tools to facilitate numerical and high-performance scientific computing, and expand modeling and simulation capabilities for mechanics of solid and structures. The project will also help upgrade the curriculum on computational modeling of structures. Engineering students will be recruited and mentored in this project. The training for students will include structural design, computational modeling, algorithm development and experimental testing.<br/><br/>Traditional structural design is an open-loop protocol, in which a design-modeling-validation procedure is followed. The main objective of this project is to create a data-driven closed-loop platform for optimal design of DPJ structures. This frame-invariant platform is new in providing a closed-loop structural design protocol. In this platform, experimental results will not only be used for model validation, but also in turn serve to provide training and testing data to further improve performance of a data-driven computational model. The loop will then be closed by using the computational model to guide initial structural design.  Toward this goal, two tasks will be carried out. The first task is to develop a stochastic approach to form finding. Traditional methods for form finding of DPJ structures require member grouping, which  relies highly on the geometric simplicity of the structure. To resolve this issue, a new method, called the stochastic fixed nodal position method, will be designed and investigated. The key benefit of this method is that it does not use member grouping or require any geometric simplicity. These features will allow the method to serve as a powerful tool in design of large-scale, complex, and irregular DPJ structures. The second task is to develop a data-driven computational modeling technique. Constitutive modeling techniques often over-simplify a DPJ structure, which results in the failure to reflect important mechanical properties of the structure. Very few recently developed techniques for computational modeling are suitable to DPJ structures, due to their special characteristics that are not commonly seen in other solids or structures. This project will develop a novel computational modeling technique based on machine learning and non-destructive testing for DPJ structures. This technique will bypass traditional constitutive modeling and provide good performance in handling DPJ structures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103989","ELEMENTS: Anharmonic formalism and codes to calculate thermal transport and phase change from first-principles calculations","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2021","08/30/2021","Keivan Esfarjani","VA","University of Virginia Main Campus","Standard Grant","Alan Sussman","08/31/2024","$515,487.00","","ke4c@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1712, 8004","054Z, 077Z, 094Z, 7923, 8004, 8396, 8611, 9216","$0.00","Most properties of crystalline materials can today be calculated with a high degree of accuracy via computation. Due to the nature of the physical theories, these properties are, however, currently calculated at absolute zero temperature. The purpose of this project is to develop methodologies and computer codes to extend the calculation of elastic and thermodynamic properties to temperatures as high as 1000C or more, where thermal expansion becomes important and structural phase transitions can occur. This work can have a great impact in the aerospace and car industries, which make engines and parts operating at very high temperatures. The tools we will develop enable accurate prediction of stability and thermophysical properties of materials, even if they have not been synthesized in the lab. The results of these calculations will also be included in materials databases and help in our understanding of the behavior of new functional materials. These tools will be freely available to the research community under an open source license so as to engage the community in further development and collaboration. During this project a postdoctoral researcher and a graduate student will be trained in computing, data processing and storage methods. We will also develop modules to teach K-12 students about energy, its conversion, storage and sustainability during summer projects organized by the project lead.<br/><br/>The mission of the proposed project is to provide to the materials physics community tools based on a new generation of quantum mechanical methodologies and input from first-principles calculations, to enable advances in two challenging areas: (1) thermodynamic, dielectric, mechanical and thermal transport properties at high temperatures where anharmonic effects become important, and (2) prediction of solid-solid phase transitions as a function of temperature, particularly in multifunctional materials, in which phonons are coupled to electronic degrees of freedom. This approach will be systematic, and applies to real materials enabling quantitative prediction of the above properties. The codes will be tested and validated on non-trivial materials such as transition metal oxides (TMOs) due to those materials having a rich number of phase transitions and emergent multiferroic  phases.  These materials and their applications in energy and information storage and processing also have a large amount of experimental and theoretical data on their phase transitions available, for validation. In summary, these timely tools will enable materials scientists to predict or understand thermophysical properties of anharmonic, complex and multifunctional materials at arbitrary temperatures with unprecedented accuracy.<br/><br/>This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103510","Collaborative Research: OAC Core: Improving Utilization of High-Performance Computing Systems via Intelligent Co-scheduling","OAC","OAC-Advanced Cyberinfrast Core","09/01/2021","08/30/2021","Patrick Bridges","NM","University of New Mexico","Standard Grant","Alan Sussman","08/31/2024","$247,461.00","","patrickb@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","090Y","026Z, 7923, 9150","$0.00","This project is aimed at increasing efficiency of high-performance computing systems by scheduling multiple jobs on the same set of nodes in a system, generally called co-scheduling. This is a break from current practice in which nodes are dedicated to one job at a time, which results in predictable execution time but inefficient use of system resources. To make this practical, the project will develop analyses to determine how to carry out co-scheduling such that overall system efficiency is improved while the performance impact on individual applications is minimized. In particular, the goal is to co-schedule jobs that can co-exist without contending for similar resources on the nodes.  The work done in this project will help achieve better efficiency on high-performance systems, which will impact application domains such as climate/weather, renewable energy, and national security. The work will be implemented and validated on systems at Lawrence Livermore and Sandia National Laboratories and then transitioned into software that will be used at these national laboratories. The project will also have an impact on education by integrating the techniques in this research into courses covering parallel and distributed computing at the PIs' institutions. In addition, the project will take place at two Hispanic-serving institutions, and the PIs have a history of advising under-represented students; the project will broaden participation in computing by recruiting Hispanic undergraduates to work on the project and sending them to national laboratories for internships.<br/><br/>The long-standing abstraction at high-end computing facilities is one of a submitted job being allocated a set of dedicated nodes. However, this makes systems much less efficient, as there are more per-node resources that will often be used inefficiently. In addition, the demand for high-end systems is increasing and dedicating nodes to jobs can increase job turnaround time and decrease overall system throughput.  One way to address this problem is for supercomputer centers to break from the current common practice of assigning each job a private, isolated portion of a supercomputer.  The intellectual merit of the project is three-fold. First, novel profile analyses will be developed that will reveal the effects on jobs due to sharing nodes. Second, novel statistical projection techniques will be developed that predict scaling behavior of jobs that are utilizing shared nodes. Third, new job-level scheduling techniques will be designed that use the interference analysis and projections to choose a set of shared nodes that will lead to good job turnaround time and maximize system throughput. The broader impact of the project is multifold.  This project will help achieve better efficiency on high-performance systems, which will benefit a broad range of applications that includes climate/weather prediction, nuclear energy, and national security.  Through a long-standing collaboration with both Lawrence Livermore and Sandia National Laboratories, the PIs will implement and validate the techniques on LLNL and SNL systems as well as transition the techniques into future resource managers at the national laboratories. In addition, both PIs will broaden participation in computing by recruiting Hispanic undergraduates to work on the project and sending them to national labs for internships.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017424","Collaborative Research: CyberTraining: Implementation: Small: INnovative Training Enabled by a Research Software Engineering Community of Trainers (INTERSECT)","OAC","CyberTraining - Training-based","08/15/2020","07/21/2020","Ian Cosden","NJ","Princeton University","Standard Grant","Alan Sussman","07/31/2023","$255,953.00","","icosden@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","044Y","026Z","$0.00","Software forms the backbone of much current research in a variety of scientific and engineering domains. The breadth and sophistication of software skills required for modern research software projects is increasing at an unprecedented pace. Despite this fact, an alarming number of researchers who develop software do not have adequate training in software development. Therefore, it is imperative for researchers who develop the software that will drive tomorrow?s critical research discoveries to have access to software engineering training at multiple stages of their career, to not only make them more productive, but also to make their software more robust, reliable, and sustainable. This project, INnovative Training Enabled by a Research Software Engineering Community of Trainers (INTERSECT), provides training on software development and engineering practices to research software developers who already possess an intermediate or advanced level of knowledge. To achieve this goal, INTERSECT provides expert-led training courses and workshops to help build the pipeline of computational researchers trained in best practices for research software development. This project serves the national interest and NSF's mission of promoting the progress of science by preparing a workforce who are trained in research software engineering best practices and addresses a gap in the current training and education available to research software developers.<br/><br/>The INTERSECT project provides training in research software engineering practices for researchers who are engaged in software development by (1) involving practicing Research Software Engineers (RSEs) in workshops to curate, develop, and refine training material and (2) using experienced RSEs to conduct training courses focused on that material. INTERSECT assembles experienced RSEs from the growing RSE community across the country who are interested in being instructors, to gather information about their  capabilities, knowledge, and expertise. In addition to developing materials, INTERSECT is curating the materials used in the workshops and training sessions into existing frameworks. This curation provides an open-source platform that allows RSEs to have ongoing engagement with the training material and helps coordinate efforts across the RSE-trainer community. To support the generation and curation of materials, INTERSECT sponsors multiple RSE-trainer workshops. These workshops connect practicing RSEs, who are also instructors, from across the country to both leverage the diversity of knowledge present in different institutions and to build and strengthen a national community of this increasingly important group of Cyberinfrastructure Professionals. By contributing to the growth of this community, INTERSECT will have a broad and long lasting impact on research software through increased access to peer support, resource and knowledge sharing, and networking opportunities for community members. By providing intermediate and advanced level training events and RSE workshops, INTERSECT will help to cultivate sophisticated Research Software Developers, bring awareness to the RSE career path, and strengthen the nascent national community of RSEs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104319","CDSE: Collaborative: Cyber Infrastructure to Enable Computer Vision Applications at the Edge Using Automated Contextual Analysis","OAC","CDS&E","09/01/2021","08/30/2021","George Thiruvathukal","IL","Loyola University of Chicago","Standard Grant","Tevfik Kosar","08/31/2024","$209,624.00","","gkt@cs.luc.edu","1032 W. Sheridan Road","CHICAGO","IL","606601537","7735082471","CSE","8084","026Z, 1504, 8084","$0.00","Digital cameras are deployed as network edge devices, gathering visual data for such tasks as autonomous driving, traffic analysis, and wildlife observation. Analyzing the vast amount of visual data is a challenge. Existing computer vision methods require fast computers that are beyond the computational capabilities of many edge devices. This project aims to improve the efficiency of computer vision methods so that they can run on battery-powered edge devices. Based on the visual data and complementary metadata (e.g., geographical location, local time), the project first extracts contextual information (such as a city street is expected to be busy at rush hour). The contextual information can help assist determine whether analysis results are correct. For example, a wild animal is not expected on a city street. Moreover, contextual information can improve efficiency.  Only certain pixels need to be analyzed (pixels on the road are useful for detecting cars, while pixels in the sky are not) and this can significantly reduce the amount of computation, thus enabling analysis on edge devices. This project constructs a cyberinfrastructure for three services: (1) understand contextual information to reduce the search space of analysis methods, (2) reduce computation by considering only necessary pixels, and (3) automate evaluation of analysis results based on the contextual information without human effort.<br/><br/>Understanding contextual information is achieved by using background segmentation, GPS-location-dependent logic, and image depth maps.  Background analysis leverages semantic segmentation and analysis over time to identify the background pixels and then generate inference rules via a background-implies-foreground relationship. If a pixel is consistently marked by the same semantic label across a long period of time, this pixel is classified as a background pixel. The background information can infer certain types of foreground objects. For example, if the background is city streets, the foreground objects can be vehicles or pedestrians; if a bison is detected, this is likely a mistake. This project processes only the foreground pixels by adding masks to the neural network layers. Masking convolution can substantially reduce the amount of computation with no loss of accuracy and no additional training is needed. Meanwhile, hierarchical neural networks can skip sections of a model based on context. For example, pixels in the sky only need to be processed by the hierarchy nodes that classify airplanes. The project provides an online service that can accept input data and analysis programs for automatic evaluation of the programs, without human created labels. The evaluation is based on the correlations of background and foreground objects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1953050","CRII: OAC: Inferring, Attributing, Mitigating and Analyzing the Malicious Orchestration of Internet-scale Exploited IoT Devices: A Network Telescope Approach","OAC","CRII CISE Research Initiation","08/20/2019","10/31/2019","Elias Bou-Harb","TX","University of Texas at San Antonio","Standard Grant","Alan Sussman","02/28/2021","$111,831.00","","elias.bouharb@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","026Y","026Z, 8228, 9102","$0.00","Despite the benefits provided by the widespread adoption and deployment of diverse Internet-enabled devices such as phones and smart home components in consumer markets and critical infrastructure - the so called Internet of Things (IoT) devices, security concerns are rising as such devices also introduce new vulnerabilities that could be leveraged by attackers to launch disrupting cyber-attacks.  The objective of this project is to enable exploration of the inherent insecurity of the IoT paradigm by exploring innovative data analytics as applied to raw cyber security data.  Insights gained will allow detection, characterization and attribution of Internet-scale compromised IoT devices, coupled with their malicious activities, in near real-time.  Several technical challenges impede addressing IoT security at large, including, the excessive diversity of IoT devices in addition to their Internet-wide deployment, the lack of IoT-relevant data and the shortage of IoT-specific actionable attack signatures.  In this context, this project serves NSF's mission to promote the progress of science by aiming to generate a first-of-a-kind, large-scale analysis of the magnitude of compromised IoT devices.  The project also promotes cyber security research and training for minorities, given that it will be executed within the boundaries of a designated Hispanic-serving institution.  Moreover, the project will contribute to operational cyber security by developing a real-time capability for storing and sharing IoT-relevant threat information.<br/><br/>The project will draw-upon macroscopic, large-scale passive measurement data collected in real-time from a network telescope to highlight the severity of the insecurity of the IoT paradigm.  Network telescopes, most commonly known as darknets, constitute a set of routable, allocated yet unused IP addresses.  The project will design and develop real-time algorithms that are capable of inferring Internet-scale exploited IoT devices by exploring darknet data.  Furthermore, the project will investigate formal correlation approaches rooted in stochastic data structures between IoT-relevant passive measurements and malware samples to aid in the attribution and thus the remediation objective.  The project will further explore the orchestration behavior of seemingly independent IoT activities, which operate within well-coordinated IoT botnets.  To this end, the project will innovate time series analytics based upon trigonometric interpolation techniques, recursive optimal stochastic estimators, and bitmap matching algorithms to infer such IoT botnets by employing passive measurements.  The project will also (1) develop a unique cyberinfrastructure for IoT cyber threat indexing by automating the proposed algorithms, techniques and methods, (2) generate IoT-specific signatures by employing piecewise hashing techniques, and (3) create access methods based on an API mechanism and a front-end service facilitated by Elasticsearch to allow the sharing of IoT-centric empirical data, threat intelligence and signatures.  <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019073","CC* Integration-Large: SciStream: Architecture and Toolkit for Data Streaming between Federated Science Instruments","OAC","CISE Research Resources","10/01/2020","06/30/2020","Rajkumar Kettimuthu","IL","University of Chicago","Standard Grant","Deepankar Medhi","09/30/2022","$850,000.00","","kettimut@mcs.anl.gov","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","2890","","$0.00","Scientific instruments are capable of generating data at very high speeds. However, with traditional file-based data movement and analysis methods, data are often processed at a much lower speed, leading to either operating the instruments at a lower speed or discarding a (significant) portion of the data without processing it. To address this issue, SciStream project will develop software tools to stream data at very high speeds from scientific instruments to supercomputers at a distant location. SciStream hides the complexities in network connections from the end user and provides a high level of security for all the network connections.<br/><br/>The data producers (e.g., data acquisition applications on scientific instruments, simulations on supercomputers) and consumers (e.g., data analysis applications on high performance computing systems) may be in different security domains (and thus require bridging of those domains) and may, further, lack external network connectivity (and thus, require traffic forwarding proxies). SciStream establishes necessary bridging and end-to-end authentication between source and destination, while providing efficient memory-to-memory data streaming. Through the exploration of architectural and design choices and addressing issues of control protocols and security, SciStream will advance the understanding of the challenges in supporting high speed memory-to-memory data streaming between remote instruments in federated science environments.<br/><br/>SciStream will benefit all scientific applications that require memory-to-memory data streaming between distributed instruments. Recent trends suggest that this is an important and growing requirement for many scientific applications. SciStream will help significantly reduce the time to solution for these applications, resulting in improved scientific productivity and thus far-reaching benefits for society. Key design choices such as application-agnostic streaming and support for best-effort streaming will make SciStream appealing to a broader science community. SciStream will engage with domain scientists, campus computing centers, and a scientific user facility to reach a wider audience. Through on-campus programs at the University of Chicago, SciStream will train under-represented students in networking. Additional details on SciStream can be found here: https://scistream.github.io/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940153","Collaborative Research: Autonomous Computing Materials","OAC","HDR-Harnessing the Data Revolu, PROJECTS","10/01/2019","10/15/2020","Sanghamitra Neogi","CO","University of Colorado at Boulder","Continuing Grant","Daryl Hess","09/30/2021","$334,947.00","","sanghamitra.neogi@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","099Y, 1978","062Z, 9263","$0.00","The recent explosion in worldwide data together with the end of Moore's Law and the near-term limits of silicon-based data storage being reached are driving an urgent need for alternative forms of computing and data storage/retrieval platforms. In particular, exabyte-scale datasets are increasingly being generated by the biological sciences and engineering disciplines including genomics, transcriptomics, proteomics, metabolomics, and high-resolution imaging, as well as disparate other scientific fields including climate science, ecology, astronomy, oceanography, sociology, and meteorology, amongst others. In this data revolution, the continuously increasing size of these datasets requires a concomitant increase in available computational power to store, process, and harness them, which is driving a need for revolutionary new, alternative substrates for, and forms of, computing and data storage. Unlike traditional data storage and computing materials such as silicon, the human brain offers a remarkable ability to sense, store, retrieve, and compute information in a manner that is unrivaled by any human-made material. In this research project, analogous modes of information sensing, data storage, retrieval, and computation will be explored in non-traditional computing molecular systems and materials. The over-arching goal of the research is to discover revolutionary new modes of data storage/retrieval, sensing, and computation that rival conventional silicon-based technology, for deployment to benefit society broadly across all domains of data science. Graduate students and postdocs across five institutions will be trained and mentored in a highly interdisciplinary manner to attain this goal and prepare the next-generation of data scientists, chemists, physicists, and engineers to harness the ongoing data revolution. The research will be disseminated to a broad community through news outlets and integration of high school student internships in participating research laboratories. <br/><br/>Large-scale datasets from spatial-temporal calcium imaging of the mouse brain will be recorded into DNA-based, nanoparticle-based, and phononic 2D and 3D soft and hard materials. Continuous spatial-temporal data will first be transformed into discrete data for mapping onto DNA-conjugated fluorophore networks, dynamic barcoded nanoparticle networks, and phononic 2D and 3D materials. Sensing, computation, and data storage/retrieval will be demonstrated as proofs-of-principle in exploiting the chemical properties of molecular networks and materials to recover the encoded neuronal datasets and their sensing and computing processes. Success with any of these three prototypical materials would revolutionize the ability to encode arbitrarily complex, large-scale datasets into complex molecular systems, with the potential to scale across diverse data domains and materials frameworks. The investigators' Autonomous Computing Materials framework will thereby enable the encoding of arbitrary ""big data"" sets into diverse materials for data storage, sensing, and computing. This project maximizes opportunities for disruptive new computing and data science concepts to emerge from a multi-disciplinary, collaborative team spanning data science, neuroscience, materials science, chemistry, physics, and biological engineering. <br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003874","CDS&E: Collaborative Research: Private Data Analytics Synthesis, and Sharing for Large-Scale Multi-Modal Smart City Mobility Research","OAC","CYBERINFRASTRUCTURE, CDS&E","07/01/2020","06/28/2021","Desheng Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Tevfik Kosar","06/30/2023","$367,000.00","Dimitris Metaxas","dz220@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7231, 8084","026Z, 8084, 9251","$0.00","Given the trend towards urbanization, understanding real-time human mobility in urban areas has become increasingly important for many research areas from Mobile Networking, to Transportation/Urban Planning, Behavior Modeling, Emergency Response, to recent Pandemic Mitigation. Many analytical models have been proposed to understand human mobility based on mobility data. However, most of these data are proprietary and cannot be accessed by the research community at large. Fortunately, based on the latest expansion of urban infrastructures, such mobility data has been collected by city government agencies and some companies that are willing to share the data for social good. However, a key challenge is the privacy concern since such data usually have sensitive information and system design details for potential privacy and security issues. To address this issue, the project aims to generate realistic yet synthetic mobility data through machine learning based on the real mobility data analytics and then share these realistic synthetic data with the research community. The objective of the project is to lower the entry barriers for interdisciplinary researchers in mobility data-intensive research aimed at addressing major scientific/societal challenges related to urban mobility.<br/><br/>The core merit of the project lies in integrating two aims, i.e., privacy-preserving data synthesis and data integration, for large-scale smart city mobility research. For the first research aim, the project plans to utilize recent advances in Generative Adversarial Networks (GANs) to enable large-scale mobility data synthesis. The goal is to achieve the individual-level release of realistic synthetic mobility data by GAN-based models targeting key characteristics of human mobility. The GAN architecture proposed has novel technical components to augment basic GAN frameworks, which optimize the fundamental trade-off between privacy (regarding removing/obfuscating sensitive mobility features) and utility (in terms of preserving non-sensitive mobility features) with long-range dependencies (in terms of repeated mobility patterns) revealed. For the second research aim, the PIs plans to perform multi-modal data integration based on aligned multi-tensor decomposition under mobility semantics. The technical approach proposed is to enable multi-modal data integration based on synthetic single-modal data for comprehensive mobility modeling with a set of machine learning techniques including novel mobility semantic learning and multi-tensor decomposition with aligned spatiotemporal granularity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031019","PRE-EVENTS Multiscale Space Weather Modeling LRAC Travel Support","OAC","Leadership-Class Computing","08/01/2020","05/06/2020","Gabor Toth","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Edward Walker","07/31/2022","$10,000.00","","gtoth@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940979","Planning for the Leadership-Class Computing Facility","OAC","Leadership-Class Computing","07/01/2019","08/26/2021","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","06/30/2024","$0.00","","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","026Z","$0.00","NSF has played a central role in the transformation of scientific research and as a leader in enabling the use of High-Performance Computing (HPC) to advance discovery for almost four decades.  The leadership-class computing program is intended to provision advanced computational capabilities to enable transformative research for all of Science and Engineering (S&E) that would not otherwise be possible.  NSF's current leadership-class computing investment is the support of the Frontera project at the Texas Advanced Computing Center (TACC) at The University of Texas at Austin.  Frontera is the Phase 1 system of a two-phase approach to eventual deployment of a much more capable Leadership-Class Computing Facility (LCCF) in approximately 2024.  This award will support the conceptual design of a Phase 2 system and facility.<br/><br/>The project design approach will extend best practices in the design/operate/evaluate cycle that TACC has used to successfully deploy some of the largest computing systems in the world for open scientific research. This process will have four keystones: science user requirements, future technology assessment, alternative design evaluation, as well as risk management for cost, scope and schedule. Testbeds feature prominently in the design process and will provide users with early access to future computational hardware and will offer system designers quantitative insights into the effectiveness of these technologies. Testbed partners will include major cloud and technology vendors as well as early exascale computing technology adopters.  In addition, the design process will be guided by science inputs and technology drivers identified by science and technology engagement teams. These teams will be composed of distinguished scientists representing a broad range of expertise and experience to guide the Phase 2 system design.  The team members will also serve as community leaders to bring together colleagues in workshops, meetings, and other engagement venues to inform the process further.  Moreover, the project will engage with the scientific software community, including library and community code providers, to ensure the project optimizes the selection of Phase 2 computing technologies to enable transformative S&E discoveries for the future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924302","Collaborative Research: CyberTraining: Implementation: Small: Multi-disciplinary Training of Learning, Optimization and Communications for Next Generation Power Engineers","OAC","CyberTraining - Training-based, EPCN-Energy-Power-Ctrl-Netwrks","09/15/2019","07/01/2019","Zhen Ni","SD","South Dakota State University","Standard Grant","Alan Sussman","10/31/2019","$299,876.00","Reinaldo Tonkoski","zhenni@fau.edu","1015 Campanile Ave","Brookings","SD","570070001","6056886696","CSE","044Y, 7607","9150","$0.00","With the increasing adoption of interconnected power/micro grid infrastructures, today's power engineering research professionals require broader knowledge and a more diverse skillset. This project provides undergraduate and graduate students in the Northern Plains region with access and opportunity to learn using state-of-the-art smart grid cyberinfrastructure. The Northern Plains region sees increasing use and potential in renewable energy, in addition to energy exports to other areas, solidifying the importance of advanced training in power engineering. The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense. The resulting curriculum and instructional materials integrate advanced skills from multiple areas into power engineering infrastructure education. Students practice the multi-disciplinary skillsets needed for the power industry using a unique, remotely connected smart grid cyberinfrastructure. They extend their academic and research portfolios and strengthen their career competitiveness as smart grid cyberinfrastructure professionals and cyberinfrastructure users for regional and national levels. The cyber training model leverages resources for underrepresented minority schools and schools with limited research cyberinfrastructure to participate, either remotely or on-site. The mobile microgrid laboratory demonstrations introduce K-12 teachers and students to Science, Technology, Engineering and Mathematics majors.<br/><br/>This project establishes a new, remotely-connected smart grid cyberinfrastructure platform between the collaborative institutes using a real-time digital simulator, and sharing software licenses and hardware resources. This project promotes the application of advanced cyberinfrastructure techniques in power system monitoring, planning, operation and control, and prepares the next-generation power engineers to face challenges in modern power systems. This remotely connected power cyberinfrastructure provides an ideal platform for interdisciplinary research and education of computational intelligence, machine learning, control, communications, and data analytics in the smart grid area. The project team collects heterogeneous smart grid measurement data from this new cyberinfrastructure to conduct real-time learning, event-detection and data integrity, online optimization and multi-level decision-making process of intelligent systems. The project team integrates results into existing undergraduate courses and expand graduate courses from a frontier interdisciplinary viewpoint. The project team also creates replicable project templates/demos with designated benchmark data so other schools can easily adopt this new educational model with or without specific resources. Feedback from an independent evaluator, institutional stakeholders, national laboratory scientists and local industry partners supports periodic improvement of the educational model and materials. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Electrical, Communications and Cyber Systems in the Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1909015","OAC Core: Small: Collaborative Research: Scalable Run-Time for Highly Parallel, Heterogeneous Systems","OAC","CSR-Computer Systems Research","07/01/2019","06/19/2019","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Alan Sussman","06/30/2022","$249,995.00","","bosilca@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7354","026Z, 9179","$0.00","Supercomputing has become an essential tool in many scientific fields, including advances in engineering and medicine, and contributes to national security. Progress in many areas depends on continued improvements in the performance of supercomputers and their usability. Communication between processes is a critical component of this effort and is the target of this project. This project departs from the traditional communication protocols. Rather, the project  focuses on providing middle ground solutions between hardware and software. This approach potentially  reduces communication overheads and better matches the functionality of the communication library to the capabilities of modern communication adapters and also improves the match between the requirements of modern parallel computing frameworks and applications. By improving the communication capabilities of computational platforms, this project will promote faster and more flexible communication capabilities and will improve the time to completion of scientific applications.  It, therefore, increases the scientific throughput of existing and future cyberinfrastructure platforms. The research and educational outcomes of this project are closely related, resulting in highly trained new generations of researchers and engineers leading to a more efficient and globally competent workforce. Therefore, this project aligns with the NSF's mission to promote the progress of science and to advance national prosperity and welfare through science, and serves the national interest.<br/> <br/>This project brings together a multidisciplinary team and aims at breaking away from the limitation of standards such as Message Passing Interface and pointing the way for handling the needs of future computational frameworks and high-end systems. To this end the project (1) designs and implements a communication library with new communication primitives to enable fast coordination with no serial bottleneck, to manage irregular, fine grain communication, and to provide new efficient synchronization mechanisms; (2) demonstrates the value of this library by using it to accelerate multiple task-based runtimes (Legion, PaRSEC) and communication libraries (MPI and GasNET); (3) demonstrates the value of hardware support by porting key components to a programmable NIC; and (4) delivers improvements and extensions to mainstream communication libraries to provide the new functionality. This work puts a special emphasis on emerging programming models, such as Legion or PaRSEC, and on emerging application domains, such as graph analytics. It aims at an orthogonal design where different mechanisms for associating producer buffer with consumer buffer can be composed with different mechanisms for synchronizing producer and consumer; and where mechanisms can be specialized so as to allow efficient hardware support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1947559","CRII: OAC: A computational framework for multiscale simulation of cardiovascular disease progression connecting cell-scale biology to organ-scale hemodynamics","OAC","CRII CISE Research Initiation","06/01/2020","04/15/2020","Amirhossein Arzani","AZ","Northern Arizona University","Standard Grant","Alan Sussman","05/31/2022","$174,999.00","","Amir.Arzani@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","026Y","026Z, 8228","$0.00","Developing computer models of cardiovascular disease growth requires an organ-scale model of blood flow, a cell-scale model of cell biology, and a framework to couple these models. Such computer models could help predict cardiovascular disease by simulating spatial and temporal patterns of disease growth. To develop these models, we need a software infrastructure that integrates modeling techniques from multiple disciplines. This project will develop software for this purpose and will apply the software to the calcific aortic valve disease (CAVD) problem, which is prevalent in aging adults. The project will also help to understand the interaction between the fundamental biological and mechanical processes involved in CAVD. The project leverages existing resources at Northern Arizona University to perform outreach activities targeting underrepresented minority students in the region. The outcome of this study will contribute to advancing the national health and improving our scientific understanding of the interaction between different processes in cardiovascular disease. <br/><br/>During the past two decades, significant advances have been made in the development of organ-scale patient-specific computational models of cardiovascular disease. These models often focus on a specific spatial and temporal scale and their goal is to quantify biomechanical biomarkers of cardiovascular disease growth. However, quantitative information about disease growth over long time scales is missing in these models. The goal of this project is to 1) Develop a software platform for multiscale two-way coupling between organ-scale biomechanics and cell-scale systems biology models. 2) Apply the computational framework to study the long-term spatial and temporal progression of CAVD. The organ-scale model will be based on continuum solid and fluid mechanics models, and the cell-scale model will be based on systems of differential equations. The developed software infrastructure could be applied to patient-specific data to model disease progression patterns. This will enable the development of transformative models that advance our knowledge of cardiovascular disease. The project will train highly interdisciplinary researchers at the interface of software development, biomechanics, and biology. Outreach activities will promote STEM participation by demonstrating the beauty of computer science and engineering blended and applied to biomedical applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1942973","CAREER: Unifying Heterogeneity of Extreme-Scale Cyberinfrastructures for Higher Productivity and Performance Portability","OAC","CAREER: FACULTY EARLY CAR DEV","07/01/2020","04/13/2020","Esam El-Araby","KS","University of Kansas Center for Research Inc","Continuing Grant","Alan Sussman","06/30/2025","$268,544.00","","esam@ku.edu","2385 IRVING HILL RD","Lawrence","KS","660457552","7858643441","CSE","1045","026Z, 1045, 9150","$0.00","The goal of this project is to improve the performance portability and productivity (ease-of-use) of application development for extreme-scale heterogeneous reconfigurable architectures. This goal is in the scope of NSF's 10 Big Ideas, i.e., ""Harnessing the Data Revolution"", and a recent national report, i.e., ""Extreme Heterogeneity 2018"". The project will enable scientists in domains such as quantum simulation, experimental High-Energy Physics (HEP), and Fossil Energy (FE) to focus on the inherent characteristics, e.g., parallelism, of their applications rather than be burdened by the technological details of the underlying hardware. The project also significantly increases the longevity of domain scientists' legacy code. This project integrates the PI's research and teaching activities. The outcome from the project will produce undergraduate and graduate students who can contribute to solving problems in large-scale cyberinfrastructure. The project aims to include women and minority groups, through The Office of Diversity, Equity and Inclusion at the University of Kansas (KU), to receive training and engage in research collaboration with our group. This project will enable higher productivity in post-exascale (extreme-scale) heterogeneous architectures, advancing the PI's long-term research goals. These project goals align with: (1) the mission of the Office of Advanced Cyberinfrastructure (OAC) of NSF and the mission of NSF at large in promoting the progress of science and advancing the national prosperity and welfare, and (2) key strategies in the Kansas Building an Environment for Science and Technology (B.E.S.T.) for Innovation report. In addition, this project contributes to improving Kansas cyberinfrastructure, with a focus on building a workforce that will improve the state economy through STEM education.<br/><br/>The project tackles three barriers to the efficient and wide spread deployment of extreme-scale high-performance reconfigurable computing (HPRC) systems: (1) the imbalance in processing heterogeneity where processing resources are integrated in different ways and proportions, (2) lack of hierarchical parallel programming models and/or insufficiency of existing models, and (3) lack of accurate formal models that are capable of describing the instantaneous and transient behaviors of HPRCs rather than only describing average behaviors. These challenges, especially the first two, have hindered wide adoption of HPRC by a broad range of HPC users and more specifically scientists in domains such as quantum simulation, experimental High-Energy Physics (HEP), and Fossil Energy (FE). Three research activities are proposed in this project: (1) development of a portable node-level abstraction layer and run-time libraries that will manage and transparently provide to the end-user a unified view of the hardware resources independent of their type (virtualized hardware), while exploiting inherent features of FPGAs, (2) creation of a new parallel domain-specific language (DSL) derived from Partitioned Global Address Space (PGAS) models using a multi-layered C-to-hardware compilation that supports a system-wide multi-level hierarchy of granularity and parallelism while also providing application portability, and (3) formulation and implementation of formal models and discrete-event simulation tools for design space exploration based on stochastic Markov chains and queueing networks.  The proposed concepts and techniques will be implemented and evaluated to demonstrate their portability on three distinct heterogeneous large-scale HPC systems: (1) an experimental multi-node HPC cluster populated with Xilinx FPGA accelerators, (2) an Intel Hardware Accelerator Research Program (HARP) system, and (3) a DS8 state-of-the-art OS-less HPRC system from DirectStream.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1949912","REU Site:  iCER ACRES: iCER Advanced Computational Research Experience for Students","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2020","03/31/2020","Brian O'Shea","MI","Michigan State University","Standard Grant","Alan Sussman","03/31/2023","$401,234.00","Arjun Krishnan","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1139","9250","$0.00","The Advanced Computational Research Experiences at Michigan State University offers undergraduate students from a wide variety of backgrounds the opportunity to conduct cutting-edge research in computational and data science.  In addition to research, students participate in a variety of technical training activities.  Such activities include Python programming, data analysis and visualization, supercomputing, and scientific software development. The program also provides a variety of professional development activities, targeting presentation skills, technical writing, and discussions relating to graduate school and a wide range of career opportunities.  This ten-week summer program trains students in computational and data science, gives them the freedom to explore a range of subject areas and career paths, and provides research opportunities to students from under-represented groups in STEM and from non-research-intensive colleges and universities.<br/><br/>The Advanced Computational Research Experiences Research Experience for Undergraduates (ACRES REU) at Michigan State University (MSU) offers students the opportunity to conduct cutting-edge research in computational and data science under the supervision of faculty who are recognized experts in their disciplines.  This 10-week summer residential program includes MSU faculty from a wide variety of STEM fields whose research focuses on algorithms pertaining to, and applications of, computational modeling and data science.  In addition, peer- and near-peer mentoring, professional development, and technical training are key components of the program.  One of the strengths of this REU site lies in the rich interdisciplinary collaborative research environment fostered between student teams consisting of pairs of REU students, their mentors who have expertise in conducting research in computational science, data science, and high-performance computing, and technical trainers.  These elements combine to empower undergraduates to conduct meaningful original research focused on the development and enhancement of algorithms for computational modeling and data science, as well as the application of these algorithms to cutting-edge applications in a wide range of STEM fields.  In addition, this project will broaden the participation of underrepresented populations in STEM research.  Furthermore, it will provide early exposure to high performance computing to students from institutions with limited access to HPC resources, especially women and other underrepresented groups, and will motivate students to pursue careers in computational fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1930003","Student Travel Support for MVAPICH User Group (MUG) Meeting","OAC","EDUCATION AND WORKFORCE","05/01/2019","05/07/2019","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Alan Sussman","04/30/2020","$10,000.00","Hari Subramoni","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7361","026Z, 7556, 9179","$0.00","Modern High-Performance Computing (HPC) systems are rapidly evolving with respect to processor, networking, and I/O technologies. In such a rapidly changing environment, it is critical that next-generation engineers and scientists are exposed to the modern architectural trends of HPC systems and learn how to use the features of these technologies to design HPC software stacks and learn about the process of open-source software developments and its sustainability. The annual MVAPICH User Group (MUG) meeting provides an open forum to exchange information on the design and usage of MVAPICH2 libraries, which are open-source, high-performance and scalable MPI libraries to take advantage of RDMA technology. Travel funding from this project will enable a set of undergraduate and graduate students to attend the MUG meeting. Their participation will help them to enter the next-generation HPC workforce with increased expertise in software design, reuse, and sustainability.  The project, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. <br/><br/>The MVAPICH project focuses on the design of high-performance MPI and PGAS runtimes for HPC systems. Over the years, this project has been able to incorporate new designs to leverage novel multi-/many-core platforms like Intel Xeons, NVIDIA GPGPUs, Open POWER, and ARM architectures, coupled with Remote Direct Memory Access (RDMA) enabled commodity networking technologies like InfiniBand, RoCE, Omni-Path, and iWARP. An annual MVAPICH User Group (MUG) meeting was created six years ago to provide an open forum to exchange information on MVAPICH2 libraries. The funding under this grant aims to achieve increased participation of undergraduate and graduate students working in the HPC area (systems and applications) in the annual MUG event. The requested student travel fund helps attract a set of students from a range of US institutions. Participation in an international event, such as MUG, enables students to obtain global picture of the developments happening in the rapidly evolving HPC domain and open-source software design. The selection committee plans to stress diversity to attract students from minority and under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923632","Ideas Labs: Data-Intensive Research in Science and Engineering","OAC","HDR-Harnessing the Data Revolu","03/01/2019","04/12/2021","James Crowley","PA","Society For Industrial and Applied Math (SIAM)","Standard Grant","Amy Walton","02/28/2021","$1,197,410.00","","jcrowley@siam.org","3600 Market St.","Philadelphia","PA","191042688","2153829800","CSE","099Y","","$0.00","In 2016, the National Science Foundation (NSF) unveiled a set of ""Big Ideas,"" 10 bold, long-term research and process ideas that identify areas for future investment at the frontiers of science and engineering.  The Big Ideas represent unique opportunities to position our Nation at the cutting edge of global science and engineering leadership by bringing together diverse disciplinary perspectives to support convergence research.  NSF's Harnessing the Data Revolution (HDR) Big Idea is a national-scale activity to enable new modes of data-driven discovery that will allow fundamental questions to be asked and answered at the frontiers of science and engineering.  This project describes a series of Ideas Labs on ""Data-Intensive Research in Science and Engineering (DIRSE)"".  Ideas Labs are intensive workshops focused on finding innovative and bold transdisciplinary solutions to grand challenge problems.  The overarching goal of the DIRSE Ideas Labs is to foster convergent approaches to enable data-intensive research in science and engineering through a series of facilitated activities bringing together scientists and engineers working on important data-intensive science and engineering problems with data scientists.<br/><br/>There are numerous science and engineering challenges that require, or will soon require, data science to help address research and technological questions.  Advancing knowledge in these areas requires solutions to many modeling and data challenges such as real-time sensing, learning, and decision making; social, political, and behavioral implications of machine learning and impacts of new data uses; issues related to ethics and fairness; and integrating heterogeneous data for explaining or predicting complex phenomena.  There is also a need for approaches that combine physical models with data driven models for learning and decision making.  Data science tools, such as signal and image processing, visualization, statistical modeling and inference, machine learning, and optimization, offer a starting point for solving important scientific and engineering challenges.  However, extracting new information and knowledge from data will benefit from new, convergent strategies that capitalize on existing NSF investments in data and cyberinfrastructure and that build synergy between the researchers with expertise in the generation or measurement of data and those with expertise in processing and analyzing that data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1949921","Collaborative Research: CyberTraining: Implementation: Small: Multi-disciplinary Training of Learning, Optimization and Communications for Next Generation Power Engineers","OAC","CyberTraining - Training-based, EPCN-Energy-Power-Ctrl-Netwrks","08/10/2019","06/08/2021","Zhen Ni","FL","Florida Atlantic University","Standard Grant","Alan Sussman","08/31/2024","$331,876.00","","zhenni@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","044Y, 7607","026Z, 9150, 9251","$0.00","With the increasing adoption of interconnected power/micro grid infrastructures, today's power engineering research professionals require broader knowledge and a more diverse skillset. This project provides undergraduate and graduate students in the Northern Plains region with access and opportunity to learn using state-of-the-art smart grid cyberinfrastructure. The Northern Plains region sees increasing use and potential in renewable energy, in addition to energy exports to other areas, solidifying the importance of advanced training in power engineering. The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense. The resulting curriculum and instructional materials integrate advanced skills from multiple areas into power engineering infrastructure education. Students practice the multi-disciplinary skillsets needed for the power industry using a unique, remotely connected smart grid cyberinfrastructure. They extend their academic and research portfolios and strengthen their career competitiveness as smart grid cyberinfrastructure professionals and cyberinfrastructure users for regional and national levels. The cyber training model leverages resources for underrepresented minority schools and schools with limited research cyberinfrastructure to participate, either remotely or on-site. The mobile microgrid laboratory demonstrations introduce K-12 teachers and students to Science, Technology, Engineering and Mathematics majors.<br/><br/>This project establishes a new, remotely-connected smart grid cyberinfrastructure platform between the collaborative institutes using a real-time digital simulator, and sharing software licenses and hardware resources. This project promotes the application of advanced cyberinfrastructure techniques in power system monitoring, planning, operation and control, and prepares the next-generation power engineers to face challenges in modern power systems. This remotely connected power cyberinfrastructure provides an ideal platform for interdisciplinary research and education of computational intelligence, machine learning, control, communications, and data analytics in the smart grid area. The project team collects heterogeneous smart grid measurement data from this new cyberinfrastructure to conduct real-time learning, event-detection and data integrity, online optimization and multi-level decision-making process of intelligent systems. The project team integrates results into existing undergraduate courses and expand graduate courses from a frontier interdisciplinary viewpoint. The project team also creates replicable project templates/demos with designated benchmark data so other schools can easily adopt this new educational model with or without specific resources. Feedback from an independent evaluator, institutional stakeholders, national laboratory scientists and local industry partners supports periodic improvement of the educational model and materials. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Electrical, Communications and Cyber Systems in the Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940160","Collaborative Research: Predictive Risk Investigation SysteM (PRISM) for Multi-layer Dynamic Interconnection Analysis","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Lan Wang","MN","University of Minnesota-Twin Cities","Standard Grant","Amy Walton","04/30/2020","$254,633.00","","lanwang@mbs.miami.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","099Y, 7231","062Z, 7231","$0.00","The natural-human world is characterized by highly interconnected systems, in which a single discipline is not equipped to identify broader signs of systemic risk and mitigation targets. For example, what risks in agriculture, ecology, energy, finance and hydrology are heightened by climate variability and change? How might risks in, for example, space weather, be connected with energy, water and finance? Recent advances in computing and data science, and the data revolution in each of these domains have now provided a means to address these questions. The investigators jointly establish the PRISM Cooperative Institute for pioneering the integration of large-scale, multi-resolution, dynamic data across different domains to improve the prediction of risks (potentials for extreme outcomes and system failures). The investigators' vision is to develop a trans-domain framework that harnesses big data in the context of domain expertise to discover new critical risk indicators, holistically identify their interconnections, predict future risks and spillover potential, and to measure systemic risk broadly. The investigators will work with stakeholders to ultimately create early warnings and targets for critical risk mitigation and grow preparedness for devastating events worldwide; form wide and unique partnerships to educate the next generation of data scientists through postdoctoral researcher and student exchanges, research retreats, and workshops; and broaden participation through recruiting and training of those under-represented in STEM, including women and underrepresented minority students, and impact on stakeholder communities via methods, tools and datasets enabled by PRISM Data Library web services.<br/><br/>The PRISM Cooperative Institute's data-intensive cross-disciplinary research directions include: (i) Critical Risk Indicators (CRIs); The investigators define CRIs as quantifiable information specifically associated with cumulative or acute risk exposure to devastating, ruinous losses resulting from a disastrous (cumulative) activity or a catastrophic event.  PRISM aims to identify critical risks and existing indicators in many domains, and develop new CRIs by harnessing the data revolution; (ii) Dynamic Risk Interconnections; The investigators will dynamically model and forecast CRIs and PRISM aims to robustly identify a sparse, interpretable lead-lag risk dependence structure of critical societal risks, using state-of-the-art methods to accommodate CRI complexities such as nonstationary, spatiotemporal, and multi-resolution attributes; (iii) Systemic Risk Indicators (SRIs); PRISM will model trans-domain systemic risk, by forecasting critical risk spillovers and via the creation of SRIs for facilitating stakeholder intervention analysis; (iv) Validation & Stakeholder Engagement; The investigators will deploy the PRISM analytical framework on integrative case studies with distinct risk exposure (acute versus cumulative) and catastrophe characteristics (immediate versus sustained), and will solicit regular input from key stakeholders regarding critical risks and their decision variables, to better inform their operational understanding of policy versus practice.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by HDR and the Division of Mathematical Sciences within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852538","REU Site: Cyberinfrastructure (CI) Research 4 Social Change","OAC","RSCH EXPER FOR UNDERGRAD SITES, S-STEM-Schlr Sci Tech Eng&Math","03/01/2019","06/21/2021","Rosalia Gomez","TX","University of Texas at Austin","Standard Grant","Alan Sussman","02/28/2022","$371,689.00","Kelly Gaither","rosie@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1139, 1536","9102, 9250, 9251","$0.00","The world is increasingly more dependent on computational power, augmented by larger and larger networks of sensors and instruments. To fuel innovation across multiple disciplines, advanced computing (data, high performance computing, analytics) is required to keep pace with and accelerate the rate of scientific discoveries. In response to the National Science Foundation's mission to promote the progress of science; advance the national health, prosperity and welfare; and secure the national defense, the Cyberinfrastructure (CI) Research 4 Social Change REU project is actively engaging ten undergraduate students each summer for nine-weeks in solving real-world problems of national relevance, teaching them to not only be critical thinkers, but to be creative and reflective as well. The REU project is preparing the future scientific workforce to use advanced CI resources, thus building capacity in research areas that support major advances in understanding across a broad range of societal challenges. Students have a strong desire to have an impact in their communities, a strong desire to have a sense of belonging, and a strong desire to be materially part of something larger than themselves. Connecting social change principals to real-world problem solving is key to long-term success for recruiting and retaining underrepresented communities to computational fields and for preparing students to leverage the national CI.<br/> <br/>The REU aims to meet three objectives: (1) train students to use the national CI by integrating learning of computational science, data-enabled science and multi-disciplinary science in preparation for graduate programs and the workforce; (2) train students to apply advanced computational skills, critical thinking, and creativity to address problems relevant to society; and (3) increase the number of diverse and computationally competent students in the STEM pipeline. The enriching and transformative experience at a world-class supercomputing center includes: training in High Performance Computing (HPC), visualization, and data intensive computing; mentoring by The University of Texas (UT) at Austin researchers; social and team-building activities on the UT Austin campus; professional development and graduate school preparation; and leadership development and opportunities to develop and enhance communication skills. Research projects emphasize advanced computing as a tool to power discoveries that will impact social change for future generations. Using the CI, students conduct cutting-edge research in engineering,  science, and computational medicine: helping automate processes in support of cleaner energy sources for production of biofuel; enhancing research resources for modeling/prediction of porous material properties in fields of petroleum, civil and environmental engineering as well as geology; using data science and visualization to illustrate interfaces between science, traditional culture, and pressing societal problems in the Pacific region; developing and analyzing numerical algorithms for multiphysics, multiscale flow and transport problems in storm surge, tides, and coastal circulation; and developing and analyzing machine learning algorithms for individualized medicine. The REU recruits at least 50% underrepresented students. Targeted recruitment also includes women, first-generation college students, and majority students from institutions with limited research opportunities, including Minority-Serving Institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845840","CAREER: Scalable Software and Algorithmic Infrastructure for Probabilistic Graphical Modeling","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","02/15/2019","05/11/2021","Jaroslaw Zola","NY","SUNY at Buffalo","Continuing Grant","Alan Sussman","01/31/2024","$428,826.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045, 7231","026Z, 1045, 9251","$0.00","The data-driven reasoning is one of the major factors propelling progress in science and engineering. In many practical applications, especially in biology and medicine, data-driven reasoning has been based on probabilistic graphical models, that are preferred because of the accuracy in data representation and ease of interpretation. In probabilistic graphical modeling, the modeled objects, for example, the attributes of a patient stored in electronic health records, are represented as random variables, and the goal is to learn dependencies between these variables. However, the methods to learn high-quality probabilistic graphical models from the data are computationally challenging, and do not scale to datasets emerging in modern applications. Therefore, this project aims to enable high-quality probabilistic graphical modeling of large datasets by using high performance computing techniques. To this end, the project introduces a framework of fundamental operations underlying probabilistic graphical modeling, including for managing data and coordinating computations, together with their software fulfillment, that can efficiently leverage large-scale parallel computers. The framework is designed to benefit both practitioners interested in the analysis of large-scale data, and researchers interested in the development of new learning algorithms. The validation and evaluation of the framework is based on the analysis of electronic health records with the goal of early prognosis and diagnosis of patients with chronic obstructive pulmonary disease - problems vital for improving quality and reducing the cost of healthcare. Furthermore, the framework provides the foundation to train the next generation of biomedical professionals in the use of data analytics on advanced cyberinfrastructure. Thus, the project is aligned with NSF's mission to promote the progress of science, and to advance the national health, prosperity and welfare.<br/><br/>This project responds to the recognized and growing demand for scalable machine learning methods that could capitalize on parallel architectures such as large clusters of multi-core processors. The research focus is on exact structure learning of probabilistic graphical models, for example Bayesian networks and Markov random fields, in the context of biomedical data analytics. The project is based on the two main components: a new high performance abstraction for managing data in machine learning applications, including memory efficient strategies for answering counting queries on multi-core processors, and a new programming model for distributed memory systems to facilitate efficient exploration of large-scale combinatorial search spaces, such as those described by tress, lattices or graphs. These abstractions are used to realize a set of new parallel, exact algorithms for structure search, and the related problems, for example Markov blankets identification, that accelerate learning by exploiting various properties of the input data and the underlying search spaces. The research component is driven by the timely and socially relevant application in personalized and preventive medicine, enabled by a massive collection of the actual electronic health records. The project aims to delivers multiple artifacts, including MPI and OpenMP-based software, benchmark data and educational materials, all released as open source for use, further development, enhancement, and incorporation by the community. The research activities are tightly coupled with multiple educational efforts, spanning development of an interdisciplinary course for medical professionals to train them in the use of advanced cyberinfrastructure, engagement of undergraduate students and underrepresented minorities in research, and outreach to middle and high school students to attract them to STEM.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845208","CAREER: Scalable Sparse Linear Algebra for Extreme-Scale Data Analytics and Scientific Computing","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","02/15/2019","10/15/2020","Hasan Metin Aktulga","MI","Michigan State University","Continuing Grant","Alan Sussman","01/31/2024","$435,569.00","","hma@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1045, 7231","026Z, 1045","$0.00","This project addresses several technical challenges and develops a computing infrastructure to enable solving very large scientific problems that require high end computing such as for physics and material sciences (""scientific computing"") and for analyzing patterns within huge amounts of data such as those generated by social media (""big data analytics"").  A unifying computational motif in the seemingly disparate fields of big data analytics and scientific computing is that the models currently used to solve the relevant problems often result in large amount of data with significant, irregular gaps (technically known as ""sparse matrices""). The scale of solving such problems typically require execution on massively parallel computers. Due to the unique characteristics associated with sparse matrix computations, achieving high performance and scalability is challenging. This project aims to develop an extensive set of scalable sparse matrix algorithms and software to address such challenges. By significantly improving the productivity of domain scientists working on big data analytics and scientific computing, this project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. Research plans are tightly integrated with educational and outreach objectives at various levels. The centerpiece of the outreach efforts is a Computer Science summer school and mentorship plans for high school students. <br/><br/>To tackle the challenges presented by the increasingly deep memory hierarchies of modern computer architectures that include cache, high-bandwidth device memories (HBM), DRAM, and non-volatile random access memory (NVRAM) and facilitate high performance execution of sparse matrix computations, a comprehensive research plan is explored. The centerpiece of this project is a data-flow middleware with a simple application programming interface, called DeepSparse, that aims to support a wide variety of sparse solvers, while ensuring architecture and performance portability. DeepSparse converts a given sparse solver code into a directed acyclic graph (DAG) where nodes represent computational tasks and edges represent the data-flow between tasks. Novel DAG partitioning and scheduling algorithms, which are also extended to their hypergraph counterparts, are developed to ensure that data movement between memory layers is minimized during execution of the task graph. Performance models based on the extended Roofline model and innovative memory management schemes that draw upon ideas from disk storage systems are explored to ensure high bandwidth and low latency access to sparse solver data on NVRAM devices. All software and tools developed in this research are distributed as open source projects for a broad impact. Overall, goals of this project are well aligned with the National Strategic Computing Initiative, which aims to foster innovations that can bring the fields of big data analytics and scientific computing closer.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807563","CDS&E: Optimization of Advanced Cyberinfrastructure through Data-Driven Computational Modeling","OAC","CDS&E","09/01/2018","12/11/2018","Patrick Bridges","NM","University of New Mexico","Standard Grant","Tevfik Kosar","08/31/2022","$523,644.00","Trilce Estrada-Piedra, Majeed Hayat","patrickb@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","8084","026Z, 8084, 9150","$0.00","Modern scientific and big-data computing systems must balance multiple system attributes such as power, <br/>performance, and reliability to meet application science demands. These systems and their designers are, however,<br/>constrained by the lack of clear and well-defined methods to guide system design and tuning. This reduces the <br/>overall effectiveness of modern strategic computing systems. This project is developing new<br/>techniques for quantitatively characterizing and optimizing system tradeoffs in a wide range of<br/>modern strategic computing systems. The short term goal of this project is to develop models, analyses,<br/>and optimizations that will enable system software, applications, and system architects to make effective <br/>end-to-end performance tradeoffs. The desired long-term impact of this research is to increase the<br/>overall efficiency and effectiveness of current and emerging strategic computing systems. The techniques<br/>developed as part of this project will also be integrated with large-scale computing educational programs<br/>at the University of New Mexico and across the country.<br/><br/>This research is grounded in stochastic inter-collective-interval model for characterizing <br/>the performance of application/system-software configurations. To this end, the project is <br/>investigating techniques for estimating how different system software and applications mechanisms change <br/>application model parameters, providing a general means for understanding application/system software <br/>performance tradeoffs. In addition, the project is examining optimization techniques that leverage <br/>these models to improve resource allocation decisions in system schedulers and runtime systems. <br/>These techniques are being evaluated based on their ability to improve the overall performance of modern <br/>strategic computing systems in areas such as scheduling, power management, and data locality. The techniques<br/>developed as part of this project will also be integrated with large-scale computing educational programs<br/>at the University of New Mexico and across the country.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808652","CDS&E: Collaborative Research: Strategies for Managing Data in Uncertainty Quantification at Extreme Scales","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/28/2018","Hari Sundar","UT","University of Utah","Standard Grant","Tevfik Kosar","08/31/2022","$396,066.00","","hari@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8069, 8084","026Z, 8084, 9263","$0.00","The exponential increase in the quantity of measurements and data holds tremendous promise for data-driven scientific discovery and decision making.  In many cases, data-driven scientific discovery is mathematically formulated as an inverse problem.  For inverse problems that serve as a basis for discovery and decision-making for complex problems, the uncertainty in its solutions must be quantified.  Though the past decades have seen tremendous advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty (UQ) in their solutions taking big-data issues into account remains challenging.  This is largely due to computationally demanding nature of existing mathematical techniques that are unable to scale up to the amount of data being generated.  Consequently, much of the available data remains unused.  This project develops UQ algorithms that are both computationally scalable as well as datascalable for making scientific progresses in geosciences and medical imaging. In particular, the proposed methods are evaluated in the context of two challenging data-driven applications: (1) from large amount of seismograms (records of the ground motion) perform geophysical imaging to infer earth's interior structure to better understand earthquakes, and (2) from magnetic resonance (MR) cine images of patients estimate the heart's function (e.g. motion, contraction) to detect early onset of heart disease (cardiomyopathy).<br/><br/><br/>The goal of this collaborative research project is to develop an integrated research program that addresses the data management and data analytics arising from both observations and scientific simulations, with applications from diverse domains at extreme scales. The project develops innovative statistical, mathematical, and parallel computational methods to manage the large amounts of simulation data as well as the ever increasing amounts of observation data required for extreme-scale UQ problems in general and Bayesian inverse problems in particular. These methods will be of immediate practical utility to scientists  and engineers dealing with big data and large-scale UQ problems in sensing-based disciplines, geosciences, climatology, medical imaging, etc. The successful completion of the project would  provide a first step towards the development of mathematical and computational methods for a wide range of data-driven  large-scale inverse and UQ challenges that can lead to original scientific discoveries and promote the progress of science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924259","Cybertraining: Pilot: Collaborative Research:  Cybertraining for Earth Surface Processes Modelers","OAC","CyberTraining - Training-based","10/01/2019","06/26/2019","Irina Overeem","CO","University of Colorado at Boulder","Standard Grant","Alan Sussman","09/30/2023","$277,623.00","Mark Piper, Leilani Arthurs","irina.overeem@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","044Y","9102, 9150","$0.00","Living sustainably on a rapidly changing planet is one of the greatest modern scientific and societal challenges. One critical aspect of global change involves the earth's surface itself: the rearrangement of its landforms, soils, and sediments by processes such as landslides, debris flows, floods, and coastal erosion. The Community Surface Dynamics Modeling System, CSDMS, creates cyberinfrastructure to enable advanced numerical models of the earth's surface, its changes through time, and the influence of human activity. However, traditional earth science education does not usually equip students with skills to become effective cyberinfrastructure users and cyberinfrastructure contributors. In order to develop innovative models for analyzing and predicting how the earth's surface responds to environmental change and human influence, the earth surface processes (ESP) modeling community needs a platform to teach modern programming practices and High Performance Computing methods. This project implements a 10-day Cyberinfrastructure in Earth Surface Processes Institute (ESPIn) for graduate students, postdoctoral fellows and early career faculty at the CSDMS Integration Facility at the University of Colorado in Boulder in the summers of 2020-2021 trains the next generation to be innovators. ESPIn aims to transcend the traditional model of department-based graduate education through interdisciplinary, problem-based, ""Just in Time Teaching"" of model use and development. Over forty participants, selected from diverse disciplinary backgrounds with explicit slots reserved for underrepresented minorities, gain direct experience in converting their research codes into open-source distributed software. ESPIn hosts developed lesson material in online open access educational repositories. ESPIn helps to train a new generation of computationally savvy, integrative scientists, while accomplishing major community science priorities. This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national prosperity and welfare by building a capable geoscience workforce.<br/><br/>The Earth Surface Processes Institute (ESPIn) is a 10-day immersive experience for graduate students, postdoctoral fellows and early career faculty, allowing them to make advances on critical earth surface processes research questions with state-of-the-art modeling tools. This project targets learners who would benefit from critical knowledge, skills, and tools to become better cyberinfrastructure users and developers through a careful, inclusive selection procedure. This project aims to help make scientific advances in the study of Earth Surface Processes (ESP) that leverage the powerful and advanced capabilities of new cybertools, such as the Python Modeling Tool. To these ends, the primary objective is to expand the use of cyberinfrastructure among members of the ESP research community with training that (1) increases their competence and confidence with using cyberinfrastructure tools, methods, and resources and (2) moves the larger ESP community towards more widely adopting tools to advance the fundamental science of predicting surface change.  Experienced scientists, visiting faculty, and software engineers assist with training and mentoring of the participants. ESPIn offers hands-on training in best programming practices, numerical methods, open source software development, advanced use of version control systems, writing unit tests, HPC-based sensitivity testing and model uncertainty quantification techniques. Several days are dedicated to working collaboratively on research and coding projects. Participants work on developing their own codes, with the intent of making codes more robust and compliant with existing ESP CI frameworks. The Summer Institute is quantitatively evaluated for learning efficacy and evaluations are used to iterate on lesson material quality. ESPIn provides all developed lesson material as online learning and teaching modules and broadly advertises these resources to the geoscience community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924154","CyberTraining: Implementation: Medium: Advanced Cyber Infrastructure Training in Policy Informatics","OAC","CyberTraining - Training-based, CAREER: FACULTY EARLY CAR DEV","08/01/2019","05/13/2021","Sukumar Ganapati","FL","Florida International University","Standard Grant","Alan Sussman","07/31/2023","$1,016,000.00","Julio Ibarra, Howard Frank, Giri Narasimhan, Kemal Akkaya","ganapati@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","044Y, 1045","026Z, 9102, 9179, 9251","$0.00","The Advanced Cyberinfrastructure Training for Policy Informatics (ACIT-PI) is an interdisciplinary program for Cyberinfrastructure training to policy scientists. The project is important and significant because policy scientists seek data-enabled scientific techniques, which requires training in advanced Cyberinfrastructure (CI) methods. Policy scientists can harvest data from different sources, analyze and interpret them, and arrive at evidence-based decisions. Such a workforce is urgently required in minority and low-income communities with low skills in data science. As stated by NSF's mission, the project serves the national interest to promote the progress of science, prosperity and welfare. The national interest is served by offering CI training to two classes of policy scientists: (a) CI users: research analysts in nonprofit and public agencies; and (b) CI contributors and professionals: research scientists specializing in policy science (graduate students). CI users are trained through training workshops, webinars, and self-paced online training modules. CI contributors and CI professionals are trained through a novel interdisciplinary certificate program on Policy Informatics. The ACIT-PI project is jointly undertaken by Florida International University (FIU) Metropolitan Center, Department of Public Policy and Administration, School of Computing and Information Sciences, and Electrical and Computer Engineering. It uses the CI facilities including High Performance Computing and Internet 2 resources available with FIU's Division of Information Technology.<br/><br/>Policy scientists include engineers and social scientists (criminologists, geographers, planners, public administrators, political scientists). The ACIT-PI project enhances researchers' abilities to lead the development of new CI tools, public domain data, and analysis. The materials and tools developed through the project are available for broader use by other universities through a Policy Information Hub. The Hub includes a Policy Informatics Data Repository of local government administrative data in standardized formats for fostering data driven research. The ACIT-PI project is both innovative and transformative for public policy and administration education. First, the project trains frontline analysts in local governments through annual workshops, which fosters a large pool (of about 120) of CI users. Second, project develops self-paced online training modules for broader CI adoption by the public administration and policy schools. Third, it offers a novel Policy Informatics certificate program for CI contributors/professionals (over 100 students), which includes curriculum innovations infused with CI tools and methods. Students take courses on policy science, computational methods (including HPC clusters use and CI tools), machine learning, IoT, and cybersecurity tools. PhD students are also trained to become CI experts in policy science. The project fosters a robust community of CI users, contributors, and professionals in policy informatics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915371","Workshop on Including Ethics in Data Science Pedagogy","OAC","IUSE, EDUCATION AND WORKFORCE","05/01/2019","04/18/2019","Shimei Pan","MD","University of Maryland Baltimore County","Standard Grant","Alan Sussman","04/30/2020","$50,000.00","James Foulds, Vandana Janeja","shimei@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1998, 7361","026Z, 062Z, 7556","$0.00","Artificial Intelligence (AI) and data science technologies are increasingly being used in making consequential decisions such as determining whether someone is hired, promoted, offered a loan, or provided housing. Many of these systems however were developed and some deployed without a careful assessment of their societal impact. For example, AI systems have been shown to exhibit gender and racial biases in making hiring and sentencing decisions. Yet, many data science students and practitioners are still unaware of the prevalence and seriousness of these issues. Equipping them with the knowledge to understand the implications of algorithm design decisions as well as tools that they can use to mitigate the biases in these systems is the key to ensure that the data-driven decision making systems they build are fair and trustworthy. This serves the national interest by furthering NSF's mission to promote the progress of science, and to advance the national health, prosperity and welfare.   <br/><br/>This two day workshop brings together prominent educators, researchers and thought leaders from academia, industry and government together to explore ideas on the best strategies to develop data science ethics curricula. During the workshop, participants explore ideas on the best strategies to develop data science ethics curricula that are guided by pedagogical principles, informed by the needs of communities, aligned with core social values and laws, and shaped by the latest evidence and solutions from the fairness, accountability and transparency in AI research.  The key outcomes of the workshop include a recommendation for incorporating ethics in data science pedagogy that contains guidelines on designing both standalone data science ethics courses as well as ethics modules that can be embedded in existing data science courses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2139536","Characteristic Science Applications for the Leadership Class Computing Facility","OAC","Leadership-Class Computing","09/01/2021","08/26/2021","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","08/31/2023","$6,999,361.00","John West, John Cazes, Omar Ghattas","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","","$0.00","The goal of this project is to select, study, and transform a set of Characteristic Science Applications (CSAs) to enable next-generation science on the NSF Leadership-Class Computing Facility (LCCF). The CSAs were chosen to represent a broad range of science domains and computational approaches, and each CSA will comprise one or more computer codes and a challenge problem to be solved on the LCCF. The set of CSAs will enable the project to verify the design of the LCCF and validate that the facility is applicable across the broad range of science disciplines supported by the NSF when it is constructed.  Furthermore, the transformations made to the codes and workflows in the CSAs will provide best practice models and implementation exemplars that will inform training materials, tutorials, and other educational content.  This will ensure that the nation's science and engineering research community will be effective on the LCCF from day one of operations.<br/><br/>The project plans to evaluate a broad selection of science applications and challenge problems, laying the foundations for a transformation of the computer codes in a way that will support the LCCF broad goal of enabling a ten-fold or more time to solution performance improvement over NSF's current leadership computing resource, Frontera. This capability improvement will come from a combination of increased scale of the LCCF primary computing system(s), increased capabilities of the components (on-node I/O bandwidth, inter-node I/O bandwidth, disk and memory bandwidth, access speed, cores, etc.) that make up the computing instrument, and improvements in the software codes that make up the set of science applications used to demonstration ten-fold performance improvement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829585","Collaborative Research: CYBER Training:  CIU: Data Streams, Model Workflows, and Educational Pipelines for Hydrologic Sciences","OAC","CyberTraining - Training-based, Special Initiatives","09/01/2018","08/23/2021","Christina Bandaragoda","WA","University of Washington","Standard Grant","Alan Sussman","08/31/2022","$446,392.00","Anthony Arendt, Bart Nijssen, Erkan Istanbulluoglu","cband@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","044Y, 1642","026Z, 062Z, 7361, 9102, 9179","$0.00","Studies of water and environmental systems are becoming increasingly complex and require integration of knowledge across multiple domains. At the same time, technological advances have enabled the collection of massive quantities of data for studying earth system changes. Fully leveraging these datasets and software tools requires fundamentally new approaches in the way researchers store, access and process data. The project serves the national interest by motivating a culture shift within the hydrologic and more broadly earth science communities toward open and reproducible software practices that will enhance interdisciplinary collaboration and increase capacity for addressing complex science challenges around the availability, risks and use of water. Project's CyberTraining approach provides virtual learning experiences throughout an academic year, with online learning modules oriented around a one-week in-person workshop (WaterHackWeek) that will focus on hands-on real-world research projects. These research projects are designed to serve the national interest by preparing for natural hazards such as floods, hurricanes and climate change, and to advance the nation's health by making tools and data accessible to health researchers, local governments, and citizens.<br/><br/>New cyberinfrastructure that emphasizes data sharing and open, reproducible software practices is currently in development, but requires a mode of knowledge transfer, or CyberTraining, that extends beyond currently available university curriculum.  Project's aim is to ensure successful use of community cyberinfrastructure to 1) publish large datasets, 2) run numerical models, 3) organize collaborative research projects, and 4) meet journal requirements to follow open data standards. The activities take advantage of HydroShare, a National Science Foundation funded cyberinfrastructure platform, operated by the Consortium of Universities Allied for Hydrologic Sciences (CUAHSI), for sharing hydrologic data and models.  The short-term goals are to develop new CyberTraining modules; the long-term goals are to have an annually recurring WaterHackWeek, to distribute curriculum CUAHSI to more than 130 member universities, and advance cyberinfrastructure education for the broader geoscience community. The use of the hackweek educational model extends the use of cyberinfrastructure to promote the progress of science by including a specific emphasis on graduate student training as instructors, training coordinators, and building research networks with data providers who are stakeholders outside of academia. For example, case studies include data and resource management by Native American tribal governments, Hurricane Maria data archive for research in Puerto Rico, improving flood forecasting, and tool-building using complex numerical models such as the National Water Model.  This project allows to test the educational model in the water research community, in addition to connecting team's research and curriculum to annually recurring hackweeks in neuro, astro, ocean, and geo sciences. The team of researchers is actively engaged in experimenting with this new model, and in testing its efficacy through robust evaluation metrics. The proposed activities encourage collaboration and support for use of cyberinfrastructure at all stages of the educational pipeline and provides participants with opportunities for networking, career development, community building and design of open-source software tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829622","CyberTraining: CIU: SJSU Data Science for All Seminar Series","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Leslie Albert","CA","San Jose State University Foundation","Standard Grant","Alan Sussman","08/31/2022","$410,060.00","Esperanza Huerta, Scott Jensen","leslie.albert@sjsu.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The Nation's research enterprise faces a shortage of data scientists. Expanding the pipeline of data science students, particularly from underrepresented populations, requires educational institutions to increase awareness of data science and inspire a passion for data in students as they begin their academic careers. Currently, few community colleges or undergraduate programs provide training in cyberinfrastructure tools or data science techniques to a broad student population. This project takes a novel approach to augmenting the Nation's data science workforce by training community college and undergraduate students to provide data analytics support to data scientists through a series of ""Data Science for All"" extracurricular seminars. The seminars require no prior data science knowledge, emphasize transferable skills, and present a feasible path into data science-related research and other careers for students from a broad array of disciplines and from underrepresented groups without extending their time to graduation. By increasing the Nation's data science capabilities and the diversity of its data science research workforce, the project serves the national interest, as stated by NSF's mission: to promote progress of science and advance the prosperity and welfare of the Nation. <br/><br/>The goals of this project are to increase undergraduate student awareness of data-driven science and to grow and diversify the population of students trained to perform data wrangling - the data acquisition, transformation, cleaning, and profiling required to prepare data for analysis. According to industry experts, data wrangling is the ""heavy lifting"" of data science, constituting up to 80% of a data scientist's daily work. Shifting this time-consuming effort to trained data analysts free data scientists to focus more of their time on research. The project achieves its goals through the development and delivery of widely consumable, extracurricular seminars providing interactive training on data science concepts and industry-leading data wrangling tools to undergraduate and community college students. Initial seminar topics, selected in collaboration with the project's advisory board, include Python, Jupyter notebooks, Apache Spark, Tableau, and demystifying artificial intelligence (AI). The seminars' focus on data wrangling also introduces students to data preparation documentation - capturing the data provenance needed for reproducible science. This project's contribution to the Nation's data science workforce is broadened through the free and open distribution of its seminar materials and supplemental resources and its online instructor support community. To encourage adoption at Bay Area community colleges and universities, instructor training is provided through co-instruction and a teaching-the-teacher model. The project contributes to pedagogical research by identifying instructional approaches most effective in teaching data science to a diverse population of undergraduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829729","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","08/01/2018","07/02/2018","G.J. Peter Elmer","NJ","Princeton University","Standard Grant","Alan Sussman","07/31/2022","$375,041.00","Ian Cosden","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","044Y, 7244","026Z, 062Z, 7361, 7569, 9102, 9179","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829708","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Eric Shook","MN","University of Minnesota-Twin Cities","Standard Grant","Alan Sussman","07/31/2022","$373,990.00","","eshook@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832190","Collaborative Research: Mentoring the Next Generation of Parallel Processing Researchers at IPDPS and other IEEE-CSTCPP Sponsored Conferences","OAC","Information Technology Researc, EDUCATION AND WORKFORCE","05/01/2018","05/08/2018","Trilce Estrada-Piedra","NM","University of New Mexico","Standard Grant","Alan Sussman","04/30/2019","$25,000.00","","estrada@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","1640, 7361","7556, 9102, 9150","$0.00","From their attendance to conferences, students can form professional connections and establish collaborations, market themselves to potential employers, get exposed to a wider network of mentors, and find inspiration for novel research avenues. Student mentoring programs play an important role in providing younger attendees with the necessary tools to take advantage of their participation at conferences. Student programs have emerged in several leading conferences, but are still far from being widely adopted. In particular the International Parallel and Distributed Processing Symposium (IPDPS) has been hosting a PhD forum and student program for over a decade. This program has evolved into a comprehensive training workshop that includes sessions on career planning and hands on tutorials on writing and presentation skills; it also provides effective opportunities for student networking. This project seeks to expand the IPDPS mentoring and outreach model into other conferences that fall under the broad field of parallel and distributed processing. Parallel and distributed processing has become increasingly important for a variety of disciplines where traditional computational methods lack the mechanisms to deal with large data volumes or expensive computations. As stated by NSF's mission, this project supports education and diversity, while promoting the progress of science by mentoring the next-generation workforce in the high performance computing discipline.<br/><br/>This project has two concrete goals: 1) supporting student participation at the 32nd IEEE International Parallel and Distributed Processing Symposium. IPDPS is an international conference for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation. 2) promoting the adoption of student mentoring programs in other IEEE TCPP conferences, such as: AICCSA, DCOSS, DS-RT, e-Energy, HiPC, ICCABS, ICPADS, NAS, PACT, PERCOM. The beneficiary conferences are required to facilitate an introductory session where students network with peers and mentors. Additionally, conference organizers are provided with the logistic plan of IPDPS?s PhD Forum and are encouraged to have similar proven activities; including a poster session and mentoring workshops. Only students currently studying at U.S. universities are eligible to receive support from this NSF-funded project. This project targets especially students that typically cannot attended these conferences without financial support, such as undergraduate students, graduate students in their first years, and students attending their first conference. In order to increase the diversity of attendees, the project strongly encourages the participation of females and other underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852454","REU Site: Interdisciplinary Research Experience in Computational Sciences","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2019","07/19/2019","Juana Moreno","LA","Louisiana State University","Standard Grant","Alan Sussman","02/28/2022","$380,900.00","","moreno@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1139","9102, 9150, 9250","$0.00","Computational simulations are rapidly emerging as a co-equal branch of inquiry with experiment and theory. In parallel, automated instruments store massive amounts of measurements. However, computational science will only fulfill its full potential if advances in undergraduate and graduate education accompany the advances in hardware. The 30 undergraduate students participating in this Research Experience for Undergraduates (REU) program are engaged with varied computational science projects, learn how to use state-of-the-art cyberinfrastructure tools, manage large amounts of data, experience activities that characterize research careers, and work in interdisciplinary research teams. The Center for Computation & Technology (CCT) at Louisiana State University provides an ideal setting for the REU student to become familiar with interdisciplinary research. For ten weeks in the summer, the students work collaboratively with their mentors on a wide variety of computational science projects. With research groups exploring gravitational waves, complex emergent phenomena in material science, or computational music and arts, the participants work on cutting edge research in computational sciences. By preparing the next generation of students in the computational sciences, this project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/><br/>There is a clear need for training the workforce on computing and computational science. But, currently, the majority of students learn little, if any, Computational Science (CSci) in the classroom and are not prepared for CSci research or data science. This proposal provides an evidence-based approach to address these issues and prepare the next generation of students. One of the current barriers in CSci education is that while CSci is intrinsically multidisciplinary, traditional academic departments set rigid boundaries between disciplines. Because LSU Center for Computation & Technology is not an academic unit, it provides an ideal setting for the REU student to become familiar with interdisciplinary research by participating in a multidisciplinary research team. During ten weeks the participants experience activities that characterize research careers, such as presenting their research in scholarly forums, and working on the soft- and hard-skills needed in interdisciplinary research. Faculty mentors work with the students in individual and group meetings, and they provide activities that help students appreciate the nature of multidisciplinary research and the value of working as a team. At the conclusion of the REU, students are encouraged to continue their CSci research and present their work at their home institution, informing others about computational science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852102","REU Site: High Performance Computing with Engineering Applications","OAC","RSCH EXPER FOR UNDERGRAD SITES","02/15/2019","06/25/2019","Daqing Hou","NY","Clarkson University","Standard Grant","Alan Sussman","01/31/2023","$379,931.00","Yu Liu","dhou@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","1139","9250, 9251","$0.00","This award provides support for a three-year REU Site at Clarkson University, Potsdam, New York, in High Performance Computing (HPC), a field critical to national security, scientific discovery, and technological innovation. The goal of this project is to encourage 30 talented undergraduate students to pursue graduate study and careers in HPC by engaging them in exciting, ongoing research projects and by cultivating their talents during ten-week summer research experiences and beyond. Sixteen ongoing research projects available for students are innovative and share the common theme of societally relevant engineering applications that originate from disciplines as diverse as aeronautical engineering, chemical engineering, civil engineering, electrical and computer engineering, software engineering, and applied mathematics.  The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/><br/>Students are recruited from targeted institutions that offer limited or no research opportunities in HPC, such as historically minority colleges and universities. Emphasis is placed on attracting underrepresented minority and female students.  Faculty with expertise in HPC applications mentor the students to be researchers and provide them with specialized training in the design and development of HPC applications that are rooted in and motivated by engaging engineering innovations. This specialized training provides the students with technical and analytical skills in mathematical modeling, algorithmic development, and parallel and distributed programming, which benefit them in future pursuits such as graduate study or industry work. The quality of this training is further reinforced by additional professional development activities, including an HPC crash course, field trips, invited speakers, weekly group meetings, and a mentor training workshop for faculty.  Seminars in career development, literature search, technical writing, and graduate school advisement, are also integrated into the program, and all student participants are given the opportunity to attend and present in professional conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1853982","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","09/21/2018","Wei Cheng","WA","University of Washington","Standard Grant","Alan Sussman","08/31/2022","$100,000.00","","uwcheng@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103942","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","Software Institutes, EarthCube","08/01/2021","07/15/2021","Patrick Heimbach","TX","University of Texas at Austin","Standard Grant","Tevfik Kosar","07/31/2025","$1,278,970.00","Karen Willcox","heimbach@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004, 8074","077Z, 1079, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103804","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","Software Institutes, EarthCube","08/01/2021","07/15/2021","Alan Edelman","MA","Massachusetts Institute of Technology","Standard Grant","Tevfik Kosar","07/31/2025","$1,200,000.00","Christopher Rackauckas, Christopher Hill","EDELMAN@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8004, 8074","077Z, 1079, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103991","Frameworks: An Interoperable Software Ecosystem for Many-Body Electronic Structure Calculations","OAC","DMR SHORT TERM SUPPORT, Software Institutes","07/01/2021","07/01/2021","Feliciano Giustino","TX","University of Texas at Austin","Standard Grant","Amy Walton","06/30/2025","$3,856,970.00","Steven Louie, Elena Roxana Margine, Daniel Stanzione","fgiustino@oden.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1712, 8004","054Z, 077Z, 7203, 7925, 8004, 8396, 8607, 9216","$0.00","The development of advanced materials is a key driver of progress in areas of national strategic importance, such as energy generation, storage, and distribution, wireless communications, and quantum technologies. For example the operation of solar panels, solid-state batteries, touch screens, flat-panel displays, and quantum computer prototypes critically relies on the properties and functionalities of advanced materials down to the scale of individual atoms. Further improving the performance of these materials, as well as designing brand new materials with novel functionalities, requires a detailed understanding of how macroscopic properties, such as the ability to carry electricity, to absorb and emit light, and to store chemical energy, emerge from the elemental composition and the atomic structure of each compound. In this context, simulating materials behavior by solving the fundamental equations of quantum mechanics on supercomputers has become an indispensable complement to experimental research. Today there exists an abundance of high-performance computing software to investigate and predict the properties of materials in their lowest energy state, or ground state. These tools are primarily based on density functional theory, an incredibly successful conceptual paradigm that allows us to find approximate yet accurate solutions of the Schrödinger equation of quantum mechanics for entire materials. While these methods are essential for predicting structural and energetic properties such as thermodynamic phase diagrams, they are not suitable to describe more advanced functional properties such as light-matter interactions, charge transport under electric and magnetic fields, and macroscopic quantum phenomena such as superconductivity. The current project fills this gap by developing a comprehensive software ecosystem to compute and predict functional properties of materials beyond what is currently possible with density functional theory. The cyberinfrastructure supported by this grant will enable the rational design of advanced functional materials at the atomic scale, and will underpin the development of next-generation materials for energy, computing, and quantum technologies. The research program will be tightly integrated with educational activities to promote scientific research in diverse communities. To this end, webinars, schools and hackathons for users and developers will be organized annually.<br/><br/>The aim of this project is to create an interoperable software ecosystem to model and design materials at the atomic scale using many-body field-theoretic approaches. Many-body electronic structure methods define a gold standard for accuracy, reliability, and predictive power, but the widespread adoption of these methods in academia and in industry is hindered by the complexity of the underlying theories and algorithms, as well as the lack of broad interoperability and shared data standards. The project expands and combines the complementary strengths of three software packages, EPW, BerkeleyGW, and SternheimerGW, into a user-centric, containerized simulation laboratory with shared data formats and built-in compatibility layers for major density-functional theory codes. This cyberinfrastructure advances understanding of the interplay between electronic and lattice degrees of freedom in advanced materials, and expands the range of properties that can be calculated with predictive accuracy, including: finite-temperature quasiparticle band structures; light absorption and emission spectra; excitons, polarons, and their couplings; superconductivity; carrier transport; and driven quantum systems. Furthermore, this cyberinfrastructure will accelerate future software development by distributing curated, reusable, and interoperable open-source code, and by providing a platform to develop and test new algorithms and software for many-body electronic structure methods. Central to the proposed effort is the training of a diverse, inclusive, and globally competitive STEM workforce cutting across data-driven materials research and cyberinfrastructure development.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103489","Collaborative Research: Elements: Simulation-driven Evaluation of Cyberinfrastructure Systems","OAC","Software Institutes","08/01/2021","06/30/2021","Henri Casanova","HI","University of Hawaii","Standard Grant","Tevfik Kosar","07/31/2024","$284,973.00","","henric@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","8004","077Z, 7923, 8004, 9150","$0.00","Most scientific breakthroughs and discoveries are now preconditioned on performing complex processing of vast amounts of data as conveniently, reliably, and efficiently as possible. This requires high-end interconnected compute and storage resources, as well as software systems to automate the processing on these resources. An enormous amount of effort has been invested in producing such ""cyberinfrastructure"" software systems. And yet, developing and evolving these systems so that they are as efficient as possible, while anticipating future cyberinfrastructure opportunities and needs, is an open challenge.  This project transforms the way in which these systems are evaluated, so that their capabilities can be developed and evolved judiciously. The traditional evaluation approach is to observe executions of these systems on real-world hardware resources. Although seemingly natural, this approach suffers from many shortcomings. Instead, this project focuses on simulating these executions. Simulation has tremendous, and untapped, potential for transforming the development cycle of cyberinfrastructure systems. Specifically, this project produces software elements that can be easily integrated into existing and future systems to afford them with simulation capabilities.  These capabilities make it possible for developers to put their systems through the wringer and observe their behaviors for arbitrary operating conditions, including ones that go beyond current hardware platforms and scientific applications. Simply put, these capabilities will make it possible to establish a solid experimental science approach for the development of cyberinfrastructure systems that support current and future scientific endeavors that are critical to the development of our society.<br/><br/>The cyberinfrastructure has been the object of intensive research and development, resulting in a rich set of interoperable software systems that are used to support science. A key challenge is the development of systems that can execute application workloads efficiently, while anticipating future cyberinfrastructure opportunities and needs. This project aims to transform the way in which these systems are evaluated, so that their capabilities can be evolved based on a sound, quantitative experimental science approach. The traditional evaluation approach is to use full-fledged software stacks to execute application workloads on actual cyberinfrastructure deployments. Unfortunately, this approach suffers from several shortcomings: real-world experiments are time- and labor-intensive, and they are limited to currently available hardware and software configurations. An alternative to real-world experiments that does not suffer from these shortcomings is simulation, i.e., the implementation and use of a software artifact that models the functional and performance behaviors of software and hardware stacks of interest. This project uses simulation to transform the way in which cyberinfrastructure systems are evaluated as part of their long-term development cycles.  This is achieved via software elements for enhancing production cyberinfrastructure systems with simulation capabilities so as to enable quantitative evaluation of these systems for arbitrary execution scenarios. Creating these scenarios requires little labor, and executions can be simulated accurately and orders of magnitude faster than their real-world counterparts. Furthermore, simulations are perfectly reproducible and observable. While this approach is general, its effectiveness will be demonstrated by applying it to a number of production systems, namely, workflow management systems.  This project capitalizes on the years of development invested in the SimGrid and WRENCH simulation frameworks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839739","Student Travel Support for MVAPICH User Group (MUG) Meeting","OAC","EDUCATION AND WORKFORCE","09/01/2018","08/17/2018","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Alan Sussman","08/31/2019","$10,000.00","Hari Subramoni","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7361","026Z, 062Z, 7556, 9179","$0.00","Modern High-Performance Computing (HPC) systems are rapidly evolving with respect to processor, networking, and I/O technologies. In such a rapidly changing environment it is critical that the next-generation engineers and scientists get exposed to the modern architectural trends of HPC systems and learn how to use the features of these technologies to design HPC software stacks, learn about the process of open-source software developments and its sustainability. The annual MVAPICH User Group (MUG) meeting provides an open forum to exchange information on the design and usage of MVAPICH2 libraries, which are open-source, high-performance and scalable MPI libraries to take advantage of the RDMA technology. Travel funding from this project will enable a set of students (undergraduates and graduates) to attend the MUG meeting. Their participation will help them to enter the next-generation HPC workforce with increased expertise on software design, reuse, and sustainability. The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science.<br/><br/>The MVAPICH project focuses on the design of high-performance MPI and PGAS runtimes for HPC systems. Over the years, this project has been able to incorporate new designs to leverage novel multi-/many-core platforms like Intel Xeon Phis, NVIDIA GPGPUs, Open POWER, and ARM architectures coupled with Remote Direct Memory Access (RDMA) enabled commodity networking technologies like InfiniBand, RoCE, Omni-Path, and 10/25/40/50/100GigE with iWARP. An annual MVAPICH User Group (MUG) meeting was created five years ago to provide an open forum to exchange information on MVAPICH2 libraries. The funding under this grant aims to achieve increased participation of undergraduate and graduate students working in the HPC area (systems and applications) in the annual MUG event. The requested student travel fund will help in attracting a set of students from a range of US institutions. The participation in an international event such as MUG will enable the students to get a global picture of the developments happening in the rapidly evolving HPC domain and open-source software design. The selection committee will stress on diversity to attract students from minority and under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755779","CRII: OAC: A Hybrid Finite Element and Molecular Dynamics Simulation Approach for Modeling Nanoparticle Transport in Human Vasculature","OAC","CRII CISE Research Initiation, CAREER: FACULTY EARLY CAR DEV, Leadership-Class Computing","03/01/2018","07/02/2021","Ying Li","CT","University of Connecticut","Standard Grant","Alan Sussman","02/28/2022","$250,999.00","","ying.3.li@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","026Y, 1045, 7781","026Z, 8228, 9179, 9251","$0.00","Through nanomedicine significant methods are emerging to deliver drug molecules directly to diseased areas for cancer treatment.  Targeted drug delivery is one of the most promising approaches which relies on nanoparticles (NPs) that carry and release drugs.  The therapeutic efficacy of NP-based drug carriers is determined by the proper concentration of drug molecules at the lesion site.  NPs need to be delivered directly to the diseased tissues while minimizing their uptake by other tissues, thereby reducing the potential harm to healthy tissue.  Therefore, the design of these NPs and hence the efficacy of the targeted drug delivery could be significantly improved by understanding how the drugs carried by NPs are transported and dispersed in human body.  This project proposes a set of computational tools to model and investigate the transport and dispersion of NPs in human vasculature.  This, in turn, can provide better imaging sensitivity, therapeutic efficacy and lower toxicity of NP-based drug carriers.  The multidisciplinary nature of the project also brings together concepts from biology, engineering and computer science to educate the next generation of computational biologists, scientists and engineers. This research, thus, aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare. <br/><br/>The technical objective of this project is to create a hybrid finite element and molecular dynamics computational approach for modeling NP transport and adhesion in human vasculature.  The realistic geometry of vascular network and fluid dynamics of blood flow are accurately captured through the finite element model.  The microscopic interactions between NPs and red blood cells within blood flow and adhesion of NPs to vessel wall are resolved through the molecular dynamics simulation.  A robust and efficient coupling interface is built to couple the finite element and molecular dynamics solvers.  Specifically, this project aims to 1) create a multiscale and multiphysics computational model for predicting the vascular dynamics of NPs under the influence of realistic geometrical and physiochemical features of human vasculature; 2) craft an interface coupling technique that enhances computational accuracy and predictability by coupling the finite element and molecular dynamics solvers; 3) build testsuits for multiscale and multiphysics simulations for coupled solution error and convergence analysis; and 4) advance the current cyberinfrastructure to accelerate the material design process and enrich the cyber-enabled materials design community.  Such a computational method can be used to explore how the vascular dynamics of NPs will be affected by their size, shape, surface and stiffness properties, as well as complex geometry of human vasculature.  The simulation results can further guide experimentalists to design NP-mediated drug delivery platforms that optimally accumulate within diseased tissue to provide better imaging sensitivity, therapeutic efficacy and lower toxicity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104078","Collaborative Research: Framework Implementation: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network Dynamics","OAC","Software Institutes","09/01/2021","05/11/2021","Sajal Das","MO","Missouri University of Science and Technology","Standard Grant","Tevfik Kosar","08/31/2025","$841,385.00","Samuel Frimpong","sdas@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","8004","077Z, 7925, 7942, 8004","$0.00","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.<br/><br/>This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104008","Frameworks: Collaborative Research: ChronoLog: A High-Performance Storage Infrastructure for Activity and Log Workloads","OAC","Software Institutes","06/01/2021","05/11/2021","Kyle Chard","IL","University of Chicago","Standard Grant","Tevfik Kosar","05/31/2025","$1,308,582.00","","chard@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","077Z, 7925, 8004","$0.00","Modern computing applications generate massive amounts of data at unprecedented rates. Beyond simply storing data, one increasingly common requirement is to store activity data, also known as log data, which describe things that happen rather than things that are. Activity data are generated by computing systems, scientific instruments, electrical devices, etc. as well as by humans. The fast growing of activity data stresses current data management systems beyond their capability and becomes a known killer performance bottleneck of high-performance computing systems. This project develops ChronoLog, a novel system for organizing and storing activity data effectively and efficiently. ChronoLog leverages modern storage hardware and provides user-focused plugins and easy-to-use interface for productivity. It will benefit a diverse range of communities in various ways, such as enabling better fraud detection in financial transactions, faster and more accurate weather predictions and simulations, reduced time-to-insight for medical and bioengineering data, autonomous computing (e.g., driving), and more secure web and mobile services. <br/><br/>ChronoLog uses physical time to provide a synchronization-free data distribution and the total ordering on a log. It first leverages multiple storage tiers, such as storage-class memories (e.g., 3D XPoint) and new flash storage (e.g., NVMe SSDs), to transparently scale the log via log auto-tiering. It then adopts a tunable parallel access model, which offers multiple-writers-multiple-readers (MWMR) semantics and highly concurrent I/O, to fully utilize the multi-tiered storage environment. ChronoLog's innovative design supports high-performance data access via I/O isolation between tails and historical operations, efficient resource utilization with newly developed elastic storage capabilities, and scalability using a novel 3D log distribution. It facilitates data processing pipelining by acting as an authoritative source of strong consistency and with the help of fast append and commit semantics. It can be used as an arbitrator offering a plethora of features such as transactional isolation and atomicity, a consensus engine for consistent replication and indexing services, and a scalable data integration and warehousing solution. ChronoLog and its plugins establish a robust, flexible, and high-performance storage ecosystem that promotes the development of scalable applications and services for high performance computing systems. The project includes a diverse group of collaborators who share a common need for a fundamentally new approach to distributed logging to address their use cases. These close partnerships will strengthen the bonds between academic and applied science, ultimately leading to new applications and driving discovery in domains as diverse as geoscience, cosmology, and astrophysics. Forming these collaborations and integrating students and junior IT professionals will create a well-trained workforce in cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103939","Collaborative Research: Elements: EXHUME: Extraction for High-Order Unfitted Finite Element Methods","OAC","Software Institutes","06/01/2021","05/11/2021","David Kamensky","CA","University of California-San Diego","Standard Grant","Tevfik Kosar","05/31/2024","$297,766.00","","dmkamensky@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8004","077Z, 7923, 8004","$0.00","Unfitted finite element methods allow for the simulation of physical systems that are difficult if not impossible to simulate using classical finite element methods requiring body-fitted meshes. For instance, unfitted finite element methods can be directly applied to the simulation of physical systems exhibiting a change of domain topology, such as the movement of blood cells through a human capillary or the flow of blood past the heart valves between the four main chambers of the human heart. Unfitted finite element methods also streamline the construction of computational design optimization technologies that optimize the geometry and material layout of an engineered system based on prescribed performance metrics. However, the computer implementation of an unfitted finite element method remains a challenging and time-consuming task even for domain experts. The overarching objective of this project is to construct a novel software library, EXHUME (EXtraction for High-order Unfitted finite element MEthods), to enable the use of classical finite element codes for unfitted finite element analysis. EXHUME will empower a large community of scientists and engineers to employ unfitted finite element methods in their own work, allowing them to carry out biomedical, materials science, and geophysical simulations that have been too expensive or too unstable to realize using classical finite element methods. EXHUME will also improve the fidelity of design optimizations being performed in academia, national laboratories, and industry on a near daily basis.<br/><br/>Unfitted finite element methods simplify the finite element solution of PDEs (Partial Differential Equations) on complex and/or deforming domain geometries by relaxing the requirement that the finite element approximation space be defined on a body-fitted mesh whose elements satisfy restrictive shape and connectivity constraints. Early unfitted finite element methods exhibited low-order convergence rates, but recent progress has led to high-order methods. The key ingredient to success of a high-order unfitted finite element method is accurate numerical integration over cut cells (i.e, unfitted elements cut by domain boundaries). EXHUME uses the concept of extraction to express numerical integration over cut cells in terms of basic operations already implemented in typical finite element codes, an integration mesh, and extraction operators expressing unfitted finite element basis functions in terms of canonical shape functions. EXHUME generates integration meshes and extraction operators outside of the confines of a particular finite element code so it may be paired with existing codes with little implementation effort. A key goal of the project is demonstration of EXHUME by connecting it to existing research codes and the popular FEniCS toolchain for finite element analysis. An effort parallel to software development explores accuracy versus efficiency trade-offs associated with 1) approximations made during extraction and 2) novel numerical quadrature schemes for cut cells. The breadth of EXHUME's technical impact is ensured by several factors: 1) the ubiquity of PDEs across nearly all disciplines of science and engineering, 2) the library's interoperability with existing finite element codes, and 3) the generic nature of the EXHUME+FEniCS demonstrative example, which can be applied to arbitrary systems of PDEs. By simplifying the setup of PDE-based computational models, EXHUME+FEniCS enables classroom demonstrations simulating complicated physical scenarios without letting the technical details of numerical methods distract from the scientific principles being taught.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104116","Collaborative Research: Elements: SciMem: Enabling High Performance Multi-Scale Simulation on Big Memory Platforms","OAC","Software & Hardware Foundation, Software Institutes","06/01/2021","05/05/2021","Dong Li","CA","University of California - Merced","Standard Grant","Tevfik Kosar","05/31/2024","$459,899.00","Liang Shi, Yanbao Ma","dli35@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","CSE","7798, 8004","077Z, 7923, 7942, 8004","$0.00","Increasing system scalability is crucial to improving nation?s computation capabilities for scientific applications. However, some applications often face the scalability challenge from the perspective of memory capacity. This is especially true in multi-scale simulations when handling massive simulation data from different scales. The emerging big memory infrastructures have shown great potential to increase the simulation scale and solve larger numerical problems. However, using big memory architectures for the multi-scale simulation is challenging, because of limited computing capability in the big memory machines and memory heterogeneity introduced by big memory. There is a lack of a software infrastructure that can release the full power of big memory to accelerate multi-scale simulation. This project aims to create a capability and a software package (named SciMem) that enables high performance multi-scale simulation on big memory platforms. The techniques presented offer a path for general use of this structure for a wide variety of applications having a broad impact on science and engineering. There will be impact on the students through their direct involvement with the project and through the integration with the educational activities.<br/><br/>The project will enable high performance multi-scale simulations on big memory platforms through more efficient utilization of large and heterogeneous memory machines. Specifically, it will replace computations with pre-computed and stored in memory data on a heterogeneous computing systems. The developed tool, SciMem, will be integrated and tested with the popular parallel molecular dynamics simulator, LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator). The developed improvements in the use of computational resources will allow more accurate models of complex physical phenomena to be carried out on the emerging hardware systems. SciMem aims to bring a 10x performance improvement for certain larger-scale multi-scale simulations widely applied in the fields of computational chemistry and material science, e.g., quantum mechanical/molecular mechanical-based molecular dynamics (MD) simulation of catalysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841694","Exploring Introduction of High Performance Computing and Big Data in High Schools","OAC","EDUCATION AND WORKFORCE","08/01/2018","08/04/2018","Yumei Huo","NY","CUNY College of Staten Island","Standard Grant","Alan Sussman","07/31/2020","$50,000.00","Feng Gu","Yumei.Huo@csi.cuny.edu","2800 Victory Boulevard","Staten Island","NY","103146609","7189822254","CSE","7361","026Z, 062Z, 7556, 9102","$0.00","Many high schools across the nation offer programs in science, technology, mathematics, and engineering (STEM), and a lot of high school students pursue related majors in the post-secondary study. To meet the urgent workforce need, early exposure to high performance computing and big data in high schools can help attract more students to choose related majors in colleges. This requires the stakeholders including academia, industry, high school administration, and instructors to work together to address the concerns. This project has the experts from academia provide training support to high school instructors in high performance computing and big data, and the professionals from industry helping bring the cutting-edge technologies and real life examples and experience to high school administration and instructors. High school instructors are expected to develop and offer high performance computing and big data courses in their schools after the training. By inviting all the stakeholders and training high school instructors, the workshop aims to collect feedback about contents of materials, training effectiveness, and covered topics for developing a complete training model for high school instructors.  This project, thus, aligns with NSF's mission to promote the progress of science by exploring scientific workforce development pipeline.<br/><br/>The workshop invites high school administration and instructors from New York Metropolitan Area in an effort to provide exposure of high performance computing and big data, especially instructors from science, technology, mathematics, and engineering, through carefully designed tutorials. In addition to improving the awareness via tutorials and invited talks, the workshop also aims to prepare for developing an informal and cross-training model for high school instructors and designing the related advanced placement or elective courses for high school students. By convening three parties including academia, industry, and high schools and looking at the problem from different perspectives, the organizers aim to gain new insight into the education of high performance computing and big data at high school level and obtain crucial evidences to design and implement the cross-training model. The trained high school teachers are expected to infuse the training materials into their computer science related courses and/or develop new advanced placement or elective high performance computing and big data courses. The findings from the workshop are broadly disseminated in the research and educational communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103986","Elements: RHAPSODY: Runtime for Heterogeneous Applications, Service Orchestration and DYnamism","OAC","Software Institutes","09/01/2021","04/26/2021","Matteo Turilli","NJ","Rutgers University New Brunswick","Standard Grant","Tevfik Kosar","08/31/2024","$599,908.00","","matteo.turilli@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8004","077Z, 7923, 8004","$0.00","The end of traditional scaling paradigms coupled with innovations in machine learning are driving unprecedented changes in the formulation of scientific applications as well as the nature of high-performance computing (HPC) software and application ecosystems. As a consequence, scientific applications increasingly depend on heterogeneous components with diverse computational characteristics and performance challenges, coordinated in agile and innovative ways. The Runtime for Heterogeneous APplications, Service Orchestration and DYnamism (RHAPSODY) addresses challenges arising from heterogeneity and need for performance. It will enable the effective and efficient execution of scientific applications at unprecedented scale and on a variety of current and upcoming HPC platforms.<br/><br/>RHAPSODY is a scalable, fault-tolerant and portable runtime system to enable scientific communities to innovate by effectively executing applications composed of heterogeneous components on current and upcoming computing platforms. RHAPSODY provides missing runtime capabilities to application and middleware developers, fostering the integration of existing software systems and, ultimately, a more efficient use of existing resources. Together, these contributions make RHAPSODY a tool to advance scientific discovery in multiple domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104104","Elements: Spatial Ecology Gateway","OAC","Capacity: Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes","07/01/2021","05/21/2021","Robert Sinkovits","CA","University of California-San Diego","Standard Grant","Amy Walton","06/30/2024","$600,000.00","Jesse Lewis, James Sheppard","rssinkovits@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","168Y, 7726, 8004","077Z, 7923","$0.00","Spatial ecology is the study of how landscape characteristics influence the distribution and movement of organisms within their environment. The Spatial Ecology Gateway (SEG) enables researchers, students and wildlife managers to upload biotelemetry data, typically GPS readings, and construct home ranges that allow them to interpret animal space use. Applications of the SEG can include classroom projects, basic research into problems in wildlife ecology, environmental impact studies and mitigation of adverse outcomes such as habitat fragmentation or increased human-wildlife interaction resulting from new development. The SEG insulates users from the underlying computational details so that they can focus on their science rather than mastering the technology. Users of the SEG have the option to generate two-dimensional (2D) or, where applicable, three-dimensional (3D) home ranges. Users are also provided with less computationally intensive tools to perform exploratory analyses.<br/><br/>The SEG is built using the HubZero platform, an open-source software platform for building websites that support scientific activities and leverages the Extreme Science and Engineering Discovery Environment (XSEDE) for more computationally demanding tasks. Data can be pulled directly into the SEG from the online community platform Movebank using their REST API. The SEG deploys the Brownian Bridge Movement Model (BBMM), Potential Path Volumes (PPV) and Continuous-Time Movement Modeling (ctmm) methods for home range construction. Tools for exploratory analysis of animal trajectories include net squared displacement, movement path tortuosity and relocation sampling rates versus deployment time. Users can apply basic filtering to their data sets to restrict analyses to specified time ranges or particular animals. The SEG insulates users from having to decide computational details such as choice of resource, number of compute cores or wall clock time. Rather, these decisions are made using logic built into the gateway and based on extensive benchmarking studies.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104004","Elements: Spatiotemporal Analysis of Magnetic Polarity Inversion Lines (STEAMPIL)","OAC","SOLAR-TERRESTRIAL, Software Institutes, EarthCube","07/01/2021","05/21/2021","Berkay Aydin","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Amy Walton","06/30/2024","$599,879.00","Petrus Martens","baydin2@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","1523, 8004, 8074","077Z, 4444, 7923, 8004","$0.00","Extreme space weather events such as solar flares, coronal mass ejections, energetic proton events and geomagnetic storms can cause massive disruptions in many technologically complex systems, including radio communications, telecommunication and navigation satellites, electrical power systems, or space and even commercial airline flights.  This project builds a detection and analysis cyberinfrastructure, and investigates one of the most distinctive features in the solar atmosphere ? magnetic polarity inversion lines, which are hotspots of the most intense eruptive activity. Analyzing these features enables solar physicists to advance understanding of extreme space weather events and provide needed predictive capabilities for space weather forecasters.<br/><br/>This project creates an innovative and sustainable software infrastructure to detect, characterize and analyze polarity inversion lines. The first step toward that objective is the identification of polarity inversion lines, and quantitative characterization of these multi-faceted features through image descriptors. In subsequent stages, this project analyzes the time series of these features and descriptors using advanced machine learning and data mining techniques, specifically for improving space weather forecasting capabilities. This includes analyzing the spatiotemporal patterns of emergence and disappearance for polarity inversion lines, selecting and understanding important shape characteristics of these lines pertinent to solar eruptive activity, and creating a prototype eruption forecasting system with discovered precursors. Automatically identifying and analyzing polarity inversion lines has several direct benefits: physically understanding solar magnetic shear layers and the transition from typical non-eruptive active region states to intense, eruptive ones; making contributions to forecasting of solar eruptions; and generating descriptors and measures that can be useful to the study of shear layers in nature.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Solar Terrestrial Physics Program and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841768","EAGER: Advanced Cyberinfrastructure Training for Modeling Physical Systems Research","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","09/01/2018","07/20/2018","Joel Giedt","NY","Rensselaer Polytechnic Institute","Standard Grant","Alan Sussman","08/31/2022","$300,000.00","Vincent Meunier, Christopher Carothers","giedtj@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","044Y, 7244","026Z, 7361, 7569, 7916, 9179","$0.00","There is a critical national need for the development of scientists with expertise in modeling and simulating physical systems on the most powerful computers available.  Many pressing problems require this scale of extreme computing in order to make significant advances that contribute to the national welfare and security, such as stewardship of the nuclear stockpile, developing advanced energy technologies, and discovering new physical interactions and materials that may revolutionize technological sectors of the economy.  In order to address this need, a series of two-week summer schools, each preceded by a three-day computational boot camp, is being hosted at Rensselaer Polytechnic Institute.  This series is aimed at graduate students and advanced undergraduate students.  The project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.  The boot camp focuses on basic computer skills that are needed for the more advanced material and methods that are to be covered in the summer school.  It will be offered to a subset of the summer school students who need this basic training, since many students from across the country come from colleges and physics programs where adoption of cyberinfrastructure, computational methods and data-intensive computing is far below the level desired by practitioners in advanced scientific computing.  The project is specifically addressing the compartmentalization between computer science versus physics curriculum by bridging that gap through an intensive hands-on learning experience from experts who have developed this type of interdisciplinary agility.  This project is performing significant outreach to encourage women and underrepresented minorities to participate, coupled with specific recruiting targets for this cyber-training series.<br/><br/>The boot camp and summer school build on existing activities at Rensselaer, which have successfully integrated computation into the physics curriculum, including three computational courses and a computational physics concentration that have been developed for physics majors.  This NSF project is scaling out these types of activities to a national stage, where students and lecturers are brought from across the country to interact and learn around lectures and activities that illustrate cutting edge physical modeling with advanced algorithms and hardware.  Participants in the summer school have access to the significant supercomputing facility at Rensselaer, the Computational Center for Innovations (CCI), which includes an IBM BlueGene/Q platform.  This facility is being used for hands-on training activities for the students, thereby exposing them to computing at the highest levels, in preparation for a career at the forefront of modeling and simulating physical systems.  The boot camp and summer school are resulting in world-class instruction in basic computer skills and the use and design of highly optimized and efficient codes and algorithms for solving important problems in the physical sciences.  Each summer school is culminating with a competition in which students work in teams to solve significant computational problems related to the physical sciences, with a poster session and prizes awarded. Domain specific topics that are included in the summer school are density functional theory, lattice gauge theory, numerical relativity, big data applications in physics and astronomy, quantum finite elements and quantum many-body simulations.  Since the goal for attendance is 36 summer school students per year (18 to the boot camp), the project activities are having a direct transformative impact on 72 scientists over the two-year period of this project.  In order to create an even greater impact, on-demand tutorials are being posted on an RPI website, based on the video lectures, exercises and activities that are developed through the course of the boot camp and summer school.  The course materials and videos of the lectures are also being made available on the website, along with e-mail announcements to physics departments around the country, in order to encourage integration and adoption of the curriculum.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2048068","CAREER: Scalable Remote Sensing Computational Framework for Near-real-time Crop Characterization","OAC","CAREER: FACULTY EARLY CAR DEV","07/15/2021","07/14/2021","Chunyuan Diao","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Alan Sussman","06/30/2026","$306,212.00","","chunyuan@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1045","1045, 9102","$0.00","The increasing proliferation of earth observation satellites, along with the  explosive growth of remote sensing data, has dramatically facilitated timely land surface characterization worldwide. With several national and international agricultural initiatives, near-real-time crop type characterization has become vital for providing early warnings on food insecurity and timely crop yield forecasting, and for global food market transparency. However, near-real-time crop type characterization remains a challenge in agricultural remote sensing, due to the difficulty in collecting timely crop ground reference data, the limited generalizability of existing characterization models, and the lack of appropriate remote sensing cyberinfrastructure. Recent advances in satellite remote sensing and computational cyberinfrastructure open a new avenue to tackle the challenge. The overarching goal of the project is to establish a scalable remote sensing computational framework for near-real-time crop type characterization and to promote computational remote sensing education. The computational framework can transform the  large-scale agricultural monitoring paradigm to meet the timely crop characterization requirements of global agricultural initiatives. The framework can substantially boost the ability to respond rapidly to emerging food crises, as well as create cross-cutting impacts in advancing a broad spectrum of remote sensing and agricultural research. The synergistic education and outreach activities offer unique learning opportunities about computational remote sensing to students from K-12 to the  graduate level, and will broaden the participation of underrepresented students in computing. These activities also facilitate the open development and adoption of the computational framework across a range of disciplines. Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity, and welfare.<br/><br/>The advanced remote sensing computational framework focuses on the development of a benchmark data repository called CropSight, a crop characterization modeling system, and a cutting-edge remote sensing cyberinfrastructure, to catalyze near-real-time crop and land surface characterizations. CropSight is a unique national-scale crop ground reference data repository, and embodies a wealth of season-long remotely sensed crop growth and environmental attributes across crop growing locations for most crop types in the U.S.  CropSight can be generalized to continental and global scales, and will be used as a large-scale, systematic, and consistent ground reference data repository. The crop characterization system comprises a suite of novel deep learning-based computational models that can fuse the imagery from a set of earth observation satellites for timely crop monitoring, as well as identify varying crop types via innovative modeling of complex crop-environment interactions. The system will increase modeling generalizability for crop type characterization, and holds considerable potential to be extrapolated over wide geographical regions. The remote sensing cyberinfrastructure will include a highly scalable and cloud native implementation of the CropSight and a near-real-time on-demand crop monitoring system. With a serverless architecture, the project will build the cloud middleware that integrates various geospatial data sources and enables data-intensive remote sensing data analytics for timely crop characterization. The cyberinfrastructure will empower the paradigm shift from conventional compute-limited remote sensing analysis to planetary-scale massive imagery analysis for timely land surface monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814225","Student Support: IEEE Cluster 2018 Conference","OAC","Information Technology Researc, EDUCATION AND WORKFORCE","05/15/2018","05/10/2018","Amanda Randles","NC","Duke University","Standard Grant","Alan Sussman","02/29/2020","$20,000.00","","amanda.randles@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1640, 7361","7556, 9102","$0.00","It is important that the STEM community identifies and develops the next generation of scientists through engagement in premier scientific conferences. The IEEE Cluster conference series, an international event for presenting the research results, problem solutions, and insights on new challenges in high performance computing in general and cluster computing in particular, has been<br/>pursuing this mission by seeking to increase student participation in the conference and the cluster computing field. To this end, the student program at the Cluster conference has been established to provide a comprehensive means for students to improve their overall research skills and planning rather than just presenting posters and papers. By attending the student program, students can now obtain unique experiences and interactions with academic and industry researchers in the cluster computing community. The overall exposure to research through the IEEE Cluster student program promotes foundational understanding of scientific methods, which will serve the students well throughout their careers, and is aligned with NSF?s mission of promoting progress of science.<br/><br/>This award supports the travel of up to 20 students from US-based institutions to participate in the student program at IEEE Cluster 2018. The student program at IEEE Cluster 2018 includes multiple sessions scheduled during the lunch breaks and after the end of the regular sessions. The sessions target research presentation training, research experience and career guidance, industry interaction, and feedback from the best paper candidates. Travel grants particularly encourage the research interests and the involvement of students in the field who are not well funded and those who are just beginning their participation in the field or are interested in entering it.<br/>Particular effort is made to solicit applications for the travel support from female students and students from under-represented communities by reaching out to graduate programs of computer science and engineering departments at universities who are members of the National Consortium for Graduate Degrees for Minorities in Engineering and Science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832257","Collaborative Research: Mentoring the Next Generation of Parallel Processing Researchers at IPDPS and other IEEE-CSTCPP Sponsored Conferences","OAC","Information Technology Researc, EDUCATION AND WORKFORCE","05/01/2018","05/08/2018","Jaroslaw Zola","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","04/30/2020","$25,000.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1640, 7361","7556, 9102","$0.00","From their attendance to conferences, students can form professional connections and establish collaborations, market themselves to potential employers, get exposed to a wider network of mentors, and find inspiration for novel research avenues. Student mentoring programs play an important role in providing younger attendees with the necessary tools to take advantage of their participation at conferences. Student programs have emerged in several leading conferences, but are still far from being widely adopted. In particular the International Parallel and Distributed Processing Symposium (IPDPS) has been hosting a PhD forum and student program for over a decade. This program has evolved into a comprehensive training workshop that includes sessions on career planning and hands on tutorials on writing and presentation skills; it also provides effective opportunities for student networking. This project seeks to expand the IPDPS mentoring and outreach model into other conferences that fall under the broad field of parallel and distributed processing. Parallel and distributed processing has become increasingly important for a variety of disciplines where traditional computational methods lack the mechanisms to deal with large data volumes or expensive computations. As stated by NSF's mission, this project supports education and diversity, while promoting the progress of science by mentoring the next-generation workforce in the high performance computing discipline.<br/><br/>This project has two concrete goals: 1) supporting student participation at the 32nd IEEE International Parallel and Distributed Processing Symposium. IPDPS is an international conference for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation. 2) promoting the adoption of student mentoring programs in other IEEE TCPP conferences, such as: AICCSA, DCOSS, DS-RT, e-Energy, HiPC, ICCABS, ICPADS, NAS, PACT, PERCOM. The beneficiary conferences are required to facilitate an introductory session where students network with peers and mentors. Additionally, conference organizers are provided with the logistic plan of IPDPS?s PhD Forum and are encouraged to have similar proven activities; including a poster session and mentoring workshops. Only students currently studying at U.S. universities are eligible to receive support from this NSF-funded project. This project targets especially students that typically cannot attended these conferences without financial support, such as undergraduate students, graduate students in their first years, and students attending their first conference. In order to increase the diversity of attendees, the project strongly encourages the participation of females and other underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762045","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","09/01/2018","08/27/2018","Rayid Ghani","IL","University of Chicago","Standard Grant","Wendy Nilsen","09/30/2020","$108,998.00","","rayid@cmu.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","024Y","028Z","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities. (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2016981","Collaborative Research: CyberTraining: Implementation: Medium: The Informatics Skunkworks Program for Undergraduate Research at the Interface of Data Science and Materials Science","OAC","CyberTraining - Training-based, DMR SHORT TERM SUPPORT","09/01/2020","10/20/2020","Mahmood Mamivand","ID","Boise State University","Standard Grant","Alan Sussman","08/31/2024","$152,000.00","","mahmoodmamivand@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","044Y, 1712","054Z, 062Z, 095Z, 1711, 9150, 9178","$0.00","The project will develop a sustainable and scalable approach to train a workforce skilled in research and application of machine learning (ML) in materials informatics. ML in materials informatics is rapidly transforming materials science and engineering (MS&E) by an unprecedented ability to extend materials databases, improve materials simulation, mine texts, automate materials research and development, and accelerate materials design. As pointed out by multiple recent studies, including from the National Academies and the Minerals, Metals & Materials Society (TMS), it is essential to train a next generation workforce in ML for materials informatics to realize its enormous potential for improving the human condition through advanced materials. Unfortunately, ML in materials informatics is almost completely absent from today's materials curricula at the undergraduate level. However, the power of informatics tools combined with their rapid evolution and relative novelty in MS&E creates an opportunity for engaging undergraduates (UGs) with active learning through impactful research. With this motivation, the project will create new infrastructure and an ecosystem for the engagement and training of UGs across the U.S. in research using applied ML in materials informatics, called the Informatics Skunkworks. The Skunkworks consists of mentor/UG teams performing research in materials informatics. The project provides the teams with new resources, consisting of curricula, software, and research problems, and with a community of practice to support research and to work effectively and collaboratively. The low cost and accessibility of ML and materials informatics creates an opportunity for Skunkworks to engage mentors and students with limited research resources, particularly at institutions serving underrepresented groups. The Skunkworks is a sustainable and scalable approach that can fulfill this unmet need by training a diverse workforce skilled in research and application of ML for materials informatics. <br/>   <br/>The project will provide freely available (a) curriculum to train UGs in relevant ML, materials informatics and research professional development, (b) software tools that augment existing ML packages to be UG accessible, and (c) authentic and appropriate-level research problems. The proposed work will also develop a community of practice to enable a network of productive mentor/UG research teams to effectively and collaboratively use the curricular, software and other resources developed by the project to support transforming the future workforce. The intellectual merit of the proposed work is to (a) develop scalable resources to increase UG experience and learning in research at the boundary of data science and materials science and engineering, (b) grow a community of mentors and UG researchers engaged in materials informatics research, and (c) increase the utilization of data science tools for solving critical problems in MS&E and related fields through workforce development and materials informatics training. The broader impact of the proposed work is to (a) freely disseminate enabling curricula and tools for materials informatics, (b) train staff, mentors and UGs in broadly applicable research and professional skills, and (c) develop a diverse community of practice for materials informatics researchers. The project will enable the development of a new workforce capable of advanced materials informatics, especially for underrepresented groups, by supporting primarily UG institutions and community colleges that often have limited research resources. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004642","Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples","OAC","ICB: Infrastructure Capacity f, Info Integration & Informatics, Software Institutes, EarthCube","08/15/2020","10/15/2020","Neil Davies","CA","University of California-Berkeley","Standard Grant","Alan Sussman","07/31/2024","$887,476.00","Christopher Meyer","ndavies@moorea.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","085Y, 7364, 8004, 8074","077Z, 7925, 8004","$0.00","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.<br/><br/>The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003740","RUI: Collaborative Research:  CDS&E:  A Modular Multilayer Framework for Real-Time Hyperspectral Image Segmentation","OAC","CDS&E","08/01/2020","07/23/2020","Semih Dinc","AL","Auburn University at Montgomery","Standard Grant","Tevfik Kosar","07/31/2023","$207,664.00","","sdinc@aum.edu","P. O. Box 244023","Montgomery","AL","361244023","3342443249","CSE","8084","026Z, 8084","$0.00","The analysis of images has been used by the scientific community to solve challenging problems and to get insight into diverse natural, social, and technical phenomena. Different types of images have been employed in various areas of study. One example is the hyperspectral images, which have higher resolution when compared to conventional camera images. Analyzing such images has its challenges. For instance, it is computationally demanding, and traditional methods have some limitations. This project provides an efficient solution to analyze such images, by exploiting high-performance computing tools and machine learning techniques. The resulting methods are applied to image-based atmospheric cloud detection.<br/><br/>The project develops a real time, multi-layer, and modular segmentation framework for hyperspectral images. The developed framework automatically identifies various regions within a hyperspectral image by classifying each pixel of the image and associating them to class segments. The developed system is multi-layer, where each layer?s responsibility is to perform an operation on its input, generate region classification data, and pass the resultant output to the next layer. Importantly, each layer analyzes its input from distinct viewpoints, utilizing spectral and spatial data, resulting in a multi-layer framework where the layers complement each other. Also, this project aims to provide an optimized high-performance (speed-up and accuracy) computational tool for real-time hyperspectral image analysis. This is achieved by adapting the algorithms used in the different parts of the model for parallel processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2037773","RAISE: A Materials Science Gateway for X-ray Imaging and Modeling of Microstructures","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","08/01/2020","07/20/2020","Jorge Vinals","MN","University of Minnesota-Twin Cities","Standard Grant","Amy Walton","07/31/2022","$993,220.00","Mark Miller, Matthew Miller","vinals@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1253, 1712, 7726","026Z, 049Z, 054Z, 062Z, 094Z, 9216","$0.00","Nontechnical Summary<br/>This award supports the development and deployment of a Science Gateway, a set of tools, computational materials-science models, and x-ray scattering data that are integrated and available through single web-based location. This project will facilitate the synergistic interaction of computational, and experimental research to accelerate progress on developing understanding of how materials deform, and in some cases fail, that will have impact on technological needs. Advances in the development of new structural materials with specified properties require improved diagnostics across a wide range of length and time scales. Examples of technological needs include: the development of high strength, low density metals that can be significantly deformed without rupturing or breaking for light weight applications in a vast variety of aerospace, power generation, and structural materials; the capability to make accurate predictions of residual stress and relaxation and recovery in additive manufacturing and metal forming; and reliability of interconnects in semiconductor chip technology. Advanced x-ray diffraction imaging from synchrotron facilities provides the necessary diagnostic information about the three-dimensional structure of complex materials with unprecedented resolution. Synchrotron facilities produce radiation, and of particular interest here, high energy x-rays, by accelerating electrons very close to the speed of light, capturing radiation that occurs from further acceleration by magnetic fields, and directing it to instrumentation that can perform experiments by scattering high energy x-rays off the atoms in materials. Instrumentation at the Cornell High Energy Synchrotron Source can perform advanced x-ray scattering experiments and provide high resolution data. Key aspects of the technique include being nondestructive in hard materials, and fast enough and of high enough resolution to observe structural changes at length scales below one micron in materials samples as they are subjected to thermal, mechanical, or other types of loading. Such unprecedented resolution results in the creation of very large datasets, uniquely rich in information, but difficult to mine and analyze, except by a very small number of experts. This is a significant bottleneck to progress. <br/><br/>The Science Gateway is aimed to facilitate the mining and analysis of extremely large datasets by bringing together software engineers, data scientists, and disciplinary scientists. This is transformational in how Gateways are architected, and equally importantly, in how large experimental facilities are operated. The Gateway is designed to provide seamless access to new data being produced, to data reduction and reconstruction codes, and to modeling tools, all in a curated and user-friendly environment. The Gateway accelerates the feedback loop between data, tool creators, and tool users, and therefore the overall discovery process. It is also empowers investigators, by providing broad community access to data and sophisticated analysis tools no investigators who are not specialists in the techniques and technical areas that created them. The developments supported under this award may prove useful in developing Science Gateways to accelerate progress on other fundamental problems relevant to materials research.<br/> <br/><br/>Technical Summary<br/>This award supports the development and deployment of a Science Gateway, a set of tools, computational materials-science models, and x-ray scattering data that are integrated and available through a single web-based location. It is well recognized that advances in the development of new structural materials with specified properties require improved diagnostics across all the relevant length and time scales. Examples of technological needs include: development of high strength, high ductility, low density metals for light weight applications in a vast variety of aerospace, power generation, and structural materials; accurate predictions of residual stress and relaxation and recovery in additive manufacturing and metal forming, for example in the automobile and defense industries; and improved reliability of interconnects in semiconductor chip technology. Advanced x-ray diffraction imaging at leading synchrotron facilities uniquely provides the necessary diagnostic information about the three-dimensional structure of complex materials with unprecedented resolution.  Key aspects of the technique include being non-destructive in hard materials, and fast enough and of sufficient resolution to observe structural responses at the sub-micron scale in samples as they are subjected to thermal, mechanical, or other types of loading. Such unprecedented resolution comes with very large datasets, uniquely rich in information, but difficult to mine and analyze, except by a very small number of experts. This is a significant bottleneck to progress.<br/><br/>This program develops an open cyberinfrastructure in the form of a public Science Gateway to serve the Cornell High Energy Synchrotron Source. The gateway is based on the Galaxy framework. Raw data is accumulated locally at the beam lines, and ingested into a Galaxy instance using standard web application programming interfaces. Data is typed, metadata is attached, and is made available through a shared data library. The gateway then provides the infrastructure for user defined transformations, including data reduction, image reconstruction, and feature analysis, while retaining metadata and provenance information. The gateway supports data transfer to XSEDE resources, as Galaxy natively orders individual codes according to category, and maps the execution of the designed workflows to the necessary resources without user awareness of their location. Workflows including visualization and modeling are also supported. The gateway is extensible by users of the facility and the broader community as new analysis and modeling tools are developed.<br/><br/>The developments supported under this award may prove useful in developing Science Gateways to accelerate progress on other fundamental problems relevant to materials research.<br/><br/>This project is jointly supported by the Office of Advanced Cyberinfrastructure in the Computer and Information Sciences Directorate, and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004601","Frameworks: Bayesian Analysis of Nuclear Dynamics","OAC","COMPUTATIONAL PHYSICS, PHYSICS AT THE INFO FRONTIER, Software Institutes","07/01/2020","08/09/2021","Daniel Phillips","OH","Ohio University","Continuing Grant","Amy Walton","06/30/2025","$3,566,619.00","Stefan Wild, Frederi Viens, Richard Furnstahl","phillid1@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","7244, 7553, 8004","075Z, 077Z, 7569, 7925, 9102","$0.00","Nuclear physicists seek an accurate description of the properties of atomic nuclei, collisions between nuclei, and extreme environments such as the first few seconds of our universe or the interior of a neutron star. These situations involve many particles interacting through complex forces. They?re each described by a number of different models that typically explain accurately results of existing experiments. The models don?t do as well predicting what will happen in future experiments or in environments that are inaccessible here on Earth. The Bayesian Analysis of Nuclear Dynamics (BAND) Framework will use advanced statistical methods to produce forecasts for as-yet-unexplored situations that combine nuclear-physics models in an optimal way. These will be more reliable than the predictions of any individual model. BAND?s forefront computer codes will be widely available and will facilitate the design of nuclear-physics experiments that can deliver the largest gain in understanding. The adoption of BAND?s tools in other sciences dealing with ?model uncertainty? could spur broad scientific innovation. Undergraduate and graduate students working on BAND will gain a broad range of technical skills in data science, machine learning, nuclear physics, and high-performance computing.<br/><br/>Nuclear physicists seek a quantitative description of strongly-interacting matter. Sophisticated models of how neutrons and protons interact in the nucleus, extreme environments, and collisions between nuclei have been key to the great progress made towards this goal. These models typically describe extant data well, but often yield divergent predictions for future experiments. The Bayesian Analysis of Nuclear Dynamics (BAND) framework will be a broadly available set of computational tools built through intensive collaboration between statisticians, computer scientists and nuclear physicists. It will combine the results of several models, incorporating prior knowledge and experimental data for each, to produce a full assessment of the uncertainty in nuclear-physics predictions. This will enable quantitative evaluation of the impact of future experiments, accelerating the theory-experiment feedback loop and spurring innovation. It will also help quantify uncertainties for terrestrially inaccessible environments, such as the core of neutron stars or the first microsecond after the Big Bang. Similar challenges are faced by researchers modeling complex dynamics in other sciences, so BAND?s tools will have broad appeal. Undergraduate and graduate students working on BAND will gain expertise in statistical methods and nuclear physics, as well as experience with large-scale computing and machine learning.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics and the CDS&E program in the Division of Mathematical Sciences within the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004157","Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics","OAC","Software Institutes","07/01/2020","04/01/2020","Steven Brandt","LA","Louisiana State University","Standard Grant","Amy Walton","06/30/2024","$427,321.00","Peter Diener","sbrandt@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","107y, 8004","069Z, 077Z, 7569, 7925","$0.00","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science.  The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.<br/><br/>The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure.  The software is designed to simulate compact binary stars as sources of gravitational waves.  This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: <br/>?  CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;<br/>?  NRPy+ -- a user-friendly code generator based on Python; and <br/>?  Canuda -- a new physics library to probe fundamental physics.  <br/>Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit.  The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components.  Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community.  The team is also creating a science portal with additional educational and showcase resources. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004658","Elements: Open Access Data Generation Engine for Bulk Power System under Extreme Windstorms","OAC","EPCN-Energy-Power-Ctrl-Netwrks, Software Institutes, CDS&E","07/01/2020","03/19/2020","Ge Ou","UT","University of Utah","Standard Grant","Amy Walton","06/30/2023","$498,032.00","Mostafa Sahraei Ardakani, Zhaoxia Pu","ge.ou@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7607, 8004, 8084","008Z, 040E, 077E, 077Z, 155E, 7923, 8084, 9102, 9263","$0.00","The electric power grid is a critical infrastructure that is regularly disrupted by natural disasters.  Currently, there is a lack of correlated data on windstorms and the physical and cyber power system infrastructure. This project addresses that gap in knowledge in dealing with high wind disaster events for preventative and restorative resiliency of electric power networks.  By integrating multiple data sources into a robust simulation tool, researchers can design new methods to mitigate the impact of hurricanes and other extreme wind events on power system operations. The outcomes of the research would positively impact reliability, resiliency, and delivery of electric power to US population centers.<br/><br/>The framework is unique and will enable inter-disciplinary research between atmospheric sciences, civil engineering, and electric power engineering. A preliminary result is provided for the Texas power system (using a digital surrogate model of the physical infrastructure and the Texas synthetic power network) under the impacts of Hurricane Harvey. The calculated damage using the model results in very similar structural damage and power systems impact as the real disaster (the ground truth model).  In addition to three deliverable test cases (Texas, New York, and Florida), the project will design automated tools to create new test cases in the future for other researchers.  The power system digital surrogate, correlation of datasets, and overall data generation engine will enhance the national cyberinfrastructure ecosystem.  <br/>  <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Civil, Mechanical and Manufacturing Innovation (CMMI) and the Division of Electrical, Communications and Cyber Systems (ECCS) within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004954","Elements: Development and Dissemination of a Slurm Simulator","OAC","Software Institutes","06/01/2020","05/19/2021","Nikolay Simakov","NY","SUNY at Buffalo","Standard Grant","Amy Walton","05/31/2023","$590,305.00","Robert DeLeon","nikolays@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8004","077Z, 7923, 9251","$0.00","Slurm is an open source resource management and job scheduling system that is widely used on small and large high-performance computing (HPC) systems.  Slurm is highly tuneable, with many settings that can significantly influence job throughput, overall system utilization and job wait times. Unfortunately, in many cases it is difficult to judge how modification of these settings will affect the overall performance of the HPC resource.  This project develops a prototype version of a Slurm simulator that allows HPC personnel to tune Slurm parameters, to optimize throughput or meet specific workload objectives without impacting an HPC system in production.  <br/><br/>The proposed Slurm simulator could have impacts far beyond computer science and the field of scheduling.  The ability to optimize scheduler parameters on production HPC resources, most of which are significantly oversubscribed, has the potential to dramatically improve job throughput for researchers and reduce the ?time to science?.  The simulator would allow many different job scheduling schemes to be rapidly evaluated, and implemented if they are useful or rejected if they are not.  Furthermore, the ability to model the impact of various features that impact job execution time (such as network traffic, parallel file system loads and node sharing) can be explored to determine if they merit additional scheduler development work.  The developed framework would allow researchers in other science and engineering fields to incorporate their models, and study a range of problems affecting the performance and execution of users' jobs on HPC systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1943036","CAREER: Scalable Approaches for Multiphysics Fluid Simulation","OAC","CAREER: FACULTY EARLY CAR DEV","04/15/2020","04/15/2020","Amanda Randles","NC","Duke University","Continuing Grant","Alan Sussman","03/31/2025","$292,452.00","","amanda.randles@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1045","026Z, 1045, 9102","$0.00","Over the past decade, we have seen an emergence of the use of personalized blood flow simulations in medical practice and biomedical research.  These models are used to help design new drugs, devices, and treatments for a wide range of diseases. Large-scale simulations capture both the fluid movement and the interaction of included particles and cells.  This interdisciplinary research will create a tool for researchers to interactively modify the geometry of the device or vessels or properties describing the cells to study how changes influence metrics that can improve treatment design. This project will also provide a framework to facilitate new educational programs at the intersection of computing and biomedical engineering with the goal to promote wider interest in STEM degrees and careers. To engage next generation scientists with computational modeling, the project aims to (i) develop virtual reality-based interactive modules for K-12 students, (ii) develop standards-aligned primary and secondary school classroom curriculum add-ons, and (iii) host implementation workshops to broadly disseminate the material and findings.<br/> <br/>The proposed research program will develop and establish new multiscale, multiphysics modeling techniques that enable users to use parallel fluid-structure-interaction (FSI) models to design new therapeutics in an intuitive and interactive manner. The program couples complementary resources including virtual reality and augmented reality interfaces, massively parallel fluid simulation, and high-fidelity cellular adhesion models. The following key components will be combined: (i) the development of a robust, efficient capability to capture a range of cell types, (ii) a parallel method to initialize high cell densities in complex geometries, and (iii) interactive techniques for design feedback and modification.  The resulting cyberinfrastructure represents a new and potentially transformative FSI engineering paradigm that will lead to advances in fundamental knowledge, more effective research techniques, enhanced clinical capabilities, and cross-cutting impacts that transcend the bioengineering and biomedical fields. The knowledge gained by development of a state-of-the-art, simulation-driven, geometry-interaction methodology will have wide impacts beyond the use cases investigated in the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1942995","CAREER: Next-Generation Infrastructure for Tensor Computations","OAC","CAREER: FACULTY EARLY CAR DEV","08/01/2020","04/09/2020","Edgar Solomonik","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Alan Sussman","07/31/2025","$290,505.00","","solomon2@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1045","026Z, 1045","$0.00","Matrices and their higher-order generalization (tensors) provide a mathematical toolbox for expressing a large variety of algorithms. Consequently, linear algebra operations on dense matrices have served as the backbone of high-performance scientific computing applications. This research aims to translate this benefit to more complex problems, by improving software infrastructure and parallel performance of sparse matrix and tensor operations. The proposed methods will be applied to accelerate analysis of large graphs, approximation of multidimensional datasets by tensor decompositions, and simulation of quantum systems. By providing a high-level library for distributed sparse tensors, the research will improve the development productivity of scientists and engineers from disciplines including chemistry, physics, and bioinformatics. Deployment of tensor-based techniques on massively-parallel computing systems will enable simulations of larger scale and higher accuracy, making new innovations in computational science possible. Additionally, development of web-based educational modules for programming with tensors and understanding parallel performance will make the software and methods accessible to the broader scientific community.<br/><br/>Tensor decompositions and tensor networks are fundamental techniques in approximation of multi-dimensional data and functions. The frontiers of tensor computations in quantum chemistry and data analysis involve methods that contract tensors of different order, size, and sparsity. Recent developments have led to provably efficient algorithms and software for contraction of a pair of dense tensors and multiplication of a pair of sparse matrices. However, in the context of sparse multi-tensor operations, opportunities for asymptotic cost improvements remain. In particular, there is a lack of software and rigorous algorithmic analysis for sparse matrix and tensor computations involving hyper-sparsity and output sparsity, as well as for all-at-once contraction of multiple tensors, which can be advantageous in the presence of sparsity. Further, at the software library level, open problems remain in leveraging layout persistence, reuse of mapping logic, and automated performance modeling. The project will address these gaps in the state-of-the-art of available computational infrastructure by developing new parallel algorithms and systems techniques for sparse multi-tensor contraction. These innovations will be integrated into the Cyclops library and studied in the context of applications in graph analysis, tensor decomposition, and tensor networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1950448","REU Site: Promoting Leadership in Advanced-Research-Computing for INterdisciplinary Sectors (PLAINS)","OAC","RSCH EXPER FOR UNDERGRAD SITES, ","04/01/2020","03/18/2020","Stephen Gent","SD","South Dakota State University","Standard Grant","Alan Sussman","03/31/2023","$400,731.00","Jung-Han Kimn","stephen.gent@sdstate.edu","1015 Campanile Ave","Brookings","SD","570070001","6056886696","CSE","1139, U135","7736, 9150, 9250","$0.00","Our nation's workforce demands graduates who are computationally proficient, can readily apply fundamental research to new challenges, and have both technical and interpersonal skills. The ability to understand and apply high fidelity modeling and simulations is becoming increasingly important in a wide variety of STEM disciplines, including engineering, biology, medicine, and digital agriculture. The PLAINS REU Site will engage STEM undergraduates in collaborative group projects that use High Performance Computing, Big Data, and computationally-intensive models as a central organizing theme. The students represent multiple STEM disciplines that employ similar simulation techniques but have significantly different applications. They will gain a competitive edge in the STEM workforce by combining a broad perspective on the theory and application of computing with training in research integrity and the development of professional skills, such as technical communication and leadership. Through experience, training, and mentoring, participants will emerge from this program with the skills, aptitude, and desire to pursue further study and careers in STEM. This project serves the national interest and advances NSF's mission to promote the progress of science by engaging undergraduate students in state-of-the art research in engineering applications and computation, and promotes the nation's health, prosperity, and welfare by strengthening the STEM workforce and increasing the participation of underrepresented groups and nontraditional students. This site is supported by the Department of Defense in partnership with the NSF REU program.<br/><br/>The goals of this REU Site are to enhance students' abilities to: 1) conduct innovative and meaningful research using high performance computing (HPC) and other computing tools and resources, 2) work in a collaborative research environment, 3) conduct research with integrity and social responsibility, 4) succeed in a professional career, and 5) pursue graduate STEM programs and/or research opportunities post-graduation. Program directors will recruit ten students per year from a diverse pool of first- and second-year STEM majors from traditionally under-represented populations (small rural colleges, community colleges, tribal colleges, and non-traditional students). Each student will work closely with graduate assistant and faculty mentors every day, and with industry mentors several times per week. Participants will learn how high fidelity models can be used to represent physical phenomena, how the models are implemented within a large computing environment, and how the simulation results are analyzed, interpreted, and used to gain insight for theoretical and applied problems. Students will gain valuable experience with simulation tools in the context of engineering analysis, state-of-the-art research tools in computation, statistical analysis based on real datasets and simulations, and advanced numerical methods including parallel algorithms in HPC. Tangible outcomes will include student-authored journal articles, conference proceedings, and technical presentations. The REU Site's success will be assessed by an external evaluator through formative and summative evaluations; student reflections, interviews, and enrollment in STEM graduate programs; and the CISE REU Toolkit.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940789","CDS&E: Compiler/Runtime Support for Developing Scalable Parallel Multi-Scale Multi-Physics","OAC","CDS&E","07/01/2019","09/23/2019","Ponnuswamy Sadayappan","UT","University of Utah","Standard Grant","Tevfik Kosar","06/30/2020","$70,867.00","","saday@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8084","026Z, 7433, 8084","$0.00","The dramatic strides in computer speed and performance over the last few decades make it feasible to accurately model increasingly complex phenomena. However, achieving high performance on massively parallel supercomputers is an extremely challenging task. With deepening memory hierarchies, significantly higher degrees of per-chip multi-core parallelism, the task of programming compute-intensive engineering applications to attain high performance on a large scale cluster system has become increasingly difficult. It is often the case that the time and effort required to develop effective and efficient software has become the bottleneck in advancing many areas of science and engineering. This challenge can be overcome by advances in compile-time/runtime systems that can ease the burden on the programmer while delivering a high performance portable instantiation of the particular application on modern and emerging high performance platforms.<br/><br/>To address this challenge, this project is developing a novel framework for transforming irregular scientific/engineering applications in a global address space framework. The research is grounded in a very different and complementary research direction to most current efforts in addressing the challenge of enhancing programmer productivity, maintaining portability, and achieving good performance on scalable distributed-memory parallel systems. The project will advance compiler/runtime techniques so that users can develop annotated sequential programs, to be automatically transformed by our system for efficient execution on distributed-memory parallel systems. This approach is motivated by the success of the popular OpenMP and OpenACC pragma based approaches to transforming annotated sequential programs for parallel execution on multicore and GPU/accelerator systems, respectively. An annotation based OpenAPP (APP - Asynchronous Partitioned Parallelism) framework is proposed for source-to-source transformation of an important class of scientific/engineering programs using the inspector/executor paradigm for execution on distributed-memory parallel systems. The proposed framework will be validated using several medium to large scale applications.<br/><br/>The project seeks to significantly lower the entry barrier associated with effective use of scalable distributed-memory computers, which are essential if more than 100x performance improvement over sequential codes is sought. A successful outcome of this project will be transformative for computational and domain scientists and engineers who seek to use next generation parallel systems for their simulation and modeling. The developed tools will be made publicly available to the community under an open source license. The project will also organize workshops that bring together compiler/runtime experts and computational scientists developing massively parallel scientific/engineering applications."
"1907321","Support of the Doctoral Symposium at the IEEE International Conference on Autonomic Computing (ICAC)","OAC","EDUCATION AND WORKFORCE","06/01/2019","03/21/2019","Gregory Ditzler","AZ","University of Arizona","Standard Grant","Alan Sussman","05/31/2020","$15,000.00","Salim Hariri","ditzler@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7361","026Z, 7556, 9179","$0.00","Research in cloud and autonomic computing spans a variety of areas, from distributed systems, computer architecture, middleware services, databases and data-stores, networks, machine learning, and control theory. The purpose of the Fifth International Conference on Autonomic Computing (ICAC) is to bring together researchers and practitioners across these disciplines to address the multiple facets of cloud and autonomic computing. ICAC is holding a doctoral symposium during the conference. The doctoral symposium is meant to engage students attending the symposium with interactions that provide them with research and career mentoring opportunities. The overall impact that such a meeting can have on Ph.D. students, both early and late in their studies, can be significant. The research presentations at the Doctoral Symposium will be made publicly. This project, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity, and welfare; or to secure the national defense.<br/><br/>To enable students to increase their interactions with other researchers, experts from both industry and academia will be invited to participate in the symposium program and/or deliver keynote talks at the ICAC doctoral symposium. The primary objectives of the doctoral symposium are to (1) provide a venue for graduate students to present their research and obtain feedback from experts in the area; (2) allow Ph.D. students to present their work at the main conference?s poster session; and (3) mentor the PhD students by providing career advice from representatives in industry as well as academia. Furthermore, the ICCAC doctoral symposium will have a special session devoted to career mentoring for Ph.D. students in attendance. The ICAC organization committee understands the diversity in the background of the Ph.D. students that attend the conference - while some will go into industry there will be those who go into academia. Therefore, the career mentoring session will allow students to listen and interact with experts in both areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845962","CAREER: A Parallel and Efficient Computational Framework for Unified Volumetric Meshing in Large-Scale 3D/4D Anisotropy","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE","03/15/2019","10/15/2020","Zichun Zhong","MI","Wayne State University","Continuing Grant","Alan Sussman","02/29/2024","$395,416.00","","zichunzhong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1045, 7231","1045","$0.00","This proposal develops a computational framework that helps the domain scientists who employ advanced cyberinfrastructure ecosystem (e.g., for engineering, manufacturing, healthcare, etc.) to realistically and efficiently reconstruct, visualize, and analyze 3D and 4D (space-time) volumetric objects with complex geometric structures and highly anisotropic properties (such properties are characterized by the presence of specified orientations and aspect ratios in the system). For example, in mechanical engineering, it is necessary to interactively design and model mechanical parts with user-required high-quality measures and standards. The computational framework enables fabrication of such mechanical parts with specified microstructure that can be efficiently produced to sustain much stronger stress and strain compared with those without endowing such properties, which leads to significant impact on the next-generation mechanical component design. As an integral part of the PI's career development, the educational plan emphasizes on the integration of education and research in different aspects through the PI's new ""3D hands-on"" education philosophy for K-12, undergraduate and graduate students. This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/> <br/>The research goal of this project focuses on a computational framework for anisotropic volumetric meshing, a foundational as well as translational research impacting a broad range of scientific domains.  The capability and usability of the meshing framework are evaluated by investigating fabrication of objects with internal microstructures and construction of anisotropic volumetric models to capture the organ and tissue shape. This work has the following primary components: (1) Computing high-dimensional geometric embedding based on Nash theorem in parallel: the computational realization of high-dimensional geometric embedding makes modeling complex objects with multiple tensor features being built and solved in parallel in a large linear system. (2) Modeling multi-shape of mesh element in a unified particle framework: the particle system flexibly and effectively generates high-quality honeycomb, tetrahedral, and hexahedral (grid) patterns, which are exactly designed for meshing structure. The optimization procedure is easily formulated for parallelism in the high-dimensional space. (3) Generating 3D/4D anisotropic mesh in parallel: the final multi-shape anisotropic meshes are computed in parallel in the high-dimensional space with simple Euclidean computations under the isotropic metric. The primary outcome of this project is a 3D/4D-ParaAnisoMesh system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835144","Elements: Software: NSCI: Efficient GPU Enabled QM/MM Calculations: AMBER Coupled with QUICK","OAC","Software Institutes","09/01/2018","08/28/2018","Kenneth Merz","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2022","$600,000.00","Andreas Goetz","merzjrke@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8004","026Z, 077Z, 7923, 8004, 8005, 9216","$0.00","The over-arching goal of this project is to develop a software cyberinfrastructure aimed at solving important molecular-level problems in catalysis, drug design, and energy conversion. The PI and his collaborators will develop open source software that will enhance the ability to tackle chemical and biological problems using a sustainable model. The PI and his collaborators are collaborating with NVIDIA on this project to help accelerate the development efforts. Finally, the projects undertaken here will train students in formal theory, computer programming, computational chemistry and biology, and manuscript preparation/publication further enhancing the technical workforce in the USA.<br/><br/>Combined quantum mechanical/molecular mechanical (QM/MM) models have enabled significant advances in the understanding of chemical reactivity and intermolecular interactions. This approach allows regions of a system where bonds are to be broken and formed to be modeled using accurate QM methods, while the surrounding environment is treated using classical models. The most widely used QM models in QM/MM studies are generally semiempirical, but the most accurate employ density functional theory (DFT), Hartree-Fock (HF) or post HF methods. The shortcoming when using the more accurate methods is the computational expense, which limits the extent of QM/MM molecular dynamics simulations. The performance of QM methods has been greatly improved over the years through algorithmic and hardware improvements. This project will focus on both: for the former the PI will add the ability to handle long-range interactions in QM/MM calculations, add GPU enabled correlated methods and create an electron repulsion interaction (ERI) engine for general use, while for the latter the PI will integrate the GPU enabled Quantum Interaction Computational Kernel (QUICK) program with the Sander and PMEMD molecular dynamics (MD) engines from the AMBER suite of programs. AMBER is one of the most popular simulations packages and has been supported and sustained by the AMBER developer community for approximately 30 years. The developments proposed here will be fully available to the community via AMBERTools, which is released using an open source model (see http://ambermd.org/AmberTools.php). <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829717","CyberTraining:CIU:Computational and Data Science Literacy Curriculum Exchange","OAC","CyberTraining - Training-based","09/01/2018","06/29/2018","Katharine Cahill","OH","Ohio State University","Standard Grant","Alan Sussman","08/31/2022","$499,734.00","Linda Akli, Dinadayalane Tandabany, Asamoah Nkwanta, Ana Gonzalez","kcahill@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","044Y","026Z, 062Z, 7361, 9102, 9150","$0.00","There is a well-established national need for a research workforce with suitable knowledge of computational and data science (CDS). Although, some strides have been made in integrating CDS competencies into the university curriculum, the pace of change has been slow. Department boundaries, course requirements for STEM majors, and resource constraints all contribute to the slow pace of CDS curriculum implementation across STEM disciplines. Ohio Supercomputer Center (OSC) in collaboration with Bethune Cookman University (BCU), Clark Atlanta University (CAU), Morgan State University (Morgan), Southeastern Universities Research Association (SURA), Southern University and A&M College (SUBR), and the University of Puerto Rico at Mayaguez (UPRM) are implementing a pilot Computational and Data Science (CDS) Curriculum Exchange (C2Exchange) to address the challenges experienced by resource constrained institutions and, in particular, those faced by Minority Serving Institutions (MSIs). This project serves the national interest, as stated by NSF's mission: to promote the progress of science and to serve the national health, prosperity, and welfare by advancing STEM education and improving the computational and data science skills of our future research workforce.<br/> <br/>The curriculum exchange approach is intended to minimize the faculty preparation time required to deliver new and updated courses, increase the number of CDS courses offered at each institution, and allow courses to be offered that may not initially have large enrollments at an individual institution by drawing students from all of the participating institutions. This pilot develops the foundation for a CDS Curriculum Exchange that can increase the capacity of small under-resourced MSIs to deliver undergraduate CDS curriculum, minors, and certificates with low investment and be extensible for future participation by additional resource constrained institutions. Students in these institutions will benefit by having access to a broader selection of computational science courses. Training in these highly sought after skills aims to increase their chances of being part of the STEM workforce or continuing on to graduate school and becoming effective researchers. Each academic institution is contributing and receiving courses, such as Introduction to Modeling and Simulation, Computational Chemistry, as well as Data Visualization, with the goal of providing a sequence of CDS courses that can form part of a certificate or minor program at each institution. The consortium delivers courses through a blended online learning model that has been tested and shown to be effective for technical subjects. This model is flexible enough to reduce preparation time for local instructors, respond to students' difficulties with material, and to accommodate institutions' varied schedules. If successful, this exchange model of instruction will be extensible to a wide range of institutions as well as to specialized instruction at the graduate level.  The project explores important insights into both the institutional and pedagogical questions associated with the implementation of shared course networks in resource constrained environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761931","Spokes: MEDIUM: MIDWEST: Collaborative: Community-Driven Data Engineering for Substance Abuse Prevention in the Rural Midwest","OAC","BD Spokes -Big Data Regional I","09/01/2018","08/27/2018","Amit Sheth","OH","Wright State University","Standard Grant","Wendy Nilsen","11/30/2019","$120,000.00","","amit@sc.edu","3640 Colonel Glenn Highway","Dayton","OH","454350001","9377752425","CSE","024Y","028Z","$0.00","The opioid crisis ravaging Ohio and the Midwest disproportionally affects small and rural communities. Harnessing and deploying data holds promise for developing a response to this crisis by policymakers, healthcare providers, and citizens of the communities. Currently, there are many barriers to getting data into the hands of individuals on the frontlines. Crucial data are siloed across law enforcement, public health departments, hospitals and clinics, and county administrations; data often are inaccurate or collected in non-standard ways across different agencies and departments; the stigma of drug abuse limits accurate reporting of drug-related deaths; and information is not shared with the community and other stakeholders because of the lack of a privacy and security framework. Such barriers, for example, prevent individuals with addictions or their families and friends from locating available treatment centers or obtaining other important information in a timely way. Similarly, it is difficult for first responders and healthcare providers to obtain critical up-to-date information. In predominantly rural counties, these challenges are especially daunting because there is often poor connectivity and communication infrastructure. This Big Data Spoke project involves developing scalable, flexible, and connectivity-rich data-driven approaches to address the opioid epidemic. The cyberinfrastructure framework, OpenOD, will be initially designed and deployed in small and rural communities in Appalachia Ohio and the Midwest, where the need for data and connection are greatest. Based upon significant community input, OpenOD will also create end-user applications or enterprise solutions to support stakeholders and communities to mount a response they feel will be most efficient and beneficial at the local level. As a Spoke to NSF?s Midwest Big Data Hub, our efforts can be efficiently scaled, disseminated, and applied to the opioid and other societal problems such as infant mortality, crime, and natural disasters. This project fits within NSF's mission to promote the progress of science (contribute to the science and engineering of large socially relevant cyberinfrastructures) to advance the health and welfare of US citizens (by linking data sources in new and useful ways to empower communities to address societal problems; establishing sustainable partnerships between academia, industry, government and communities; increasing data literacy and community engagement with data science; and enhancing research and education via development/adaptation of training modules and courses in data analytics).<br/><br/>The main goal of this project is to help small and rural communities in the Midwest address the opioid epidemic via BIGDATA (BD) technology. While no communities have been spared, small and rural communities face unique challenges in confronting the opioid epidemic: knowledge and data exist in siloes across multiple organizations with varying jurisdictional boundaries; efforts to collect, link, and analyze data are hampered by a lack of infrastructure and tools; rural areas are plagued by ""dead zones"" in cellular connectivity; communities lack capacity for data collection, and analytics; needs and resources across effected communities are not uniform and require BD approaches that are flexible, open, leverage significant community input, and can be dutifully validated. Our proposed solution is OpenOD, a framework that provides uniform, relevant and timely access to data. Working integrally with the Midwest Big Data Hub (MBDH) and our partners, our three main objectives are to: (1) Work with local communities to understand strengths and gaps in cyberinfrastructure, data availability, and need for data analytics workforce skills. (2) Assemble flexible cyberinfrastructure that includes a data commons, stakeholder-usable and cloud-amenable data analytics and visualization tools, and internet connectivity with both mobile and non-mobile capabilities. (3) Validate, evaluate, and disseminate cyberinfrastructure and data analytics tools to stakeholder groups throughout the region while fostering new partnerships. OpenOD will create approaches that will allow governing units to deploy openly available tools rather than rely on proprietary tools. In this way, existing disparities in data access and ensuing responses are effectively addressed. The potential contributions of the project are to: (1) Increase BD and STEM literacy and community engagement in underrepresented groups given the operating milieu of OpenOD in rural areas where the population is indigent and lacks adequate skills to join the modern workforce. (2) Improve well-being of individuals in society by linking data sources in new and useful ways to empower communities to address the opioid crisis; improved connectivity and timely delivery of critical information will accelerate community responsiveness and improve preventive strategies. (3) Provide infrastructure for research and education will be improved given that project activities will deliver linked, curated data sets to community stakeholders, researchers and educators. Training modules and courses adapted and developed and shared with local/regional educators and will remain with the communities after the funding period has ended. In addition, new and established partnerships will allow sustainability of the project in the communities for the long-term.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761887","Spokes: MEDIUM: MIDWEST: Collaborative: An Integrated Big Data Framework for Water Quality Issues in the Upper Mississippi River Basin","OAC","BD Spokes -Big Data Regional I","08/01/2018","07/29/2018","Witold Krajewski","IA","University of Iowa","Standard Grant","Martin Halbert","07/31/2022","$599,999.00","Larry Weber, Keith Schilling, Ibrahim Demir, Christopher Jones","witold-krajewski@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","024Y","8083","$0.00","This project will develop a cyberinfrastructure framework to facilitate research on the efficient management of agricultural practices and their impact on water resources in the Upper Mississippi River Basin (UMRB).  Large-scale data acquisition, integration, analysis, and visualization using data-enabled information technologies will accelerate the dissemination of knowledge, experience, and shared resources (e.g., technology, equipment, and people) among communities and partners.  The key element of the project is a new cyber platform, the Upper Mississippi Information System (UMIS), which will provide water quality data within a rich spatio-temporal hydrologic context.  The UMIS directly addresses three of the Grand Challenges for Engineering identified by the National Academy of Engineering: i) provide access to clean drinking water; ii) manage the nitrogen cycle; and iii) engineer the tools of scientific discovery.  The UMIS will immediately begin facilitating data access, integration, and scientific discovery for water quality challenges in the UMRB. <br/><br/>UMIS will offer internet-based open access to water quality information in its meteorological, hydrological, and geographical context, providing almost endless potential benefits for stakeholders.  For example, the experimental design of the UMIS will enable researchers to study spatial scaling, efficiency of various land use and agricultural practices to improve water quality, and the impact of climate change on land management and water quality.  Decision-makers, producers, and extension staff will be able to assess the relative efficacy of local (e.g., best management practices) versus system-level (e.g., state programs) solutions designed to reduce pollution, optimize the use of resources, and evaluate tradeoffs among competing objectives.  For all stakeholders, the UMIS will support partnerships and collaborations, increase dissemination of information about a critical natural resource to empower stakeholders at all levels, and set new standards in the communication of scientific data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751161","CAREER: Building an Advanced Cyberinfrastructure for the Data-Driven Design of Chemical Systems and the Exploration of Chemical Space","OAC","CAREER: FACULTY EARLY CAR DEV, Chem Thry, Mdls & Cmptnl Mthds","03/01/2018","02/08/2018","Johannes Hachmann","NY","SUNY at Buffalo","Standard Grant","Alan Sussman","02/28/2023","$561,685.00","","hachmann@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045, 6881","026Z, 062Z, 1045, 8084, 9263","$0.00","Innovation in chemistry and materials is a key driver of economic development, prosperity, and a rising standard of living.  It also offers solutions to pressing problems on energy, environmental sustainability, and resources that shape our society.  This research program is designed to boost the chemistry community's capacity to address these challenges by transforming the process that creates underlying innovation. The research promotes a shift away from trial-and-error searches and towards rational design.  These combine traditional chemical research with modern data science by introducing tools such as machine learning into the chemical context.  This project enables and advances this emerging field by building a cyberinfrastructure that makes data-driven research a viable and widely accessible proposition for the chemistry community, and thereby an integral part of the chemical enterprise.  Tools and methods developed in this research provide the means for the large-scale exploration of chemical space and for a better understanding of the hidden mechanisms that determine the behavior of complex chemical systems.  These insights can potentially accelerate, streamline, and ultimately transform the chemical development process.  The project also tackles the concomitant need to adapt education to this new research landscape in order to adequately equip the next generation of scientists and engineers, to build a competent and skilled workforce for the cutting-edge R&D of the future, and to ensure the competitiveness of US students in the international job market.  By promoting minority participation in this promising field, it contributes to a sustained push towards equal opportunity in our society.  This project thus promotes the progress of science and advances prosperity and welfare as stated by NSF's mission. <br/><br/>While there is growing agreement on the value of data-driven discovery and rational design, this approach is still far from being a mainstay of everyday research in the chemistry community.  This work addresses three key obstacles: (i) data-driven research is beyond the scope and reach of most chemists due to a lack of available and accessible tools, (ii) many fundamental and practical questions on how to make data science work for chemical research remain unresolved, and (iii) data science is not part of the formal training of chemists, and much of the community thus lacks the necessary experience and expertise to utilize it.  This research centers around the creation of an open, general-purpose software ecosystem that fuses in silico modeling, virtual high-throughput screening, and big data analytics (i.e., the use of machine learning, informatics, and database technology for the validation, mining, and modeling of resulting data sets) into an integrated research infrastructure.  A key consideration is to make this ecosystem as comprehensive, robust, and user-friendly as possible, so that it can readily be employed by interested researchers without the need for extensive expert knowledge.  It also serves as a development platform and testbed for innovation in the underlying methods, algorithms, and protocols, i.e., it allows the community to systematically and efficiently evaluate the utility and performance of different techniques, including new ones that are being introduced as part of this project.  A meta machine learning approach is being developed to establish guidelines and best practices that provide added value to the cyberinfrastructure.  The work is driven by concrete molecular design problems, which serve to demonstrate the efficacy of the overall approach.  The educational challenges that arise from the qualitative novelty of data-driven research and its inherent interdisciplinarity are addressesed by leveraging a new graduate program in Computational and Data-Enabled Science and Engineering for cross-cutting course and curricular developments, the creation of interactive teaching materials, and a skill-building hackathon initiative.  This award is jointly made with the Division of Chemistry's, Chemical Theory, Models and Computational Methods Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750549","CAREER: HiPer: A CFD solver for High-Performance Turbulent Flow Simulations on Massively Parallel Machines","OAC","CAREER: FACULTY EARLY CAR DEV, CYBERINFRASTRUCTURE, Software & Hardware Foundation","03/01/2018","10/15/2020","Aparna Chandramowlishwaran","CA","University of California-Irvine","Continuing Grant","Alan Sussman","02/28/2023","$500,000.00","","amowli@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","1045, 7231, 7798","026Z, 062Z, 1045, 9102","$0.00","Evolution of physics-based simulations and Computational Fluid Dynamics (CFD) in particular has fundamentally reshaped the design and engineering process in the last several decades.  However, in spite of noteworthy success, today's CFD still remains limited to a small design space.  One of the grand challenge problems is the simulation of a full aircraft envelope.  Today's computing platforms are based on massive parallelism and heterogeneous processor designs to deliver petascale performance (10^15 floating point operations per second).  This research has the potential to effectively harness this level of performance and significantly address grand challenge problems through advances in numerical schemes, efficient parallel algorithms, and implementation strategies to enable underlying simulations that are currently infeasible.  It also aims at transforming CFD simulation capabilities to dramatically reduce the cost and time needed to solve complex multi-physics problems.  As such, this research fills a gap in the understanding that lies at the intersection of computational fluid dynamics and parallel computing.  This interdisciplinary research also shapes the next-generation of students and researchers with a multi-faceted skill set required to solve challenging problems at the boundary of domain sciences, applied mathematics, and computer science to promote the progress of science and advance national prosperity and welfare as stated by NSF's mission.<br/><br/>This research creates HiPer, a CFD solver for high-performance turbulent flow simulations on massively parallel machines.  HiPer solves the Navier-Stokes equations on multi-block structured grids for complex geometries by combining the following key components: (i) a novel time-delayed implicit time-marching scheme tailored for heterogeneous architectures;  (ii) parallelization strategies for shared- and distributed-memory systems aimed at reducing the synchronization and communication time; and  a hybrid multi-block structured grid enhanced by geometric multigrid to increase convergence as well as reduce the number of grid cells.  Applications that are drivers in the near- and long-term serve as benchmarks for continually measuring progress towards the grand challenge goals.  In particular, two application case studies serve as drivers for the near-term: (a) the simulation of the complex unsteady flow through multi-stage compressors and turbines, and (b) noise generation and propagation in a high-speed turbulent jet.  The intellectual merit of this work is the development of novel numerical techniques, parallelization strategies, and scalable software that enable turbulent-separated flow simulations that are computationally intractable today.  To engage and inspire young generations to this approach, this project strives to (i) organize hands-on workshops at relevant conferences, (ii) design and develop an educational kit targeted to teaching CFD and high-performance computing (HPC) concepts, (iii) design lab-based courses on HPC for computational scientists, including hands-on labs using HiPer on large-scale systems for on-site students, and (iv) release videos and guest lectures through University of California's Early Academic Outreach Program. <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031921","Frontera Travel Grant: High Resolution Thunderstorm Modeling and Analysis","OAC","Leadership-Class Computing","10/01/2020","06/03/2020","Leigh Orf","WI","University of Wisconsin-Madison","Standard Grant","Edward Walker","09/30/2022","$7,518.00","","leigh.orf@ssec.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916252","BD Hubs: Collaborative Proposal: Midwest: Midwest Big Data Hub: Building Communities to Harness the Data Revolution","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/28/2021","Jeffrey Peterson","MN","University of Minnesota-Twin Cities","Cooperative Agreement","Martin Halbert","05/31/2023","$255,000.00","Shashi Shekhar, James Wilgenbusch","jmpeter@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","024Y","062Z, 8083","$0.00","This project builds on a prior Midwest Big Data Hub effort. In 2015 stakeholders in the Midwest region of the United States formed a consortium of partners and working groups called the Midwest Big Data Hub (MBDH).  MBDH aimed to help member organizations working in Big Data coordinate current activities and launch new collaborative projects.  The project included stakeholders in the twelve states of the Midwest Census region (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin) and six leading universities that support hundreds of researchers, technologists, and students.  This hub provides a basis for collaboration and outreach that increases the potential for benefitting society. <br/><br/>The current award is a collaboration among five academic sites (Indiana University, Iowa State University, UIUC/NCSA, the University of Michigan, the University of North Dakota, and the University of Minnesota - Twin Cities).  The project focuses on priority areas that are important to the region and can also be influential on the national stage. <br/>  -  The five thematic areas of focus, and the institutional partner leading that thematic area, are: Digital Agriculture (led by Iowa State); Smart, Connected, and Resilient Communities (Indiana University); Water Quality (University of Minnesota); Advanced Materials and Manufacturing (UIUC); and Health and Biomedicine (University of Michigan). <br/>  -  Three cross-cutting areas that are emphasized across the project are: data science education and workforce development; cyberinfrastructure, data access and use; and communication and community development.<br/>The priority areas have regional relevance and also have the prospect for integration into societal contexts at the national level. The overall goal is to enable the use of existing and emerging cyberinfrastructure and best practices to improve access to and use of data.  The project plans to reach out to the Midwest community at large and to connect people, resources, and organizations. Ties to Big Data Hubs in three other regions provide a means to advance knowledge across these fields at the national level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112167","Category II: A Prototype National Research Platform","OAC","Innovative HPC","07/01/2021","08/17/2021","Frank Wuerthwein","CA","University of California-San Diego","Cooperative Agreement","Robert Chadduck","06/30/2026","$5,999,999.00","Tajana Rosing, Thomas DeFanti, Mahidhar Tatineni, Derek Weitzel","fkw@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7619","","$0.00","Advances in data-intensive science and engineering research, supported by ongoing developments in cyberinfrastructure, enable new insights and discoveries. Among these are progress in understanding fundamental processes and mechanisms from human public health to the health of the planet; predicting and responding to natural disasters; and promoting the increasing interconnectedness of science and engineering across many fields, including in astronomy, extreme-scale systems management, cell biology, high energy physics, social science, and satellite image analyses. Fundamentally new system architectures are required to accelerate such advances, including capabilities that integrate diverse computing and data resources, research and education networks, edge computing devices, and scientific instruments into highly usable and flexible distributed systems. Such systems provide both technological platforms for conducting research, and can catalyze distributed and multidisciplinary teams, which are developing new and transformative approaches to addressing disciplinary and multidisciplinary research problems. <br/><br/>Recent reports, informed through community visioning, including the NSF supported report  ?Transforming Science Through Cyberinfrastructure?, note that  a cyberinfrastructure (CI) ecosystem  designed to be open and scalable, and to grow with time may advance through in kind contributions of compute and data resources by the national science and education community.  This CI ecosystem may be viewed, ?more holistically as a spectrum of computational, data, software, networking, and security resources, tool and services, and computational and data skills and expertise that can be seamlessly integrated and used, and collectively enable new, transformative discoveries across S&E [science and education]?.<br/><br/>Aligned with this vision of a national scale CI ecosystem, the San Diego Supercomputer Center (SDSC) at the University of California, San Diego (UCSD), in association with partners at the University of Nebraska, Lincoln (UNL) and the Massachusetts Green High Performance Computing Center (MGHPCC), will deploy the ?Prototype National Research Platform? (NRP). This  novel, national scale, distributed testbed architecture includes:  a high performance subsystem to be deployed at SDSC that integrates advanced processors to be available in association with extremely low latency national Research and Educational (R&E) networks  operating at multiple 100Gbps speeds;  additional highly optimized subsystems each constituting 288 Graphics Processing Units (GPUs) to be deployed at the University of Nebraska, Lincoln (UNL) and the Massachusetts Green High Performance Computing Center (MGHPCC), to be also interconnected to the R&E networks at 100Gbps speeds at each location; a minimum of  additional 1 PB of high performance disk storage to be deployed at each of the three sites to establish a Content Delivery Network (CDN) providing prototype caliber access to data anywhere in the nation within a round trip time (RTT) of ~10ms to be available through a set of eight optimally positioned 50TB Non Volatile Memory (NVMe)-based network caches; and an innovative  system software environment enabling both centralized  management of the nationally distributed testbed system. Additionally, the system architecture will remain open to future growth through additional integration of capabilities to be achieved through a novel ?bring your own resource? program.    <br/><br/>The project is structured as a three-year testbed phase, followed by a two-year allocations phase. During the testbed phase, SDSC researchers, working closely with collaborators at UNL and MGHPCC, as well as with small numbers of research teams, will evaluate the NRP architecture and performance of constituent components. Semiannual workshops will bring teams together to share lessons learned, develop the knowledge and best practices to inform researchers, and explore the innovative architecture to accelerate S&E discoveries from ideas to publications. During the allocations phase, NRP will be available to researchers with projects deemed meritorious by an NSF-approved allocation process. Workshops continue through the allocations phase.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925716","CC* Compute: Shared Computing Infrastructure for Large-scale Science Problems","OAC","Campus Cyberinfrastructure","08/01/2019","07/17/2019","Richard Jones","CT","University of Connecticut","Standard Grant","Kevin Thompson","07/31/2022","$400,000.00","Vernon Cormier, Kyungseon Joo, Jun Yan, Cara Battersby","Richard.T.Jones@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","8080","","$0.00","Advances in scientific instrumentation over the past decade have led to phenomenal growth in both the size and complexity of data across a wide variety of scientific domains. Researchers at the University of Connecticut are expanding the capabilities of the existing shared high-performance computer facility to meet this challenge by the addition of a 28-node cluster, each node with 40 Intel cores and 192 GB of memory, and 1000 TB of shared storage.<br/><br/>This balance of resources has been chosen to accommodate applications in the areas of particle physics, astrophysics, geophysics, and statistics in use by UConn researchers, with a view to meet a rising need for data-intensive computing across all of science and engineering at the University. Furthermore, the cluster will be configured as a shared scientific computing resource on the Open Science Grid (OSG). OSG is a consortium of universities and national research facilities who are using grid technology to aggregate their individual computer clusters into a single unified national compute infrastructure for science. In joining this cluster to the OSG, UConn researchers will see a much higher throughput than they would see running on local resources alone, granting them faster turn-around and access to bigger data than could be stored and processed locally. The cluster will enable the introduction of a new big data component within an existing course on scientific computing at the graduate level. It will also be used to produce visualizations of astronomical observations that will be used in K-12 outreach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931283","Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research","OAC","EnvS-Environmtl Sustainability, Software Institutes","10/01/2019","08/17/2019","Claire Welty","MD","University of Maryland Baltimore County","Standard Grant","Seung-Jong Park","09/30/2024","$410,000.00","","weltyc@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7643, 8004","026Z, 077Z, 7925, 8004","$0.00","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. <br/><br/>SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126319","CC* Regional: A Purpose-built SoCal Science DMZ for Catalyzing Scientific Research Collaborations","OAC","Campus Cyberinfrastructure","10/01/2021","08/19/2021","Carl Kesselman","CA","University of Southern California","Standard Grant","Kevin Thompson","09/30/2023","$999,777.00","Byoung-Do Kim, Yul Pyun","carl@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8080","","$0.00","The Los Nettos Regional Network is a long-standing regional research and education (R&E) network with a history of supporting science and engineering research for its more than 30 members and associates in the greater Los Angeles area. This project builds a friction-free regional Science DMZ network across multiple Southern California college campuses, catalyzing collaborative research capabilities at the institutions. The project establishes the network infrastructure and software necessary to facilitate high speed transfers of large-scale research data for regional and national scientific collaborations. The campuses included in the network are Loyola Marymount University, Occidental College, and The Claremont Colleges consortium, which consists of Claremont Graduate University, Claremont McKenna College, Harvey Mudd College, Keck Graduate Institute, Pitzer College, Pomona College, and Scripps College. <br/><br/>This purpose-built science network is specially customized for each institution?s unique needs and follows the well-known Science DMZ guidelines established by ESnet. The new network interconnects with state, national, and international networks, such as CENIC?s California Research and Education Network (CalREN), Internet2, and Pacific Wave. Many projects in various science domains benefit from the significant network capacity increase that this project supports. Coordinated activities at the regional level, including technical training for administrators and researchers at each campus, ensure uniform standards are maintained. This scalable R&E network can be expanded in the future for researchers and students at other smaller regional institutions (e.g., Charles R. Drew University of Medicine and Science, ArtCenter College of Design) as their need for collaboration widens.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2122110","Collaborative Research: IRNC: Testbed: FAB: FABRIC Across Borders","OAC","International Res Ret Connect","03/01/2021","08/19/2021","Anita Nikolich","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Kevin Thompson","03/31/2024","$40,020.00","","anitan@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7369","","$0.00","Global science relies on robust, interconnected components - computers, storage, networks and the software that ties them together - collectively called the scientific cyberinfrastructure (CI). Improvements to individual components are made at varying paces, often creating bottlenecks in the flow of information - the scientific workflow - and slowing down scientific discovery. FABRIC Across Borders (FAB) enables domain scientists and CI experts to jointly develop a more tightly integrated, flexible, intelligent, easily programmable workflow that takes advantage of rapid changes in technology to improve global science collaboration. FAB enables domain scientists to perform global, end-to-end experimentation of new CI workflow ideas on a platform with one of a kind capabilities. The project expands the NSF-funded FABRIC testbed to encompass four additional, International locations, creating an interconnected resource on which an initial set of scientists from High Energy Physics (HEP), Astronomy, Cosmology, Weather, Urban Science and Computer Science work with cyberinfrastructure experts to conduct cyberinfrastructure experiments. In addition to domain scientists, FAB collaborates in the area of Internet freedom and maintains strong partnerships with human rights groups, which serve to expand the results beyond domain sciences.<br/><br/>FABRIC nodes contain programmable networking hardware, storage, CPUs and GPUs, measurement devices and software in a single, integrated rack. FAB enables placement of four additional nodes in partner data centers in Tokyo, Amsterdam, Bristol and the particle physics lab CERN in Geneva and connects them via NSF-funded International networks, on which it?s possible to conduct experiments without impacting production science. FAB offers programmable peering with production networks and specialized testbeds, allowing experimenter topologies to be joined with production networks, vastly expanding the possibilities for the types of resources and users that can utilize the infrastructure. FAB creates new software services and tools for researchers at the facilities, and interfaces with existing and evolving data delivery services to efficiently move and process scientific data globally and test novel data analysis approaches that scale to massive volumes. Metrics of success are driven by the science experiments themselves: more efficient handling of both high energy physics data from CERN experiments to worldwide collaborators and Cosmic Microwave Background data collected in South America and the South Pole; successful proofs of concept for the sharing of Smart City sensor data for urban planning as well as the establishment of global, private 5G networks. All software associated with FAB will be open source and posted in a publicly available repository: https://github.com/fabric-testbed/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925747","CC* Network Infrastructure: Enabling Research with Mines' Underground Research Facility","OAC","Campus Cyberinfrastructure","08/01/2019","07/27/2019","Matthew Ketterling","CO","Colorado School of Mines","Standard Grant","Kevin Thompson","04/30/2022","$480,591.00","Michael Erickson, Jorge Ricardino Csapo, Juergen Brune","mketterling@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","8080","","$0.00","This award provides funds to install extensive cyberinfrastructure and expand Wide-Area Network (WAN) speeds to the Colorado School of Mines' Edgar Underground Educational and Research Facility.  Such cyberinfrastructure upgrades further establish the Edgar Underground Educational and Research Facility as the premier facility for innovative research in underground environments.  Furthermore, this facility provides an environment for students, educators, researchers, and external partners to collaborate across a wide variety of scientific and engineering disciplines.  As a result of the project, researchers can better leverage campus, public, and private resources efficiently and expand research opportunities in areas such as underground communications and networking; sub-surface studies of microbial genetics and environmental DNA; remote monitoring of geologic features, seismic, electric or magnetic anomalies; and remote and autonomous control of terrestrial and aerial robots.<br/><br/>The Colorado School of Mines' Edgar Underground Educational and Research Facility is located 25 miles west of main campus in Idaho Springs, CO.  Due to the challenges of the surrounding terrain and remote location of the Edgar Facility, commodity ISP provides only one bonded pair of DSL lines, for a maximum download speed of 20Mbsec.  One goal of this project to establish a 10Gb network connection to the Edgar Facility using a combination of leased fiber from the FrontRange GigaPOP and 10Gb P2P wireless communication to connect the final half-mile that spans rugged terrain.  A second goal is to create robust cyberinfrastructure within the Edgar Facility, allowing connections to instrumentation and providing redundant network pathways within the mine.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2134956","EAGER: Dryad BRIDGE: Building Repository Interconnections with Dryad Guidance and Extensions","OAC","NSF Public Access Initiative","08/15/2021","08/18/2021","Ray Habermann","CO","METADATA GAME CHANGERS LLC","Standard Grant","Martin Halbert","07/31/2023","$299,940.00","Mark Musen","ted@metadatagamechangers.com","4524 14TH ST UNIT H","BOULDER","CO","803041265","3143699954","CSE","7414","7916","$0.00","This project will explore methods to increase the quality of data publication in a large multidisciplinary repository (Dryad) using techniques that can be broadly deployed in many scientific communities.  The project will be undertaken by a metadata consulting firm in collaboration with Stanford University, and the Dryad repository.  The quality of metadata associated with datasets is fundamental to the success of open science.  Datasets with poor quality metadata are far less likely to be discovered or usefully described for open science purposes of reproducibility of results and generation of new research findings.  Scalable methods for improving the quality of metadata for datasets will be extremely advantageous for open science purposes broadly.  <br/><br/>The project has three broad technical goals, as follows.  First, the project team will work with Dryad sub-disciplinary communities in Earth and Earth Sciences to develop enhanced metadata templates that better characterize datasets beyond the minimally required metadata fields.  Second, the project team will assess and enhance metadata in Dryad by developing connectivity metadata metrics and baselines in consultation with Dryad user groups, integrating missing funder metadata into records, and attempting to apply the enhanced metadata templates to records.  Third, the project team will apply both the enhanced metadata templates and enhanced metadata records produced to approximately 40K records in Dryad to identify potential connections to other repositories and service providers (especially with the EarthCube Council of Data Facilities repositories) and interlinkages with articles and funders to further enhance the quality of the metadata in Dryad as a proof of concept.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834807","Elements:  Software - Harnessing the InSAR Data Revolution: GMTSAR","OAC","XC-Crosscutting Activities Pro, EarthCube","10/01/2018","08/15/2018","David Sandwell","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Seung-Jong Park","09/30/2022","$281,747.00","","dsandwell@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","7222, 8074","026Z, 062Z, 072Z, 077Z, 7923, 8004","$0.00","The award supports the development of a software tool aimed at lowering the barriers for the use of Interferometric Synthetics Aperture Radar (InSAR) data. Natural processes such as earthquakes, volcanoes, landslides, and glaciers cause deformation of the surface of the earth that can now be monitored to 10 mm precision globally.  These surface measurements provide a new tool for investigating processes in the interior of the Earth. InSAR is a powerful and low-cost way to monitor subsurface magma movement and is especially useful when combined with other tools such as GPS and seismic data.  InSAR is also used to understand subsurface fluid movement, such as caused by groundwater withdrawal, geothermal production, or in oil and gas fields. A wide variety of new SAR satellites are currently operating and the US will have an even more capable mission in the near future.  These massive data sets need to be transformed into information to promote the progress of science, mitigate natural  hazards, and enhance use of underground resources. The software, named GMTSAR will develop robust and sustainable software to take full advantage of the satellite generated data for both science and applications.  The main innovation of this project is to develop open and robust software to simplify the InSAR data processing and enable routine processing of thousands of SAR images on state-of-the-art computer facilities.  Open distribution of software, and the GMTSAR theoretical basis document provide a foundation for education in the field of space geodesy and ensures availability to a diverse audience.<br/><br/>This proposed investigation will use standard software engineering practices to harden the GMTSAR code, improve the geodesy, and make it more accessible to novice and advanced users on a wide array of UNIX platforms.  This will be achieved through improved and automated testing, partial redesign and simplification of the work flows, UNAVCO short courses, user feedback, and eventual migration of the code distribution and maintenance to a national facility. Work will be performed by a postdoctoral researcher in collaboration with the GMTSAR and GMT development teams and with assistance from the XSEDE program. The expected outcome is to provide sustainable, open, geodetically-accurate software to move InSAR time series analysis from the intermediate-scale methods published today to large spatial and time scale analyses that are becoming possible using the new data streams.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115148","CICI:UCSS:Securing an Open and Trustworthy Ecosystem for Research Infrastructure and Applications (SOTERIA)","OAC","Cybersecurity Innovation","09/01/2021","05/06/2021","Robert Gardner","IL","University of Chicago","Standard Grant","Robert Beverly","08/31/2024","$500,000.00","Brian Bockelman","rwg@hep.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8027","7923, 8027","$0.00","Managing a secure software environment is essential to a trustworthy cyberinfrastructure.<br/>Software supply chain attacks may be a top concern for IT departments, but they are also an<br/>aspect of scientific computing. The threat to scientific reputation caused by problematic software<br/>can be just as dangerous as an environment contaminated with malware. The issue of<br/>managing environments affects any individual researcher performing computational research<br/>but is more acute for multi-institution scientific collaborations as they often preside over complex<br/>software stacks and must manage software environments across many distributed computing<br/>resources. Increasingly, these collaborations and individual investigators have turned to Linux<br/>container images (packing application software, operating system and other needed libraries<br/>into one entity) for their platform portability and scientific reproducibility advantages. However, in<br/>doing so new software sources from both public and private repositories are introduced into the<br/>supply chain, thus bringing new risks. The Securing an Open and Trustworthy Ecosystem for<br/>Research Infrastructure and Applications (SOTERIA) project is an element within NSF's fabric of<br/>coordinated Cyberinfrastructure that helps collaborations avoid security pitfalls while reducing<br/>the burden of scientific software management. SOTERIA aims to provide researchers with<br/>improved discoverability, visibility, and traceability of their software environments.<br/><br/>SOTERIA operates a container registry for open science. The registry has been customized to<br/>meet the unique needs of the scientific environment, including associating the researcher?s<br/>identity with container images, providing image security scanning and introspection (visibility),<br/>and integration with other digital object identification and archiving services. SOTERIA also<br/>operates a container distribution service with tools to trace image provenance through the<br/>ecosystem. Finally, as the challenge of managing secure software environments goes far<br/>beyond container security, SOTERIA provides training and education on best practices tailored<br/>to researchers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031838","Frontera Travel Grant: Multi-Scale Dynamics of Kinetic Turbulence and Dynamo in Collisionless Astrophysical Plasmas","OAC","Leadership-Class Computing","09/01/2020","05/12/2020","Matthew Kunz","NJ","Princeton University","Standard Grant","Edward Walker","08/31/2022","$7,703.00","","mkunz@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835120","Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/31/2018","Jonathan Pober","RI","Brown University","Standard Grant","Bogdan Mihaila","08/31/2022","$250,933.00","","Jonathan_Pober@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed.  These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The invetigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.<br/><br/>One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042055","Collaborative Research: EAGER: Leveraging Advanced Cyberinfrastructure and Developing Organizational Resilience for NSF Large Facilities in the Pandemic Era","OAC","CESER-Cyberinfrastructure for","09/01/2020","10/14/2020","Kerk Kee","TX","Texas Tech University","Standard Grant","Bogdan Mihaila","08/31/2022","$169,491.00","","kerk.kee@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7684","020Z, 097Z, 7916","$0.00","The COVID-19 global pandemic in 2020 has created major disruptions to the research enterprise. NSF-supported large facilities are critical elements of US research infrastructure and are increasingly dependent on advanced cyberinfrastructure (CI) ? comprising advanced computing, data and software assets, networking, and the related specialized workforce ? to accomplish their science missions. This study investigates how large facilities are impacted by, and are responding to, the pandemic challenge with a focus on understanding factors related to the use of existing CI. This project will also explore the value of CI in the broader social context of how people and the facilities perceive and respond to major disruptive events. The goal is to determine how to design large facility organizations to be more resilient during crises and major disasters and what CI capabilities are needed to support these and other large science projects to accomplish their science missions during such disruptions.<br/><br/>This study comprises three main research questions related to NSF large facilities and CI during the pandemic: (a) What types of research activities remain ""business as usual"" and what types of activities must adapt or stop completely under pandemic conditions? (b) If facilities could turn back time, what would they have done to better prepare? And (c) What lessons are facilities learning from the current disruptions, and how can these be best disseminated to the facility, CI, and research communities? The approach is grounded in Weick's Theory of Organizing, and examines disruptions from the environment (ecological change) through the stages of enactment (immediate actions), selection (rules establishment), and retention (identification of approaches worth re-utilizing in future events), with feedback loops linking the stages and the environment. The project?s goals will be accomplished primarily through interviews with domain scientists, CI users, developers, and administrators who are engaged in NSF large facility science and operations. The project will also analyze and document the organizational structures of the facilities to identify the key engagement points with national CI resources and services, towards enhancing the ability of the broader CI community to engage with the facilities. The ultimate objective of this project is to provide a framework for facilities and other large science projects to mitigate disruptions to their scientific and operational activities in current and future times of crisis. Study outcomes and findings will be widely disseminated to the stakeholder communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042054","Collaborative Research: EAGER: Leveraging Advanced Cyberinfrastructure and Developing Organizational Resilience for NSF Large Facilities in the Pandemic Era","OAC","CESER-Cyberinfrastructure for","09/01/2020","10/14/2020","Ewa Deelman","CA","University of Southern California","Standard Grant","Bogdan Mihaila","08/31/2022","$130,000.00","","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7684","020Z, 097Z, 7916","$0.00","The COVID-19 global pandemic in 2020 has created major disruptions to the research enterprise. NSF-supported large facilities are critical elements of US research infrastructure and are increasingly dependent on advanced cyberinfrastructure (CI) ? comprising advanced computing, data and software assets, networking, and the related specialized workforce ? to accomplish their science missions. This study investigates how large facilities are impacted by, and are responding to, the pandemic challenge with a focus on understanding factors related to the use of existing CI. This project will also explore the value of CI in the broader social context of how people and the facilities perceive and respond to major disruptive events. The goal is to determine how to design large facility organizations to be more resilient during crises and major disasters and what CI capabilities are needed to support these and other large science projects to accomplish their science missions during such disruptions.<br/><br/>This study comprises three main research questions related to NSF large facilities and CI during the pandemic: (a) What types of research activities remain ""business as usual"" and what types of activities must adapt or stop completely under pandemic conditions? (b) If facilities could turn back time, what would they have done to better prepare? And (c) What lessons are facilities learning from the current disruptions, and how can these be best disseminated to the facility, CI, and research communities? The approach is grounded in Weick's Theory of Organizing, and examines disruptions from the environment (ecological change) through the stages of enactment (immediate actions), selection (rules establishment), and retention (identification of approaches worth re-utilizing in future events), with feedback loops linking the stages and the environment. The project?s goals will be accomplished primarily through interviews with domain scientists, CI users, developers, and administrators who are engaged in NSF large facility science and operations. The project will also analyze and document the organizational structures of the facilities to identify the key engagement points with national CI resources and services, towards enhancing the ability of the broader CI community to engage with the facilities. The ultimate objective of this project is to provide a framework for facilities and other large science projects to mitigate disruptions to their scientific and operational activities in current and future times of crisis. Study outcomes and findings will be widely disseminated to the stakeholder communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838955","Encouraging Data Sharing and Reuse in the Field of Collective Behavior through Hackathon-Style Collaborative Workshops","OAC","NSF Public Access Initiative","10/01/2018","08/30/2018","Simon Garnier","NJ","New Jersey Institute of Technology","Standard Grant","Martin Halbert","09/30/2020","$24,997.00","Jason Graham","garnier@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7414","7556","$0.00","The investigators will bring together diverse researchers who work in the field of collective and emergent behavior. Collective and emergent behavior is the study of complex biological and social systems, ranging from bacterial colonies to human groups.  The hackathon-style workshop draws researchers around identifying best practice mechanisms for sharing data, communicating methods of data analysis, and reusing publicly available data.   The investigators propose a series of two workshops where teams of 2-5 participants work on a specific project during the duration of the 3-day event. An objective of the workshop is to foster novel collaborations between researchers in biology, data science, mathematics, computer science and physics, all of whom have an interest in collective behavior.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005369","Category II: Exploring Neural Network Processors for AI in Science and Engineering","OAC","Innovative HPC","06/01/2020","08/16/2021","Amitava Majumdar","CA","University of California-San Diego","Cooperative Agreement","Robert Chadduck","12/31/2026","$11,250,000.00","Rommie Amaro, Robert Sinkovits, Mai Nguyen, Javier Duarte","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7619","","$0.00","Artificial Intelligence (AI) is increasingly applied to science and engineering problems, and as a part of a growing data science field that is enabling new insights and discoveries not possible with traditional high-performance computing architectures. In 2019 the President issued an executive order announcing a national AI strategy involving the private sector, academia, and the public. Among other things, this strategy calls for investments in AI research and development, providing AI resources, and training an AI-ready workforce. Researchers are increasingly applying machine learning (ML) techniques to science and engineering problems including those from astronomy, climate modeling, extreme-scale systems management, cell biology, high energy physics, drug discovery, social science, satellite image analysis, among others. In addition to advances in algorithms and software, the performance of AI systems is heavily dependent on the underlying hardware. Evaluation of hardware optimized for AI algorithms is of keen interest to the AI research community.<br/><br/>To extend the application of AI to evermore challenging problems in science and engineering, the San Diego Supercomputer Center (SDSC), working closely with their vendor partner Supermicro, will deploy Voyager, a high-performance, innovative resource for conducting AI research across a wide range of science and engineering domains. Based on AI processors optimized for deep learning (DL) operations, Voyager will be a first-of-its-kind system available in the NSF resource portfolio. This will give researchers the opportunity to explore Voyager?s unique hardware and software using well-established deep learning frameworks like PyTorch, Keras, and Tensorflow to implement deep learning techniques such as convolutional neural networks (CNNs) and generative adversarial networks (GANs). Researchers will also be able to develop their own AI techniques using software tools and libraries built specifically for Voyager?s innovative AI architecture.<br/><br/>The project is structured as a three-year Testbed phase followed by a two-year Allocations Phase. During the Testbed phase SDSC researchers and collaborators will work closely with a small number of research teams to evaluate the performance of Voyager?s innovative deep learning (DL) hardware, specialized compilers, and system libraries. Semiannual workshops will bring teams together to share lessons learned, and develop the knowledge and best practices that inform future users who will be given access during the Allocations Phase. During the Allocations Phase, Voyager will be available to researchers with projects deemed meritorious by an NSF-approved allocation process. Lessons learned from the Testbed Phase is used to develop documentation, best practices, allocations models, and user support strategies. Semiannual workshops continue in the Allocations Phase. Through SDSC?s AI Technology Lab, the project will engage with industry to explore how technologies like those in Voyager can improve the global competitiveness of private sector companies, and prepare the next generation workforce.<br/><br/>SDSC will deploy Voyager in SDSC's energy-efficient data center on the UCSD campus. Voyager will be connected to multiple high-performance research and education networks at 100 Gbps. Supporting Voyager is a nationally recognized team of application and systems experts at SDSC. The Voyager External Advisory Board will assist in recruiting early users and providing guidance to the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827243","CC* Network Design: Improve Network on Campus for Research and Education in Agriculture, Science, and Engineering at Prairie View A&M University","OAC","Campus Cyberinfrastructure","08/15/2018","06/15/2020","Suxia Cui","TX","Prairie View A & M University","Standard Grant","Kevin Thompson","07/31/2022","$531,964.00","Yoonsung Jung, Hua-Jun Fan, Yonghui Wang, Seungchan Kim, Shumon Alam","sucui@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","CSE","8080","9251","$0.00","Prairie View A&M University (PVAMU), a Historically Black College and University, is implementing a Science DMZ to Improve Network on Campus for Research and Education in Agriculture, Science, and Engineering (INCREASE), which is an upgrade to current cyberinfrastructure that allows researchers to effectively work in the fields of Big Data and Data Intensive science. This INCREASE project enables data driven research in the areas of cyber security, high performance computing, computational chemistry, brain imaging, genomics, and bioinformatics. It provides better visualization and collaboration mechanisms for PVAMU researchers from seven departments in three colleges. It leads to better educational resources to train students at a minority serving institution. The INCREASE project specifically ties researchers from four research centers closely together to form a new foundation of campus cyberinfrastructure.<br/><br/>The INCREASE network, bypassing PVAMU-IT's firewall, integrates with existing infrastructure and provides improved connectivity between PVAMU and Internet2 via Lonestar Education and Research Network (LEARN). It also provides high bandwidth interconnectivity to other research universities and High Performance Computing resources nationwide. As part of the network design, the need for Software Defined Networking solutions that can selectively steer Science DMZ traffic around the campus firewall solution as well as look into implementing a deep-packet inspection and Intrusion Prevention Service to route unregistered connections in and out of the Science DMZ is considered. The study and exploration of the INCREASE network can provide guidance for universities of similar scale to pursue and utilize research resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031682","Travel support for 2020 Frontera PI Users Meeting","OAC","Leadership-Class Computing","07/01/2020","05/12/2020","Ganesh Balasubramanian","PA","Lehigh University","Standard Grant","Edward Walker","06/30/2022","$10,000.00","","gab317@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017194","Collaborative Research: CyberTraining: Pilot: Interdisciplinary Training of Data-Centric Security and Resilience of Cyber-Physical Energy Infrastructures","OAC","CyberTraining - Training-based, Secure &Trustworthy Cyberspace","09/01/2020","06/24/2020","Zhuo Lu","FL","University of South Florida","Standard Grant","Joseph Whitmeyer","08/31/2022","$140,000.00","Nathan Fisk, Mahshid Rahnamay Naeini","zhuolu@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","044Y, 8060","","$0.00","In this project, students and researchers are provided with mentored, hands-on training combining expertise across electrical engineering, communication, science and technology studies, and data science.  This establishes a novel model for energy cyberinfrastructure resilience education. The curriculum and instructional materials that are developed integrate advanced skills from multiple areas under the umbrella of cyber-physical energy systems. Participants develop and refine the multi-disciplinary skillsets needed for the data-centric energy industry using unique, remotely connected smart grid cyberinfrastructure. Participants extend their academic research portfolios, strengthening their career competitiveness as future cyberinfrastructure professionals and users. The two-week workshop immerses undergraduate/graduate students and research scientists in a unique training opportunity through laboratory demonstrations and mini projects. <br/><br/>Three main technical challenges are tackled in the project: (i) Establishment of a new remotely connected cyberinfrastructure platform among the collaborating universities. An existing hardware-in-the-loop power testbed using a real-time digital simulator is connected with a virtual network laboratory to characterize cyber-physical energy systems. The combined infrastructure is equipped with state-of-the-art hardware and software modules, where humans, machines, and power girds can interact and cooperate in a near-to-real learning environment. (ii) Implementation of a cybersecurity module in the virtual lab to simulate cyber threats, such as denial-of-service attacks and man-in-the-middle attacks. Participants create attack scenarios that are played out, where the attacking process and consequence can be visualized on the physical system. (iii) Development of data analytics techniques based on machine learning as a set of cyber defense mechanisms, such as data-driven adversarial event detection. Participants execute their cyber defense algorithms alongside the attack, and the effectiveness of these defenses is validated and visualized using the testbed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018919","CC* Networking Infrastructure: Enhancing SIUC Campus Cyberinfrastructure to Accelerate Data-Driven Research and Education","OAC","Campus Cyberinfrastructure","07/01/2020","08/23/2021","Ning Yang","IL","Southern Illinois University at Carbondale","Standard Grant","Kevin Thompson","06/30/2022","$399,923.00","Ning Weng, Thomas Imboden, Ning Yang, Scott Bridges","nyang@siu.edu","Ofc. of Sponsored Projects Admin","Carbondale","IL","629014308","6184534540","CSE","8080","","$0.00","This project implements a high-performance research and education network architecture over the existing campus cyberinfrastructure (CI) at Southern Illinois University Carbondale. The project solves the performance bottleneck of the current campus core network and establishes new infrastructures enabling high-throughput data transfer with peer institutions and on-campus private wireless broadband service. The updated CI enables efficient and high-throughput data movement needed by fast-growing data-intensive research and education applications across broad academic domains on campus including Chemistry and Biochemistry, Physics, Engineering, Plant Biology, Statistics, Computer Science, etc. The project builds upon a strong partnership among campus information technology experts, networking faculties, and domain researchers. It also provides an exceptional opportunity to train the CI workforce. <br/><br/>The project includes three tasks: 1) enhance the campus core network by adding redundancy across 9 core locations with new routers and 10 Gbps links. The updates add the capacity and redundancy needed to offer reliable and high-performance network service to the whole campus; 2) establish a Science DMZ and the associated network performance monitoring framework, which enables researchers to obtain and share data with external hosts a high speed; and 3) establish a private LTE network dedicated to research and education over the 3.65 GHz CBRS spectrum. Five data-intensive science and education applications are prototyped to demonstrate the new CI capabilities and workflows enabled by this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835904","Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet","OAC","Data Cyberinfrastructure, Software Institutes","01/01/2019","08/23/2021","Alexander Lex","UT","University of Utah","Standard Grant","Seung-Jong Park","12/31/2022","$1,899,694.00","Bryan Jones, Alexander Lex","alex@sci.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7726, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system.  <br/><br/><br/>MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019144","MRI: Acquisition of Pinnacles -- Raising Research Computing to New Heights in California's Central Valley","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE","08/01/2020","07/22/2020","Hrant Hratchian","CA","University of California - Merced","Standard Grant","Alejandro Suarez","07/31/2023","$700,000.00","Ashlie Martini, Suzanne Sindi","hhratchian@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","CSE","1189, 1640, 7231","1189","$0.00","Scientific computing plays a central role in knowledge discovery and scientific advancement. This award will result in the acquisition of a shared high-performance computing (HPC) cluster named Pinnacles at the University of California, Merced (UCM). Building on an increasing strength in scientific computing at UCM, the nation?s first new research-intensive university of the 21st century, this project will deploy a new state-of-the-art HPC to enable exciting cutting-edge science and facilitate advanced research training that will promote the progress of science. UCM, an Hispanic Serving Institution, is also uniquely positioned to leverage the new HPC to provide access and training opportunities for students from underrepresented groups.<br/><br/>Pinnacles will serve as a centerpiece for high-impact work in three scientific research pillars: (1) modeling and simulation of complex systems; (2) data enabled science; and (3) numerical optimization. This shared HPC cluster will be used by researchers to advance a broad array of conventional disciplines. Indeed, senior personnel involved in the project span all three of UCM?s academic schools, with work ranging from quantum chemistry to cognitive science. With a central focus on HPC and serving as a collaboration incubator, the Pinnacles cluster will facilitate knowledge discovery by supporting researchers working in their individual research areas and enhancing interdisciplinary collaborations to strengthen advancements at the boundaries of traditional disciplines. In addition to exciting broad scientific opportunities, Pinnacles will support early career researchers and advance learning and research experience opportunities for a diverse and underserved student population.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019007","CC* Compute: Interactive Data Analysis Platform","OAC","Campus Cyberinfrastructure","07/01/2020","06/16/2020","Klara Jelinkova","TX","William Marsh Rice University","Standard Grant","Kevin Thompson","06/30/2022","$397,600.00","Meng Li, Yingyan Lin, Jonathan Ajo-Franklin, Christopher Tunnell","klaraj@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8080","","$0.00","Rice University researchers engaged in groundbreaking data-intensive science and engineering increasingly depend on access to real-time data analysis facilities required for their research. These research activities include image processing, computer vision, and machine learning, spanning multiple fields, such as geological sciences, statistics, computer science, and physics. Each of these problems areas or use cases can be addressed by shared computational infrastructure leveraging GPU accelerators for interactive computing. The system provides a significant resource for enabling science but also for educating the next generation of computational scientists in the latest GPU-computing techniques through the outreach of the Center for Research Computing. <br/><br/>The resource includes nine compute nodes, each with 40 cores, 384GB RAM, 4TB NVMe storage, and 8 NVIDIA Quadro RTX 6000 GPUs. The systems are interconnected via high-performance networking and hosted on a Science DMZ integrating them with the Open Science Grid as well as commercial cloud allowing both increased utilization as part of national OSG efforts and the ability to utilize cloud resources for load bursting. The system leverages an open-source software stack designed to support containerization, enabling each researcher to utilize their own unique set of software and toolkits while sharing common hardware and a common cloud access platform. Moreover, the infrastructure is part of a larger technology ecosystem that leverages federated identity and access management as part of InCommon, advanced networking with science DMZ, and Information Security Office that supports not only university data and technology security but includes targeted outreach for research data and protocol security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2032010","Frontera Travel Grant: 3-D Stellar Hydrodynamics  of Convective Boundary Mixing and Shell Mergers in Massive Stars","OAC","Leadership-Class Computing","08/01/2020","05/12/2020","Paul Woodward","MN","University of Minnesota-Twin Cities","Standard Grant","Edward Walker","07/31/2022","$9,977.00","","paul@lcse.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835402","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Leadership-Class Computing, Software Institutes","09/01/2018","04/04/2019","Michael Norman","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","08/31/2022","$488,727.00","James Bordner","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","1253, 1798, 7781, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004544","Elements: Cyberinfrastructure Service for IoT-Based Construction Research and Applications","OAC","Software Institutes","08/01/2020","04/29/2020","Aaron Costin","FL","University of Florida","Standard Grant","Robert Beverly","07/31/2022","$455,114.00","Janise McNair, Sanjeev Koppal, Idris Jeelani","aaron.costin@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","8004","077Z, 7923","$0.00","Wireless infrastructure is steadily evolving into wireless access for all humans and most devices, from 5G to Internet-of-Things. This widespread access creates the expectation of custom and adaptive services from the personal network to the backbone network. In addition, challenges of scale and interoperability exist across networks, applications and services, both requiring an effective wireless network management infrastructure. At the same time, there has been a rising imperative to capitalize on the current technological advancements to address to the most pressing issues surrounding construction and the built environment to increase health and safety; productivity; and sustainability. This Elements project combines the areas of computer science, electrical engineering, and construction and building to develop a robust cyberinfrastructure (CI) service for construction research, as well as applications that utilize state-of-the-art emerging technologies and software to address the current challenges faced by the construction industry. The major contribution of the project is the development of the IoT-ACRES (IoT-Applied Construction Research and Education Services) system,  a central, interoperable framework hub that can incorporate a variety of heterogeneous sensors, technology, software, managed by a software-defined network infrastructure and optimized by machine learning and artificial intelligence techniques. The prototype system will help to augment the works and/or safety manager's ability to detect hazards and subsequently improve safety performance in construction, which is one of the greatest challenges faced by the construction industry. In addition, the framework can be used to increase autonomy in applications that require simultaneous tracking of multiple entities (people, vehicles, equipment, etc.), detecting multiple objects of interests, analyzing real-time biometric data, and making autonomous decisions. Results will be disseminated to industry and research communities through publications and presentations at workshops, training courses and online professional certification programs. The project will also be used as a research, education, and training tool to (1) mentor and teach K-12 students about STEM, and (2) to develop and enhance courses to educate the current and next generations of students, users, and workers, on the latest technology and the latest approaches to cyber security techniques. <br/><br/>This project develops a robust cyberinfrastructure (CI) system and service for construction research and applications to address the current challenges faced in the construction industry. The outcomes and services that this proposal aims to provide are 1) a distributed SDN-managed and AI-assisted IoT-based system that can be adapted and extended based on needs of the research and application; 2) identification of the data and data security requirements needed to address the challenges in the construction industry and potential technologies that can provide those data; 3) evaluation of reliable real-time multi-sensor fusion techniques for ruggedness, usability, and limitations of IoT-based components deployed in the dynamic construction environments; 4) robust prototype system for real-time safety monitoring based on the IoT system framework; and 5) recommendations of potential configurations of the system with the appropriate technology and sensors to meet the needs of the application. The empirical data resulting will be delivered through yearly NSF reports on the progress and findings, journal publications of the intellectual merit and scientific findings, and conference proceedings discussing the broader impacts and future research objectives. The framework of the hardware and software, including an instructional manual will also be published. The software will be made available through request via a project website, open source posts, and conference and workshop dissemination. The project will explore the use of various delivery mechanisms, such as NSF's eXtreme Science and Engineering Discovery Environment (XSEDE). The IoT-ACRES will utilize IBM IoT Continuous Engineering and Cloud Computing Servers Cloud (e.g. Amazon AWS) for the data analysis and performance metrics. This novel convergence research project will ultimately advance the development of sustainable CI communities and stewardships of sustainable CI services that can enhance productivity and accelerate innovation in science and engineering. This work will advance practices of safety controls by developing a tool for safety monitoring on construction sites, presented to safety managers with interfaces that visualize, and report real-time safety hazards. Significantly, it will address fundamental research challenges in computer vision and construction management: improving context-based object recognition and tracking; and formalizing rules for integrating visual, textual, biometric data to proactively recognize safety hazards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031580","Frontera - Travel","OAC","Leadership-Class Computing","08/01/2020","05/01/2020","Ken Dill","NY","SUNY at Stony Brook","Standard Grant","Edward Walker","07/31/2022","$9,982.00","","dill@laufercenter.org","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2034617","Towards the Future Direction of the NSF Program on the Cyberinfrastructure for Sustained Scientific Innovation (CSSI)","OAC","Software Institutes","06/01/2020","06/02/2020","Ritu Ritu","TX","University of Texas at San Antonio","Standard Grant","Seung-Jong Park","07/31/2022","$12,538.00","","ritu.arora@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8004","026Z, 7556, 8004","$0.00","The NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program has been critical in not only supporting the development and deployment of innovative software and data products, but also in several workforce development and community-building initiatives. The projects funded through the CSSI program have promoted the nation's competitiveness and leadership in the fields of data, High-Performance Computing (HPC), networking, cybersecurity, software, and workforce development. A workshop on the ""Future Direction of the NSF CSSI Program"" can be instrumental in gathering the community-feedback on the current scope and objectives of the CSSI program and guiding its future direction such that the nation's leadership in the aforementioned fields is maintained. <br/><br/>The goal of the workshop on the ""Future Direction of the NSF CSSI Program"" is to bring together researchers and practitioners from the industry, academia, and government laboratories to assess the impact of the CSSI program, share best practices in supporting sustainable software and data products in present and future, and identify gaps (if any) in the scope of the CSSI program and needs of the community. A report resulting from this workshop will help inform NSF of future directions for the CSSI program. The workshop will strengthen multi-disciplinary collaborations on developing the cyberinfrastructure (software and data) for supporting research and innovation in different fields of science and technology. This project is critical for enabling the organization of the aforementioned workshop and supporting the participation of a diverse range of researchers and practitioners in the development of the future CSSI solicitations. Some participants may need financial support for traveling to the workshop. This project will help in supporting the travel of such participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1902308","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","02/01/2019","Timo Heister","UT","University of Utah","Standard Grant","Stefan Robila","02/29/2020","$700,000.00","","heister@clemson.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004894","Collaborative Research: Frameworks: funcX: A Function Execution Service for Portability and Performance","OAC","Software Institutes","05/01/2020","04/17/2020","Ian Foster","IL","University of Chicago","Standard Grant","Seung-Jong Park","04/30/2024","$2,658,096.00","Kyle Chard","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","077Z, 7925, 8004","$0.00","The funcX project is developing, deploying, and operating a new distributed computing cyberinfrastructure platform to enable researchers to build applications from programming functions that execute on different computing resources, from laptops to supercomputers. This cloud-hosted service democratizes access to advanced computing by providing intuitive interfaces for both registering remote computers as function executors and executing functions on these computers reliably, securely, and with high performance. Researchers can thus decompose monolithic applications into collections of reusable lightweight functions that can be run wherever makes the most sense, for example where data reside or where excess capacity is available. By simplifying access to specialized and high performance cyberinfrastructure and decreasing the time to discovery, the project serves the national interest, as stated in NSF's mission, by promoting the progress of science. A total of 33 diverse science, cyberinfrastructure, and software institute partners working with cutting-edge science applications and research cyberinfrastructure will directly benefit from the funcX platform.<br/><br/>This project develops funcX, a scalable and high-performance federated platform for managing the remote execution of (often short-duration) functions across diverse cyberinfrastructure systems, from edge accelerators to clusters, supercomputers, and clouds. funcX allows developers to decompose applications into collections of functions that can each be executed in the best location, in terms of cost, execution time, data movement costs, and/or energy consumption. It thus integrates the extreme convenience of the function as a service (FaaS) model, developed in industry for specific industry applications, with support for the specialized needs of scientific research. funcX addresses important barriers to these new uses of research cyberinfrastructure systems, by enabling the intuitive, flexible, and scalable execution of functions without regard to physical location, scheduler architecture, virtualization technology, administrative domain, or data location. Flexible open-source funcX agent software makes it easy to expose arbitrary computing systems as funcX computing platforms, thereby transforming existing cyberinfrastructure systems into high-performance function serving environments (endpoints). The cloud-hosted funcX service provides a REST interface for registering functions, discovering available endpoints, and managing the execution of functions on endpoints, all via a universal trust fabric and standard web authentication and authorization mechanisms. It dynamically creates and deploys containers that incorporate function dependencies and provide a secure and isolated environment for safe function execution. The project engages a diverse set of 11 science partners, 18 research computing and cyberinfrastructure projects, and 4 NSF Software Institutes, each supporting many NSF-funded researchers, to provide use cases for funcX, shape its design, and evaluate its implementation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940247","Collaborative Research: Biology-guided neural networks for discovering phenotypic traits","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","10/15/2020","Anuj Karpatne","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Peter McCartney","09/30/2022","$422,000.00","","karpatne@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","099Y, 7231","1165, 7231","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811600","Molecular and Coarse-Grained Simulations of Biomolecular Processes at the Petascale","OAC","Leadership-Class Computing","04/01/2018","03/22/2018","Gregory Voth","IL","University of Chicago","Standard Grant","Edward Walker","03/31/2019","$8,100.00","","gavoth@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7781","","$0.00","Computer simulations of biological systems can offer insights that are difficult or impossible to access with conventional experimental techniques, providing significant benefits for basic scientific research.  However, the large range of characteristic time and length scales observed in biological processes makes the use of any single computational technique difficult.  While atomic-resolution simulations can furnish a scientist with exquisite levels of detail, the sheer computational expense of these simulations sometimes presents a significant barrier for their application to large-scale biological problems.  This project proposes to us coarse-grained (CG) molecular models to expand the reach of computer simulations to cellular scales. The project propose to integrate atomic-resolution and CG simulations to study a range of biologically relevant systems, in close collaboration with an international cohort of scientists from various experimental fields. This work will involve not only elucidating and explaining biomolecular processes, but also the development and dissemination of cutting-edge simulation software to the wider scientific community.<br/><br/>The project aims to combine experimental data with cutting-edge computer simulations to investigate a number of important biomolecular systems.   The systems of interest can be grouped into two main categories: critical stages of the viral lifecycles of HIV-1 and influenza, and studies of the actin filaments and microtubules of the cellular cytoskeleton.   The project will develop an integrated pipeline which allows scientists to convert experimental data into computer models capable of investigating biomolecular processes at scales that are inaccessible to other approaches. While the results of atomic-scale simulations will clearly be important in and of themselves, they will also, in combination with experimental data, form the basis for generating and parameterizing rigorous UCG models. Results and predictions made by these models will be validated by close collaboration with experimental scientists, and used to suggest new directions in both the theoretical and experimental field.  In addition to the development and deployment of advanced biomolecular simulation techniques, this proposal will also assist in the dissemination of the advances to the wider research community by the integration of the UCG model generation and simulation algorithms with the popular LAMMPS software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1761795","Spokes: SMALL: SOUTH: Smart Privacy for Smart Cities: A Research Collaborative to Protect Privacy and Use Data Responsibly","OAC","BD Spokes -Big Data Regional I, ","09/01/2018","08/30/2018","Jules Polonetsky","DC","FPF Education and Innovation Foundation","Standard Grant","Martin Halbert","02/28/2021","$349,858.00","","julespol@fpf.org","1400 Eye Street","Washington","DC","200056503","2026429142","CSE","024Y, 033y","028Z, 042Z, 8083","$0.00","Today's cities and communities, and their residents, are increasingly connected, and decisions are informed by actionable, real-time data. Data is making modern cities and local communities faster and safer, as well as more sustainable, livable, and equitable. At the same time, smart city technologies raise concerns about individuals' privacy, autonomy, choice, and possible misuse. Government officials are raising important questions about who should own data, how privacy protections for public-facing technologies work, who and how to communicate with the public about privacy, and more. The long-term vision of the project is to help municipal leaders strengthen their ability to collect, use, and share data in a responsible manner. This will help grow privacy-preserving innovations across applications and geographic boundaries for the public good. In this way, the Smart Privacy for Smart Cities Spoke will serve to increase public knowledge, understanding and engagement with privacy-related concerns, and ultimately promote the public's trust in smart city technologies and in their local government.<br/><br/>The Smart Privacy for Smart Cities Spoke is a collaborative project led by the FPF Education and Innovation Foundation (FPF EIF). It will undertake three interrelated activities to help city/community innovators understand and incorporate privacy considerations and protections into smart city projects. First, the Spoke will produce a privacy risk assessment framework for smart city projects, including guidance on what policies, practices and accountability structures should be put in place and how to weigh benefits of new or expanded data techniques against attendant privacy and security risks. The framework will feature a de-identification component as de-identification is one of the primary measures that organizations can take to protect and share data in a privacy-preserving manner. Second, the Spoke will establish a network of privacy leaders for smart cities and communities, helping municipal officials better understand, communicate, and collaboratively address privacy issues and principles. Lastly, the Spoke will hold two workshops in partnership with MetroLab Network, a national network of 40+ city-university partnerships advancing technological approaches to urban challenges. The workshops will serve to launch the privacy leaders network; educate network members about the privacy risk assessment framework; and identify areas for further collaboration and research, particularly in applying the framework to specific technologies and data uses. Expected project outcomes include wide dissemination of the privacy risk assessment framework and its adoption in specific localities; development of new collaborations and partnerships through the privacy leaders network and associated workshops; and as an overall outcome, the advancement of civic leaders' knowledge, understanding, and application of privacy protection(s) toward standards of practice for smart cities and privacy. The project will support and leverage the Hub Network, in particular the South Big Data Hub, NSF's Smart and Connected Communities program, and the MetroLab Network of city-university partners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923810","Collaborative Research: CyberTraining: Conceptualization: Planning a Sustainable Ecosystem for Incorporating Parallel and Distributed Computing into Undergraduate Education","OAC","CyberTraining - Training-based","09/01/2019","08/17/2019","Ramachandran Vaidyanathan","LA","Louisiana State University","Standard Grant","Almadena Chtchelkanova","02/28/2021","$21,961.00","","vaidy@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","044Y","026Z, 9150","$0.00","In this era of pervasive multicore machines, GPUs, cloud services, big data, machine learning, and the Internet of Things, there is a critical need for an institute to create a sustainable, discipline-wide ecosystem for incorporating parallel and distributed computing (PDC) into undergraduate computing curricula. Such an institute would support the community of educators, students, and other stakeholders, with the goal of developing a workforce that is ready to meet the challenges of working with current and future computing fabrics. The investigators propose planning for such an institute (iPDC) that can help eliminate the longstanding barrier of the sequential computing paradigm such that, analogous to the establishment of the object oriented paradigm, the PDC paradigm is naturally integrated into Computer Science (CS) and Computer Engineering (CE) curricula across various institutions as recommended by the 2013 ACM/IEEE Computer Science Curricula and now by ABET.<br/><br/>Through the network of funded and unfunded collaborators, established contacts with instructors at institutions serving underrepresented groups, and outreach efforts, the project will robustly engage with stakeholder communities through four well-structured planning workshops, weekly teleconferences, and feedback and dissemination activities to formulate the key attributes of the institute.Broadening PDC education will further enable advances in science and engineering, which depend increasingly on PDC systems, by providing the next generation of practitioners and researchers with the necessary skills and knowledge to effectively exploit them. The curriculum standards, adoption, and dissemination activities will have synergistic international components. Overall, this project will facilitate a rich exchange of ideas within the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840003","CICI: SSC: Securing Science Gateway Cyberinfrastructure with Custos","OAC","Cybersecurity Innovation","08/15/2018","08/16/2018","Marlon Pierce","IN","Indiana University","Standard Grant","Robert Beverly","07/31/2022","$997,672.00","James Basney, Suresh Marru, Enis Afgan","marpierc@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8027","","$0.00","Science gateways are web-based portals for scientists, educators, and students to easily access advanced computing infrastructure, perform reproducible research, and share scientific data. Science gateways are used by tens of thousands of researchers worldwide; thus, gateways are an increasingly attractive target for a wide range of cybersecurity attacks. In cybersecurity terms, science gateways are a collection and integration of a rich set of assets, including user identities, access to a wide range of third party computing and storage resources, sensitive data, licensed or otherwise restricted software, and generated scientific results. The consequences of compromises to science gateways are wide ranging, including unauthorized access to computing resources and data as well as more subtle attacks such as alterations of computed results that may require retraction of publications and otherwise undermine the integrity of the scientific enterprise.  <br/><br/>Through the integration and extension of best-of-breed software, the Custos project provides a single, open source software solution that improves science gateways' management of users identities, secrets such as security tokens used to access a wide range of resources needed for scientific research, and groups and sharing permissions on digital research objects created by science gateway users. Custos software is used to operate a secure, online service with a language-independent programming interface for the science gateway community, providing both an operational platform and a proving ground for research into multi-tenanted, science-focused cyberinfrastructure. The entire Custos environment, including provisioning and operations tools, is openly governed following the meritocracy and diversity principles of the Apache Software Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822378","Leveraging Heterogeneous Data Across International Borders in a Privacy Preserving Manner for Clinical Deep Learning","OAC","BD Spokes -Big Data Regional I","03/15/2018","03/15/2018","Gari Clifford","GA","Emory University","Standard Grant","Alejandro Suarez","02/28/2021","$300,000.00","Shamim Nemati","gari.clifford@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","024Y","7916","$0.00","There is a growing awareness of the need for multi-center clinical databases and multi-institutional analyses of healthcare data to ensure reproducibility and generalizability of research findings. Single-instance database algorithms are prone to three distinct problems. First, in the context of Big Data science, the size of the data compared to the number of variables makes it difficult to develop complex predictors without overfitting, and more traditional learning algorithms may lead to over-simplified models that do not capture important related influences or interactions between different types of healthcare information. Second, training and testing predictive models on a single database can lead to learning noise or other irrelevant local practices or differences in definitions that are correlated with, but not causally related to, the outcome in question. This leads to models that do not work in other institutions or in the future when practices or the environment changes. Third, sharing data between institutions, and in particular, across borders, is extremely problematic because of trust, legal issues, privacy issues and national policies. The significance of solving these issues is threefold: 1) it would allow the creating of strong generalizable data science models, which leverage enormous pools of data from around the world; 2) it would also allow the identification of rare diseases or patient types, which, as we compile databases, become less rare; and 3) perhaps most importantly, it would allow the free exchange of data science models and generalized approaches to solving medical problems in the cloud.<br/><br/>This project aims to develop a set of distributed deep learning and cloud computation techniques for cross-institution and cross-border machine learning on health and medical data without the need for protected health information to leave the generating institution. The goals are to create demonstration programs which illustrate feasibility and open source the architecture. The scope of this project encompasses the broad set of machine learning-based tasks multiple institutions may want to apply to their healthcare data in the cloud, as well as the technical issues surrounding transfer learning of knowledge across domains (e.g., institutions/demographics) and tasks (e.g., types of classification and prediction problems). The project has three specific aims: 1) develop a cloud-based infrastructure which preserves regional autonomy of data, but allows the sharing of parameters of the partially trained deep neural network (including weights and hyperparameters) between regions, to allow transfer learning across domains and tasks; 2) develop a standardized coded model for deep learning approaches in medical applications; and 3) evaluate the effect of training and testing the model across multiple centers and national boundaries, by comparing improvement in performance with cross-institutional training without loss of privacy protection, using metrics of sensitivity, specificity, positive predictive value, area under the receiver operating characteristic (ROC) curve and model calibration. Aims 1-3 will be achieved by taking four databases (including, a database of intensive care unit patients with sepsis, a free text corpus of nursing progress notes, voice recordings taken from a public corpus classically used for speaker identification, and a public database of full-face images used for classification of facial expressions) and placing them in the cloud (Google, AWS and Azure) at different geopolitical locations (namely US and Europe) and developing a distributed deep learning architecture that learns to improve its performance by sharing weights across borders, but not sensitive patient data. This project has the potential to make several contributions to the field. First, it will demonstrate that medical data across geopolitical boundaries can be made available in an interoperable manner (using the FHIR standard) and can be used for training of deep learning algorithms in a privacy-preserving manner, thus addressing both the concerns of Health Insurance, Portability and Privacy Act (HIPPA) and interoperability. Secondly, it will provide open-source deep learning algorithms for several medical datasets and data types that can be used across institutions to solve similar problems with some fine-tuning (e.g., via transfer learning). Third, it will provide a set of open-source meta algorithms for transfer learning (across domains and tasks) implemented on the cloud in containers (dockers) that can be downloaded for local use or transferred across the different cloud vendors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1953051","OAC Core: Small: Devising Data-driven Methodologies by Employing Large-scale Empirical Data to Fingerprint, Attribute, Remediate and Analyze Internet-scale IoT Maliciousness","OAC","OAC-Advanced Cyberinfrast Core","08/14/2019","03/18/2020","Elias Bou-Harb","TX","University of Texas at San Antonio","Standard Grant","Robert Beverly","06/30/2022","$496,898.00","","elias.bouharb@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","090Y","026Z, 9179","$0.00","At least 20 billion devices will be connected to the Internet by 2023. Many of these devices transmit critical and sensitive system and personal data in real-time. Collectively known as ""the Internet of Things"" (IoT), this market represents a $267 billion per year industry. As valuable as this market is, security spending on the sector barely breaks 1%. Indeed, while IoT vendors continue to push more IoT devices to market, the security of these devices has often fallen in priority, making them easier to exploit. This drastically threatens the privacy of the consumers and the safety of mission-critical systems. While a number of research endeavors are currently taking place to address the IoT security problem, several challenges hinder their success. These include the lack of IoT monitoring capabilities once such devices are deployed, the shortage of remediation techniques when they are compromised, and the inadequacy of methodologies to permit the comprehension of the underlying IoT malicious infrastructures. To this end, this project will serve NSF's mission to promote the progress of science by developing data science methodologies to identify and remediate infected IoT devices in near real-time. The project will also promote cyber security research and training for minorities and K-12 students. Moreover, the project will contribute to operational cyber security by developing a large-scale cyberinfrastructure for IoT-relevant data and threat sharing, enabling hands-on cyber-science at large. <br/> <br/>The project will scrutinize close to 100 GB/hr of real-time unsolicited Internet-scale traffic to devise and develop efficient deep learning classifiers to fingerprint IoT devices, identifying their types and vendors, and disclosing their large-scale vulnerabilities and hosting environments. The project will design and develop fast greedy approximation algorithms for L1-norm Principal Component Analysis (PCA) data-dimensionality reduction, enabling the real-time execution of the Density Based Spatial Clustering of Application with Noise (DBSCAN) technique for detecting and attributing IoT orchestrated botnets. The project will also design scalable offensive security algorithms based on Internet-wide active measurements to offer macroscopic remediation strategies. The project will curate close to 3.5 million malware samples/day and around 1.3 million passive DNS records/day to build graph-theoretic models to uncover and characterize inter-related components which form the concept of IoT malicious cyberinfrastructure. Further, the project will analyze the evolution of such infrastructures to comprehend their modus operandi by devising efficiency graph similarity techniques in linear time, by designing and implementing algorithms rooted in graph kernels and min-hashing methods. The project will also (i) develop a unique cyberinfrastructure for IoT empirical data and cyber threat indexing and sharing, (ii) automate the devised algorithms and techniques by leveraging high speed, in-memory data processing technologies, (iii) generate IoT-specific detection signatures by exploring fuzzy hashing algorithms, and (iv) enable at-large access to the generated IoT artifacts through a secure API and a front-end mechanism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835426","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Brian O'Shea","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2022","$480,055.00","","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, PIs at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The PIs have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835499","Elements: Software: Roundoff-Error-Free Algorithms for Large-Scale, Sparse Systems of Linear Equations and Optimization","OAC","Software Institutes","06/01/2019","08/31/2018","Erick Moreno-Centeno","TX","Texas A&M Engineering Experiment Station","Standard Grant","Seung-Jong Park","05/31/2022","$600,000.00","Timothy Davis","emc@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Solving systems of linear equations is central to solving problems in numerous applications within healthcare, power generation, national defense, economics, physics, chemistry, mathematics, computer science and engineering.  Nowadays, a large number of these critical applications have an ever-increasing need for faster, more reliable, and more accurate solutions. For example, more accurate treatment plans are proven to be less costly, less invasive, safer, and more reliable outcomes for prostate-cancer brachytherapy (placement of radioactive ""seeds"" inside a tumor). Similarly, millions of dollars can be saved, and cleaner energy can be produced, by solving the power generation dispatch optimization problem with more accuracy. Paradoxically, today's state-of-the-art software tools are limited to calculating limited-precision solutions (e.g., treatment plans and power dispatches).  This is due in part to the prevalence of computing methods relying on floating-point arithmetic (i.e., arithmetic using truncated decimal numbers). At the same time, real-life problems in a wide range of applications are becoming larger and so more prone to incorrect results due to roundoff errors (errors introduced when truncating the decimal numbers). The primary goal of this project is to design, create, and deploy computational tools to solve large-scale, sparse systems of linear equations and optimization problems without any error at all. Because of the ubiquity of solving systems of linear equations and optimization problems, the outcomes of this project will directly translate in software that is more reliable for applications across academia, industry, and government.  <br/><br/>Large-scale, sparse systems of linear equations (SLEs) and linear optimization problems (LPs) are routinely solved and the accuracy/correctness of solvers is taken for granted. However, state-of-the-art solvers commonly report incorrect results, some as striking as misclassifying feasible problems as infeasible and vice versa or even failing altogether.  Moreover, exactly solving SLEs and LPs is of fundamental importance for applications where fixed-precision standards have been deemed inadequate, including specific applications in healthcare, power generation, biology, combinatorial auctions, and formal verification of mathematical proofs.  Therefore, the first objective of this project is to devise efficient algorithms and implement robust software to reliably and exactly solve large-scale, sparse SLEs, free of any roundoff error. This objective will build on our recently devised roundoff-error-free (REF) LU and Cholesky factorizations for dense matrices. The second objective of this project is to devise efficient algorithms and  implement robust software to reliably and exactly (REF) solve  large-scale, sparse LPs.  The specific outcomes of this project  include: (1) Devise an efficient REF factorization framework for  large-scale sparse matrices, including devising good fill-reducing  orderings that consider the bit-size growth of the entries; (2)  Devise REF optimization algorithms to exactly solve large-scale,  sparse linear programs; (3) Our software will be rigorously tested,  with a full 100% test coverage suite and scaffolding code to test  loop invariants and data sanity. The software products will be  submitted as algorithm papers to the ACM Transactions on Mathematical  software, where the code itself, test suite and documentation undergo  rigorous peer review. Finally, we will incorporate our solvers into  our existing SuiteSparse installations, including all Linux distros  with the ultimate goal of being integrated into MATLAB and thus  accessible to a wide user base.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107283","Collaborative Research: OAC Core: Enabling Extremely Fine-grained Parallelism on Modern Many-core Architectures","OAC","OAC-Advanced Cyberinfrast Core","07/01/2021","06/28/2021","Kyle Chard","IL","University of Chicago","Standard Grant","Seung-Jong Park","06/30/2024","$166,302.00","","chard@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","090Y","026Z, 7923","$0.00","Computer systems are becoming increasingly complex: multisocket systems with many-core processors and general graphic processors have the potential to address the needs of demanding applications at the node level. Programmability and efficiency are often not easy to find together due to the hardware growing several orders of magnitude in degree of parallelism to thousands of computing units on a chip. Task parallelism is an important type of parallelism in which computation is broken down into a set of inter-dependent tasks which can be executed concurrently on various computing units. To achieve strong scaling and high levels of effective parallelism, there is a growing need in today's parallel languages with supporting over-decomposition (many more tasks than cores) in order to improve performance, hide latency caused by blocking operations, and otherwise achieve maximum speedup. By enabling the efficient support of fine-grained parallelism across the growing range of scales seen in modern and future hardware, it is expected that the productivity of parallel programmers will be enhanced. Trends show evidence that most of the Top500 high-performance computing systems will likely employ hardware that this work directly targets. The project aims to conduct a high-impact education program in distributed parallel programming with broad reach, encouraging student internships grounded in real-world challenges, and paving the way for technology transfer from research to open-source projects. Special emphasis is placed on engaging women and underrepresented minorities. This education facet will create a new and more accessible foundation for fluency in parallel computing for scientists and engineers.<br/><br/>This work explores novel data-structures and algorithms that allow for scalable runtime and execution models for fine-grained parallelism at sub-microsecond timescales. Preliminary work by the PIs at the language and runtime levels suggests a path to achieving this. The project objectives are: 1) unifying runtime enabling task granularities measured in cycles: design, analysis, and implementation of building blocks for efficient fine-grained computing on diverse node hardware; 2) evaluating performance of these building blocks in the context of real parallel systems and application kernels on a range of computer architectures; 3) measuring performance and scalability impact of runtime on benchmark kernels and real applications; and 4) integrating this research with education programs from undergraduate to graduate levels through new course material on parallel computing. This high-risk/high-reward research is geared towards yielding transformative improvements in the ease and efficiency of programming parallel machines at every scale. The contributions lie in the realization of productive, implicitly parallel high-level languages optimized for single node deployments with many-core architectures to support fine-grained parallelism measured in cycles, enabling an entirely new class of many-task computing applications. The dataflow architecture makes implicit parallelism tractable with a programming model whose impact could rival that of MATLAB, R, and Python, with the added benefit that the same code could also run in a distributed system or large-scale HPC systems. Thus, the scientist would be able to write a program once, run it at any suitable scale, and have it seamlessly use the most appropriate granularity for each component of the hardware. This work?s innovations in dataflow architecture will be broadly applicable to a number of existing parallel programming systems such as OpenMP, Swift/Parsl, and CUDA/OpenCL, in terms of both efficiency in executing fine grained parallelism and adding support for implicit parallelism where possible. Target hardware includes Intel/AMD x86, ThunderX/2 ARM, IBM Power9, and NVIDIA/AMD GPUs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019506","EAGER: Extending the Productive Lifetime of Scientific Computing Equipment","OAC","XD-Extreme Digital","03/01/2020","03/18/2020","Andrew Chien","IL","University of Chicago","Standard Grant","Edward Walker","02/28/2022","$300,000.00","","achien@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7476","7916","$0.00","The growing environmental impacts of hardware acquisition and operation are significant concerns for scientific computing infrastructure deployed at universities and national laboratories. While significant interest in better models for systems operation and lifecycle exist, there are ecosystem challenges including lack of proven pathways and best practices. The Zero Carbon Compute project will convene leaders in the scientific computing infrastructure community, and connect them with leading ecosystem innovators in this space.  The goal of the project is to create ""best practices"" that will catalyze the birth and growth of new lifecycle paths that both reduce the e-waste impact and carbon footprint of research scientific computing. The project will also work to create economic models of the extended lifecycles and broader ecosystem, focusing on economic incentives and the potential impact on the carbon footprint of scientific computing.  The project will both increase computing capacity for the national scientific community and create radical reductions in power carbon footprint. <br/><br/>The Zero Carbon Compute project will create new opportunities for computing innovation by leveraging the growing excess renewable power in US power grids and around the world. This rapidly growing excess provides both low-cost power and low-cost operation, even for aging computing equipment without increasing the carbon footprint of computing.  The project will also convene the scientific computing community through a series of workshops and other methods to understand challenges and communicate the potential benefits of the proposed solutions. Materials generated from these community engagements will be used for wider outreach to the broader scientific computing community, and the cloud computing community. In addition, the project will create economic models of the extended lifecycles and broader ecosystem, focusing on economic incentives, market equilibria, and the potential impact on carbon footprint of scientific computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925689","CC* Regional: Tribal Consortium Research Network","OAC","Campus Cyberinfrastructure","07/01/2019","07/16/2019","Jason Arviso","NM","Navajo Technical University","Standard Grant","Kevin Thompson","06/30/2022","$792,181.00","Mario Montes-Helu, Steven Burrell, Joy Thompson","jarviso@navajotech.edu","LOWER POINT ROAD","Crownpoint","NM","873130849","5057864112","CSE","8080","9150","$0.00","Navajo Technical University (NTU) is one of the nation's leading tribal colleges and a leader in delivering academic and research programs for the region and for the Navajo Nation. Tribal colleges regularly work in cooperation and in collaborative organizations.  In 2017, NTU facilitated the development of a tribal consortium to build cooperative support and technology services.  The tribal consortium includes Navajo Technical University, Crownpoint, NM, Dine College, Tsaile, AZ, Tohono O'odham Community College (TOCC), Sells, AZ, and A:shwi College and Career Readiness Center (ACCRC), Zuni, NM. Although Native American students have access to cultural, language, vocational, and Science, Technology, Engineering, and Math (STEM) programs, campus and student connectivity options are expensive and limited to cellular or broadband capacities below national standards. Students are not well connected to the internet and to the larger research and education community; there is a fundamental lack of internet connectivity with sufficient bandwidth to successfully participate in the ever-increasing opportunities of online courses and programs. This project improves campus network performance in support of education and research, by increasing external connectivity to each campus by connecting to NTU as a regional aggregator.<br/><br/>The project addresses distance education challenges by implementing an advanced wireless testbed to deliver homework gap solutions for local students and faculty leverages a strong existing regional relationship with the Northern Arizona University, a leader in tribal programs and services. The project emphasizes meetings to map needs to requirements, followed by design and training workshops, which benefit the regional Native American communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924023","Collaborative Research:CyberTraining:Conceptualization: Planning a Sustainable Ecosystem for Incorporating Parallel and Distributed Computing into Undergraduate Education","OAC","CyberTraining - Training-based","09/01/2019","05/01/2020","Charles Weems","MA","University of Massachusetts Amherst","Standard Grant","Almadena Chtchelkanova","02/28/2021","$36,815.00","Neena Thota","weems@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","044Y","026Z, 9251","$0.00","In this era of pervasive multicore machines, GPUs, cloud services, big data, machine learning, and the Internet of Things, there is a critical need for an institute to create a sustainable, discipline-wide ecosystem for incorporating parallel and distributed computing (PDC) into undergraduate computing curricula. Such an institute would support the community of educators, students, and other stakeholders, with the goal of developing a workforce that is ready to meet the challenges of working with current and future computing fabrics. The investigators propose planning for such an institute (iPDC) that can help eliminate the longstanding barrier of the sequential computing paradigm such that, analogous to the establishment of the object oriented paradigm, the PDC paradigm is naturally integrated into Computer Science (CS) and Computer Engineering (CE) curricula across various institutions as recommended by the 2013 ACM/IEEE Computer Science Curricula and now by ABET.<br/><br/>Through the network of funded and unfunded collaborators, established contacts with instructors at institutions serving underrepresented groups, and outreach efforts, the project will robustly engage with stakeholder communities through four well-structured planning workshops, weekly teleconferences, and feedback and dissemination activities to formulate the key attributes of the institute.Broadening PDC education will further enable advances in science and engineering, which depend increasingly on PDC systems, by providing the next generation of practitioners and researchers with the necessary skills and knowledge to effectively exploit them. The curriculum standards, adoption, and dissemination activities will have synergistic international components. Overall, this project will facilitate a rich exchange of ideas within the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842623","EAGER: Measuring Real World Application Performance on Next-Generation Computing Systems","OAC","XD-Extreme Digital","10/01/2018","08/03/2018","Robert Henschel","IN","Indiana University","Standard Grant","Edward Walker","03/31/2021","$300,000.00","Rudolf Eigenmann, Sunita Chandrasekaran","henschel@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7476","7916","$0.00","The project proposes to create a benchmark suite using real-world scientific applications. The benchmark suite will be developed jointly with the High-Performance Group (HPG) of the Standard Performance Evaluation Corporation (SPEC), a non-profit organization with the goal of creating and maintaining standardized benchmarks to evaluate performance and energy efficiency for the newest generation of computing systems. The project will leverage SPEC infrastructure to ensure continuous maintenance of the benchmark suite, as well as support for result submission, review, and publication on the SPEC website. Furthermore, the project will use the published results of the benchmark suite to create a public ranking of High-Performance Computing (HPC) systems deployed across the world. Partnership with SPEC will ensure that the results of the project will be disseminated to industry.<br/><br/>The creation of a new real-world application benchmark suite jointly with SPEC/HPG, as well as the development of performance metrics suitable for application benchmarks, will lead to improved HPC system design and enable the better understanding of how next-generation computing systems must evolve. In addition to evaluating HPC systems, the proposed application benchmark suite will also enable the comparative performance evaluation of novel software and hardware ecosystems.  Moreover, the proposed work builds on NSF investment in domain science application development by including these applications in the suite. Importantly, the project will be an opportunity to bring the benchmark community together to develop and define performance and throughput metrics for real world scientific applications. Finally, the project will deliver a benchmark suite that will be freely available to non-profit organizations and sustained by SPEC.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821926","CICI: Data Provenance:  Collaborative Research: Provenance Assurance Using Currency Primitives","OAC","Cybersecurity Innovation","10/01/2017","08/01/2019","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Robert Beverly","12/31/2020","$283,731.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","8027","021Z, 7434, 9150, 9251","$0.00","Data provenance is the ability to track data history including things such as where the data resided, who handled it, and what systems stored, forwarded and processed it. This research builds on the architecture of the digital currency Bitcoin. It develops distributed data ledgers - similar to bookkeeping ledgers - that maintain data history so it can't be manipulated by hackers trying to hide their activities. Data consistency guarantees  that everyone gets the right answers about where, who, and what regardless of which ledger is read.  This software advances the security of computing systems by making data accountable, especially for online commerce and big data (""the cloud'').  It secures forensic information taken from compromised computers  for further analysis. It validates whether privacy requirements are being met for medical records.  The key outcome is a software prototype that implements the complete system and illustrates the ability to store, maintain, and update provenance information for real data. <br/><br/>A data provenance framework will be designed, prototyped, evaluated and then delivered as an Application Programmer Interface, software library, and distributed service. This work will produce a reusable distributed service architecture achieving scalability by using distributed services that maintain ledger information. The system leverages Bitcoin cryptocurrency by building on Bitcoin's block-chain architecture to maintain provenance metadata securely. It leverages existing tools for provenance data exploration and visualization.  Digital signatures from both the server/system as well as the user creates dual information about possession, while distributed ledgers remove control and maintenance of metadata from the user who creates it. The prototype enables research into long-term provenance creation, maintenance, and utilization for workflows in the area of cybersecurity as well  studies of how to integrate and secure provenance into existing file systems and network services.  Opt-in and passive (involuntary) provenance systems will be enabled using the API, library, and distributed ledgers prototyped, enabling data provenance for systems where needed, notably high assurance cloud computing and scientific workflow systems. The tool can be used  to enable reproducibility of published results from archived data and artifacts."
"2127548","CI CoE: CI Compass: An NSF Cyberinfrastructure (CI) Center of Excellence for Navigating the Major Facilities Data Lifecycle","OAC","CiCoE-Cyberinfrastructure Cent","07/15/2021","07/23/2021","Ewa Deelman","CA","University of Southern California","Standard Grant","Kevin Thompson","06/30/2026","$8,000,000.00","Valerio Pascucci, Anirban Mandal, Jaroslaw Nabrzyski, Angela Murillo","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","139Y","020Z, 9102","$0.00","Innovative and robust Cyberinfrastructure (CI) is critical to the science missions of the NSF Major Facilities (MFs), which are at the forefront of science and engineering innovations, enabling pathbreaking discoveries across a broad spectrum of scientific areas. The MFs serve scientists, researchers and the public at large by capturing, curating, and serving data from a variety of scientific instruments (from telescopes to sensors). The amount of data collected and disseminated by the MFs is continuously growing in complexity and size and new software solutions are being developed at an increasing pace. MFs do not always have all the expertise, human resources, or budget to take advantage of the new capabilities or to solve every technological issue themselves. The proposed NSF Cyberinfrastructure Center of Excellence, CI Compass, brings together experts from multiple disciplines, with a common passion for scientific CI, into a problem-solving team that curates the best of what the community knows; shares expertise and experiences; connects communities in response to emerging challenges; and builds on and innovates within the emerging technology landscape. By supporting MFs to enhance and evolve the underlying CI, the proposed CI Compass will amplify the largest of NSF?s science investments, and have a transformative, broad societal impact on a multitude of MF science and engineering areas and the community of scientists, engineers, and educators MFs serve. CI Compass will also impact the broader NSF CI ecosystem through dissemination of CI Compass outcomes, which can be adapted and adopted by other large-scale CI projects and thus empower them to more efficiently serve their user communities.<br/><br/>The goal of the proposed CI Compass is to enhance the CI underlying the MF data lifecycle (DLC) that represents the transformation of raw data captured by state-of-the-art scientific MF instruments into interoperable and integration-ready data products that can be visualized, disseminated, and converted into insights and knowledge. CI Compass will engage with MFs and contribute knowledge and expertise to the MF DLC CI by offering a collection of services that includes evaluating CI plans, helping design new architectures and solutions, developing proofs of concept, and assessing applicability and performance of existing CI solutions.  CI Compass will also enable knowledge-sharing across MFs and the CI community, by brokering connections between MF CI professionals, facilitating topical working groups, and organizing community meetings. CI Compass will also disseminate the best practices and lessons learned via online channels, publications, and community events.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126240","CC* Networking Infrastructure: Cyberinfrastructure improvements for multi-disciplinary data-intensive scientific research","OAC","Campus Cyberinfrastructure","09/01/2021","07/14/2021","Edward Aractingi","VA","College of William and Mary","Standard Grant","Kevin Thompson","08/31/2023","$491,051.00","Kostas Orginos, Yinglong Zhang, Eric Walter, Gary Anderson","earactingi@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","8080","","$0.00","This project addresses four areas of improvement in the College of William & Mary networking that expands its research capacity throughout its two campuses and aims at modernizing the data transfer capabilities. These areas are: Greater transfer speeds between main campus and the Virginia Institute of Marine Science; a Science DMZ for high-speed connectivity for research computing and a dedicated data transfer node which are monitored for quality of service with perfSONAR; addition of dedicated 10Gbps ports for offices and labs of major research network users; and the addition of a Globus license to enable efficient transfers to/from the campus research network. This work upgrades William & Mary?s research computing network to continue supporting multi-disciplinary research which demands higher transfer rates for larger data files needed to support state-of-the-art research.<br/><br/>These network enhancements support cross-disciplinary transformative research and discovery by investigators from W&M?s School of Marine Science (Virginia Institute of Marine Science or VIMS) and W&M?s Global Research Institute, as well as by researchers from Physics, Computer Science and Data Science. The research from this group of scientists contributes largely to the advancement and transformation of various frontiers of knowledge and discovery. Projects like 3D Environmental Flow Modeling, Lattice Quantum Chromodynamics (LQCD, Coastal Biogeochemical Modeling, Deep Learning for Estimates of Development Indicators, Location AI for National Security, the building of novel ML model execution optimizations, and simulations of accelerator designs are examples of the projects with great potential to advance knowledge and dependent on the faster network this new cyberinfrastructure expansion provides.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925192","CC* Compute: Building a state-of-the-art campus compute resource at Franklin & Marshall College","OAC","Campus Cyberinfrastructure","07/01/2019","07/14/2020","Carrie Rampp","PA","Franklin and Marshall College","Standard Grant","Kevin Thompson","06/30/2022","$400,000.00","Peter Fields, Christina Weaver, Fronefield Crawford, Joshua Booth, Jason Brooks","crampp@fandm.edu","415 Harrisburg Ave.","Lancaster","PA","176043003","7173584517","CSE","8080","","$0.00","Franklin & Marshall College (F&M) is building and deploying a campus cluster resource to better meet the needs of our researchers and their students who need greater access to high performance compute resources to support intensive data analysis and computation. This project provides much needed local compute nodes for F&M's researchers and students while also contributing to the growing fabric of shared computing clusters across the country. This project contributes these new resources to the Open Science Grid (OSG) which is a national, distributed computing partnership that allows participants to share their resources with other researchers to maximize the impact these investments have on scientific research and discovery.  As an institution, F&M has many top-tier scientific researchers who also partner with students in research that is regularly funded by public and private agencies.  Providing substantially improved infrastructure to support this research will advance and expand the institution's capacity to support important investigations, from the search for pulsars to brain science. F&M has a demonstrated commitment to recruiting and supporting STEM students and this infrastructure improvement and investment allows the institution to continue to be a leader in this arena, providing access to the best resources and opportunities for future scientists.  This project, similar to other recent initiatives, demonstrates how it is possible to design and implement significant research infrastructure, even at a smaller institution, that advances scientific discovery both on and beyond our campus.<br/><br/>The compute cluster maximizes available resources for research that requires both HPC and HTC solutions, comprised of a master node running dual Xeon Gold 6130 2.10GHz CPUs with 192GB of 2666 MHz ECC memory, dual 480GB SSD drives configured in RAID 1, and 38TB of usable SSD storage in a Raid 6 array. There are 12 standard compute nodes each using dual Xeon Gold 6130 2.10 GHz CPUs each node with 192GB of 2666 MHz ECC memory and 480GB SSD. There is one GPU node for use with software that takes advantage of cuda compiled software and GPU co-processing. It mirrors the same specs as standard compute node, but includes two Nvidia V100 GPU cards featuring 32 GB HBM2 memory and 5120 stream processors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833317","Open Compass: Leveraging the Compass AI Engineering Testbed to Accelerate Open Research","OAC","XD-Extreme Digital","05/01/2018","04/25/2018","Paola Buitrago","PA","Carnegie-Mellon University","Standard Grant","Robert Chadduck","04/30/2022","$300,000.00","Nicholas Nystrom","paola@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7476","7916","$0.00","Artificial intelligence (AI) has immense potential to contribute to advances spanning progress in science, the national health, prosperity and welfare, education, benefit society, or secure the nation's defense. Research initiatives, conferences, investments, and products based on AI abound and are expanding rapidly, while the methods, performance, and understanding of AI are in their infancy. Researchers face vexing issues such as how to improve performance, transferability, reliability, comprehensibility, and how better to train AI models with only limited data. Future progress depends on advances in hardware accelerators, software frameworks, system architectures, and creating cross-cutting expertise between scientific and AI domains. One way to accelerate progress on these topics is to create an engineering testbed, which provides a controlled environment that allows investigators to explore solutions to these - and other - challenges.  Open Compass is an exploratory research project to conduct academic pilot projects on an advanced engineering testbed for artificial intelligence, culminating in the development and publication of best practices. <br/><br/>Open Compass will: 1) engage pilot projects and research groups, documenting approaches, experiences, and lessons learned; 2) conduct training events, in-person and using the Pittsburgh Supercomputing Center's wide area classroom; 3) organize and conduct a workshop focusing on advanced AI technologies; 4) integrate experiences gained through open research collaboration with those of industry experiences to identify a comprehensive set of best practices; and 5) publish results and best practices in peer-reviewed journals, conferences, and technical reports. The broad community will benefit from publication of research results, experiences, and a knowledge base of best practices. The research community will gain access to new technologies on which to develop algorithms and applications, along with insight across new fields of those technologies' applicability and important trends for AI. Open Compass will promote workforce development through student involvement in pilot projects and training and provide feedback to industry for to enable more efficient future AI technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920462","CICI: RDP: Enforcing Security and Privacy Policies to Protect Research Data","OAC","Cybersecurity Innovation","08/01/2019","07/16/2019","Yuan Tian","VA","University of Virginia Main Campus","Standard Grant","Robert Beverly","07/31/2022","$924,503.00","Byoung-Do Kim","yt2e@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8027","","$0.00","Advances in computer systems over the past decade have laid a solid foundation for data collection at a staggering scale. Data generated from end-user devices has tremendous value to the research community. For example, mobile and Internet-of-Things devices can participate in large-scale Internet-based measurement or monitoring of patient's health conditions. While ground-breaking discovered may occur, malicious attacks or unintentional data leaks threaten the research data. Such a threat is hard to predict and difficult to recover from once it happens. Preventative and defensive measures should be taken where data is generated in order to protect private, valuable data from the attackers. Currently, there are efforts that try to regulate data management, for example, a research application might have a privacy policy that describes how the user data is being collected and protected. However, there is a disconnect between these documented policies and the implementations of a research project. In this project, the investigators propose to interpret the documented policies and enforce them in research projects, in order to protect the privacy of research data. This work can significantly reduce researchers' overhead in implementing policy-compliant code and reduce the complexity of protecting research datasets.<br/><br/>In this project, the investigators provide a solution that protects research data using policies mandated by different regulatory entities, such as an application store and an Institutional Review Board (IRB). The system utilizes Natural Language Processing (NLP) techniques to extract security and privacy requirements from unstructured regulatory documents and translates these requirements to code that can patch a program that does not comply with the policies. The solution covers the lifetime of research data protection, from data collection to data storage, and data processing. This research has two thrusts. First, the investigators will build novel NLP techniques to extract security and privacy policies from unstructured, sparsely-labeled documents such as IRB protocols, and privacy disclosure of research applications. Second, the investigators will enforce these extracted policies in code, through context-aware program analysis to discover inconsistencies between a researcher's implementation and the extracted policies, and instrument researcher?s code to enforce compliant program behavior. The results of this work will have a transformative impact on the development of the next generation research data protection techniques, and more defensive security and privacy practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018149","CC* Compute: GPU Infrastructure to Explore New Algorithmic & AI Methods in Data-Driven Science and Engineering at Tufts University","OAC","Campus Cyberinfrastructure","07/01/2020","05/22/2020","Christopher Sedore","MA","Tufts University","Standard Grant","Kevin Thompson","06/30/2022","$400,000.00","Abani Patra, Lionel Zupan","chris.sedore@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","8080","","$0.00","Advanced scientific and engineering research at Tufts University is employing increasingly complex models, algorithms, simulations and machine learning approaches to large datasets. The larger the dataset or the more complex the model, the longer it takes to compute, slowing down researchers' progress and limiting their ability to innovate. Tufts' addition of six Graphics Processing Unit (GPU) enhanced compute nodes to its high-performance computing cluster accelerates scientific and engineering research in the areas of Algorithm Parallelization & Acceleration and Machine Learning & Deep Learning. Researchers in biology, chemistry, computer science, mathematics, physics, and urban planning leverage the GPU enhanced infrastructure to develop new algorithms and models and accelerate scientific discoveries. Through collaboration with the NSF-funded T-TRIPODS (Transdisciplinary Research in Principles of Data Science) project and the Center for STEM diversity at Tufts, the infrastructure provides new opportunities for underrepresented students to acquire and extend data science and high-performance computing skills.<br/><br/>The six GPU enhanced compute nodes are each configured with dual 20-core Intel Xeon Gold 6248 CPUs, 768GB of RAM and 8 NVIDIA Tesla V100 (32GB) GPUs interconnected with NVLink to improve scaling of multi-GPU computation. The nodes are linked with a 100 gigabit network and are accessible to researchers at Tufts and externally through the Open Science Grid (OSG).   The large core count, large RAM, interlinked GPU architecture provides the greatest flexibility for researchers to mix traditional and GPU-enhanced approaches in complex computational analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103524","Elements:Software A Scalable Open-Source hp-Adaptive FE Software for Complex Multiphysics Applications","OAC","Software Institutes","09/01/2021","07/09/2021","Leszek Demkowicz","TX","University of Texas at Austin","Standard Grant","Robert Beverly","08/31/2024","$589,762.00","","leszek@oden.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","077Z, 7923, 8004","$0.00","Computer models can be used to augment, inform, and even replace expensive experimental measurements in science and engineering. However, complex models of engineering applications can quickly exceed computational capability, driving the need for advanced simulation tools. Applications in high-frequency wave simulation--such as submarine sonar (acoustics), fiber optics (electromagnetics), and structural analysis (elastodynamics)--pose a significant challenge for large-scale simulation. This project advances computational modeling capabilities through the development, documentation, and dissemination of a leading-edge simulation software. The effort builds on decades-long research and code development by the investigators and their project collaborators. Distributed as open-source, the software is accessible to the broader scientific community, thereby contributing to fundamental research and education for computer modeling in science and engineering. Furthermore, the project expands the national workforce by training young computational mathematicians at the graduate and postdoctoral levels. The project results are disseminated through conference presentations, workshops and seminars, as well as publications in scientific journals.<br/><br/>The hp3D software leverages hybrid MPI/OpenMP parallelism to run efficiently on NSF extreme-scale computing facilities and interfaces with state-of-the-art third-party scientific libraries. In addition to publishing the hp3D code and documentation, this project focuses on the development of a scalable multigrid (MG) solver based on the pre-asymptotically stable discontinuous Petrov-Galerkin (DPG) finite element method. This DPG-MG solver represents a significant advancement in solver technology as 1) the first robust, scalable solver for problems with highly-indefinite operators, such as high-frequency wave propagation; and 2) the first multigrid solver with support for fully anisotropic hp-adaptive hybrid meshes and a reliable built-in error indicator. Serial implementations of the DPG-MG solver have demonstrated near-linear scaling with respect to degrees of freedom in both time and memory; its parallel implementation significantly expands scientific compute capabilities and enables solution of currently intractable problems in 3D wave simulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827098","CC* Networking Infrastructure: Jackson State University (JSU)-Research Network","OAC","Campus Cyberinfrastructure","07/01/2018","06/22/2018","Deborah Dent","MS","Jackson State University","Standard Grant","Kevin Thompson","06/30/2022","$500,000.00","Tor Kwembe, Natarajan Meghanathan, Hung-Chung Huang","deborah.f.dent@jsums.edu","1400 J R LYNCH ST.","Jackson","MS","392170002","6019792008","CSE","8080","9150","$0.00","This project provides Jackson State University (JSU) researchers with a campus cyber infrastructure (CI) capability allowing expansion of big data and other network intensive collaborative research that depend on high-speed network access to local, regional, cloud and national compute and storage resources.  The project increases research network capacity to 100G, re-architects the campus research network, and strategic relocation of equipment to utilize a Science DMZ (Demilitarized zone - i.e., free from firewalls and other friction devices).  The research network includes centrally pooled High-Performance/Throughput Computing (HPC/HTC) resources designed to meet immediate and future research needs.  An increased focus on domain scientist talent is possible due to management and monitoring of the CI assets by a centralized, well-trained enterprise information technology team. This project enhances the end-to-end network services for researchers and is an important catalyst for the growing campus-wide interdisciplinary data science program. <br/><br/>Application-specific network and computational needs for big data analytics, visualization, nanotoxicity, complex network analysis, science drivers and education are addressed.  Faculty, students, and the IT staff on campus and across campuses are engaged to leverage the new environment for their research, education, and operational needs. The project is disseminated through outreach workshop activities with Historical Black Colleges and Universities and other universities or the community college systems within the state of Mississippi who may be planning similar campus network upgrades.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017673","CyberTraining: Pilot: Linear Algebra Preparation for Emergent Neural Network Architectures (LAPENNA)","OAC","CyberTraining - Training-based","07/01/2020","06/22/2020","Kwai Wong","TN","University of Tennessee Knoxville","Standard Grant","Joseph Whitmeyer","06/30/2022","$299,839.00","Stanimire Tomov","kwong@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","044Y","","$0.00","In this project, the Linear Algebra Preparation for Emergent Neural Network Architectures (LAPENNA) program is organized to provide essential knowledge to advance literacy in AI to sustain the growth and development of the workforce in the cyberinfrastructure (CI) ecosystem of data-driven science. This program provides integrated expertise to faculty, students, and researchers the knowledge in numerical mathematics, linear algebra software, data-driven methods, and machine learning tools to tackle day to day problems in data science applications. This program aims to prepare college researchers to enable, design, and direct their own in-house data-driven science programs and incorporate perspectives from their research into their course curricula. The knowledge and experiences gathered under the direction of LAPENNA will be beneficial to CI practitioners as well as to general interested parties leading to fostering new collaborative partners and potential research initiatives and workforce training programs.<br/><br/>LAPENNA focuses on delivering algorithmic and computational techniques, numerical and programming procedures, and AI software implementation on emergent CPU cloud systems and GPU platforms. It runs two training sessions every year. Ten webinars/lectures are delivered with supporting online tutorials available for general public use.  In each session, eight teams of faculty/researchers and students are selected to participate in the LEPENNA program. Each team consists of two members from an institution. The training for each cohort lasts for six months and concludes with an on-site one-week workshop. Follow-up Q & A sessions connect the college teams and PIs during and after the training events and continue to provide hardware and software support to them.  LAPENNA delivers online materials that are useful and available to general CI practitioners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835213","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","John Wise","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2022","$481,436.00","","jwise@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018892","CC* Planning: Designing a Process to Improve Research Computing Infrastructure at City Tech","OAC","Campus Cyberinfrastructure","07/01/2020","06/11/2020","Justin Vazquez-Poritz","NY","CUNY New York City College of Technology","Standard Grant","Kevin Thompson","06/30/2022","$99,183.00","Ariyeh Maller, Andrea Ferroglia, Jeremy Seto, Mai Zahran","jvazquez-poritz@citytech.cuny.edu","300 Jay Street","Brooklyn","NY","112012902","7182605560","CSE","8080","","$0.00","New York City College of Technology, a comprehensive college within the City University of New York and a Hispanic-Serving Institution, is creating a five-year strategic plan for cyberinfrastructure (CI) and future investments and strategic management in research computing resources.  A team of faculty engaged in international high-performance computing projects are convening a broad-based stakeholder group to consider issues of CI governance and leadership, high-performance computing, data infrastructure, and faculty training and workforce development. The ecosystem of CI users includes researchers working on particle physics, galaxy formation, genomics and integrative systems biology, and protein dynamics and drug discovery, as well as faculty in a broad range of domains who need to learn the fundamentals of research computing, and students who are exposed to CI through their faculty mentors. These stakeholders are considering how to construct laterally and vertically integrated partnerships that draw upon the expertise of practitioners from New York State Education and Research Network, The Carpentries, and the NSF Established Program to Stimulate Competitive Research.<br/><br/>The goal of the CC* planning initiative is to design a flexible, extensible, accessible research cyberinfrastructure (CI) that is capable of meeting the evolving computational needs of City Tech?s most advanced CI users while cultivating a long tail of impact within the larger CI ecosystem. Identified institutional needs include a faculty-led governance structure, systematic rather than ad hoc resource acquisition policies to avoid the creation of silos, increased professional CI support for users, and the creation of a stable and sustainable starting point for systemic CI planning and governance. This planning grant provides that starting point. The work of the project will proceed through a combination of forums, Working Groups, and consultations from entities with expertise in CI development. Topics for investigation include a faculty led leadership structure, data, HPC, and stakeholder engagement. Project deliverables are a five-year Cyberinfrastructure Master Plan and planning for future proposal submissions to the NSF MRI and CC* programs to seek partial support for implementation of the Master Plan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031839","Frontera Travel Grant: Supporting the Science of Hypersonics","OAC","Leadership-Class Computing","07/01/2020","06/03/2020","Daniel Bodony","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","06/30/2022","$10,000.00","","bodony@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849113","FCTaaS: Federated Cybersecurity Testbed as a Service","OAC","Cybersecurity Innovation","01/01/2019","12/17/2018","Salim Hariri","AZ","University of Arizona","Standard Grant","Robert Beverly","12/31/2021","$300,000.00","Nizar Al-Holou, Utayba Mohammad, Cihan Tunc","hariri@ece.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8027","7916","$0.00","With the advent of smart infrastructures, smart buildings, smart grids, smart industry and manufacturing, and smart cities and governments have created more new vulnerabilities than would exist if they were isolated from one another. Sophisticated cyberattacks can exploit these vulnerabilities to disrupt or even completely disable the operations of these infrastructures and consequently can severely impact our national security and all aspects of our life and economy. To overcome the cybersecurity challenges introduced by smart infrastructures, researchers and educators need to better understand the interdependencies among these infrastructures, their implications on cybersecurity issues and how to develop effective defense and protective protection solutions.<br/><br/>The main goal of this project is to explore innovative algorithms to allow the investigators and students to access resources across multiple and heterogeneous testbeds. This approach has the potential to provide new capabilities to conduct  important research such as:<br/><br/>1. How to model, and predict operations and interactions among complex, large, heterogeneous, and dynamic federation of cybersecurity and cyberphysical testbeds; <br/>2. How to secure and protect smart infrastructure resources and services and their interactions under normal and abnormal situations that may be caused by nature, accident, or malicious actions; and <br/>3. How to develop an innovative teaching and training experiments to provide hands-on experiences on how to discover existing or newly created vulnerabilities within an infrastructure or caused by the interactions with other infrastructures, detect and protect their operations against malicious attacks. <br/><br/>Service Oriented Architecture (SOA) are adopted to develop the federated smart infrastructure testbed in order to enable researchers and educators to publish/discover  testbeds that are needed for their research and educational programs. Initially, the Ford  Breadboard Smart Car testbed available at the University of Detroit-Mercy, and  UA testbeds including IoT Testbed, Virtual Cybersecurity Testbed that is currently hosted on Amazon public cloud, and Wireless Security Testbed are used for federation, experimentation and evaluation.  Open communication standards and security tools that are developed at the NSF Center for Cloud and Autonomic Computing are used to maintain the security and privacy of the federated security testbed. These services allow heterogeneous testbeds to communicate their data syntactically and semantically, enabling accurate interpretation of the semantics of data received and the dependencies among these testbeds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019035","CC* Compute: Acquisition of a Lehigh University HPC cluster to enhance collaboration, research productivity and educational impact","OAC","Campus Cyberinfrastructure","07/01/2020","06/05/2020","Edmund Webb III","PA","Lehigh University","Standard Grant","Kevin Thompson","06/30/2022","$399,607.00","Ganesh Balasubramanian, Srinivas Rangarajan, Alexander Pacheco, Lisa Fredin","ebw210@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","8080","","$0.00","This project is constructing and deploying high performance computational capability at Lehigh University that will enable new research collaborations across Physics, Chemistry, Biology, Computer Science, Engineering.  The work directly supports NSF?s mission to promote the progress of science by also providing critical infrastructure for broader incorporation of computation in science and engineering research pursuits.  Further, a portion of the new resources is dedicated to education around computation, including support of efforts to increase the number of members of underrepresented populations in STEM-related professions. Finally, a portion of the resources is dedicated to contributing to the national open science grid (OSG). <br/><br/>The new resource combines CPU and GPU nodes to further broaden the applications and associated research that are supported including electronic structure calculations, atomistic and coarse-grained molecular simulations, Monte Carlo simulations, data science, and deep learning models.  It also allows researchers who have traditionally utilized CPU-based architectures to explore GPU-based computation.  Some of the specific scientific drivers around which new collaborations will be built include (i) understanding fundamental mechanisms of heterogeneous catalytic processes, (ii) realizing predictive design of bulk heterojunction organic solar cells, (iii) predicting thermo-mechanical properties of advanced materials, and (iv) elucidating mechanisms behind flow responsive proteins in human blood.  Work under this support expands the Lehigh community?s research connections and advances Lehigh?s ability to generate high impact research, educate the next generation of scientists and engineers, and expand the usage of high performance computation and simulation-guided science in research and education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925678","CC* Networking Infrastructure: Network for Data Driven Science in Allied 21st Century Smart Multi-Campus System: A Use Case Design Through Kent State's Sharable Science DMZ","OAC","Campus Cyberinfrastructure","07/15/2019","01/30/2020","Javed Khan","OH","Kent State University","Standard Grant","Kevin Thompson","06/30/2022","$383,734.00","Paul Schopis, Philip Thomas, Mark Fullmer","javed@kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","CSE","8080","","$0.00","This project from Kent State University (KSU) designs a ScienceDMZ sharable by the ten KSU campuses spread across northeastern Ohio aligned with NSF's goal of to innovate more scalable approaches to expand advance cyber infrastructure for massive data drive science. A ScienceDMZ at KSU's main campus connects to OARnet's optical exchange to have 100 Gbps unimpeded transfer rate capacity. KSU and OARNet teams to build a virtual DMZ perimeter over a highly-responsive regional WAN allowing researchers from bandwidth constrained regional campuses to access the cyber-facility with uniform access. IPv6, and shared network innovations like PerfSONAR and InCommons are employed. <br/><br/>The ScienceDMZ leverages a broad set of compelling big-data projects. The project launches outdoor ultra-high speed wireless access infrastructure in campuses connected to the ScienceDMZ to facilitate big-data-driven dense sensor and IoT research projects. This project is unique in the sense that a shared regional science DMZ is leveraged across ten campuses throughout north-eastern Ohio. The region of the Kent's allied campuses is the Rust Belt of America. The project brings the world of data-driven STEM research closer to this mass of students, a large percentage of whom are Pell-eligible and/or first-generation college students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2029312","IRNC Core Improvement: SXTransPORT Pacific Islands Research and Education Network","OAC","International Res Ret Connect","08/15/2020","10/14/2020","David Lassner","HI","University of Hawaii","Continuing Grant","Kevin Thompson","07/31/2025","$3,060,763.00","Gwen Jacobs, Jason Leigh, Christopher Zane, Garret Yoshimi","david@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","7369","","$0.00","The SX-TransPORT Pacific Islands Research and Education Network (PIREN) project supports the primary Research & Education (R&E) network backbone connecting Australia, New Zealand, Guam, and points beyond.  Based in Hawaii, the project enables major advances in astronomy, oceanography, coral reef research, high energy physics, biomedical research, and data science. Primary PIREN partners are the Pacific Wave distributed exchange, which ensures that all PIREN-connect networks have full access to the entire domestic and global R&E network fabric, and the Network Startup Research Center (NSRC), which supports the development of campus networks as well as training and education for Pacific Islander network engineers, a highly underrepresented group in STEM.<br/><br/>the PIREN project leverages both the international fiber optic systems that connect to multiple Hawaiian islands as well as the unparalleled international astronomy resources on Maunakea and Haleakala.   The PIREN project also established and operates, in partnership with the University of Guam, the Guam Open Research and Education Exchange (GOREX), strategic new R&E network infrastructure in the Pacific that interconnects multiple submarine cable systems from the US, Asia and Australia to provide more resilient and lower latency paths among major research partners including Japan, Hong Kong, Singapore, and Southeast Asia.  PIREN is also the principal U.S. initiative to advance R&E networking among underserved Pacific Islands, which now have increased access to fiber-optic systems that interconnect in major hub locations such as Hawaii and Guam, and which are on the forefront of sustainability challenges arising from climate change and sea-level rise.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018551","CC* Compute: SIUE Campus Cluster","OAC","Campus Cyberinfrastructure","07/01/2020","06/05/2020","Daniel Chace","IL","Southern Illinois University at Edwardsville","Standard Grant","Kevin Thompson","06/30/2022","$395,580.00","Carolyn Butts-Wilmsmeyer","dchace@siue.edu","Campus Box 1046","Edwardsville","IL","620250001","6186503010","CSE","8080","","$0.00","The goal of this project is to enhance high performance computing resources at Southern Illinois University Edwardsville to support a variety of research and education activities.  Specifically, current campus facilities do not provide sufficient GPU or storage resources needed to enable work with large data sets.  Additionally, SIUE campus IT staff participation increases experience supporting cyberinfrastructure resources to better support faculty, students, and collaborators.<br/><br/>This hardware directly supports ongoing projects and teaching activities which include: the use of machine learning models to predict complex phenotypic traits, development of high order accurate numerical methods for problems governed by hyperbolic partial differential equations, understanding the mechanism behind the quantum phenomenon in chemical reactions, drug interactions, and cybersecurity.  Additional activities include faculty outreach in incorporation of high-performance computing into classroom exercises and curriculum.<br/><br/>Commitment to sharing this computing environment through the Open Science Grid (OSG) and other collaborative efforts increases awareness and adoption of other shared resources by the SIUE community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916613","BD Hubs: Collaborative Proposal: Midwest: Midwest Big Data Hub: Building Communities to Harness the Data Revolution","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/30/2021","Catherine Blake","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Martin Halbert","05/31/2023","$2,145,844.00","William Gropp, Catherine Blake, Elif Ertekin","clblake@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","024Y","062Z, 8083, 9102","$0.00","This project builds on a prior Midwest Big Data Hub effort. In 2015 stakeholders in the Midwest region of the United States formed a consortium of partners and working groups called the Midwest Big Data Hub (MBDH).  MBDH aimed to help member organizations working in Big Data coordinate current activities and launch new collaborative projects.  The project included stakeholders in the twelve states of the Midwest Census region (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin) and six leading universities that support hundreds of researchers, technologists, and students.  This hub provides a basis for collaboration and outreach that increases the potential for benefitting society. <br/><br/>The current award is a collaboration among five academic sites (Indiana University, Iowa State University, UIUC/NCSA, the University of Michigan, the University of North Dakota, and the University of Minnesota - Twin Cities).  The project focuses on priority areas that are important to the region and can also be influential on the national stage. <br/>  -  The five thematic areas of focus, and the institutional partner leading that thematic area, are: Digital Agriculture (led by Iowa State); Smart, Connected, and Resilient Communities (Indiana University); Water Quality (University of Minnesota); Advanced Materials and Manufacturing (UIUC); and Health and Biomedicine (University of Michigan). <br/>  -  Three cross-cutting areas that are emphasized across the project are: data science education and workforce development; cyberinfrastructure, data access and use; and communication and community development.<br/>The priority areas have regional relevance and also have the prospect for integration into societal contexts at the national level. The overall goal is to enable the use of existing and emerging cyberinfrastructure and best practices to improve access to and use of data.  The project plans to reach out to the Midwest community at large and to connect people, resources, and organizations. Ties to Big Data Hubs in three other regions provide a means to advance knowledge across these fields at the national level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916481","BD Hubs: Collaborative Proposal: West: Accelerating the Big Data Innovation Ecosystem","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/30/2021","Christine Kirkpatrick","CA","University of California-San Diego","Cooperative Agreement","Martin Halbert","05/31/2023","$755,059.00","Christine Kirkpatrick","christine@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","024Y","062Z, 8083, 9102","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The West Big Data Innovation Hub builds and strengthens strategic partnerships -- harnessing the data revolution to address scientific and societal challenges. Whether working towards the future of data-informed healthcare or tackling projects in disaster recovery, the Hub envisions a diverse community empowered to contribute to areas of national priority. The Hubs focus on data science activities and initiatives that inspire cross-sector collaboration and exemplify the need for multi-disciplinary approaches.<br/><br/>With this award, the West Big Data Innovation Hub will: (1) Develop and enable translational data science (TDS) pilot projects in our thematic areas to highlight the value of cross-sector collaboration, enhance fluency with real-world use cases, and emphasize a pragmatic and holistic view of the data and analytics lifecycles. We envision our signature TDS initiatives for 2019-2023 to include: Fire and Water: Data Collaborative for the Future of Natural Resource Management; Stress-Testing Access for Road Video; and Housing Instability: Trusted Data Collaborative for Responsible Data Management. (2) Facilitate team formation across different stakeholder groups through our activities, capturing inspirational stories and encouraging teams to reflect and share their insights about cross-sector collaboration. (3) Raise awareness of regional opportunities and inspire work in priority areas including Natural Resources & Hazards, Metro Data Science, Health & Medicine, Data-Enabled Discovery & Learning, Data Sharing, Cloud Computing, and Responsible Data Science. (4) Support data science education and workforce development. Recognizing that a diverse, multi-faceted workforce is key to addressing current scientific and societal challenges, we will continue to expand our portfolio of education and workforce development efforts, including a focus on Train-the-Trainer sessions, Pedagogy and Practice, Data Science for Social Good and the Data Science Corps, Findable Accessible Interoperable and Reusable (FAIR) data, and institutional change -- providing a platform for broadening participation in data science. Core to our progress in Programmatic Activities, Socio-Technical Shared Resources and Services, and Education and Workforce Development Activities will be a coordinated evaluation, opportunities for scaling regional successes to the national network of Big Data Hubs, and strategic efforts for Hub sustainability including the development of external funding streams. These efforts will be designed to enable community input and to strengthen channels for ongoing dialogue.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018112","CC* Networking Infrastructure: Deploying a Science DMZ to Advance Research at the University of Montana","OAC","Campus Cyberinfrastructure","07/01/2020","06/11/2021","Zachary Rossmiller","MT","University of Montana","Standard Grant","Kevin Thompson","06/30/2022","$307,645.00","Erin Landguth, Tung-Chung Mou, Jeffrey Good","zachary.rossmiller@mso.umt.edu","32 CAMPUS DRIVE","Missoula","MT","598120001","4062436670","CSE","8080","9150","$0.00","A major component of the University of Montana's (UM) mission is to generate world-class research and creative scholarship. To that end, UM established a state-of-the-art Modular Data Center, ensured dedicated high-speed connections for key campus sites, and deployed a shared computing cluster. A team of researchers and IT professionals, reviewing the cyberinfrastructure plan alongside researcher needs, determined that a well-configured dedicated network for high-performance dataflow is needed to advance UM's mission. This project installs a Science DMZ (UM-DMZ), a separate friction-free network dedicated to scientific data transfer, into UM's research infrastructure. The UM-DMZ, along with a robust research infrastructure, serves and attracts quality students, educators and researchers who require access to high performance end-to-end data transfers to conduct their work.<br/><br/>Hardware includes using an Arista network switch that allows for low-latency and deep packet buffering. It supports sFlow and meets the needs for IPv4 and IPv6 scientific applications. Various sized Data Transfer Nodes are deployed and a solid security foundation using Zeek IDS system and network security policies are being implemented. The UM-DMZ serves research efforts from diverse disciplines that impact the well-being of society. This includes research on the environment and climate change, genomic studies, the development of tools used to further scientific discovery, and innovative student led research projects. Common themes uniting these diverse projects are enhanced collaboration, high-speed data transfer, and educational opportunities for students. Student participation through advanced network courses, internships, and student led research, are important aspects of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925484","CC* Networking Infrastructure: Building a Science DMZ for Data-intensive Research and Computation at the University of South Carolina","OAC","Campus Cyberinfrastructure","07/01/2019","07/01/2019","Jorge Crichigno","SC","University of South Carolina at Columbia","Standard Grant","Kevin Thompson","06/30/2022","$498,525.00","Steffen Strauch, Andreas Heyden, Neset Hikmet, Paul Sagona","jcrichigno@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","8080","9150","$0.00","The University of South Carolina (UofSC) is establishing a new network, namely a Science DMZ, operating at 100 Gbps. The Science DMZ supports current research moving terabyte-scale data between UofSC and national laboratories (e.g., Argonne, Fermi, Oak Ridge, Savannah River, Los Alamos), university collaborators, and the national network of supercomputer centers (XSEDE). The project serves the national interest, as it addresses the need to connect UofSC to the national ""cyber-highway"" system to share big science data, hence promoting collaboration and national competitiveness, aligned with NSF's mission. The new cyberinfrastructure also permits researchers to exchange large datasets with collaborators geographically distributed across the world. Examples include nuclear physics results from the Paul Scherrer Institute in Switzerland and observation files from the Cryogenic Underground Observatory for Rare Events (CUORE) in Italy. <br/><br/>The elements of UofSC's Science DMZ include: i) data transfer nodes (DTNs), built for sending/receiving data at a high speed over wide area networks; ii) high-throughput, friction-free paths connecting DTNs, instruments, storage devices, and computing systems; iii) measurement devices to monitor end-to-end paths; and iv) security policies tailored for high-performance environments. The proposed Science DMZ substantially increases the bandwidth to compute and XSEDE resources, permitting their use on digital image correlation, semiconductor material development, DNA/RNA sequencing, and other areas. Additionally, UofSC hosts key national resources and centers including the first U.S. deployed Time of Flight-Inductively Coupled Plasma-Mass Spectrometer, NOAA's National Estuarine Research Reserves database, McNair Aerospace Center, and Baruch Institute. These resources are now more efficiently used by researchers and collaborators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004235","Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC)","OAC","Software Institutes","06/01/2020","05/01/2020","Michael Mahoney","CA","International Computer Science Institute","Standard Grant","Seung-Jong Park","05/31/2024","$718,740.00","","mmahoney@icsi.berkeley.edu","2150 Shattuck Ave, Suite 1100","Berkeley","CA","947041345","5106662900","CSE","8004","077Z, 7925, 8004","$0.00","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.<br/><br/>The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004645","Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying  algorithms for a fully GPU-based first trigger stage","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2020","05/01/2020","Mike Williams","MA","Massachusetts Institute of Technology","Standard Grant","Robert Beverly","06/30/2023","$310,000.00","","mwill@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253, 7244, 8004","075Z, 077Z, 7569, 7923","$0.00","The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.<br/><br/>The LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839021","EAGER: Environmental drivers of biodiversity: leveraging a history of NSF-funded research to test models of butterfly responses to global change","OAC","NSF Public Access Initiative","10/01/2018","07/31/2018","Leslie Ries","DC","Georgetown University","Standard Grant","Martin Halbert","09/30/2021","$299,989.00","","Leslie.Ries@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7414","7916","$0.00","Our most pressing ecological priority is to determine how human activity is driving global shifts in biodiversity and how we can balance preserving ecosystem function with the needs of a growing human population. The PI proposes new ecological research on the Pierid family of butterflies that have been the subject of research on thermal responses over the last 40 years.  The overarching goal of this research is to extend and generalize Species Distribution Models (SDM), the dominant modeling approach to understanding large-scale shifts of biodiversity in the face of global change.   The PI will leverage that 40 year legacy of thermal response research to develop general models of how butterflies respond to changing environments.  In order to verify how well these models perform and their ability to make both species-specific and general predictions, models will be validated with large-scale monitoring data sets.  There is a growing resource of citizen-science monitoring data but there are also several NSF-funded academic monitoring programs that have occurred over the years.  Another aspect of this project is to bring together those academic and citizen science data into a unified publicly-available data set.  The research is expected to advance macrosystems ecology study of thermal ecology and responses of biodiversity, especially ectotherms while also serving as a strong demonstration of data reuse by focusing on a well-studied and wide-spread group of butterfly species, the Pierids.<br/> <br/><br/>Research in macrosystems ecology requires two disparate ecological data types that are rarely generated or employed by the same research community: mechanistic, experimental data and data from large spatiotemporally-replicated monitoring programs. The two dominant environmental factors driving the distribution and abundance of butterflies are thermal environment (impacted by climate) and host-plant availability (impacted by land use change and climate). There are 14 previous and current NSF-funded projects focused on primarily thermal, but also nutritional drivers of performance in one family of butterflies, Pierids. Thermal environments have several impacts on butterfly performance. Indirectly, temperature drives the seasonal timing and distribution of host-plant (food) resources. It also provides energy for growth for developing caterpillars. Host-plants are also a key component of butterfly development, not just for the obvious reason that they acquire all their nutrition for growth from these plants, but the quality of the plants also determines their growth rate. There has been a long legacy of research on the thermal and nutritional constraints of growth for many insects, but these models have not yet been fully employed to make range-wide predictions for butterflies. We propose to bring together a legacy of NSF-funded research on butterfly responses to environmental change with long-term monitoring data that can be used, respectively, to generate and test mechanistic SDMs over multiple spatiotemporal scales.<br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104003","Collaborative Research: Elements: Shared Data-Delivery Infrastructure to Enable Discovery with Next Generation Dark Matter and Computational Astrophysics Experiments","OAC","PHYSICS AT THE INFO FRONTIER, Software Institutes","08/01/2021","07/14/2021","Amy Roberts","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Robert Beverly","07/31/2024","$335,984.00","","amy.roberts@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","7553, 8004","077Z, 7569, 7923, 8004","$0.00","Modern laboratories provide unprecedented sensitivity to the many different galactic-messengers that stream through our planet by the minute: cosmic rays, light from distant galaxies, elusive neutrinos, and possibly dark matter.  Combining this information with models and data from simulations provides insight into how our universe began and continues to evolve -- the scales at which objects first collapsed, the development of stars and galaxies, and the dynamics within our own galaxy.<br/> <br/>However, this data is often inaccessible: scientists within an experiment or community struggle with the complex, custom-built programs they use to access the data.  And switching to a standard format is usually not an option: these data formats are designed for requirements that often do not include cross-experiment synthesis or linking.<br/> <br/>Junior scientists - let alone the public - can struggle to generate new insights from the data because the data is difficult to access, understand and analyze.  The cross-cutting inquiry that could arise from clever reuse and combination of data from different experiments and simulations is rarely conducted.<br/> <br/>This project makes data accessible both within and across collaborations, providing the infrastructure to search for signals in detectors across the globe.  Extending existing efforts to improve data access makes this project possible: yt is software that provides uniform access to simulation data; Kaitai is a data-description language that enables easy access to any data format; Rucio and other tools provide a standard interface that allows data downloads; and ServiceX can identify, subset and process data with little effort from the end user.<br/> <br/>Scientists have built experiments that offer an incredible wealth of information about our world.  This project works to make that information accessible to everyone.<br/><br/>Technical Description<br/><br/>The Personal Data-Delivery infrastructure (PONDD) addresses the data challenges of existing dark matter and astrophysics experiments while requiring no changes to existing data formats.  This non-invasive, no-changes-necessary support for any file format provides opportunities to expand beyond our two identified use cases, dark matter searches and astrophysics simulations, into many other data-driven science domains that rely on custom file formats. <br/> <br/>This work delivers an infrastructure that seamlessly delivers data in a well-supported format (such as Parquet) from multiple sources.  To successfully deliver cross-experiment data to end users, we bring together ongoing projects from High Energy Physics and the broader NSF community; while this project will involve development of software products (yt and Kaitai) it will also include synthesis of existing investments in cyberinfrastructure and efforts to improve their long-term sustainability. <br/><br/><br/>This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019000","CC*Compute: From classroom to the lab: NMSU responds to the changing HPC landscape in New Mexico","OAC","Campus Cyberinfrastructure","07/01/2020","05/27/2020","Diana Dugas","NM","New Mexico State University","Standard Grant","Kevin Thompson","06/30/2022","$399,869.00","Phillip De Leon, Piyasat Nilkaew","dugasdvt@nmsu.edu","Corner of Espina St. & Stewart","Las Cruces","NM","880038002","5756461590","CSE","8080","9150","$0.00","New Mexico State University (NMSU), a Minority-Serving Institution and Hispanic-Serving Institution, recognizes the vital need for universal access to a high performance computing (HPC) facility.  Increasing the computational resources at NMSU, including storage, supports NMSU?s high research need, instructors interested in incorporating HPC into their classroom activities, and state-wide collaborations with faculty who are excited to have a supportive HPC team to assist their classrooms.  New Mexico as a state has a high need for personnel experienced with HPC use, but lacks resources dedicated to student learning.  By utilizing existing relationships between NMSU and other NM-based universities, the new resources increase HPC-based classroom activities around the state through a dedicated queue.  <br/><br/>Students trained in HPC use are in high demand both in industry and academia, providing our students with new opportunities.  Much of the research performed on the HPC is either unfunded or funded through smaller grants that cannot purchase dedicated HPC resources and many users start on their HPC journey without the basic knowledge of Unix or how to use an HPC.  <br/><br/>Time on regional and national super clusters is valuable and not a good learning environment for those who are beginning their venture into computing-intensive science.  With extensive collaborations across institutions and already having streamlined NMSU-affiliated user on-boarding and account creation, NMSU is key in increasing HPC knowledge across the entire state.<br/><br/>This activity expands the existing HPC resources at NMSU by roughly 30% through the acquisition of 10 compute nodes each with 36 CPU cores, at least 256 GB of RAM and 960 GB of local storage for a total of 360 cores, two GPU nodes similar to the compute nodes but with dual Tesla V100 GPUs, 600 TB of network connected storage, and Infiniband interconnect hardware. The cluster supports both research and educational activities in a range of fields from biology and environmental sciences to physics, chemistry, and material science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762493","Spokes: MEDIUM: SOUTH: Collaborative: Enhanced 3-D Mapping for Habitat, Biodiversity, and Flood Hazard Assessments of Coastal and Wetland Areas of the Southern US","OAC","BD Spokes -Big Data Regional I","08/15/2018","08/16/2018","Frank Muller-Karger","FL","University of South Florida","Standard Grant","Alejandro Suarez","07/31/2022","$857,998.00","Timothy Dixon","carib@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","024Y","8083","$0.00","The risk to coastal populations and infrastructure from flooding due to sea level rise, severe storms, and river discharge will increase for U.S. southern states. The vision of this project is that communities occupying low-lying coastal areas of the southern US will be protected and develop in a sustainable manner through planning based on knowledge, conservation, and wise use of sensitive lands. Researchers from the University of South Florida's College of Marine Science and the School of Geosciences, Texas A&M University Corpus Christi, and Google Earth Engine are collaborating with the South Big Data Hub through this project to develop more accurate, ultra-high resolution topographic, land cover, and urban environment geospatial products. The project examines in detail areas that were directly impacted by Hurricanes Harvey and Irma in 2017, and identifies flood-prone areas across the region. The 3D maps show habitat diversity, needed to plan for conservation and development in these important ecosystems.<br/><br/>This project will develop the improved topographic and land cover maps of the south States within 50 Km of the coast from Texas to Florida (an area >220,000 square Km). The maps will be constructed using a Big Data approach, using detailed historical airborne LiDAR (Light Detection and Ranging) data collected from airplanes merged with high spatial resolution (<2 m pixel) multispectral commercial satellite imagery. The project will also include research into detailed 3D mapping of urban areas using Structure-from-Motion (SfM) methods; specifically the project will map portions of Houston/Corpus Christi in Texas, and Tampa/Saint Petersburg in Florida, using Kite Photography and light aircraft. The production of land cover maps and digital elevation models requires the fusion of very large amounts of disparate data and efficient, automated techniques. The project will develop the strategies to aggregate these data into useful products using Google Earth Engine and a High Performance Computing cluster. The project will distribute all products openly via NOAA's Digital Coastal portal.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925752","CC* Regional: Integrating the Colorado Western Slope Research and Education (R&E) Community into the National R&E Infrastructure","OAC","Campus Cyberinfrastructure","08/01/2019","07/16/2019","Marla Meehl","CO","University Corporation For Atmospheric Res","Standard Grant","Kevin Thompson","07/31/2022","$84,994.00","Scot Colburn, Fabian Guerrero","marla@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","8080","","$0.00","High-speed, reliable networking infrastructure is vital to an organization's ability to thrive in today's rapidly evolving scientific and technical environments. Such network connectivity is essential for success in business, science, communication, global collaboration, education, and outreach. The network connectivity in the Western Slope Rocky Mountain region of Colorado is challenged. This project will bring together research and education (R&E) stakeholders to identify specific areas of need, work with public and private partners to identify infrastructure and other network resources, define the scope of the network to be constructed, and design a network. Stakeholders benefit from gaining knowledge of available resources for better network connectivity, building relationships with the regional and national network providers, technical training, and access to high-performance network connectivity. Project goals include ensuring underserved populations from rural areas of Colorado and Northern New Mexico have a diverse, well-educated workforce and that students are offered the highest caliber educational opportunities and resources. Improved network access supports economic development and growth, increased diversity, and development and expansion of employment opportunities.<br/><br/>The project team identifies and documents network resources and provides a network design that addresses feasibility, costs, and deployment of the network. The team performs site visits to identify, document, and assess network paths, and one-time and recurring costs. Regular meetings with stakeholders and vendors are conducted. A workshop is planned to teach stakeholders about advanced network tools and technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2135446","EAGER: Computer-Assisted Redaction and Anonymization of Scholarly Communications and Products (CARASCAP)","OAC","NSF Public Access Initiative","07/15/2021","07/09/2021","Christopher Lee","NC","University of North Carolina at Chapel Hill","Standard Grant","Martin Halbert","06/30/2022","$299,858.00","","callee@ils.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7414","7916","$0.00","The Computer-Assisted Redaction and Anonymization of Scholarly Communications and Products (CARASCAP) project will produce a proof-of-concept open-source application stack to assist research teams and individual scholars in identifying, documenting, and redacting sensitive and personally identifying information within their research products.  The potential presence of personally identifying information (PII) and other sensitive information is a significant inhibitor to public access to datasets and other products of publicly funded research.  Without reliable and cost-effective processes for identifying such information, the default response is most often to indefinitely prevent the public from accessing entire collections of research products.  By developing new components and tools for iterative redaction functions incorporated into workflows to prepare datasets for public dissemination, this project will foster a stronger ecosystem of research data publishing efforts.  <br/><br/>The software will be developed primarily in Python, MIT Licensed, and packaged for distribution on the Python Package Index (PyPI).  Independent modules will interpret and modify the source material data structures. For this prototype phase, the project will focus on formats likely to be of interest to a broad range of collections, including open text scraped from web pages at specific URLs, text formats, and modern office formats (e.g., PDF,.odt, .docx, .pst, .ost).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126229","CC* Compute: A Balanced Cluster For Science and Engineering in the Great Lakes Region","OAC","Campus Cyberinfrastructure","10/01/2021","07/07/2021","Philip Chang","WI","University of Wisconsin-Milwaukee","Standard Grant","Kevin Thompson","09/30/2023","$400,000.00","Ryo Amano, Allen Evans, Mahsa Dabaghmeshin","chang65@uwm.edu","P O BOX 340","Milwaukee","WI","532010340","4142294853","CSE","8080","9102","$0.00","The University of Wisconsin-Milwaukee (UWM) is building and deploying a modern, high-performance, high-throughput, and compute-on-demand cluster that balances the growing but distinct research needs for computing, storage, and memory in the diverse areas of astronomy/astrophysics, atmospheric sciences, biomedical sciences, engineering, freshwater sciences, physics, and other fields. This cluster enables UWM researchers to push the computational frontiers of their research, create cutting-edge computational tools, and scale their computations to run on national resources such as XSEDE. The system is also integrated into the Open Science Grid to enable full utilization of idle computing resources to enhance local, regional, and national scientific computational capabilities. Finally, the cluster enhances undergraduate and graduate education and research by providing superior access to large-scale computing resources for student research and computationally intensive classes.<br/><br/>The cluster has a total system capacity of 1,568 cores, 9 TB of RAM, 504 TB of storage, and 27K GPU cores.The cluster is comprised of a login/storage server, eight compute nodes, two high-memory nodes, two GPU nodes, a 100 Gbps InfiniBand interconnect, and a 10 Gbps Ethernet network. The nodes are locally accessible to UWM researchers, available to external researchers through the Open Science Grid, and secured with the best practices in cybersecurity and monitoring. The cluster achieves a balance between large CPU counts, GPU cores, large memory, and storage to provide a flexible and responsive resource for a diverse group of researchers to perform their complex computational analyses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018979","CC* CRIA: Building CI Strategies and Capacity at the Tribal Colleges","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","Algirdas Kuslikis","VA","American Indian Higher Education Consortium","Continuing Grant","Kevin Thompson","06/30/2022","$249,995.00","","akuslikis@aihec.org","121 Oronoco Street","Alexandria","VA","223142015","7038380400","CSE","7231, 8080","","$0.00","The AIHEC CC* CRIA project: Building CI Strategies and Capacity at the Tribal Colleges, addresses the issue of bringing the nation?s 37 Tribal Colleges and Universities (TCUs) into the national cyberinfrastructure-enabled STEM research and education community. This comprehensive CI strategy focuses on CI training, planning and community-building involving both STEM faculty and TCU IT organizations, advancing the capacity of TCU faculty and students to participate in the national STEM research and education infrastructure. CI research and education stakeholders participating in the project include Internet 2, the Texas Advanced Computing Center (TACC), the Center for Computationally Assisted Science and Technology (CCAST), the University of Colorado Boulder Research Computing Center, and the Oklahoma University Supercomputing Center or Education and Research (OSCER). The project implements a cost-effective model for broadening participation of an important historically underrepresented population in STEM.  <br/><br/>The project implements a connectivist model of knowledge management in which interactions among members of a network adaptively distribute resources needed to support individual and group activities to achieve common goals of the network membership. In this project, TCU STEM faculty and IT professionals and national CI STEM domain-specific researchers comprise this extended adaptive network. Project-related activities involve interactions within the network (e.g. training events, work with data analytic tools) that generate new knowledge and skills, build the collective capacity of the TCU members of the network to develop, implement and support research and education programming. Capacity-building interactions among members of this extended network include knowledge-sharing, mentoring and collaborative problem-solving.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115075","CICI:UCSS:Improving the Privacy and Security of Data for Wastewater-based Epidemiology","OAC","Cybersecurity Innovation","07/01/2021","05/21/2021","Stephanie Forrest","AZ","Arizona State University","Standard Grant","Robert Beverly","06/30/2024","$499,592.00","Rolf Halden, Ni Trieu, Heewook Lee","Stephanie.Forrest@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8027","7923, 8027","$0.00","The field of Wastewater-Based Epidemiology (WBE) collects and analyzes data from sewage systems that relate to public health. These data include genetic and chemical biomarkers of identity, ethnicity, behavior, consumption, pollution, and pathogenic infections. Interest in WBE exploded recently, as researchers turned to wastewater samples to estimate SARS-CoV-2 infection levels in local populations and inform public health responses to the pandemic. Because data collected from wastewater are aggregated at the population level, they are typically assumed to be anonymous, and are therefore not subject to health privacy or other regulatory protections. These data are most useful when shared, yet sharing raises security and privacy issues, for individuals as well as neighborhoods, schools and governments.  The project is developing technology and tools to support legitimate uses of WBE datasets while minimizing abuse, and it is designing protocols for protecting individual and organizational privacy.  These tools will encourage wide participation in WBE consortia by enabling organizations to protect their data from adverse uses, which in turn will help improve the health of the general public and reduce morbidity and mortality.<br/><br/>The project addresses three key security threats: those posed by individual queries; those posed by sharing, whether through joint queries or large-scale association studies; and those posed by the presence of human genetic material in wastewater samples.  These threats are mitigated by: (1) supporting secure queries on wastewaster data using homomorphic encryption; (2) enabling federated data analysis based on secure computation; and (3) protecting genomic privacy with differential privacy. These components are being implemented to allow different entities to execute query processing and train Machine Learning models on joint databases while maintaining the privacy and format of each database. The system is being evaluated using real-world datasets with use cases that match important threat scenarios.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924256","CyberTraining: Pilot: Modeling Excited State Dynamics in Solar Energy Materials","OAC","CyberTraining - Training-based, OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Chem Thry, Mdls & Cmptnl Mthds","09/01/2019","07/11/2019","Alexey Akimov","NY","SUNY at Buffalo","Standard Grant","Bogdan Mihaila","08/31/2022","$299,621.00","Jeanette Sperhac","alexeyak@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","044Y, 1253, 1712, 6881","026Z, 1711, 8396, 8607, 9179, 9263","$0.00","Conversion of solar energy into electricity and fuels has the potential to revolutionize environmentally-friendly and sustainable energy production. The widespread adoption of solar panels is partially limited by the relatively low efficiencies and relatively high costs of the underlying solar-harvesting materials. The design and discovery of new efficient and inexpensive solar energy conversion materials can be accelerated by computational modeling of excited states dynamics in these systems. Studies of excited states dynamics can reveal unwanted energy loss and charge transfer in these systems, and suggest ways to control them. These modeling strategies require scientists to master advanced theories, specialized software and cyberinfrastructure for modeling excited states. Nonetheless, training in this area remains relatively scarce; the community is often unaware of the available cyberinfrastructure, lacks the best practice guidelines, and may experience entry barriers to employing these advanced tools. This pilot project fills the above gaps by providing targeted training to young scientists in the proficient use of advanced cyberinfrastructure for modeling excited states dynamics in solar energy materials, raising new workforce capable to pursue work that expressly promotes energy security. In this way, the project directly serves the national interests, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/>The project leverages and combines existing cyberinfrastructure to produce a versatile platform for advanced training: the general-purpose Libra code library, for modeling excited states dynamics; and the Virtual Infrastructure for Data Intensive Analysis (VIDIA) platform, for web-based data analysis and visualization. The resulting cyberinfrastructure tightly integrates these two components into a gateway to host training in modeling excited states dynamics of solar energy materials. Using this gateway, intensive training for 50 graduate students from across the United States is provided in two two-week summer school sessions. Building on the capabilities and resources of Libra/VIDIA, the summer school covers the fundamentals of nonadiabatic and quantum dynamics theories; best practices for scientific code development; and practical hands-on sessions that focus on a variety of existing tools for excited states dynamics. Experts in the fields of nonadiabatic dynamics and solar energy materials modeling provide instruction on special topics. The Libra/VIDIA software ecosystem also serves as a scientific gateway that enables online education, interactive classroom teaching, and the broader participation of the community in a variety of educational and research projects covering the theory of quantum dynamics and material science. Both the software platform and the training sessions provided by this project facilitate the adoption of advanced methods and tools for modeling nonadiabatic and excited states dynamics developed by the scientific community.  The Divisions of Chemistry and Materials Research in the Mathematical and Physical Sciences Directorate and the Office of Cyberinfrastructure in the Computer and Information Science and Engineering Directorate contribute funds to this award.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106738","Collaborative Research: OAC Core: Robust, Scalable, and Practical Low Rank Approximation","OAC","OAC-Advanced Cyberinfrast Core","07/15/2021","07/02/2021","Haesun Park","GA","Georgia Tech Research Corporation","Standard Grant","Seung-Jong Park","06/30/2024","$275,000.00","Richard Vuduc","hpark@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","090Y","026Z, 7923, 9102","$0.00","Nearly all aspects of society are affected by data being produced at a faster rate in recent years. The data from experiments, observations, and simulations are not only in more classical science and engineering domains but also in numerous other areas such as businesses tracking more and more facets of consumer behavior, and social networking capturing vast amounts of information on the relationships between people and their actions and interactions. There is a strong need to distill a set of data into a smaller representation that separates useful information from noise and captures the most important trends, patterns, and underlying relationships.? Such a representation can be used for direct interpretation of hidden patterns or as a means of simplifying other data analytic tasks.? This project addresses these challenges by studying a concept from linear algebra called low rank approximation.? The project develops techniques that faithfully distill the meaningful information within a data set.? The algorithms are also designed to exploit high-performance computers so that analysts can get results more quickly and tackle larger problems.? The overall effort in the project is expected to close the gap between algorithms that can effectively handle very large-scale problems and the data analyst?s ability to convert raw input into meaningful representations and actionable insight.<br/><br/><br/>The matrix and tensor low rank approximations being studied in this project serve as foundational tools in numerous science and engineering applications. Imposing constraints on the low rank approximations enables the modeling of many key problems, and designing scalable algorithms enables new applications that reach far beyond classical science and engineering disciplines. In particular, mathematical models with nonnegative data values abound, and imposing nonnegative constraints allows for more accurate and interpretable models. Variants of these constraints can be designed to reflect additional characteristics of real-life data analytics problems. The primary goals of this project are (1) to develop robust techniques for evaluating computed low rank approximations for rank and model determination, (2) to develop scalable parallel algorithms for large and robust low rank approximations on today?s extreme-scale machines, and (3) to provide end users the practical tools required to compute and analyze solutions at scale. Typical data and application scientists use Python or Matlab to iteratively compute, visualize, and evaluate solutions, and they are limited to small data sets with feasible memory and computational requirements. While high-performance algorithms and implementations exist, end users would not leverage these tools if they cannot rely on the robustness and generalizability of the results. This project aims to close this gap, developing an end-to-end system with scalable solutions for all steps of the data analytics workflow.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925744","CC* Regional: Extended Vital Education Reach Multiple Organization Regional OneOklahoma Friction Free Network (EVER-MORe-OFFN)","OAC","Campus Cyberinfrastructure","07/15/2019","07/09/2019","Stephen Wheat","OK","Oral Roberts University","Standard Grant","Kevin Thompson","06/30/2022","$500,000.00","Kelly McClure, Vladimir Ufimtsev","swheat@oru.edu","7777 South Lewis","Tulsa","OK","741710003","9184957547","CSE","8080","9150","$0.00","The CC* Networking Infrastructure project, Extended Vital Education Reach Multiple Organization Regional OneOklahoma Friction Free Network (EVER-MORe-OFFN), expands the network capabilities of Cameron University (CU), East Central Oklahoma University (ECU) (both upgraded to 10Gbps), and Oral Roberts University (ORU) (upgraded to 100 Gbps) by connecting them to the OneOklahoma Friction Free Network (OFFN) through community-based Science DMZ technologies.  Using 10Gb and 100Gb channels, OFFN interconnects Oklahoma's research organizations. OFFN allows the researchers at each institution to reliably connect to supercomputers and big data repositories, increase computational capabilities, and increase inter-institution collaboration.<br/> <br/>Previously, researchers at the EVER-MORe-OFFN institutions lacked throughput speed when dealing with large data sets. The EVER-MORe-OFFN expansion of OFFN affords these researchers access to a high-speed and efficient network allowing for analyses and simulations by researchers and an expanded repertoire of course projects and assignments available to professors. Additionally, the High Performance Computing Center at ORU is open to the collaborating universities through this expansion.<br/> <br/>Adding these three institutions increases the total OFFN Institutional participation to 12 institutions.  As OFFN expands, so does the serviced population which provides new access to the necessary resources to create research opportunities previously not afforded.  This project has broader implications for the scientific community including the next generation of researchers and scientists unlocking the door to many exciting discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107020","Collaborative Research: OAC Core: Advancing Low-Power Computer Vision at the Edge","OAC","OAC-Advanced Cyberinfrast Core","07/01/2021","07/01/2021","George Thiruvathukal","IL","Loyola University of Chicago","Standard Grant","Seung-Jong Park","06/30/2024","$250,000.00","Neil Klingensmith","gkt@cs.luc.edu","1032 W. Sheridan Road","CHICAGO","IL","606601537","7735082471","CSE","090Y","075Z, 7923","$0.00","This proposal enables low-power edge computers, such as mobile phones, drones, and Internet-of-Things devices, to benefit society. Computer vision is the technology to automatically analyze images and videos. Computer vision on these devices can keep humans safe, for example by spotting dangers in a factory or at a construction site. This project addresses two challenges that hamper practical adoption of computer vision on edge devices. The first challenge is that current computer vision approaches require powerful computers, but these computers are too far away and have long response time. This project brings the computers to the places where data is acquired. The project makes computer vision more efficient, so that visual data can be analyzed by small edge devices like phones and drones. The second challenge is that building complex software for computer vision is difficult. This project provides software engineering support for emerging computer vision technologies. As a result of addressing these two challenges, computer vision on the edge can become feasible.<br/><br/>Bringing computer vision (CV) to devices on the network edge is an essential component of realizing NSF's goal of distributed cyberinfrastructure. This project makes CV on the edge feasible and enables scientific and engineering innovation through improved response time, reduced need for network coverage, and decreased storage costs. This project solves two critical challenges that hinder the transition of edge-based CV into practice. (1) This project makes CV more efficient and edge-friendly. Current CV techniques (e.g., deep neural networks) assume server-class resources (such as graphics processing units, gigabytes of memory); these resources are not available at the edge. This project reduces the resource requirements needed for CV. The methods consider alternative neural network architectures and eliminate redundancies while processing visual data. This project also develops CV-specific distribution techniques to enable edge devices to collaborate on large vision tasks. (2) This project provides software engineering support for CV technologies. Solving real-world CV problems requires engineering new CV applications, often by re-implementing research model architectures as components in new designs. This project develops a library of exemplary CV model implementations for low-power platforms. These exemplars can be used as high-quality components in new CV applications. The project identifies factors that promote and inhibit the reproducibility of CV models. This project also identifies engineering best practices by surveying and interviewing experts in low-power CV and by studying their errors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2136085","Community Needs for Research Data Management in Aquatic Ecosystems","OAC","NSF Public Access Initiative","12/01/2021","07/09/2021","William MacMullen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Martin Halbert","11/30/2022","$40,154.00","","wjohn@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7414","7556","$0.00","This workshop will bring together multiple communities that cross disciplinary boundaries and sectors to focus on shared Aquatic invasive species (AIS) data challenges in the Upper Mississippi River Basin (UMRB). The Midwest Big Data Hub (MBDH) team will work in partnership with a committee of AIS experts and with national organizations including the other three NSF Big Data Innovation Hubs. Intended workshop participants include researchers, outreach and education specialists, and government agency staff.  AIS are a growing threat to freshwater resources, aquatic ecosystems, and wetland plant communities in the Midwest.  In the UMRB the potential for spread and impacts are especially high due to the proximity to the Great Lakes, the interconnected stream network, and an abundance of glacial lakes used for recreation. Much of the data needed to address the questions around aquatic invasive species movement and impact are currently being collected by various academic institutions, government agencies, and other stakeholders. However, the UMRB Region currently lacks a comprehensive inventory of the data available, information about accessibility, and data format standards. Further, AIS management is a multifaceted issue requiring data on numerous interconnected processes. Advances in technologies as well as data management skills are needed to advance data interoperability and replicability to address the challenge of AIS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126301","CC* Planning: Precision Agriculture and the Community","OAC","Campus Cyberinfrastructure","09/15/2021","07/08/2021","Derek Masseth","AZ","Arizona State University","Standard Grant","Kevin Thompson","08/31/2022","$99,967.00","","dmasseth@suncorridor.org","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8080","","$0.00","Water shortages in California, Arizona, and Nevada will impact crop yield and agriculture in the region for years to come.  By further advancing farming innovations in Yuma, Arizona along the Colorado River, the prospects of sustainable growth, improved research potential, and new infrastructure collaborations benefit STEM programs, industry, and the community.  Yuma is Arizona?s Salad Bowl.  The continued introduction of specialized networking and collaborations with higher education and K-12 institutions in the area improves the opportunity to plan and implement broadband and networking aligned with the science of agriculture.  The use of technology to improve crop yields, or precision agriculture, is a required step forward in the deployment of cyberinfrastructure in rural communities.<br/><br/>The planning effort to improve networking, connect science-oriented programs, and experiment with homework gap services is led by Sun Corridor Network in collaboration with Yuma?s agricultural research community, community college service and agricultural programs, Yuma?s largest elementary and high school programs, the University of Arizona?s experimental farms and regional academic organization, and the desert agricultural research organization. The Yuma collaboration planning activities will improve community-based cyberinfrastructure models, inform the next generation of agricultural infrastructure, and bridge science, engineering, and technology programs by addressing the complex needs of the business and science of agriculture.  The Yuma Collaboration introduces broadband and infrastructure planning in support of precision agriculture during a period of extreme drought and the impacts of water shortages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126199","CC* Planning: Virtual Research-Education Ohio (VROhio)","OAC","Campus Cyberinfrastructure","07/15/2021","07/07/2021","Pankaj Shah","OH","Ohio State University","Standard Grant","Kevin Thompson","06/30/2022","$99,994.00","Mark Fullmer","pshah@oar.net","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8080","","$0.00","Governor Mike DeWine allocated $12.1M through the Governors Emergency Education Relief (GEER) program to upgrade OARnet's last-mile internet connections to 40 smaller higher education institutions (HEI) through a unique opportunity. This funding is a result of growing reliance on cyberinfrastructure to support in-person and remote teaching and research. For smaller institutions, having access to additional advanced CI enriches existing programs while enabling new opportunities for research and training that rely on external, shared, and co-developed resources. To foster full use of new connectivity resources, OARnet and a team from nine smaller HEIs (Northwest State Community College, Chatfield College, Lorain County Community College, Terra State Community College, Franciscan University of Steubenville, Sinclair Community College, Columbus State Community College, Xavier University, Baldwin Wallace University), service providers (OSC and the CWRU Electron Microscopy Facility) supported by national entities (EPOC, The Quilt, InCommon/Internet2, Trusted CI), are planning two CC* proposals - 1. ""Regional Connectivity for Small Institutions of Higher Education"" track and 2. ""CI-Research Alignment"" track. <br/><br/>The regional proposal will develop statewide research and teaching DMZ and VPN networks and a federated identity management system for secure access to shared network-accessible resources in classrooms and labs. The CI-Research Alignment proposal will provide human infrastructure and processes needed to co-develop shared educational resources and curricula, such as interactive laboratory simulations and other virtual environments for STEM training that will also enhance the high school to higher education STEM pipeline (College Credit Plus Program.)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019216","CC* Compute: GPU-based Computation and Data Enabled Research and Education (G-CoDERE) at PSU","OAC","Campus Cyberinfrastructure","07/01/2020","06/08/2020","Feng Liu","OR","Portland State University","Standard Grant","Kevin Thompson","06/30/2022","$395,926.00","Christof Teuscher, Jay Nadeau, Bruno Jedynak, Steve Reichow","fliu@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","8080","","$0.00","Data and computationally intensive research and education is increasingly important at Portland State University (PSU). Scientists and students at PSU are producing massive quantities of data and investigating machine learning and data science approaches to research problems in many different fields. Graphics processing units (GPUs) excel at large-scale parallel computing and are critical for analyzing and visualizing such massive quantities of research data and developing data-driven technologies. This project establishes PSU's first GPU computing infrastructure and will support PSU research groups in a wide variety of fields, including Computer Science, ECE, Physics, Chemistry, Statistics, and Speech & Hearing Science, and benefit researchers in partner universities, including Oregon Health and Science University and Lewis & Clark University. It provides undergraduates, graduate students, and postdoctoral researchers new training opportunities for data-driven research; enables the creation of new machine learning, data analytics, and visualization courses; and supports upgrading existing courses with emerging data-driven paradigm. It facilitates the K12 outreach programs such as Oregon Mathematics, Engineering, Science Achievement and Saturday Academy?s Apprenticeships in Science and Engineering with GPU-enabled project and internship opportunities. This infrastructure allows PSU, Oregon's most diverse public university, to provide the state-of-the-art GPU facility and learning opportunities to students from underrepresented groups.<br/><br/>This project establishes PSU's first GPU computing infrastructure by acquiring twenty GPU servers with the related high-performance data storage. This GPU infrastructure complements PSU's Coeus high-performance computing cluster to support GPU-enabled research and education at PSU and share with external users through the Open Science Grid.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004364","Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying algorithms for a fully GPU-based  first trigger stage","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2020","05/01/2020","Michael Sokoloff","OH","University of Cincinnati Main Campus","Standard Grant","Robert Beverly","06/30/2023","$289,603.00","","mike.sokoloff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 7244, 8004","075Z, 077Z, 7569, 7923","$0.00","The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.<br/><br/>The LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916589","BD Hubs: Collaborative Proposal: SOUTH:The South Big Data Innovation Hub","OAC","BD Spokes -Big Data Regional I, HDR-Harnessing the Data Revolu","06/01/2019","06/28/2021","Srinivas Aluru","GA","Georgia Tech Research Corporation","Cooperative Agreement","Martin Halbert","05/31/2023","$1,716,711.00","Surya Kalidindi, Madhav Marathe, Renata Rawlings-Goss, Patrick Sullivan","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","024Y, 099Y","062Z, 8083","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The South Big Data Regional Innovation Hub nucleates community across organizations.  It does this through stimulating new activities and provides continuity and a neutral place for activities to advance.  It works in the context of regional and national data science priorities. The South Big Data Regional Innovation Hub will serve the critical national need to educate new, and retrain current, work-force in data science, and increase data literacy.   It will achieve this by tapping into the collective expertise of its member universities, create a clearinghouse for resources, materials, and collaborations, engage with industry for training and placement of individuals, assist non-research intensive universities and community colleges develop data science curriculum, and promote collaborative team science approaches to research and project execution.<br/><br/>The initial priority areas of the South Big Data Regional Innovation Hub are in data science cyberinfrastructure; health analytics for mitigating health disparities that largely impact the South; geospatial mapping for assessment of environmental hazards such as flooding along the Gulf coasts; data science for advanced materials and manufacturing to integrate and enhance the materials-manufacturing value chain; digital modeling as well as data-driven policies for smart cities; and education and workforce training in data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916454","BD Hubs: Collaborative Proposal: South: The South Big Data Innovation Hub","OAC","BD Spokes -Big Data Regional I, HDR-Harnessing the Data Revolu","06/01/2019","06/25/2021","Stanley Ahalt","NC","University of North Carolina at Chapel Hill","Cooperative Agreement","Martin Halbert","05/31/2023","$1,529,060.00","Stephen Fiore, Jay Aikat, Lea Shanley","ahalt@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","024Y, 099Y","062Z, 8083","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The South Big Data Regional Innovation Hub nucleates community across organizations.  It does this through stimulating new activities and provides continuity and a neutral place for activities to advance.  It works in the context of regional and national data science priorities. The South Big Data Regional Innovation Hub will serve the critical national need to educate new, and retrain current, work-force in data science, and increase data literacy.   It will achieve this by tapping into the collective expertise of its member universities, create a clearinghouse for resources, materials, and collaborations, engage with industry for training and placement of individuals, assist non-research intensive universities and community colleges develop data science curriculum, and promote collaborative team science approaches to research and project execution.<br/><br/>The initial priority areas of the South Big Data Regional Innovation Hub are in data science cyberinfrastructure; health analytics for mitigating health disparities that largely impact the South; geospatial mapping for assessment of environmental hazards such as flooding along the Gulf coasts; data science for advanced materials and manufacturing to integrate and enhance the materials-manufacturing value chain; digital modeling as well as data-driven policies for smart cities; and education and workforce training in data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126116","CC* Networking Infrastructure: Advanced Network For Research at UNC Charlotte","OAC","Campus Cyberinfrastructure","09/01/2021","07/15/2021","Christopher Maher","NC","University of North Carolina at Charlotte","Standard Grant","Kevin Thompson","08/31/2023","$497,187.00","Daniel Janies, Aidong Lu, Adam Reitzel, Samira Shaikh","cmaher9@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","8080","9102","$0.00","A diverse team of faculty researchers, research computing, and networking experts from across UNC Charlotte are collaborating to develop and deploy an advanced network for research across the campus.  This network dramatically improves digital communication between researchers, scientific instrumentation, visualization workstations, high performance computing (HPC) infrastructure and external collaborators. Access to the advanced network enables higher speed to existing research workflows and allows for new research processes previously unavailable to the team.  For example, massive datasets from multiple DNA sequencing instruments are streamed to powerful HPC clusters thus removing data analysis bottlenecks.  Research applying motion analysis and artificial intelligence to Future of Work research, natural language processing and cyber physical systems are also supported. Applications include fluid dynamics simulation of airflow, which carries infectious microbes, in public transportation.  Research collaborations between UNC Charlotte and other universities is enhanced by speeding acquisition and sharing of research results. Students from varied backgrounds, including UNC Charlotte?s large cohorts from underrepresented groups and first generation college students will work with this state-of-the-art cyberinfrastructure in their coursework and research projects.<br/> <br/>The design is a campus wide 100Gb fibre-based network with Science DMZ including a Data Transfer Node enabling data flows separate from the day-to-day traffic of University business. The network is implemented as a spine-leaf architecture with two spines (64x100GbE) interconnected to each other with 100Gb links by MLAG. One spine is placed in the HPC Data Center and the other is placed in a separate campus Data Center. The network has 6 Leafs (32x100 GbE) supporting connectivity between buildings housing academic departments and HPC infrastructure. Performance is monitored and tuned with PerfSonar nodes. Security is monitored through Zeek (40Gb/s) nodes.  The network and Science DMZ are tuned and optimized in collaboration between researchers and network architects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018933","CC* Compute: High-Performance Computing Backbone for Accelerating Campus-Wide and Regional Research","OAC","Campus Cyberinfrastructure","07/01/2020","06/16/2020","Aaron Wemhoff","PA","Villanova University","Standard Grant","Kevin Thompson","06/30/2022","$397,196.00","Ryan Jorn, David Cereceda, Jonathan Graziola","aaron.wemhoff@villanova.edu","800 Lancaster Avenue","Villanova","PA","190851676","6105194220","CSE","8080","","$0.00","Villanova University is acquiring a high-performance computing resource to expand the capabilities of at least 27 identified researchers in engineering, physical sciences, and social sciences. The grant also prepares students for the STEM workforce through engagement in computational research, education via the creation and modification of ten undergraduate and graduate courses, and student training in high performance computing operations. The impact of this grant extends well beyond Villanova University by establishing the Southeastern Pennsylvania High-Performance Computing Consortium to create new collaborative opportunities between Villanova and non-Villanova researchers, and by connecting Villanova to the broader Open Science Grid network to distribute resources to researchers nationally.<br/><br/>The grant focuses on three objectives to expand Villanova?s computational infrastructure for research and education. First, this work establishes new computational hardware ? including 1,184 central processing units (CPUs), 10,240 graphical processing units (GPUs), and 448 terabytes (TB) of data storage ? along with complementary software and networking resources.  Second, resource usage expands fundamental research in seven project areas relevant to the mission of the National Science Foundation, including (1) materials for fusion energy applications, (2) causes of various nasal sinus diseases, (3) ion transport in energy storage devices, (4) speech perception and language processing, (5) river behavior, (6) nonlinear mechanical behavior, and (7) machine learning algorithms.  Finally, practices are developed to mitigate the costs associated with growing and maintaining the computing resource.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2020446","CC* Compute: Deep Bayou: Accelerating Scientific Discoveries with A GPU Cluster","OAC","Campus Cyberinfrastructure","07/01/2020","06/02/2020","Le Yan","LA","Louisiana State University","Standard Grant","Kevin Thompson","06/30/2022","$398,760.00","Thomas Kutter, Lu Peng, Zachary Byerly, FENG CHEN","lyan1@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","8080","9150","$0.00","Modern science increasingly relies on high performance computing (HPC) and data analytics to make discoveries about our world. In recent years, graphics processing units (GPUs), a specialty computer hardware originally developed for graphic applications with a unique, highly parallel architecture, has become a key enabling technology for both types of workloads. Through this project, Louisiana State University (LSU) expands its existing computing facilities with the addition of Deep Bayou, a GPU cluster consisting of 12 compute nodes and 26 NVIDIA GPU devices.<br/><br/>The initial research projects enabled by Deep Bayou include particle physics, gravitational wave source characterization with LIGO, ocean and ocean-atmosphere modeling, bioinformatics, infrastructure modeling for disaster management, material sciences, computational biology, and fundamental GPU architecture design. The Deep Bayou infrastructure and organization is also open to additional initiatives and projects through an application process. In addition to researchers across LSU campus, the project partners with the Open Science Grid (OSG) to share the GPU resources among the research community across the nation. Deep Bayou will make a significant contribution to the building of future HPC workforce, as many training and education activities plan to leverage its availability. Through existing state- and federal-sponsored programs such as Louisiana Optical Network Infrastructure (LONI), Baton Rouge: Bringing Youth Technology, Education and Success (BRBYTES) and Research Experiences for Undergraduates (REU), K-12 students and undergraduates can have access to courses and workshops facilitated on the cluster, many of whom are from underrepresented minority communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2138314","FAIR for US","OAC","NSF Public Access Initiative","09/01/2021","07/14/2021","Melissa Cragin","CA","University of California-San Diego","Standard Grant","Martin Halbert","08/31/2022","$49,999.00","Christine Kirkpatrick","mcragin@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7414","7556, 9102","$0.00","This workshop will build on the widespread interest in the Findable, Accessible, Interoperable, and Reusable (FAIR) data principles for Open Science to synthesize and develop a consensus about next steps for the FAIR movement in the U.S. among representative organizations and thought leaders in the country.  The workshop will bring together a diverse group of experts and representative stakeholders from data and research communities, U.S. funders and mission agencies, and science societies, to seek input on the most promising concrete pathways for expanding uptake, integration, and implementation of the FAIR Principles in research practice and services.  <br/><br/>The primary aim of the workshop is to produce a scoping report that can serve as an agenda for the most promising strategies and approaches for FAIR capacity development incorporating the voices of academia, government, and industry.  This will be an invitational meeting which will seek diverse participants who are outside the well-known public voices on Research Data Management systems and services, while also including leaders who are working at the intersections of data infrastructure development, domain sciences, and open science.  International partners engaged in related initiatives will also be assembled for perspectives and experiences that will inform US efforts.  The goal for the workshop is to structure it as a series of highly interactive facilitated sessions with specific targeted outcomes that will contribute to the project report.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018766","CC* Compute: GP-ARGO: The Great Plains Augmented Regional Gateway to the Open Science Grid","OAC","Campus Cyberinfrastructure","07/01/2020","05/21/2020","Daniel Andresen","KS","Kansas State University","Standard Grant","Kevin Thompson","06/30/2023","$378,599.00","Pratul Agarwal, Timothy Middelkoop, Stephen Wheat, Ryan Johnson","dan@k-state.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","8080","9150","$0.00","This project creates a regional distributed Open Science Grid (OSG) Gateway led by the Great Plains Network (GPN) to support computational and data-intensive research across the region through the development of specialized CI resources, workforce training, and cross-support methodologies and agreements. The GPN Augmented Regional Gateway to OSG?s (GP-ARGO) primary goal accelerates the adoption and experience of advanced high-throughput computing and data resources by developing a model for enhanced distributed computational systems, including design, implementation, and training. This project multiplies the number of OSG sites in the GPN region by 8, adding at least 2,048 cores dedicated to OSG use, and giving OSG potential access to over 42,000 additional existing cores at participating institutions. This project accomplishes the following key objectives: 1) Improves campus awareness and adoption of advanced HTC-oriented computing and data resources for STEM research and education activities. 2) Increasing the number and capabilities of campus research computing and data professionals. 3) Increasing the capabilities of campus high-throughput computing cyberinfrastructure resources such as advanced computing systems, data caches, and networks. 4) Enabling deployment, and operation of research and education cyberinfrastructure to make science more efficient, trusted, and reproducible.<br/><br/>This project advances both the regional infrastructure and regional research efforts by increasing the number  of local CI resources across the region. GP-ARGO provides a distinctive model for distributed support teams, in particular institutions that lack a critical mass of personnel to support the key areas: OSG awareness, HTC resources, researcher support, workforce development.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018373","CC* Networking Infrastructure: Creation of a Science DMZ and 10Gb/s Connection to Internet2 for Tennessee Tech University","OAC","Campus Cyberinfrastructure","07/01/2020","05/19/2021","Susmit Shannigrahi","TN","Tennessee Technological University","Standard Grant","Kevin Thompson","06/30/2022","$276,322.00","Michael Rogers","sshannigrahi@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","8080","9251","$0.00","This project facilitates national and international research collaborations at Tennessee Technological University by establishing a dedicated 10 Gbps connection to Internet2 and creating a high-speed Science DMZ. <br/><br/>The new cyberinfrastructure benefits a large number of researchers across multiple colleges and research centers. It enables data-driven research in areas including next-generation Networking, Cybersecurity, High-performance Computing, Chemical Engineering, Biology, High-energy Physics, Earth Sciences, and Civil & Environmental Engineering. The upgraded connectivity enables researchers to share data both internally and externally in a fast, secure, and reliable manner over the dedicated research-only connection. The Science DMZ equips researchers with use-case specific segmented resources (network, disk space, and compute nodes) as well as increased autonomy and flexibility to deploy isolated research prototypes.<br/><br/>The new infrastructure bypasses Tennessee Tech's perimeter firewall while integrating with the existing infrastructure. High-speed data transfer nodes (DTNs) provides faster data delivery to and from the campus. The network design provides dedicated connectivity for research traffic to most buildings.  Several PerfSonar nodes deployed at strategic network locations report real-time network and application performance, allowing the operators to optimize network flows. Finally, this new infrastructure brings IPv6 deployment to the campus. A study of network and application performance before and after the deployment can demonstrate the impact of a science DMZ on science workflows in terms of performance, latency, and flexibility. The experience in implementing this project at Tennessee Tech can provide guidance to other universities pursuing similar cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925590","CC* Compute: High Performance Campus Computing for Institutional Research at the American Museum of Natural History","OAC","Campus Cyberinfrastructure","07/01/2019","06/21/2019","Juan Montes","NY","American Museum Natural History","Standard Grant","Kevin Thompson","06/30/2022","$399,258.00","Cheryl Hayashi, Michael Benedetto, Samuel Tran","jmontes@amnh.org","Central Park West at 79th St","New York","NY","100240000","2127695975","CSE","8080","","$0.00","Through the National Science Foundation CC* program, the American Museum of Natural History (AMNH) expands the High-Performance Computing (HPC) capabilities that directly support the Museum's research. AMNH conducts scientific research and educational activities across astrophysics, anthropology, biology, and geosciences. The increasingly data-intensive nature of this research requires greater access to computational resources and to ever more sophisticated tools, including local and remote HPC clusters.<br/><br/>In this project, AMNH is expanding its on-premise computing cluster capacity and consolidating all existing clusters into a unified open-source software framework. These clusters are connected to the Museum's Science DMZ, a high-performance network specifically designed for research data flows, which provides high-speed network access between the Internet2 and the AMNH on-premise clusters. Additionally, AMNH researchers can execute complex workloads at scale using cloud resources at Amazon via the same local HPC management framework. Federation with InCommon provides both AMNH researchers and outside collaborators with secure access to these resources via a common authentication and authorization framework. Finally, the AMNH clusters are integrated with the Open Science Grid allowing AMNH to offer idle computing cycles to the wider research community while providing AMNH researchers with the same access to remote computing resources. These improvements greatly expand the overall HPC capacity available to AMNH scientists, increasing the speed and effectiveness of their research and decreasing time to discovery. Additionally, the work of AMNH scientists informs the Museum's educational and curatorial programs, directly benefiting AMNH students and the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915774","BD Hubs: Collaborative Proposal: WEST: Accelerating the Big Data Innovation Ecosystem","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/30/2021","Edward Lazowska","WA","University of Washington","Cooperative Agreement","Martin Halbert","05/31/2023","$728,521.00","Bill Howe, Sarah Stone","lazowska@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","024Y","062Z, 8083","$0.00","The BD Hubs foster regional networks of stakeholders and cooperate nationally on US priorities of importance to a region and to the nation. The activities of the BD Hubs contribute to a vibrant national data innovation ecosystem. The West Big Data Innovation Hub builds and strengthens strategic partnerships -- harnessing the data revolution to address scientific and societal challenges. Whether working towards the future of data-informed healthcare or tackling projects in disaster recovery, the Hub envisions a diverse community empowered to contribute to areas of national priority. The Hubs focus on data science activities and initiatives that inspire cross-sector collaboration and exemplify the need for multi-disciplinary approaches.<br/><br/>With this award, the West Big Data Innovation Hub will: (1) Develop and enable translational data science (TDS) pilot projects in our thematic areas to highlight the value of cross-sector collaboration, enhance fluency with real-world use cases, and emphasize a pragmatic and holistic view of the data and analytics lifecycles. We envision our signature TDS initiatives for 2019-2023 to include: Fire and Water: Data Collaborative for the Future of Natural Resource Management; Stress-Testing Access for Road Video; and Housing Instability: Trusted Data Collaborative for Responsible Data Management. (2) Facilitate team formation across different stakeholder groups through our activities, capturing inspirational stories and encouraging teams to reflect and share their insights about cross-sector collaboration. (3) Raise awareness of regional opportunities and inspire work in priority areas including Natural Resources & Hazards, Metro Data Science, Health & Medicine, Data-Enabled Discovery & Learning, Data Sharing, Cloud Computing, and Responsible Data Science. (4) Support data science education and workforce development. Recognizing that a diverse, multi-faceted workforce is key to addressing current scientific and societal challenges, we will continue to expand our portfolio of education and workforce development efforts, including a focus on Train-the-Trainer sessions, Pedagogy and Practice, Data Science for Social Good and the Data Science Corps, Findable Accessible Interoperable and Reusable (FAIR) data, and institutional change -- providing a platform for broadening participation in data science. Core to our progress in Programmatic Activities, Socio-Technical Shared Resources and Services, and Education and Workforce Development Activities will be a coordinated evaluation, opportunities for scaling regional successes to the national network of Big Data Hubs, and strategic efforts for Hub sustainability including the development of external funding streams. These efforts will be designed to enable community input and to strengthen channels for ongoing dialogue.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003800","Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution","OAC","Software Institutes","05/15/2020","06/21/2021","Andrew Johnson","IL","University of Illinois at Chicago","Standard Grant","Seung-Jong Park","04/30/2025","$2,282,000.00","Andrew Johnson, Luc Renambot","ajohnson@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8004","075Z, 077Z, 079Z, 7925, 8004, 9251","$0.00","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world.<br/> <br/>SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126308","CC* CIRA: Southwest Higher Education Knowledge and Technology Exchange (SHEKATE)","OAC","Campus Cyberinfrastructure","10/01/2021","07/09/2021","Lev Gonick","AZ","Arizona State University","Standard Grant","Kevin Thompson","09/30/2023","$197,470.00","Derek Masseth","lev.gonick@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8080","","$0.00","The COVID-19 pandemic exposed the deficiencies of long haul, middle mile, and last mile broadband connectivity in the West.  Long distances, sparse populations, and isolated tribal communities characterized new challenges for the higher education community.  An influx of new federal funding and state allocations from the CARES Act released new opportunities to plan, build, and deliver a new foundation for cyberinfrastructure.  Whereas planning and collaboration were needed and required in a pre-pandemic environment, the demand for regional-scale thinking and planning by leadership organizations is now an essential element in rebuilding the mountain west.  As a regional, multi-state, multi-institution collaboration, SHEKATE is prepared to engage, plan, and implement the new cyberinfrastructure that supports and improves science in a place where connectivity is expensive and isolated from researchers, and where regional CIOs and state research and networking collaborations work collectively to solve regional-scale research problems.<br/> <br/>SHEKATE?s collaborative planning events introduce and connect researchers from the region?s research universities to cross state and international boundaries and to work with federal and state broadband initiatives to establish a new research core across the region. Led by Arizona State University, the Sun Corridor Network, and the Utah Education and Telehealth Network as planning leads, SHEKATE conference events focus on researcher enabled urban biometrics, artificial intelligence, broadband in the West, cyberinfrastructure sustainability, and the underlying organizational structures that inform a new approach to building CI in one of the most complex urban and rural regions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007991","OAC Core: Small: Next-Generation Communication and I/O Middleware for HPC and Deep Learning with Smart NICs","OAC","OAC-Advanced Cyberinfrast Core","07/01/2020","05/21/2020","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Robert Beverly","06/30/2023","$500,000.00","Hari Subramoni","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","090Y","7923","$0.00","In-network computing technologies, or the ability to offload significant portions of compute, communication, and I/O tasks to the network, have emerged as fundamental requirements to achieve extreme scale performance for end applications in the areas of High-Performance Computing (HPC) and Deep Learning (DL). Unfortunately, current generation communication middleware and applications cannot fully take advantage of these advances due to the lack of appropriate designs in the middleware-level. This leads to the following broad challenges: 1) Can middleware that are ?aware? of the computing capabilities of these emerging in-network computing technologies be designed in the most optimized manner possible for HPC and DL applications?, and 2) Can such a middleware be used to bene?t end applications in HPC and DL to achieve better performance and portability? A synergistic and comprehensive research plan is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL middleware and applications.  Several graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses at The Ohio State University. Tutorials and workshops will be organized at various conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)<br/><br/>The proposed innovations include: 1) Designing scalable communication primitives (point-to-point and collectives) for using emerging switch and NIC based in-network computing features, 2) Exploiting in-network computing features to o?oad complex and user de?ned functions, 3) Designing high-performance I/O and storage subsystems using NVMe over Fabrics, 4) Designing enhanced in-network datatype processing schemes for MPI library,  5) Designing and optimizing in-network computing-based solutions for emerging cloud environment, and 6) Carrying out integrated development and evaluation of the proposed designs with a set of representative HPC and DL applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available to the public. The project team members will work closely with collaborators to facilitate wide deployment and adoption of released software. The transformative impact of the proposed research is to achieve scalability, performance, and portability for HPC and DL frameworks/applications by leveraging emerging in-network computing technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2135954","EAGER: Developing a framework to identify and mitigate perceptual and technical barriers in code sharing to facilitate reproducible and transparent research","OAC","NSF Public Access Initiative","09/01/2021","07/09/2021","Serghei Mangul","CA","University of Southern California","Standard Grant","Martin Halbert","08/31/2023","$199,934.00","","mangullabatusc@gmail.com","University Park","Los Angeles","CA","900890001","2137407762","CSE","7414","7916","$0.00","This project will conduct exploratory research into the barriers and strategies for making research code publicly accessible.  While there has been significant progress in making research datasets accessible for purposes of open science and reproducibility, code and other forms of software developed in research projects are less frequently made available persistently.  In scientific research, it is not only imperative to publish a detailed description of the study design, methodology, results, and interpretation, but there is a pressing need to make all the research products (including code and software) publicly available, shareable, well documented, and organized to facilitate reproducible and transparent research.<br/><br/>An important research product that is an essential element ensuring reproducible research is the analytic code used for the analysis. In contrast to sharing data, which is widely enforced by scientific journals and research organizations, there is limited guidance on code sharing which in addition to data represents an essential component of reproducible and rigorous research.  The project will result in: A) Descriptive information which will be collected through one-to-one interviews through teleconferencing, as well as by distributing surveys across 5,000 randomly selected principal investigators who were supported by NSF and/or NIH, B) Recommendations for code sharing to be developed through a series of structured teleconferences with informed experts, notably editors of scientific journals with a strong interest in code sharing and reproducible research, and C) Formation of a Research Code Alliance (RCA) to develop a framework to mitigate perceptual and technical barriers in code sharing to facilitate reproducible and transparent research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2114202","CICI: UCSS: Enhancing Integrity and Confidentiality for Secure Distributed Data Sharing","OAC","Cybersecurity Innovation","07/01/2021","06/24/2021","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Robert Beverly","06/30/2024","$500,000.00","","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8027","7923, 8027, 9102","$0.00","The promised benefit of research data sharing lies in the ability to reuse the data. While many scientific disciplines are starting to share data, there are others who are restricted in the data they can share and with whom they can share. More and more interdisciplinary research and cross-institutional science collaborations are conducted on datasets that result from multiple funding sources with different constraints and policies and are stored in repositories that require specific metadata standards and formats and are subject to access restrictions. Providing a secure method to efficiently share and verify the data and metadata while maintaining privacy restrictions becomes necessary for the reuse of the scientific data.<br/><br/>The Open Science Chain ? Integrity Services (OSC-IS) strengthens cybersecurity controls with the application of blockchain technology primitives to preserve the integrity and the provenance of public and private data assets on research platforms and hubs. For private data assets, additional consideration of confidentiality exists, which may extend to fields in metadata. OSC-IS leverages a combination of smart contracts and off-chain storage to ensure the data confidentiality and privacy in sharing private research data in collaborative research. The project creates an API-based data integrity verification management service for data-driven research platforms and hubs that will reduce data information loss and build support for managing various metadata standards and access controls. The project enhances the security and trustworthiness in scientific data sharing that leads to increased usage in exploration and utilization of datasets within scientific communities. OSC-IS informs a wide array of different areas, including best practices in data management, demonstrating the use of blockchain in academic research, and incorporating computer security in education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1831393","2018 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop","OAC","Software Institutes","06/01/2018","09/15/2018","Francis Timmes","AZ","Arizona State University","Standard Grant","Robert Beverly","05/31/2019","$85,065.00","Paul Bauman, Rafael Ferreira da Silva, Kyle Niemeyer, Sandra Gesing","fxt44@mac.com","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8004","026Z, 7556, 8004","$0.00","This project will host a 2-day workshop in Washington, DC, which will bring together the community of Software Infrastructure for Sustained Innovation (SI2) awardees (with the goal of involving one principal investigator from each Scientific Software Elements (SSE), Scientific Software Integration (SSI), and Scientific Software Innovation Institutes (S2I2) project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from Computational and Data-Enabled Science and Engineering (CDS&E), Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP), and Venture funded PIs as well as SI2 Early Concept Grants for Exploratory Research (EAGER) and Rapid Response Research (RAPID) awardees. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability with the broader agenda for national software ecosystem. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031611","Collaborative Research: Travel Supplement for Frontera's ""Multi-scale, MHD-Kinetic Modeling of the Solar Wind and its Interaction with the Local Interstellar Medium""","OAC","Leadership-Class Computing","06/01/2020","05/12/2020","Nikolai Pogorelov","AL","University of Alabama in Huntsville","Standard Grant","Edward Walker","05/31/2022","$7,082.00","","np0002@uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","CSE","7781","9150","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916391","BD Hubs: Collaborative Proposal: Midwest: Midwest Big Data Hub: Building Communities to Harness the Data Revolution","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/23/2021","James Reecy","IA","Iowa State University","Cooperative Agreement","Martin Halbert","05/31/2023","$149,084.00","Joe Colletti, Sarah Nusser, Carolyn Lawrence-Dill","jreecy@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","024Y","062Z, 8083","$0.00","This project builds on a prior Midwest Big Data Hub effort. In 2015 stakeholders in the Midwest region of the United States formed a consortium of partners and working groups called the Midwest Big Data Hub (MBDH).  MBDH aimed to help member organizations working in Big Data coordinate current activities and launch new collaborative projects.  The project included stakeholders in the twelve states of the Midwest Census region (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin) and six leading universities that support hundreds of researchers, technologists, and students.  This hub provides a basis for collaboration and outreach that increases the potential for benefitting society. <br/><br/>The current award is a collaboration among five academic sites (Indiana University, Iowa State University, UIUC/NCSA, the University of Michigan, the University of North Dakota, and the University of Minnesota - Twin Cities).  The project focuses on priority areas that are important to the region and can also be influential on the national stage. <br/>  -  The five thematic areas of focus, and the institutional partner leading that thematic area, are: Digital Agriculture (led by Iowa State); Smart, Connected, and Resilient Communities (Indiana University); Water Quality (University of Minnesota); Advanced Materials and Manufacturing (UIUC); and Health and Biomedicine (University of Michigan). <br/>  -  Three cross-cutting areas that are emphasized across the project are: data science education and workforce development; cyberinfrastructure, data access and use; and communication and community development.<br/>The priority areas have regional relevance and also have the prospect for integration into societal contexts at the national level. The overall goal is to enable the use of existing and emerging cyberinfrastructure and best practices to improve access to and use of data.  The project plans to reach out to the Midwest community at large and to connect people, resources, and organizations. Ties to Big Data Hubs in three other regions provide a means to advance knowledge across these fields at the national level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019089","CC* Compute: Accelerating Science and Education by Campus and Grid Computing","OAC","Campus Cyberinfrastructure","07/01/2020","06/16/2020","Jan Mandel","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Kevin Thompson","06/30/2022","$399,938.00","Kannan Premnath, Carlos Infante, Amy Roberts, Yaning Liu","Jan.Mandel@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","8080","","$0.00","High-performance computing resources are a fundamental need of modern research that unites almost all disciplines. Experience with these resources is also an important tool for preparing today's students for a wide range of careers. A group of researchers and educators at the University of Colorado Denver, in partnership with its Office of Information Technology, are building a state-of-the-art computing resource on its Downtown Campus. The new facility provides the first campus-wide high-performance computer system to support both research and teaching efforts. The computing cluster is integrated with the Open Science Grid (OSG), enabling access to additional computing resources from partner institutions, and sharing unused time with the wider community. A high-priority educational queue is dedicated for teaching and course-based research. <br/><br/>The resource will include 2048 AMD EPYC compute cores and 16TB memory distributed across 32 compute nodes; 2 high-memory nodes, each with 2TB memory and 64 cores; one NVIDIA Tesla V100 32GB GPU; 1PB (raw) storage; and InfiniBand interconnect. For data-intensive research and access of OSG jobs to distributed data, the cluster is configured with full end-to-end 10gb/s connectivity from each node to Internet 2. A graphical Jupyter notebook interface increases accessibility. <br/><br/>The configuration addresses computing requirements based on a survey and an analysis of the needs of science and educational drivers in fields including earth and environmental sciences, biotechnology and genomics, computer science and engineering, applied mathematics, physics, and business. The resource will broaden participation in computational science and have a significant impact on the supported research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004012","Elements: HPN-SSH","OAC","Software Institutes","05/01/2020","04/29/2020","Christopher Rapier","PA","Carnegie-Mellon University","Standard Grant","Robert Beverly","04/30/2022","$439,505.00","","rapier@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8004","077Z, 7923","$0.00","The transfer of large data sets between computing resources is an integral component of the scientific workflow. Multiple tools have been developed to aid in this task but, despite significant performance bottlenecks, secure shell (SSH) based tools like secure copy protocol (SCP) and secure file transfer protocol (SFTP) remain popular due to ubiquity, ease of use, and minimal administrative burdens. HPN-SSH, the project tool, was initially developed in 2004 to address this need. This award will allow the PIs to expand on the original work of HPN-SSH and address new challenges created by advances in computer technology and the needs of users. They will incorporate hardware accelerated encryption; efficiently use modern CPUs by dynamically sizing the number of threads; accelerate the cryptographic workflow by allowing it to work in parallel; investigate making the default open source secure shell OpenSSH cipher use multiple cores; create a ?resume on failure? feature enabling users to restart transfers from the point of failure; and incorporate networking metrics to aid in troubleshooting and performance analysis of HPN-SSH. HPN-SSH will support and enhance research efforts across a wide range of scientific domains by lowering the costs of entry to big data and remote computation without compromising security or functionality. These benefits will extend to business and industry, educational communities, and the general public as well. <br/><br/>The transfer of large data sets between computing resources is an integral component of the scientific workflow. Multiple tools have been developed to aid in this task but, despite significant performance bottlenecks,  SSH based tools remain popular due to ubiquity, ease of use, and minimal associated costs. To address these bottlenecks we developed HPN-SSH; a series of patches that enable high performance throughput for the OpenSSH application. These patches were initially released in 2004 and have become widely used throughout the research, academic, financial, and technology communities. This award gives the PIs opportunity to foster innovative development in HPN-SSH that will benefit the community by significantly increasing performance. With this grant the PIs will: incorporate on-die hardware accelerated encryption in their multithreaded AES counter cipher; efficiently use multicore CPUs by dynamically sizing the number of threads; introduce pipelining and parallelization into the cryptographic workflow; investigate the parallelization of the default OpenSSH cipher CHACHA20; create a ?resume on failure? feature enabling users to restart transfers from the point of failure; and incorporate inline network telemetry to aid in troubleshooting and performance analysis. This work will also advance the field of computer science through the development and improvement of parallelization methods to enhance the performance of cryptographic routines. As most widely used cryptographic libraries and methods are highly serial in nature they are unable to take advantage of multicore processors. As processor speed has remained relatively stable over the past ten years we must distribute the cryptographic workload over multiple cores in order to significantly increase throughput. HPN-SSH will democratize access and extend the reach of the national cyberinfrastructure by lowering the costs of entry without compromising security or functionality. These benefits will extend to business and industry, educational communities, and the general public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018360","CC* Planning: Shepherd University CI and Regional Connectivity","OAC","Campus Cyberinfrastructure","07/01/2020","06/16/2020","Robert Warburton","WV","Shepherd University","Standard Grant","Kevin Thompson","06/30/2022","$99,999.00","Jason Miller","rwarburt@shepherd.edu","P.O. Box 5000","Shepherdstown","WV","254435000","3048765358","CSE","8080","","$0.00","This project explores the benefits and costs of bringing fiber optic connectivity and Internet2 access to the Shepherd University campus in West Virginia. This project benefits students, researchers and technology developers in the four-state region (West Virginia, Maryland, Pennsylvania, and Virginia). Network connectivity for research and education is a critical issue in many parts of the United States. West Virginia has several cyber-isolated regions including the Shepherdstown area that is home to Shepherd University. Rivers, mountain ranges, state lines and right-of-way issues stand between the campus and the major technology corridors and dense communication networks nearby. Bridging these barriers would boost the research and educational capacity at this state-supported primarily undergraduate institution.<br/><br/>This planning project incorporates the following components. (1) Identify Regional Optical Networks (RONs) capable of bringing Internet2 connectivity to the Shepherd University campus. Identify possible fiber routes from each RON to the campus region. Evaluate costs including for the problematic last-mile portion. (2) Select the most cost-effective route. Work with one or more dark fiber providers and government agencies to engineer a detailed plan that addresses geographical and political boundaries including county and state lines. Enumerate local work permits that would be required. (3) Determine sustainability with projections of ongoing costs. Plan for preferred and alternative sources of revenue. Engage critical partners from the faculty and administration of Shepherd University and other colleges in the region. (4) Create a detailed cyberinfrastructure plan for the campus. Design the Science DMZ and define the relationship between the Internet2-enabled research network and the university?s enterprise networks. Plan for secure access to Internet2 and related networks such as EduRoam. Plan for preferred and alternative network architectures. (5) If implementation is justified, identify one or more funding mechanisms for full Internet2 connectivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031913","Frontera Travel Grant: Fundamental Studies of Compressible Turbulence and Turbulent Mixing","OAC","Leadership-Class Computing","06/01/2020","05/12/2020","Diego Donzis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Edward Walker","11/30/2021","$9,598.00","","donzis@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925598","CC* Networking Infrastructure: Advancing High-speed Networking at UTC for Research and Education","OAC","Campus Cyberinfrastructure","07/01/2019","07/30/2020","Farah Kandah","TN","University of Tennessee Chattanooga","Standard Grant","Kevin Thompson","06/30/2022","$515,663.00","Anthony Skjellum, Hope Klug, Mina Sartipi, Dennis Gendron","Farah-Kandah@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","8080","9251","$0.00","A diverse PI team including researchers and the Office of Information Technology at the University of Tennessee at Chattanooga (UTC) is upgrading the campus network infrastructure to improve the ability for professors and students to perform, enhance, and expand R&D activities. This project complements other significant and ongoing investments/upgrades at UTC to enhance campus intra- and inter-networking and research computing cyberinfrastructure. UTC's growing research portfolio of faculty-driven, data-oriented research drives the network upgrades. 100Gbit/s fiber networking infrastructure, switching, and routing infrastructure are upgraded by this project. A Science DMZ infrastructure and Data Transfer Node (DTN) server, both components used for inter-campus collaborations and tuned for large-scale data transfers, support the emerging uses of big data and data-centric external collaborations.<br/><br/>The project upgrades networking to eight UTC campus buildings representing all four university colleges (Engineering & Computer Science, Arts & Sciences, The Gary W. Rollins College of Business, and the College of Health, Education, and Professional Studies), as well as the Multidisciplinary Research Building (MDRB) and the campus library. In these buildings, at least fourteen science drivers (projects) on campus benefit from the proposed infrastructure; several of the projects are either NSF funded or otherwise externally supported. The co-location of three research centers plus UTC's research computing facilities in the MDRB adds to the campus impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2106920","Collaborative Research: OAC Core: Robust, Scalable, and Practical Low-Rank Approximation","OAC","OAC-Advanced Cyberinfrast Core","07/15/2021","07/02/2021","Grey Ballard","NC","Wake Forest University","Standard Grant","Seung-Jong Park","06/30/2024","$225,000.00","","ballard@wfu.edu","1834 Wake Forest Road","Winston Salem","NC","271098758","3367585888","CSE","090Y","026Z, 7923","$0.00","Nearly all aspects of society are affected by data being produced at a faster rate in recent years. The data from experiments, observations, and simulations are not only in more classical science and engineering domains but also in numerous other areas such as businesses tracking more and more facets of consumer behavior, and social networking capturing vast amounts of information on the relationships between people and their actions and interactions. There is a strong need to distill a set of data into a smaller representation that separates useful information from noise and captures the most important trends, patterns, and underlying relationships.? Such a representation can be used for direct interpretation of hidden patterns or as a means of simplifying other data analytic tasks.? This project addresses these challenges by studying a concept from linear algebra called low rank approximation.? The project develops techniques that faithfully distill the meaningful information within a data set.? The algorithms are also designed to exploit high-performance computers so that analysts can get results more quickly and tackle larger problems.? The overall effort in the project is expected to close the gap between algorithms that can effectively handle very large-scale problems and the data analyst?s ability to convert raw input into meaningful representations and actionable insight.<br/><br/>The matrix and tensor low rank approximations being studied in this project serve as foundational tools in numerous science and engineering applications. Imposing constraints on the low rank approximations enables the modeling of many key problems, and designing scalable algorithms enables new applications that reach far beyond classical science and engineering disciplines. In particular, mathematical models with nonnegative data values abound, and imposing nonnegative constraints allows for more accurate and interpretable models. Variants of these constraints can be designed to reflect additional characteristics of real-life data analytics problems. The primary goals of this project are (1) to develop robust techniques for evaluating computed low rank approximations for rank and model determination, (2) to develop scalable parallel algorithms for large and robust low rank approximations on today?s extreme-scale machines, and (3) to provide end users the practical tools required to compute and analyze solutions at scale. Typical data and application scientists use Python or Matlab to iteratively compute, visualize, and evaluate solutions, and they are limited to small data sets with feasible memory and computational requirements. While high-performance algorithms and implementations exist, end users would not leverage these tools if they cannot rely on the robustness and generalizability of the results. This project aims to close this gap, developing an end-to-end system with scalable solutions for all steps of the data analytics workflow.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931389","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","CFS-Combustion & Fire Systems, Software Institutes","01/01/2020","09/04/2019","Richard West","MA","Northeastern University","Standard Grant","Seung-Jong Park","12/31/2022","$260,674.00","","R.West@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1407, 8004","026Z, 077Z, 7925, 8004","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2132549","EAGER: The FAIR Island Project for Place-based Open Science","OAC","NSF Public Access Initiative","07/01/2021","06/30/2021","John Chodacki","CA","University of California, Office of the President, Oakland","Standard Grant","Martin Halbert","06/30/2023","$296,234.00","Neil Davies, Erin Robinson, Matthew Buys","john.chodacki@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","7414","7916","$0.00","This exploratory project will test FAIR (Findable, Accessible, Interoperable, and Reusable) data principles for Open Science purposes in a comprehensive manner for field station data gathering on the Pacific atoll of Tetiaroa.  This project is a timely opportunity to leverage the new field station on the atoll of Tetiaroa which has close ties to U.S. institutions through the University of California, including a nearby NSF Long Term Ecological Research (LTER) site, as well as international research programs.  <br/><br/>Place-based data have a unique quality in that they span the sciences and humanities with time and space (geolocation) acting as foundational metadata used to assign data to ?place? (or nested ?places?). The FAIR Island Project will build interoperability between pieces of critical research infrastructure -- Data Management Plans (DMPs), research practice, DOIs, and publications contributing to the advancement and adoption of Open Science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916425","BD Hubs: Collaborative Proposal: Midwest: Midwest Big Data Hub: Building Communities to Harness the Data Revolution","OAC","BD Spokes -Big Data Regional I","06/01/2019","06/30/2021","H. Jagadish","MI","Regents of the University of Michigan - Ann Arbor","Cooperative Agreement","Martin Halbert","05/31/2023","$149,957.00","Ivo Dinov","jag@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","024Y","062Z, 8083","$0.00","This project builds on a prior Midwest Big Data Hub effort. In 2015 stakeholders in the Midwest region of the United States formed a consortium of partners and working groups called the Midwest Big Data Hub (MBDH).  MBDH aimed to help member organizations working in Big Data coordinate current activities and launch new collaborative projects.  The project included stakeholders in the twelve states of the Midwest Census region (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin) and six leading universities that support hundreds of researchers, technologists, and students.  This hub provides a basis for collaboration and outreach that increases the potential for benefitting society. <br/><br/>The current award is a collaboration among five academic sites (Indiana University, Iowa State University, UIUC/NCSA, the University of Michigan, the University of North Dakota, and the University of Minnesota - Twin Cities).  The project focuses on priority areas that are important to the region and can also be influential on the national stage. <br/>  -  The five thematic areas of focus, and the institutional partner leading that thematic area, are: Digital Agriculture (led by Iowa State); Smart, Connected, and Resilient Communities (Indiana University); Water Quality (University of Minnesota); Advanced Materials and Manufacturing (UIUC); and Health and Biomedicine (University of Michigan). <br/>  -  Three cross-cutting areas that are emphasized across the project are: data science education and workforce development; cyberinfrastructure, data access and use; and communication and community development.<br/>The priority areas have regional relevance and also have the prospect for integration into societal contexts at the national level. The overall goal is to enable the use of existing and emerging cyberinfrastructure and best practices to improve access to and use of data.  The project plans to reach out to the Midwest community at large and to connect people, resources, and organizations. Ties to Big Data Hubs in three other regions provide a means to advance knowledge across these fields at the national level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2008690","SCenE - Self-Assessment and Continual Learning on Edge Devices","OAC","OAC-Advanced Cyberinfrast Core","05/15/2020","05/13/2020","Ghulam Rasool","NJ","Rowan University","Standard Grant","Seung-Jong Park","04/30/2023","$499,978.00","Nidhal Bouaynaya","rasool@rowan.edu","Office of Sponsored Programs","Glassboro","NJ","080281701","8562564057","CSE","090Y","075Z, 079Z, 7923","$0.00","Artificial intelligence (AI) systems and machine learning algorithms lay at the heart of modern autonomy in theory but have experienced a bottleneck in expansion into real-world systems. Typically, in the case of real-world environments, AI systems are considered untrustworthy and are lacking the ability to adapt to an ever-changing environment, requiring continuous maintenance and tuning to stay relevant. Most current AI systems are constrained by their knowledge gathered during training and development. In order for a system to be truly intelligent, they must incorporate learning frameworks that are aware of their own limitations, have an expandable knowledge base in case of failure after deployment, and have the capabilities to operate within available energy budgets in a continuous and dynamic real-world environment. The goal for this project is to develop a rigorous and scalable learning framework that will enable the development of data-driven algorithms that can self-assess their performance and continually expand upon their prior knowledge while operating in real-time on a limited energy budget. This work will equally impact academic research and economic development through collaborations with industrial partners as well as local, regional and federal government agencies. The case study examples include healthcare, intelligent transportation systems, surveillance, severe weather and flood monitoring, aviation and rotorcraft safety, agriculture, vegetation, and endangered species monitoring, and smart and connected campus and communities. Collaboration with the Atlantic Cape Community College will serve as a basis to disseminate the research contributions to the next generation of STEM students. The developed algorithms, source code, and hardware configurations will be made available to the public through open-source data-sharing platforms.<br/> <br/>We aim to tackle the limitations of the current AI systems and learning algorithms, which are based on deterministic and over-confident deep neural networks. The learned parameters of these models are frozen after training and deployed on possibly energy-constrained edge platforms. These models cannot adapt to non-stationary environments resulting in failures in continuously changing environments. The objective of this project is to develop a rigorous, scalable, and open-source learning framework that would facilitate the development and deployment of data-driven algorithms, which can self-assess performance and continually adapt to streaming datasets while operating in real-time on a limited energy budget. We propose a new fundamental approach to machine learning systems that will: (1) provide a theoretical foundation for self-assessment of modern learning algorithms via quantifying confidence in network decisions through the propagation of distribution moments over unknown network parameters, (2) spur the development of self-assessment methods through the monitoring of variance-covariance parameters of the estimated predictive distribution, (3) derive new training methods that allow for algorithms to operate within a given power budget while achieving continual adaptation from streaming datasets through leveraging metrics of kernel importance based on variance-covariance information, and (4) assess the validity of the mathematical derivations and subsequently developed algorithms using benchmark public datasets and real-world applications with our government, industry and academic collaborators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027514","Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations","OAC","COVID-19 Research","05/15/2020","05/11/2020","Ashok Srinivasan","FL","University of West Florida","Standard Grant","Seung-Jong Park","04/30/2022","$50,000.00","","asrinivasan@uwf.edu","11000 UNIVERSITY PKWY","PENSACOLA","FL","325145750","8504742825","CSE","158Y","077Z, 096Z, 7914, 8004","$0.00","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.<br/><br/>We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of  COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104273","OAC Core: Data-driven Methods and Techniques For Protecting Research and Critical Cyberinfrastructure By Characterizing and Defending Against Ransomware","OAC","OAC-Advanced Cyberinfrast Core","07/01/2021","06/08/2021","Elias Bou-Harb","TX","University of Texas at San Antonio","Standard Grant","Seung-Jong Park","06/30/2024","$500,000.00","","elias.bouharb@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","090Y","026Z, 7923","$0.00","Ransomware is an extortion-type of malicious software (malware) that encrypts, locks and exfiltrates data from local and networked assets for financial gains, hindering the availability of such resources while causing immense reputational damages. Recent ransomware attacks on high-valued cyberinfrastructure (CI) in the health, educational, IT, and critical sectors demanded ransoms up to $50M while causing collateral losses estimated to reach $20 billion in the next few years. While there are number of ongoing research efforts that address the ransomware phenomena, they are hindered by several challenges. These include the lack of ransomware-specific analysis methods that permit the comprehension of (state-sponsored) attacks that specifically target US CI, the ineffectiveness of current network-based methods that are capable of thwarting ransomware propagation attempts, and the shortage of host-based techniques that would proactively mitigate the threat. To this end, this project serves NSF's mission to promote the progress of science by developing data-driven methods, techniques and algorithms to offer a first-of-a-kind multidimensional approach to provide CI resiliency against evolving ransomware attacks. The project empowers numerous CI communities, minorities and K-12 students with open source tools, virtual training material and empirical data to facilitate forward-looking research and education. The project further supports the operational cyber situational awareness community by indexing the generated threat intelligence in an open source platform, making it readily available to support near real-time, ransomware-centric mitigation. <br/><br/>The project draws upon close to 2M (US-targeted) ransomware samples per month provided by an industry partner. The project develops binary authorship methods that are resilient against common obfuscation and refactoring techniques to (1) provide empirical evidence related to the orchestration behavior of the attack entity, and (2) facilitate the large-scale measurements and characterization of such orchestrated events. Along this vein, the project initially leverages pre-processing data methods based on opcode frequencies to subsequently devise feature engineering processes as applied on binary code to extract salient coding habits; related to memory usages, utilization of specific data structures, function terminations, etc. Moreover, the project ingests run-time behavioral reports of ransomware and develops learning methodologies by innovating techniques rooted in natural language processing and attention mechanisms. This aims at engineering models that could provide resiliency from the network level, while applying concept drift notions to capture and comprehend the mutating behaviors of such ransomware. The project also designs and implements data carving techniques by applying the devised learning models on streaming network traffic. Additionally, the project explores host-based prevention methodologies by exploiting a set of ransomware-specific behaviors. Herein, the project conducts large-scale ransomware instrumentation, models ransomware sensing activities based on DLL calls, while devising data mining methods based on a priori methods. The project further develops data sharing capabilities to facilitate access to raw data, and the generated threat intelligence. The project also devises virtual labs? material to enable large-scale, cloud-based research and training activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937953","Workshop: Extending Open Access By Bridging the Data Steward Gap: GO FAIR Train-the-Trainer (T3)","OAC","NSF Public Access Initiative","09/01/2019","08/06/2019","Christine Kirkpatrick","CA","University of California-San Diego","Standard Grant","Martin Halbert","08/31/2020","$49,999.00","Melissa Cragin","christine@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7414","","$0.00","There are national calls for improved production of research data that are FAIR (Findable, Accessible, Interoperable, and Reusable) in order to increase scientific discovery and enhance the societal and economic benefit of federal research and development (R&D) funding. To stimulate growth of such a FAIR data ecosystem in the U.S., the concrete action taken by this project is to increase expert knowledge of FAIR-oriented tools and techniques, thereby creating new capacity for increasing the value of new and existing data sets. Developing a community of practice for making data FAIR and increasing the network of available expert trainers available will serve to accelerate awareness and build new knowledge in application of FAIR techniques in the U.S. <br/><br/>By means of a two-and-a-half-day intensive training session, the project will bring together practitioners and consultants from academia, non-profits, and industry.  The intended audience for the course is seasoned data stewards and researchers, research computing staff with depth in data-driven research, and library and informaticists. The trained teachers will be prepared to teach researchers, data experts, and research library staff at universities and research institutions who are dealing with the ever-growing complexity of data integration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018851","CC* Compute: High-Memory Compute Resources for Maine","OAC","Campus Cyberinfrastructure","07/01/2020","05/26/2020","Bruce Segee","ME","University of Maine","Standard Grant","Kevin Thompson","06/30/2022","$399,813.00","Jayendran Rasaiah, Stephen Cousins, Damian Brady, Samuel Roy","segee@maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","CSE","8080","9150","$0.00","Maine researchers are advancing the state of the art in areas including landslide prediction, hydrodynamic modelling, fluid-structure interaction and modelling the electro-chemical properties of organic molecules. Strong, scientifically compelling investigations have previously been hampered or stalled by the lack of adequate computational resources.<br/> <br/>This project advances research at the University of Maine in two ways through the addition of approximately 1000 processing cores in high RAM nodes along with a growth in CEPH disk storage. It enables research to move forward in areas such as landslide prediction, coastal modelling, DNA sequencing from single strands of DNA, and high resolution modeling of the cardiovascular system. It facilitates an increase in collaboration with the Eastern Regional Network, the Open Science Grid, the Open Storage Network, and with other institutions, particularly other EPSCoR sites in the Northeast. Data and code from this grant is disseminated to the public through tools such as github and EarthCube. The increase in computational resources as a result of this project allows opportunities for undergraduate and graduate students to engage in state-of-the-art numerical modeling. By having these new resources to meet the needs of researchers, previously existing resources are utilized to offer courses for which there was not previously the capacity. Thus the instrumentation advances research and also enables the project team to better train the next generation of scientists and engineers. The research projects facilitated by this cluster all include plans to distribute, and visualize model output for relevant stakeholders.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031547","Investigating electromagnetic precursors to neutron star merger gravitational wave events","OAC","Leadership-Class Computing","06/01/2020","05/04/2020","Alexander Philippov","NJ","Princeton University","Standard Grant","Edward Walker","04/30/2022","$9,477.00","","philippo@astro.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007100","OAC Core: Small: Collaborative Research: Conversational Agents for Supporting Sustainable Implementation and Systemic Diffusion of Cyberinfrastructure and Science Gateways","OAC","OAC-Advanced Cyberinfrast Core","06/01/2020","05/21/2020","Kerk Kee","TX","Texas Tech University","Standard Grant","Robert Beverly","05/31/2023","$199,975.00","","kerk.kee@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","090Y","7923","$0.00","By designing a chatbot that harnesses the power of machine learning to connect scientists/educators with existing tools, datasets, and other experts, this sociotechnical project aims to remove significant user support barriers in the US STEM community. A chatbot called Vidura will be developed to provide personalized user support to novice and expert users in the form of a conversational agent to facilitate interdisciplinary research and education collaborations. The chatbot investigations in the project benefit the investments in science gateways (SG) and cyberinfrastructure (CI) made by the NSF and other federal agencies for over two decades. Vidura chatbot works around the clock and makes the greatest impacts when user support by human agents (i.e., domain specialists or CI support persons) is not available and/or is too costly as a science gateway surges in user uptake. The Vidura chatbot will initially be prototyped in CyNeuro, a neuroscience SG, but will be made accessible to be adapted in SGs across multiple domains to benefit the broader research/education communities.<br/><br/>This project integrates human communication science, conversational agent design, recommender algorithms, machine learning techniques, domain topic modeling in a synergistic way that advances social science, computer science, and neuroscience. In addition, this sociotechnical project provides research opportunities to benefit undergraduate and graduate students in both social and computer sciences and creates new interdisciplinary courses. The project activities will benefit students with diverse backgrounds as it will be carried out at two large public universities, one of which is a Hispanic serving institution. The project objectives and activities will focus on answering three main research questions: (i) How to design a chatbot for gathering user requirements and creating user profiles with proficiency in order to provide personalized expert service support and maintain adoption? (ii) How to equip the chatbot communication and custom dialogue flows during support actions with underlying recommender algorithms using un-supervised machine learning within/across science domains? (iii) How to implement a chatbot framework in research and education workflows of data-intensive/computation-intensive application communities (such as the neuroscience community, as well as in CI provider communities of SGCI, XSEDE, NSG, CyVerse, and JetStream) to evaluate utility across domains and identify best practices for ongoing relevance? The project findings will ultimately advance knowledge on how to enable conversational agents implemented as chatbot interfaces with pertinent underlying recommender algorithms in order to provide expert services to scientific domain users with reduced cost and increased convenience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031737","Hadron-hadron Scattering in Lattice QCD: Travel Supplement","OAC","Leadership-Class Computing","07/01/2020","05/06/2020","Colin Morningstar","PA","Carnegie-Mellon University","Standard Grant","Edward Walker","06/30/2022","$10,000.00","","colin_morningstar@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018432","CC* Networking Infrastructure: A Science DMZ For Quantitative Biology and Precision Agriculture","OAC","Campus Cyberinfrastructure","07/01/2020","05/15/2020","Brian O'Shea","MI","Michigan State University","Standard Grant","Kevin Thompson","06/30/2022","$483,122.00","Melissa Woo","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8080","","$0.00","Moore?s Law has ushered in a scientific data revolution.  This is particularly acute in the life sciences, where the devices used to collect data and the theoretical tools used to generate models have benefited tremendously from the advent of inexpensive digital sensors and general-purpose graphics processing units, which have led to an explosive increase in the volume of high-quality data.  Sharing large amounts of this data for analysis by other researchers will result in tremendous benefits to the scientific community.<br/><br/>This project creates a Science DMZ at Michigan State University, which will facilitate MSU researchers? ability to share huge volumes of data with the external research community at very high bandwidth.  The project supports the networking hardware and software necessary to implement up to 100Gbps network connections used for sharing data already stored at MSU?s High Performance Computing System and on the NSF-funded MI-OSIRIS file system.  The project uses data provided by four research groups on campus as a testbed, making cryo-electron microscopy images, hyperspectral imaging of crops using drones, three-dimensional volumetric images of plants generated via X-ray computed tomography, and a databank of biomolecular simulation data available to other researchers and the public. This project enhances the impact of MSU scientists and leverages prior NSF scientific and cyberinfrastructure investments.  In addition, it involves the participation of students in the deployment and usage of the science DMZ.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838981","Type-Based Automation of Scientific Data Management","OAC","NSF Public Access Initiative","10/01/2018","08/13/2018","Giridhar Manepalli","VA","Corporation for National Research Initiatives (NRI)","Standard Grant","Martin Halbert","09/30/2020","$297,796.00","","gmanepalli@cnri.reston.va.us","1895 Preston White Drive","Reston","VA","201915434","7036208990","CSE","7414","7916","$0.00","An approach to scientific data interoperability and reuse is through global, persistent, and uniquely identified data types that can be assembled to characterize research data sets.  This project proposes to identify data types using persistent identifiers (PIDs). The PIDs resolve to records that specify the way in which metadata, such as the provenance of the data, is structured and recorded.  The basic premise is that machine interpretable data is a critical goal to achieving FAIRness (findability, accessibility, interoperability, and reuse) of data as data discovery at a global scale depends on automated processing of the information in digital form.  A type based approach to data interpretability that utilizes persistent IDs at the granularity of data types can overturn the Internet and stimulate an ecosystem of new tools for FAIR data. This pilot effort involves evaluating the approach through, in part, by constructing a critical mass of use cases.<br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2006816","OAC Core: Small: Collaborative Research: Conversational Agents for Supporting Sustainable Implementation and Systemic Diffusion of Cyberinfrastructure and Science Gateways","OAC","OAC-Advanced Cyberinfrast Core","06/01/2020","05/21/2020","Prasad Calyam","MO","University of Missouri-Columbia","Standard Grant","Robert Beverly","05/31/2023","$300,004.00","Satish Nair","calyamp@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","090Y","7923, 9150","$0.00","By designing a chatbot that harnesses the power of machine learning to connect scientists/educators with existing tools, datasets, and other experts, this sociotechnical project aims to remove significant user support barriers in the US STEM community. A chatbot called Vidura will be developed to provide personalized user support to novice and expert users in the form of a conversational agent to facilitate interdisciplinary research and education collaborations. The chatbot investigations in the project benefit the investments in science gateways (SG) and cyberinfrastructure (CI) made by the NSF and other federal agencies for over two decades. Vidura chatbot works around the clock and makes the greatest impacts when user support by human agents (i.e., domain specialists or CI support persons) is not available and/or is too costly as a science gateway surges in user uptake. The Vidura chatbot will initially be prototyped in CyNeuro, a neuroscience SG, but will be made accessible to be adapted in SGs across multiple domains to benefit the broader research/education communities.<br/><br/>This project integrates human communication science, conversational agent design, recommender algorithms, machine learning techniques, domain topic modeling in a synergistic way that advances social science, computer science, and neuroscience. In addition, this sociotechnical project provides research opportunities to benefit undergraduate and graduate students in both social and computer sciences and creates new interdisciplinary courses. The project activities will benefit students with diverse backgrounds as it will be carried out at two large public universities, one of which is a Hispanic serving institution. The project objectives and activities will focus on answering three main research questions: (i) How to design a chatbot for gathering user requirements and creating user profiles with proficiency in order to provide personalized expert service support and maintain adoption? (ii) How to equip the chatbot communication and custom dialogue flows during support actions with underlying recommender algorithms using un-supervised machine learning within/across science domains? (iii) How to implement a chatbot framework in research and education workflows of data-intensive/computation-intensive application communities (such as the neuroscience community, as well as in CI provider communities of SGCI, XSEDE, NSG, CyVerse, and JetStream) to evaluate utility across domains and identify best practices for ongoing relevance? The project findings will ultimately advance knowledge on how to enable conversational agents implemented as chatbot interfaces with pertinent underlying recommender algorithms in order to provide expert services to scientific domain users with reduced cost and increased convenience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939486","CSSI: Elements: First Workshop on NSF and DOE High Performance Computing Tools","OAC","Software Institutes","09/01/2019","08/22/2019","Sameer Shende","OR","University of Oregon Eugene","Standard Grant","Robert Beverly","08/31/2020","$20,954.00","","sameer@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","8004","026Z, 7556, 8004","$0.00","High Performance Computing (HPC) software has become increasingly complex to install. The complex inter-package dependency can lead to significant loss of productivity. This project will conduct a two-day hands-on workshop that brings together NSF resource providers and system administrators, NSF PIs and HPC application developers, and scientists and facilities administrators from the DOE national laboratories, and industry to develop a strong collaboration on HPC software deployment. The Software Development Kit (SDK) that will be used is available through a containerized distribution as well as Spack - an app store for supercomputers. The workshop will have a significant impact on enabling the delivery of HPC software to NSF and other supercomputing sites. It will better equip application developers in the use of modern software delivery infrastructures including HPC containers and the Spack platform. <br/><br/>High Performance Computing (HPC) software has become increasingly complex to install. The complex inter-package dependency can lead to significant loss of productivity. This workshop on HPC tools will bring together NSF resource providers and system administrators, NSF PIs and HPC application developers, and scientists and facilities administrators from the DOE national laboratories, and industry to develop a strong collaboration on HPC. DOEs Exascale Computing Project (ECP) has produced an Extreme-Scale Scientific Software Stack (E4S) of HPC libraries and tools in a Software Development Kit (SDK). This SDK is available through a containerized distribution as well as Spack - an app store for supercomputers. Spack includes recipes for building packages from source code and is the primary means of deploying ECP software. This two-day workshop will focus on the Spack platform and E4S. The expected outcome of this meeting will be actual deployment of the ECP SDK software stack and container-based runtimes on the HPC systems and an understanding of how to develop custom recipes for Spack based builds. The proposed workshop will have a significant impact on enabling the delivery of HPC software to NSF and other supercomputing sites. It will better equip application developers in the use of modern software delivery infrastructures including HPC containers and the Spack platform.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841625","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","OFFICE OF MULTIDISCIPLINARY AC, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Patrick Brady","WI","University of Wisconsin-Milwaukee","Standard Grant","William Miller","09/30/2020","$262,279.00","David Kaplan, Philip Chang","prbrady@uwm.edu","P O BOX 340","Milwaukee","WI","532010340","4142294853","CSE","1253, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841617","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Erotokritos Katsavounidis","MA","Massachusetts Institute of Technology","Standard Grant","William Miller","09/30/2020","$32,362.00","","kats@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841594","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","David Hogg","NY","New York University","Standard Grant","William Miller","09/30/2020","$36,469.00","Federica Bianco","david.hogg@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841370","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Ignacio Taboada","GA","Georgia Tech Research Corporation","Standard Grant","William Miller","09/30/2020","$26,589.00","","itaboada@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743185","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Mark Shephard","NY","Rensselaer Polytechnic Institute","Continuing Grant","Stefan Robila","08/31/2020","$65,000.00","Onkar Sahni, Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743180","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Beverley McKeon","CA","California Institute of Technology","Continuing Grant","Stefan Robila","08/31/2020","$44,284.00","","mckeon@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031219","Travel grant for LRAC proposal AST20004:  The role of low collisionality in compressible, magnetized turbulence","OAC","Leadership-Class Computing","07/01/2020","05/01/2020","Brian O'Shea","MI","Michigan State University","Standard Grant","Edward Walker","06/30/2022","$9,515.00","","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925096","Planning for the Leadership-Class Computing Facility","OAC","Leadership-Class Computing","07/01/2019","09/30/2020","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","12/31/2020","$2,000,000.00","John West, Omar Ghattas","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","026Z","$0.00","NSF has played a central role in the transformation of scientific research and as a leader in enabling the use of High-Performance Computing (HPC) to advance discovery for almost four decades.  The leadership-class computing program is intended to provision advanced computational capabilities to enable transformative research for all of Science and Engineering (S&E) that would not otherwise be possible.  NSF's current leadership-class computing investment is the support of the Frontera project at the Texas Advanced Computing Center (TACC) at The University of Texas at Austin.  Frontera is the Phase 1 system of a two-phase approach to eventual deployment of a much more capable Leadership-Class Computing Facility (LCCF) in approximately 2024.  This award will support the conceptual design of a Phase 2 system and facility.<br/><br/>The project design approach will extend best practices in the design/operate/evaluate cycle that TACC has used to successfully deploy some of the largest computing systems in the world for open scientific research. This process will have four keystones: science user requirements, future technology assessment, alternative design evaluation, as well as risk management for cost, scope and schedule. Testbeds feature prominently in the design process and will provide users with early access to future computational hardware and will offer system designers quantitative insights into the effectiveness of these technologies. Testbed partners will include major cloud and technology vendors as well as early exascale computing technology adopters.  In addition, the design process will be guided by science inputs and technology drivers identified by science and technology engagement teams. These teams will be composed of distinguished scientists representing a broad range of expertise and experience to guide the Phase 2 system design.  The team members will also serve as community leaders to bring together colleagues in workshops, meetings, and other engagement venues to inform the process further.  Moreover, the project will engage with the scientific software community, including library and community code providers, to ensure the project optimizes the selection of Phase 2 computing technologies to enable transformative S&E discoveries for the future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827314","MRI: Acquisition of a GPU Accelerated Vermont Advanced Computing Core","OAC","Major Research Instrumentation","09/01/2018","08/23/2018","Adrian Delmaestro","VT","University of Vermont & State Agricultural College","Standard Grant","Alejandro Suarez","08/31/2020","$893,120.00","Joshua Bongard, Yolanda Chen, Hugh Garavan, Juan Vanegas","Adrian.DelMaestro@utk.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","1189","026Z, 062Z, 075Z, 1189, 8089, 8091, 9150","$0.00","This project will enable interdisciplinary science through the acquisition of a high-performance computer cluster, named DeepGreen.  Based on cutting-edge massively parallel graphics processing unit (GPU) technologies, DeepGreen will be utilized by the over 300 users from six Colleges at the University of Vermont, and throughout the Northeast. The unique hybrid architecture was designed to optimize artificial intelligence (AI) applications and will allow for rapid progress on problems of great societal importance. They include: quantum computing, drug discovery and design, safe robotics, control of adaptive crop pests, and new computer vision tools for use in the health care and transportation industries.  As an example, DeepGreen will allow the training of neural networks on the world's largest brain imaging datasets of illicit drug users, yielding novel health and policy strategies to combat the opioid epidemic.  A focus of the scientific and technical team is to broaden the number of personnel able to exploit GPU hardware for problem solving, producing the highly trained and diverse technical workforce required for the current and future AI economy. <br/><br/>DeepGreen was designed by a team of experts from the physical, medical, biological, computational, and agricultural sciences, partnered with an experienced group of information technology professionals.  It will be capable of over 8 petaflops of mixed precision calculations based on the latest NVIDIA Tesla V100 architecture with a hybrid design allowing high bandwidth message passing across heterogeneous compute nodes.  Its extreme parallelism will facilitate research in three interconnected areas: quantum many-body systems, molecular simulation and modeling, and deep learning, artificial intelligence and evolutionary algorithms.  DeepGreen will forge transformative research pipelines. It will enable the study of thousands of quantum entangled atoms, and millions of interacting components in biological systems providing insights into structure-function mechanisms.  Machine learning and deep neural networks will exploit DeepGreen's Tensor Cores to solve diverse problems. These problems include: the development of coarse grained potentials for use in molecular dynamics simulations, real time dynamic processing of crowd sourced decision making for robotics, genomic sequencing of invasive pests, and feature recognition in medical imaging to distinguish cancerous tumors from benign nodules.  Software designed for use on DeepGreen will be released to the public as open source, with other scientists and researchers being able to immediately use and extend it. This project will also support the next generation of data scientists. Training workshops focused on GPU computing and machine learning frameworks, new university courses, and partnerships with existing local NSF-funded graduate training initiatives, will drive broad utilization of DeepGreen.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811123","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","03/26/2019","Nancy Ide","NY","Vassar College","Standard Grant","Stefan Robila","12/31/2019","$177,639.00","Anton Nekrutenko","ide@brandeis.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","8004","026Z, 7916, 8004, 9251","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838960","FAIR Publishing Guidelines for Spectral Data and Chemical Structures in Support of Chemistry and Related Disciplinary Communities","OAC","NSF Public Access Initiative","09/01/2018","08/17/2018","Vincent Scalfani","AL","University of Alabama Tuscaloosa","Standard Grant","Martin Halbert","08/31/2020","$24,671.00","","vfscalfani@ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","CSE","7414","7556, 9150","$0.00","In this project the PIs will convene a meeting to establish publishing guidelines for sharing machine-readable files of commonly reported chemical data classes.  The workshop organizers seek involvement from researchers, publishers, funding agencies, libraries, database and repository managers, tool developers, chemical societies, and standards organizations. Topics to discuss include formulating value propositions for data (that is, what data should be kept and for how long), and formulating criteria and stakeholder roles to better support data that are Findable, Accessible Interoperable, and Reusable (FAIR).  Several outcomes are expected, including best-practice publishing guidelines specifying FAIR criteria for spectral and chemical structure data; and workflow models for capturing, managing and sharing these data in domain repositories. This workshop will build on momentum from the Research Data Alliance (RDA), the International Union of Pure and Applied Chemistry (IUPAC), the American Chemical Society (ACS) and other related scientific societies. <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839032","Community Meeting on Scalable Data Publication Infrastructure","OAC","NSF Public Access Initiative","11/01/2018","08/31/2018","Guenter Waibel","CA","University of California, Office of the President, Oakland","Standard Grant","Beth Plale","10/31/2019","$49,888.00","","guenter.waibel@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","7414","7556","$0.00","Disciplinary (non-institutional) data repositories have much higher rates of adoption than Institutional Repositories (IRs) in part because they are directly integrated with key points in researcher workflows, particularly during the publishing of scientific articles. But community repositories lack partnerships with institutional expertise found in librarians and data curators.  This project addresses researcher need through taking the first steps in an open, community-supported initiative in research data curation and publishing.  To effectively leverage institutional knowledge and serve researchers as end users, more and varied types of institutions need to have a say in the values and necessities. To build broad community support, this invitational workshop will explore community requirements, identify impediments, study sustainable business models, and make recommendations regarding the widespread promotion and adoption of effective, scalable, and sustainable institutional data publication infrastructure.<br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004840","Elements: FastTract: Web-Based Exploratory Visualization of Gigapixel Astronomical Images","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","06/01/2020","05/19/2020","Peter Williams","DC","American Astronomical Society","Standard Grant","Robert Beverly","05/31/2023","$431,261.00","","peter.williams@aas.org","1667 K Street NW","Washington","DC","200060000","2023282010","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923","$0.00","Like scientists in virtually every field, astronomers are struggling under a<br/>""data deluge"". While the ever-larger images obtained by the newest<br/>observatories lead to cutting-edge science, they also overwhelm traditional<br/>tools designed to work with files hundreds or thousands of times smaller<br/>than the state-of-the-art. In particular, astronomers are rapidly losing the<br/>ability to simply look at the images of the sky that are coming out of their<br/>telescopes. The FastTract Project will solve this problem by marrying existing<br/>Web-based visualization technologies in the AAS WorldWide Telescope software<br/>system with new features and tools needed to efficiently work with the large<br/>astronomical images of the 2020's, enabling US astronomers to fully exploit<br/>their world-class facilities, in particular the ones that open new Windows on<br/>the Universe. The project team will synthesize the insight gained in this<br/>undertaking to establish the Little Big Data University (LBDU), a learning<br/>resource for scientists across disciplines who are struggling to stay afloat<br/>in the ""data deluge"". LBDU will follow the Khan Academy model and extensively<br/>use interactive environments to help researchers and others learn strategies<br/>for Harnessing the Data Revolution. These efforts will especially benefit<br/>people who do not have access to top-tier computational resources, such as<br/>those at small institutions and interested non-specialists.<br/><br/>The FastTract Project will create a sustainable cyberinfrastructure (CI)<br/>system and associated community of practice that enable exploratory scientific<br/>visualization of large (gigapixel+) astronomical images over the Web. The CI<br/>will build on the NSF-funded AAS WorldWide Telescope (WWT) software system.<br/>The research team will extend WWT's image tiling architecture to work with<br/>FITS data files, develop the tooling needed for easy creation of such tiles,<br/>and produce workflows and tutorials allowing the broad astronomical community<br/>to take ownership of the infrastructure. The team will work with three<br/>NSF-funded partners on specific science applications. Annual workshops will<br/>nucleate a core group of project user-contributors and ensure that development<br/>effort meets the needs of the broader community. The project will adopt best<br/>practices in the open-source, open-development paradigm, laying the groundwork<br/>for FastTract infrastructure to be leveraged by non-astronomical communities<br/>facing similar data challenges. In parallel, the team will distill its<br/>expertise to create the Little Big Data University (LBDU), an online<br/>professional development resource aimed at domain scientists who do not intend<br/>to become CI specialists. These scientists will learn core strategies for<br/>coping with ever-larger data sets through the empirically successful Khan<br/>Academy model. Telemetry analysis and focus groups will guide curriculum<br/>design and assess efficacy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800946","Workshop on Clusters, Clouds, and Data Analytics in Scientific Computing","OAC","Software Institutes","08/01/2018","08/04/2018","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Robert Beverly","07/31/2019","$19,278.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","026Z, 7556, 8004","$0.00","This workshop will be held in La Maison Des Contes, Dareize, France, and will bring together international experts from the US, France, and other countries to discuss advancements in cyberinfrastructure (CI). The specific focus will be on commonalities that exist between cluster, cloud, and high-end data analytics. CI has become of growing importance to nearly all NSF research, which makes increasing use of computational simulation and large-data analysis methods. The results will be published in an open report as well as in several journal papers. They will contribute to and leverage new ideas in cluster, cloud, and data analytics computing for the benefit of the research community.<br/> <br/><br/>The architectural similarities between the above issues, and the fact that high performance clusters typically make up the major compute nodes of computational grids and clouds, means that deployment, operational, and usage issues surrounding computational clouds form a superset of the issues that revolve around clusters. The workshop will focus on five tasks: (1) survey and analyze the key deployment, operational, and usage issues for clusters, clouds, and data analytics-- focusing especially on discontinuities produced by multicore and hybrid architectures, data-intensive science, and the increasing need for wide-area/local-area interaction; (2) document the current state-of-the-art in each of these areas, identifying interesting questions and limitations; (3) discuss experiences with clusters, clouds, and data relative to the research communities and science domains benefiting from the technology; (4) explore interoperability among disparate clouds and between various clouds and grids and their impact on domain sciences; and (5) explore directions for future research and development against the background of disruptive trends and technologies and the recognized gaps in the current state-of-the-art.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838628","Integrative Structural Determination Federation (iSDF) Workshop","OAC","NSF Public Access Initiative","09/01/2018","08/04/2018","Helen Berman","NJ","Rutgers University New Brunswick","Standard Grant","Beth Plale","08/31/2019","$50,000.00","","berman@rcsb.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7414","7556","$0.00","The PIs will organize a workshop that brings together members and stakeholders of the integrative Structure Determination Federation (iSDF), a new entity that is envisioned as a network of model and data resources that contribute to structural biology. The proposed workshop is aimed at initiating engagement of diverse experimental data and structural model communities in order to plan for an interoperating network of model and data repositories.  The workshop builds upon three years? effort on building blocks required for developing  coordinated network of structural biology resources.   This earlier effort stems from an action in 2014 by wwPDB to create an Integrative/Hybrid Methods task force which had, as one of its key recommendations, to create a federated system of model and data archives.  The outcome of the workshop is a set of requirements for creating well-aligned data standards and protocols for efficient data exchange among participating repositories.  <br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838990","Leveraging Scientific Societies for Open and FAIR Scientific Data","OAC","NSF Public Access Initiative","11/01/2018","06/11/2021","Shelley Stall","DC","American Geophysical Union","Standard Grant","Martin Halbert","10/31/2021","$50,000.00","","sstall@agu.org","2000 FLORIDA AVE NW","Washington","DC","200099123","2024626900","CSE","7414","7556","$0.00","Scientific societies are a critical stakeholder and leader in bringing about cultural change toward open data across the sciences. The American Geophysical Union (AGU) has played a key role in being a change agent in open science specifically by leading the Earth and space science communities in enabling research data to be findable, accessible, interoperable, and reusable (FAIR). The AGU proposes to host a workshop and one-on-one follow-up conversations with targeted societies within the Earth and space sciences and in adjacent disciplines. The goal of the proposal is to engage and empower more scientific societies on their way to adoption of open science principles through education, tools, materials, and consultation. The intellectual merit is in evaluation of the effectiveness of the open and FAIR data tool suite for societies and their members and in moving societies forward in open science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031862","Frontera Travel Grant: Milky Way Galaxy Simulation at Unprecedented Resolution","OAC","Leadership-Class Computing","07/01/2020","05/12/2020","Alyson Brooks","NJ","Rutgers University New Brunswick","Standard Grant","Edward Walker","06/30/2022","$9,922.00","","abrooks@physics.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103433","Leveraging Data Communities to Advance Open Science","OAC","NSF Public Access Initiative","04/01/2021","04/13/2021","Danielle Cooper","NY","JSTOR","Standard Grant","Martin Halbert","03/31/2022","$99,599.00","","Danielle.Cooper@ithaka.org","101 Greenwich Street, 18th Floor","New York","NY","100061852","3475732317","CSE","7414","7556","$0.00","This workshop will advance scientific community readiness to share dataset metadata and create evidence-based practices to maximize the impact of that sharing. The workshop will do this by addressing the need for focused collaboration between scientists and information and technology professionals to understand, support, and promote the growth of ?data communities,? formal or informal networks of scientists who voluntarily share and reuse a particular type of data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945938","Implementing Effective Data Practices","OAC","NSF Public Access Initiative","09/01/2019","08/29/2019","Guenter Waibel","CA","University of California, Office of the President, Oakland","Standard Grant","Martin Halbert","08/31/2020","$49,531.00","","guenter.waibel@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","7414","","$0.00","The open science movement is gaining momentum across the academic landscape. Since the National Academies of Science, Engineering, and Medicine (NAS) published the Open Science by Design: Realizing a Vision for 21st Century Research report in 2018, many institutions, organizations, and faculty have begun assessing their current practices and infrastructure to support a more open research ecosystem. To fully realize the vision for open science and scholarship, stakeholders need to adopt key infrastructure, standards, and practices necessary to facilitate responsible data practices. Drawing inspiration from sources such as the May 2019 NSF Dear Colleague Letter (DCL) the organizers propose an expert convening to discuss persistent identifiers (PIDs) for datasets and creating machine readable data management plans (DMPs).  The conference is organized by California Digital Library (CDL) and Association of Research Libraries (ARL), in partnership with the Association of American Universities (AAU) and the Association of Public and Land-grant Universities (APLU). <br/><br/>The conference will engage approximately 40 experts in a multi-day workshop in Washington DC winter of 2019 with the goal to identify and determine:<br/><br/>1. What barriers remain to implementing the widely recognized good practices in the NSF DCL<br/>2. What kinds of model workflows might address those barriers, while minimizing faculty burden<br/>3. What implementation of the NSF DCL means for institutional data governance (e.g. sharing DMPs across campus units, between institutions, and publicly)<br/>4. Findings to bring back to policymakers, funding agencies, and other similar institutions<br/>5. Recommendations of effective practices for grants offices, including guidance to their researchers<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939291","EAGER: Open Infrastructure to Reduce Burden on Researchers and Federal Agencies","OAC","NSF Public Access Initiative","10/01/2019","07/22/2019","Golam Choudhury","MD","Johns Hopkins University","Standard Grant","Martin Halbert","09/30/2021","$247,601.00","","sayeed@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7414","7916","$0.00","Johns Hopkins University, in partnership with six other institutions, proposes to design a set of recommendations, use cases, specifications, and workflows that will transform the deposit of manuscripts into federal agency repositories, thereby reducing burden on researchers, universities, and federal agencies. This early exploratory research undertakes to develop solutions for issues that arise when projects are multi-institution, federal agency workflows are vastly different, and the integrity of information is critical. As research becomes increasingly convergent, there is a major risk of an increasingly complex environment for investigators, universities, and funding agencies. By binding grants or award data to primary research objects such as articles, data, or software, the proposed research represents an important step toward data analytics on researcher and university productivity and collaboration. The proposed work has the potential to positively impact every university that receives federal funding and every federal funding agency with a public access compliance policy.<br/><br/>An early implementation of the PI's Public Access Submission System (PASS) system has been shown to work with the NIH PubMed Central.  But scaling beyond PubMedCentral requires exploratory research such as to establish the logic for simultaneous submission of a publication to multiple federal grant repositories and institution repositories.   Research questions include how a third-party applications such as the PASS system, manage cross-institutional grants data, identities, and deposit workflows.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838958","FAIR Publishing Guidelines for Spectral Data and Chemical Structures in Support of Chemistry and Related Disciplinary Communities","OAC","NSF Public Access Initiative","09/01/2018","08/17/2018","Leah McEwen","NY","Cornell University","Standard Grant","Martin Halbert","08/31/2020","$24,611.00","","lrm1@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7414","7556","$0.00","In this project the PIs will convene a community workshop to establish publishing guidelines for sharing machine-readable files of commonly reported chemical data classes.  The workshop organizers seek involvement from researchers, publishers, funding agencies, libraries, database and repository managers, tool developers, chemical societies, and standards organizations. Topics to discuss include formulating value propositions for data (that is, what data should be kept and for how long), and formulating criteria and stakeholder roles to better support data that are Findable, Accessible, Interoperable, and Reusable (FAIR).  Several outcomes are expected, including best-practice publishing guidelines specifying FAIR criteria for spectral and chemical structure data; and workflow models for capturing, managing and sharing these data in domain repositories. This workshop will build on momentum from the Research Data Alliance (RDA), the International Union of Pure and Applied Chemistry (IUPAC), the American Chemical Society (ACS) and other related scientific societies. <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005259","Elements: ALE-AMR Framework and the PISALE Codebase","OAC","Hydrologic Sciences, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","07/01/2020","04/13/2020","Alice Koniges","HI","University of Hawaii","Standard Grant","Seung-Jong Park","06/30/2023","$599,996.00","Jonghyun Lee","koniges@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","1579, 7222, 8004, 8074","077Z, 4444, 7923, 8004, 9150","$0.00","The solution of partial differential equations (PDEs) on modern high performance computing (HPC) platforms is essential to the continued success of research and modeling for a wide variety of areas of importance to the national interest. This project will make available software for modeling with PDEs. It will also apply the code for simulations of complex groundwater flow processes in Hawaiian islands characterized by highly heterogeneous volcanic rocks and dynamic interaction between freshwater and seawater. Roughly half the population in the US lives near coastal areas where groundwater supplies much of the domestic, agricultural, and industrial water supply.  Almost all of Hawaii?s domestic water use is pumped from volcanic aquifer systems since the islands are completely surrounded by the Pacific Ocean. In Hawaii?s groundwater resources, freshwater accumulates on top of the denser underlying saltwater, making it highly susceptible to anthropogenic activities and saltwater intrusion induced by possible sea water and volcanic events. It is essential that Hawaii?s groundwater resources are properly modeled and managed for sustainable use. Island-scale numerical groundwater flow modeling with PDEs on HPC will play an important role in predicting the sustainable yields for the volcanic aquifer systems and planning groundwater resources management. The software in this project will be used both for applications and as a nexus for student involvement in HPC. Curriculum material associated with the project will be developed and offered at university level groundwater modeling classes.<br/> <br/>The PDE software developed, distributed, and applied in this project uses an innovative combination of advanced mathematical techniques for the solution of PDEs including parallel software tools to dynamically adapt the grids and special Lagrangian-flow methods that allow for solution of equations that can reproduce the sharp freshwater-seawater interface observed in sea water monitoring locations. The software is particularly appropriate for applications with equations that can be couched in a conservation law form. Source terms of these applications can be included using complex numerical techniques including the ability to handle anisotropic tensors components.The software is based on techniques of ALE (Arbitrary Lagrangian Eulerian Methods) with AMR (Adaptive Mesh Refinement) to create a publicly available sustainable branch of the software known as PISALE for Pacific Island Structured-amr with ALE.  In addition to the subsurface flow and transport application in Hawaiian aquifers, the project will provide capability for collaborative research in a variety of fields that require efficient solution of PDEs on advanced HPC architectures.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, part of the Division of Earth Sciences, within the NSF Directorate of Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836797","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2017","09/19/2018","Matthew Knepley","NY","SUNY at Buffalo","Standard Grant","Stefan Robila","07/31/2020","$215,715.00","","knepley@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1841612","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Joshua Bloom","CA","University of California-Berkeley","Standard Grant","William Miller","09/30/2020","$29,542.00","","joshbloom@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841591","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Peter Couvares","CA","California Institute of Technology","Standard Grant","William Miller","09/30/2020","$23,291.00","","peter.couvares@ligo.org","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841590","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","03/15/2019","Claudio Kopper","MI","Michigan State University","Standard Grant","William Miller","09/30/2020","$16,002.00","","koppercl@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841544","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Dale Howell","CA","Las Cumbres Observatory Global Telescope Network","Standard Grant","William Miller","09/30/2020","$28,356.00","","ahowell@lco.global","6740 Cortona Dr Suite 102","Goleta","CA","931175575","8058801608","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841546","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Peter Couvares","CA","California Institute of Technology","Standard Grant","William Miller","09/30/2020","$55,094.00","","peter.couvares@ligo.org","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827138","CC* Networking Infrastructure: CyberInfrastructure Technology Advancement for Delaware (CITADel) - 100 Gb/s Connection Upgrade to Internet2","OAC","Campus Cyberinfrastructure","07/01/2018","06/18/2018","Jason Cash","DE","University of Delaware","Standard Grant","Kevin Thompson","08/31/2020","$447,089.00","Fraser Gutteridge, William Totten, Doke Scott, Sharon Pitt","cash@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","8080","9150","$0.00","The CC* Networking Infrastructure project at the University of Delaware (UD) is constructing a high-bandwidth, low-latency Internet2 connection to support the university's research and education initiatives by fulfilling the need for efficient and timely data transfer to collaborating institutions, national high-performance computing resources, and service for the global research community. This project enables the sharing of large and time-sensitive data sets without impediment. It provides the necessary infrastructure to support major initiatives for the Delaware Biotechnology Institute, department of Chemistry & Biochemistry, department of Mechanical Engineering, Center for Bioinformatics and Computational Biology, Bartol Research Institute, department of Chemical & Biomolecular Engineering, Entomology and Wildlife Ecology, and many more data intensive programs across the university. This is a transformational grant which enables DNA sequence, energy flux, molecule property, molecular dynamics trajectory, protein, solid particle, visualization, weather, and other data to be transferred between HPC clusters and scientists for immediate use with shorter delays between successive simulations and resulting in faster overall research schedule.<br/><br/>This project provides two enhancements to the University of Delaware network infrastructure: (1) creation of a 100Gbps capable network for connectivity to the Internet2 network community, and (2) creation of a ScienceDMZ to enable researchers to take full advantage of the new high-bandwidth capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743191","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Robert Moser","TX","University of Texas at Austin","Continuing Grant","Stefan Robila","08/31/2020","$45,000.00","","rmoser@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743179","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Charles Meneveau","MD","Johns Hopkins University","Continuing Grant","Stefan Robila","08/31/2020","$22,821.00","","meneveau@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1933353","2019 NSF Workshop on Connecting Large Facilities and Cyberinfrastructure","OAC","CESER-Cyberinfrastructure for","07/01/2019","07/02/2019","Ewa Deelman","CA","University of Southern California","Standard Grant","William Miller","06/30/2020","$96,402.00","","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7684","7556","$0.00","Cyberinfrastructure (CI) is a fabric that pervades and enables modern science and has been long-supported at NSF. CI comprises advanced computing, data, software, and networking infrastructure, as well as the necessary specialized human capital. NSF-supported Large Facilities are major scientific research platforms and represent some of the NSF's most substantial investments. Facilities rely heavily on existing CI and new CI capabilities and solutions to support their scientific communities. However, CI used by facilities is currently predominantly built independently by each facility. In 2015, NSF began to support a series of workshops focused on CI for Large Facilities to bring together the facility and CI communities to share common experiences and challenges, discuss potential collaborations and opportunities for leveraging CI within the community. The 2019 Workshop on NSF Large Facilities and CI aims to continue and advance the discussion, exchange, and community building, through a forum for sharing of ideas and experiences and, importantly, prepare for future CI research, development, and deployment. <br/><br/>Specific goals of the workshop include 1) identifying common cyberinfrastructure challenges among facilities, 2) understanding the facility data lifecycle, including the commonalities and differences between data lifecycle stages, 3) exploring opportunities for joint training and education among facilities and large CI projects, 4) sharing experiences in CI project management, and 5) discussing approaches to building a community of CI professionals. An important focus of the workshop will be the CI needed to support the entire facility science lifecycle, which spans data capture and processing, data storage and archiving, and data access, analysis, visualization, and dissemination. The exploration of potential collaborations on common CI challenges is another important aim. Other workshop topics may include computation, network management, and education and workforce development.  A workshop report will be posted online to disseminate the discussions and findings to the broader CI community. Participation will be encouraged from a diverse set of CI researchers and professionals at various stages in their careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925467","CC*:  Campus Computing and the Computing Continuum:  Campus Cluster Resource:  Expanded High Performance Computing at Wayne State","OAC","Campus Cyberinfrastructure","07/01/2019","06/03/2019","David Cinabro","MI","Wayne State University","Standard Grant","Kevin Thompson","06/30/2020","$399,944.00","Patrick Gossman","david.cinabro@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","8080","","$0.00","Wayne State University has made investments in research computing including construction of a new data center, deployment of high-speed networks with global reach, and massive storage for globally accessible data.  This NSF CC* project augments those existing high-performance computing resources through the addition of six high-speed, multiprocessor systems with a large amount of memory each with 1.5 terabytes and 3 NVIDIA graphic processing units (GPUs) capable of up to 120 trillion calculations per second in each system. Simulations that used to take months can be completed in days, thereby speeding research in many areas and expanding resources for the University's 275 research teams and 1,500 unique users of which 1,000 are students. <br/><br/>The impact of this new equipment spreads across multiple disciplines including Computational Chemistry, Membrane Biophysics, Molecular Dynamics, Complex Materials and ground-breaking work in the study of Nuclear Matter and its various phases.  Modeling relativistic heavy ion collisions that produce the Quark Gluon Plasma in reasonable wall times was beyond the capability of the old hardware, but with the new system such simulations can be made and help shed light on strongly coupled systems and inform the nature of the early Universe. The GPUs have special features called Tensor Flow Cores, of particular use in Artificial Intelligence, Deep Learning and Analytics.  These augment research in urban and environmental health.  The GPUs are also a boon for education in Wayne State's new Master of Science in Data Science joint program in Engineering, Computer Science and School of Business<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841527","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Kelly Holley-Bockelmann","TN","Vanderbilt University","Standard Grant","William Miller","09/30/2019","$20,075.00","","k.holley@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835511","Cyberinfrastructure for Sustained Scientific Innovation - Software Elements: Cloud WRF for the Atmospheric Research and Education Communities","OAC","LARS SPECIAL PROGRAMS, Software Institutes, EarthCube","10/01/2018","07/31/2018","Jordan Powers","CO","University Corporation For Atmospheric Res","Standard Grant","Seung-Jong Park","09/30/2020","$284,201.00","Yuh-Lang Lin, Russ Schumacher","powers@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","7790, 8004, 8074","026Z, 062Z, 077Z, 1525, 4444, 7923, 8004","$0.00","This award supports the establishment of an officially-supported version of the Weather and Research Forecast (WRF) model in the cloud environment.  WRF is the world's most popular numerical weather prediction model and is supported by the National Center for Atmospheric Research (NCAR) for a community of users across universities, research labs, and operational weather centers.  This project will address fundamental issues such as modeling system accessibility, improvement of model support to the research and educational user communities, student and scientist training, and facilitation of model development. Given WRF's prominence in both atmospheric research and real-time weather forecasting, this work will not only promote the advancement of science, but also will contribute to the vigor of development and application of one of the nation's cyberinfrastucture (CI) assets, a key weather prediction model used at operational centers and for public purposes.  Furthermore, this project will contribute to education at minority-serving institutions and will yield tools to better the training of new generations of the nation's atmospheric scientists. Lastly, the project will leverage the resources and support of commercial cloud service providers to serve these national interests, in a collaboration of the principal researchers with industry.<br/><br/>The viability of running the WRF in the cloud has been previously demonstrated, and this project would advance the field by configuring and supporting the official version of the WRF in the cloud, with an up-to-date, cloud-configured version of the WRF system code synced to the WRF GitHub repository.  Other materials that will be available include the WRF tutorial materials and system documentation, WRF input and output datasets, and the WRF Testing Framework code analysis package.  The project will produce a configured and documented cloud WRF system that will open a new arena for model use and that will enhance the efficiency of both user support and the integration of contributed model improvements.  This cloud capability will exploit an emerging common cyber-ground to facilitate code development for the target model and allow for critical reproducibility in code analysis and testing.  It will advance the delivery of modeling system tutorials, and thus scientist training and education, through establishing globally-accessible modeling spaces, enhancing instruction portability and access, decreasing costs for host institutions, and improving usability of on-line materials.  These new cloud capabilities will be addressing bottlenecks in model support and development, and once developed, the CI can be adapted for other community models, thus benefiting other scientific disciplines and their tools via its reuse.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841399","Collaborative: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, Software Institutes","06/01/2018","07/21/2018","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","William Miller","04/30/2020","$75,000.00","","taufer@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084, 9150","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOÕs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826915","CC* Networking Infrastructure: Building a Friction-free High-speed Science Network and DMZ to Support Data-intensive Research and Education at The College of New Jersey","OAC","Campus Cyberinfrastructure","07/01/2018","06/25/2018","Sharon Blanton","NJ","The College of New Jersey","Standard Grant","Kevin Thompson","06/30/2020","$499,987.00","Jeffrey Osborn","blantons@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","8080","","$0.00","This project proposes a program of high-speed network upgrades at The College of New Jersey (TCNJ) to support cutting-edge computational research and education. Insufficient access to TCNJ's High Performance Computing cluster previously created a bottleneck that limited scientific research and training. These enhancements to TCNJ's science network will expand research capacity and efficiency, particularly in computationally intensive fields, including biochemistry, astrophysics, phylogenomics, high-resolution microscopy, and undergraduate science education, and will provide students with greater access to research opportunities in laboratories and classrooms while contributing to the preparation of a computationally literate workforce. The upgrades enable faster data acquisition and propagation and enhancing the research and external collaborations of the College's many computationally intensive researchers, including both faculty members and undergraduate students. Finally, as part of this project, a new data visualization short course is exposing a greater number of undergraduate students to TCNJ's computing resources.<br/> <br/>This project implements a new high-speed science network and DMZ at The College of New Jersey (TCNJ) to support cutting-edge computational research and education where insufficient access to TCN's High Performance Computing cluster previously created a bottleneck that limited scientific research and training. This project directly connects TCNJ's science buildings to its High Performance Computing cluster via a new 80 Gigabit per second (Gbps) backbone and 10 Gbps connections to targeted labs, classrooms, and offices. A new Science DMZ that uses the SciPass OpenFlow application is also being implemented. SciPass supports friction-free interconnectivity by automatically allowing ""good"" data flows to bypass the Science Network firewall in real-time. Network performance is being assessed and nationally benchmarked using perfSONAR, and a new collaboration with Open Science Grid is allowing TCNJ to connect directly to this distributed computing federation's national infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823405","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Ewa Deelman","CA","University of Southern California","Standard Grant","William Miller","04/30/2020","$75,000.00","","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOÕs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811605","GPU-Enabled General Relativistic Simulations of Misaligned Black Hole Accretion Systems","OAC","XD-Extreme Digital, Leadership-Class Computing","05/01/2018","05/16/2019","Alexander Chekhovskoy","IL","Northwestern University","Standard Grant","Edward Walker","10/31/2019","$17,979.00","","atchekho@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7476, 7781","026Z","$0.00","A black hole accretion disk is a structure formed by diffused materials in orbital motion around a black hole (BH).  From observations, the presence of tilted accretion disks around a BH is detected in some systems.  However, the physics of accretion disks is poorly understood.  This project will run very large scale simulations on the Blue Waters supercomputer to improve our fundamental understanding of BH accretion disks.  Results of the proposed simulations will address long-standing questions in the way supermassive black holes consume and expel gas, and thereby exert feedback on their environment.  These results will allow the physics community to gain first-principles understanding of disk physics in typical tilted BH accretion systems.<br/><br/>Gas falling into a BH from large distances is unaware of BH spin direction, and misalignment between the accretion disk and BH spin is expected to be common. However, the physics of tilted disks is poorly understood, even for the ""standard"", geometrically thin, radiatively efficient accretion disks that power active galactic nuclei known as quasars and thought to provide the best observational tests of general relativity and disk physics. In particular, it is still not understood how the curved space-time of a spinning black hole imprints itself on the structure of the tilted disks. This project will make use of the fact that, at their core, BH accretion disks are well-described by the general relativistic magnetohydrodynamics (GRMHD) equations of motion. By carrying out direct GRMHD simulations of tilted thin and thick disks, the project will obtain the first-principles understanding of disk physics in typical, tilted BH accretion systems. To surmount the prohibitively expensive nature of these simulations, the project has constructed the first GPU-accelerated GRMHD code, H-AMR, which is capable of adaptive mesh refinement and is ideally suited for the Blue Waters supercomputer.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1910469","OAC Core: Small: Shape-Image-Text: A Data-Driven Joint Embedding Framework for Representing and Analyzing Large-Scale Brain Microvascular Data","OAC","OAC-Advanced Cyberinfrast Core, Engineering of Biomed Systems","06/15/2019","06/03/2019","Zichun Zhong","MI","Wayne State University","Standard Grant","Seung-Jong Park","05/31/2022","$499,978.00","Jing Hua","zichunzhong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","090Y, 5345","026Z, 9179","$0.00","Today, many areas of science and engineering face increased challenges in synthesizing information from the ever-growing amount of data available. Such challenges are even more complex when the data type and format vary, as is the case for three-dimensional objects. Such objects can be described by different representations (modalities): shapes, images, and texts. Recently, deep learning methods were shown to be effective processing techniques by exposing object relationships without relying on hard-coded metrics. However, such methods focus on single modalities. This project seeks to design and develop a model that unifies multiple types of data such as three-dimensional shapes, images and text in a single quantitative model. Following a rigorous approach, the team of researchers from Wayne State University will map the multimodal and heterogeneous representations and features onto a universal high-dimensional encoding space, characterized by uniform representation and metric. The team will then validate the work by applying the research results to MICRO Magnetic Resonance Imaging (MICRO-MRI) microvascular data collected in collaboration with area health science professionals. The project bridges a significant gap in neuroscience data analysis and will produce a cyberinfrastructure framework that will stimulate research in the field. The project will also provide educational activities for undergraduate and graduate students, as well as outreach to local middle school students. This project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>The research goal of this proposal centers around the unified theoretical multimodality data-driven joint embedding framework and involves design of a high-dimensional multimodal feature vector, probability-based joint embedding, and deep neural networks, hence making it possible to effectively represent and process the large-scale microvascular networks from a brand-new perspective. The proposed computational realization of deep neural networks can transform a three-dimensional shape with heterogeneous imaging, textual, and other features obtained from a large dataset to a novel high-dimensional isometric multi-view (shape, image, and text) probability space. The proposed joint embedding space preserves all intrinsic geometric, imaging, and textual characteristics and has the capability to integrate other multimodality properties. The generalized joint embedding space through the unified metric vector field allows formal and diverse study of geometry scalability and variability in shape processing and measurement intensively involved in 3D multimodal data informatics. In the proposed joint embedding space, the global and local shape comparison and analysis can be easily computed and measured by using the unified metric, which will significantly increase system's automation, reduce human's interventions, and discover new knowledge in vascular diseases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839030","Findable Accessible Interoperable and Reusable (FAIR) Hackathon Workshop for Mathematical and Physical Sciences (MPS) Research Communities","OAC","NSF Public Access Initiative","09/01/2018","07/25/2018","Michael Hildreth","IN","University of Notre Dame","Standard Grant","Beth Plale","08/31/2019","$49,917.00","Natalie Meyers","hildreth.2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7414","7556","$0.00","The FAIR principles (findable, accessible, interoperable, and reusable) when applied to scientific research data have the potential to both ease the burden of readying data created from NSF funded research for sharing and enhance the use and reuse of the data.  The FAIR Hackathon Workshop for Mathematics and the Physical Sciences (MPS) research communities will bring together innovative data scientists and developers from across physical sciences projects funded by NSF to solve real-world data challenges using the principles of FAIR: Findable, Accessible, Interoperable and Reusable Data. Designed to address issues of public access to data and to provide tools and relevant hands-on experience for researchers, the workshop will lay out the FAIR principles and metrics in the context of a hackathon using the successful model of The Bio-IT World FAIR Data Hackathon organized by the European Union's GOFAIR initiative of the European Open Science Cloud. The hackathon will unite MPS and research data management teams to tackle actual datasets with maximum impact potential.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838572","Community Track DCL 18-060: Exploring Roles of Universities, Data Centers, Data Repositories and Publishers in Data Re-Use","OAC","NSF Public Access Initiative","10/01/2018","07/25/2018","Richard Hooper","MA","Tufts University","Standard Grant","Beth Plale","09/30/2019","$50,000.00","","richard.hooper@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7414","7556","$0.00","Scientists are faced with an ever increasing array of options for data publication and a growing set of requirements to meet grant and journal directives. General purpose data repositories, data archives with journals or university libraries, domain-specific data repositories or broader project management and collaborative environments that provide some data publication services (e.g., issuing DOIs) are just a few of the options available. How should scientists choose among these options? How effective are these alternatives in meeting the multiple objectives for data publication, including discoverability, recording of sufficient metadata for reliable re-use, reproducibility of scientific results, convenience for data provider, and persistence. The investigator proposes to organize a workshop that brings together key stakeholders in data reuse.  The workshop targets research carried out in the geosciences and is an activity connected with the South Big Data Regional Innovation Hub.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810584","Probing the Fossils of the Local Group using Petascale Adaptive Mesh Galaxy Simulations","OAC","Leadership-Class Computing","08/01/2018","03/22/2018","Brian O'Shea","MI","Michigan State University","Standard Grant","Edward Walker","07/31/2019","$13,950.00","Britton Smith, John Wise","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7781","","$0.00","The goal of this project is to unlock fundamental breakthroughs in galaxy formation with large-scale numerical simulations on the Blue Waters supercomputer. These simulations will resolve the smallest building blocks of galaxies and their star formation histories, allowing for direct measurements of the consequences of the numerical calculations. The complexity of the included physics will lead to the creation of rich data sets that address many key issues relating to star formation histories, chemical evolution, early galaxy assembly, and the observable properties of galaxies. Furthermore, the simulation data produced during the course of this project, as well as a wide range of other data products, will be made publicly available for use by the broader astrophysical community.<br/><br/>The project will answer several pressing, observational motivated questions about low-mass and metal-poor galaxies by using the Blue Waters supercomputer to perform a suite of sophisticated, high dynamic range adaptive mesh simulations of cosmological structure formation. These simulations will probe the early history of the Local Group of galaxies in great detail, and will directly connect the largest of these early galaxies to their relics, which can be found among the present-day Local Group dwarf galaxies.  The project will address four specific questions about galaxy formation: 1. What are the key physical mechanisms that control galaxy formation at the earliest epochs, and how do they differ from the ones in larger galaxies forming at later times? 2. If the seeds of supermassive black holes form from the first generation of stars, how do they grow over the first billion years of cosmic evolution? 3. What are the physical characteristics of the remnants of early galaxy formation in the Local Group, and what information about their formation is retained by the stellar populations of those galaxies? 4. What are the unique observational signatures of the earliest galaxies, both at high redshift and the present day? The team contains experts in galaxy formation and high performance computing, as well as the use of a sophisticated numerical tool, specifically the Enzo code.  The Enzo code that has been demonstrated to scale and perform well on the Blue Waters supercomputer. The project will apply the simulations to the interpretation of measurements of both local and distant galaxies from current astronomical surveys, and to motivate future observational campaigns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841636","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Gabrielle Allen","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","09/30/2020","$38,210.00","","gdallen@uwyo.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811402","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Stefan Robila","12/31/2019","$99,344.00","","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809073","Three-Dimensional Simulations of Core-Collapse Supernovae","OAC","Leadership-Class Computing","04/01/2018","03/22/2018","Adam Burrows","NJ","Princeton University","Standard Grant","Edward Walker","03/31/2020","$8,100.00","","burrows@astro.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","Core-collapse supernovae dramatically announce the death of massive stars and the birth of neutron stars. These supernovae occur when the iron core of a massive star collapses to a neutron star. Releasing its gravitational binding energy in a violent explosion as neutrinos, the resulting neutron star, for a few seconds, outshines the rest of the observable universe.  Viewed as a nuclear physics laboratory, core-collapse supernovae produce the highest densities of matter and energy in the modern universe.  However, the precise mechanism of this explosion has not been unambiguously pinned down and this fifty-year old conundrum is one of the central remaining unsolved problems in theoretical astrophysics.  Supernova theory is an amalgam of much of physics, and represents one of the most complex computational problems of modern science. Supercomputers, such as Blue Waters, are essential to make progress in simulating and understanding the evolution of the supernova event and neutron star birth.  A solution to the core-collapse supernova problem would benefit ongoing efforts of observers and instrument designers in the U.S. and around the world engaged in projects to determine the origin of the elements, measure gravitational waves (LIGO), and other important physics experiments.<br/><br/>The project will conduct three-dimensional radiation/hydrodynamic simulations of core-collapse supernovae with the goal of constraining, and ultimately determining, the mechanism of explosion.  During this explosion process, a combination of high-density nuclear physics, multi-dimensional hydrodynamics, radiation transport, and neutrino physics determines whether and how the star explodes.   The project will use the newly developed and tested code FORNAX incorporating state-of-the-art microphysics and methodologies, with excellent scalability to beyond 100,000 cores per tasks.   Various observational diagnostics, such as neutrino and gravitational-wave signatures and residual neutron star masses and kicks, will be derived for all models calculated.  Supernova explosions and their products are central to the origin of the elements, the birth of pulsars and black holes, and the dynamics of galaxies and the interstellar medium, and a fundamental theoretical understanding of such explosions will inform the interpretation of data derived from, among other platforms, NASA's Chandra X-ray Observatory, the Hubble Space Telescope, Swift, NuStar, and the upcoming James Webb Space Telescope.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810976","Petascale Polar Topography Production","OAC","Leadership-Class Computing","05/01/2018","03/22/2018","Paul Morin","MN","University of Minnesota-Twin Cities","Standard Grant","Edward Walker","04/30/2020","$14,999.00","Ian Howat, Charles Nguyen, Myoung-Jong Noh, Claire Porter","lpaul@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7781","","$0.00","The polar regions are changing faster than any other region of the Earth, with accelerating rates of coastal erosion, permafrost loss, glacier thinning, and other changes. Yet the poles are also among the least well observed regions, due to both their relative inaccessibility and coverage limitations in prior global surveys by remote sensing.  The project will use the high performance computing capabilities of Blue Waters to produce high-resolution, high-quality, time-dependent, and openly distributed digital elevation models (DEMs) for the Earth's poles. This data set is needed for measuring and understanding rapid, ongoing changes to the polar landscapes, as well as for impact mitigation and adaptation planning by polar communities. The produced DEMs will be distributed at no cost to the science community and public at large.  <br/><br/>The project will use Blue Waters to produce 2m resolution DEMs with absolute accuracies of approximately 1m or better and relative accuracies of 0.2m over the Antarctic and Arctic regions.  The project incorporates the capability for repeat, high-resolution and high-precision topographic mapping in the form of sub-meter stereoscopic imagery from the DigitalGlobe constellation of five polar-orbiting sun-synchronous Earth imagers (WorldView 1-4 and GeoEye-1), as well as the archive of IKONOS and QuickBird.  These satellites images allow for precise stereo photogrammetric DEM construction from overlapping image pairs over a wide range of terrain types and light conditions, including the ""flat white"" interior of ice sheets and shadowed mountain faces. The time-dependent collection of DEMs will be invaluable to scientists studying change in the polar region.   Resources for obtaining and maintaining the imagery, tools for post-processing the results, software development time, and methods for distributing the DEMs have been secured by the project.  All DEMs will be publically distributed through the Polar Geospatial Center (PGC) website and a consortium including the National Geospatial Agency (NGA) and NASA, as well as Amazon web services and the geospatial software company ESRI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811685","Structural and Dynamical Determinants of Influenza Transmissibility","OAC","Leadership-Class Computing","05/01/2018","03/22/2018","Rommie Amaro","CA","University of California-San Diego","Standard Grant","Edward Walker","12/31/2019","$10,500.00","","ramaro@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7781","","$0.00","Both seasonal and pandemic influenza have been responsible for millions of deaths worldwide.  The persistence of seasonal influenza strains costs between 3,000 and 49,000 lives annually in the United States alone.  The project aims to use Blue Waters to run simulations of the influenza virus to explain flu transmissibility.  These simulations will be of a very large scale, involving over 200 million atoms, and are expected to provide unprecedented insights into the mechanisms of influenza virulence and drug resistance.  In addition, simulations will set the stage for the longer-term goal of developing an atomic-level understanding of the influenza infection process, leading to new pathways for pharmacological intervention.<br/><br/>The project has used molecular dynamics (MD) simulations of the individual influenza membrane glycoproteins hemagglutinin (HA) and neuraminidase (NA) to identify experimentally validated antiviral compounds that bind to new glycoprotein pockets never before captured by experimental techniques.  However, influenza glycoproteins are components of a much larger virion surface that collectively participates in host recognition and infection processes.   The surface glycoprotein distributions and their decoration with glycans poses many interesting unanswered questions related to the roles of HA, NA, glycans, and substrate (and substrate mimicks) play in the viral infection and assembly process.  Additionally, the functional balance of HA, which binds to host-cell receptors, and NA, which abrogates receptor binding via cleavage of terminal sialic-acid residues, has not yet been fully characterized  The project request time to run two whole virion simulations: (1) H1N1 with glycans attached to key glycolysation sites on HA and NA and (2) with sialic acid bound in the sialic acid binding sites (mimicking a substrate bound complex). With the addition of the glycans and substrates, both systems will consist of over 200 million atoms.  The simulations will for the first time combine large-scale, subcellular / cellular level modeling at atomic detail with Markov state model theory; effectively aggregating the many copies of the individual glycoproteins into one statistics based network of states type model that enables it to extract long timescale dynamics from the short time scale simulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836997","Workshop Proposal:  Rethinking NSF's Computational Ecosystem for 21st Century Science and Engineering","OAC","XD-Extreme Digital","05/15/2018","05/15/2018","Daniel Reed","IA","University of Iowa","Standard Grant","Robert Chadduck","05/31/2019","$26,645.00","","dan.reed@utah.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7476","7556","$0.00","For nearly four decades, NSF has effectively supported the broad availability and innovative uses of leading edge computational resources to accelerate advances in science and engineering. These NSF investments have spanned discipline-specific instruments and facilities; computational systems of varying capabilities and architectures optimized for different applications; virtual organizations for allocating resources and interfacing with users; and network backbones connecting and providing access to these resources. Recent profound advances across science and engineering interests and priorities, as well as in enabling technologies, contribute to the current view of cyberinfrastructure supporting research as appropriately embodying a model of an ""ecosystem"" constituting interconnected, appropriately synergistically evolving, elements.<br/><br/>The objective of this workshop is to further foster and promote the NSF supported cyberinfrastructure ecosystem, including its makeup of interconnected, appropriately evolving, elements, to continue to be both motivated by scientific interests and priorities, as well as to evolve responsively to an increasing pace of technologies developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1930161","National Cyberinfrastructure Coordination Conference","OAC","XD-Extreme Digital","06/01/2019","05/22/2019","Alan Blatecky","NC","Research Triangle Institute","Standard Grant","Robert Chadduck","11/30/2019","$96,375.00","","arblatecky@gmail.com","3040 Cornwallis Road","Research Triangle Park","NC","277092194","9195416000","CSE","7476","","$0.00","The National Cyberinfrastructure Coordination Service Conference supports community-based efforts contributing to the exploration of the nature, composition, and execution of NSF supported national scale coordinated cyberinfrastructure services that are responsive to evolving technologies, solutions and user needs, with the overarching goal of supporting the full range of computational- and data-intensive research across all of science and engineering.<br/><br/>The conference will enable the exploration national scale coordinated cyberinfrastructure services that support the next generation of science and research in light of rapidly evolving technology landscape and increases in capability, which have the potential to change and transform the future of science. The conference will also directly address critical issues regarding workforce development and preparedness to support next general science, including training, education, and developing the next generation of scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832767","BD Spokes: PLANNING: NORTHEAST: Collaborative: Planning for Privacy and Security in Big Data","OAC","Secure &Trustworthy Cyberspace","08/01/2017","07/27/2018","Adam Smith","MA","Trustees of Boston University","Standard Grant","Beth Plale","08/31/2019","$9,395.00","","ads22@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8060","027Z, 7433, 7434, 8083","$0.00","Security and privacy play a key role in areas such as health, energy, and smart cities, as well as constituting a grand challenge in and of themselves.  Privacy and security are critical for realizing Big Data's promise to advance society.  If data are used without regard to privacy of individuals or protection of the data, then individuals may be hurt.  If the data?s authenticity is not guaranteed, or if data are not permitted to be used at all due to privacy and security concerns, then the data's value is not realized. As the information that is collected about us grows in quantity, scientific and commercial value, and sensitivity, addressing these challenges is crucial to accessing the benefits of big data while ensuring that privacy and associated basic rights such as free speech and association are respected.   Through two workshops hosted by DIMACS, this planning project will advance the research agenda on privacy and security for big data, build a community of interested researchers and practitioners, and propose regional activities in security and privacy related to the Northeast Big Data Hub.  The long-term vision of the project is to catalyze both foundational and practical advances in privacy and security for big data that have the ability to positively impact society by (1) expanding the extent to which individuals can have control over protection of their personal data, and (2) enabling data?s value to be harnessed for advances in areas including health, energy, smart cities and regions, finance, and education. <br/><br/>The first event, a workshop on ""Overcoming Barriers to Data Sharing including Privacy and Fairness"", will bring together computer scientists, legal scholars, social scientists, and consumers of data to understand the extent to which privacy currently limits the sharing of data, including but not limited to research data, and to develop standards and best practices to enable new information flows in domains from healthcare to energy.  The second event, the ""NE BD Hub Workshop on Privacy and Security for Big Data"", will bring together privacy and security experts as well as experts in a variety of big data application areas to highlight privacy and security issues associated with each of the Northeast Big Data hub spokes.  Expected project outcomes include broad participation, development of new collaborations and partnerships, production and dissemination of tutorial and talk videos, and reports on planned activities and potential best practices.<br/><br/><br/>This award is co-funded by the CISE Division of Computer and Network Systems (CNS) Secure and Trustworthy Computing (SaTC) Program."
"1826574","EAGER: Empirical Software Engineering for  Computational  Science","OAC","Software & Hardware Foundation, Software Institutes","05/01/2018","04/16/2018","Timothy Menzies","NC","North Carolina State University","Standard Grant","Stefan Robila","07/31/2019","$124,628.00","","timm@ieee.org","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","7798, 8004","026Z, 7916, 7944, 8004","$0.00","Science has become increasingly reliant on Computational Science methods implemented in software. These methods are complex, and therefore the software that implements them is prone to errors. This projects seeks to transformatively improve the state of the practice in the development of Computational Science software by applying systematic, data-driven methods (known as empirical methods) to evaluate how software is being developed and to suggest improvements. Improving the software engineering methods of Computational Science would result in higher quality software, and consequently increase our confidence in the research in scientific phenomena conducted by Computational Scientists, <br/><br/>Much of the work in Computational Science is related to the software that implements it. In this project, the researcher will apply state of the art empirical software engineering methods to Computational Science software. Qualitative methods will be applied to conduct large scale surveys of computational science. Quantitative data mining tools (classifiers, intelligent data preprocessor, automatic hyperparameter optimizers) will be used to can learn predictive models of time series of SE data such as ""Where in this system should we look for current bugs?"" and ""How many bugs are left on the system?"". These models can be used to guide developer effort in building new code or maintaining old code.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004541","Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC)","OAC","Software Institutes","06/01/2020","05/01/2020","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","05/31/2024","$1,223,264.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","077Z, 7925, 8004","$0.00","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.<br/><br/>The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004850","Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC)","OAC","Software Institutes","06/01/2020","05/01/2020","Julien Langou","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Seung-Jong Park","05/31/2024","$646,234.00","","julien.langou@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","8004","077Z, 7925, 8004","$0.00","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.<br/><br/>The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004763","Collaborative Research:Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC)","OAC","Software Institutes","06/01/2020","05/01/2020","James Demmel","CA","University of California-Berkeley","Standard Grant","Seung-Jong Park","05/31/2024","$611,758.00","","demmel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8004","077Z, 7925, 8004","$0.00","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.<br/><br/>The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1847753","Network Embedded Storage and Compute (NESCO)","OAC","Campus Cyberinfrastructure","08/15/2018","02/14/2019","Xi Yang","MD","University of Maryland, College Park","Standard Grant","Kevin Thompson","07/31/2020","$299,817.00","Xi Yang","maxyang@umd.edu","3112 LEE BLDG 7809 Regents Drive","College Park","MD","207425141","3014056269","CSE","8080","7916","$0.00","The Network Embedded Storage and Compute (NESCO) project is motivated by a premise that future Research and Education networks should include ""embedded compute and storage"" functions as a mechanism to provide advanced services for domain science application workflows. This is a natural follow-on to the current cyberinfrastructure evolution revolving around ""Cloud"" and ""Edge"" computing.  The NESCO project is developing the next important trend based on a distributed ecosystem of compute and storage facilities which are embedded deeply within networks.  These will be similar in function to on-premise edge cloud deployments but will be customized for placement within networks.  Application workflow developers will have access to a new class of services which can be rapidly tailored to specific flows and use cases.  One result anticipated is the ""democratization of middlebox"" functions in a manner where application workflow developers can customize their workflows and data processing on real-time basis with flow level granularity.  This project includes a climate science workflow as a prototype use case.  It is anticipated that these new services will be of interest to a broad range of domain science applications.  Individual and small research groups who cannot afford access to dedicated resources may find these services of particular value.<br/><br/>A NESCO facility is defined which can be placed inside of Wide Area Networks, Regional Networks, and Exchange Points.  These facilities will contain advanced programmable hardware-based network functions, compute and storage resources, and be architected to provide services on a highly customizable per flow basis.  An open source orchestration system known as StackV is being modified to develop a new class of realtime ""Flow Management"" services.  The StackV system includes a realtime modeling framework to ""datafy"" infrastructure, modular pluggable computation elements, and intelligent workflow processes to build on-demand topologies across heterogeneous resource types and service providers.  This system also includes a user facing application programming interface which allows workflow agents to request resource topologies which span network, compute, and storage resources via a single integrated request.  This project is developing a NESCO reference implementation which will be deployed for testing and experimentation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1936847","2019 NSF Campus Cyberinfrastructure and Cybersecurity Innovation for Cyberinfrastructure PI Workshop","OAC","Campus Cyberinfrastructure","08/01/2019","07/17/2019","Gwendolyn Huntoon","PA","Keystone Initiative for Network Based Education and Research","Standard Grant","Kevin Thompson","07/31/2020","$90,000.00","Jennifer Leasure, Jennifer Oxenford","huntoon@kinber.org","5775 Allentown Blvd","Harrisburg","PA","171124051","7179637490","CSE","8080","","$0.00","The 2019 Campus Cyberinfrastructure and Cybersecurity Innovation for Cyberinfrastructure PI Workshop builds upon the success of the previous Campus Cyberinfrastructure Workshops providing an opportunity for recipients of all active NSF Campus Cyberinfrastructure (CC*) and Cybersecurity Innovation for Cyberinfrastructure (CICI) awards to meet in person, exchange project findings, interact with national cyberinfrastructure experts and collaborate across project areas and project regions. By again co-locating with the Quilt Fall Member Meeting, the broader scope encourages relationships between campus cyberinfrastructure, science driven applications, cybersecurity, and regional and national cyberinfrastructure resources. Based on last year's experience, the meeting schedule has been updated to provide additional opportunities for interaction between PIs themselves as well as between the three groups participating in the meetings. Included in the 2019 PI Workshop is an expanded pre-workshop set of breakout sessions half-day breakout sessions on topics relevant to academic research networking challenges facing campuses and CC* and CICI PI teams. <br/><br/>The intellectual merit of this project is the exchange of project findings, interaction between national cyberinfrastructure experts, collaboration across project areas and project regions that will lead to new ideas, relationships and collaborations associated with an in person workshop. The half-day cyberinfrastructure engineering session will provide timely insights into growing practice of cyberinfrastructure facilitation in the campus environment that can be shared with the co-located meeting attendees as well as the community in general. The broader impact associated with the workshop includes the dissemination of workshop information, including the workshop presentations. By co-locating with the Quilt meeting the workshop continues to bring together stakeholders with a vested interest in leveraging campus cyberinfrastructure investments to provide the opportunity to develop stronger ties between campus cyberinfrastructure, science driven applications and regional and national cyberinfrastructure resources, leading to a broader set of collaborations and lasting impact on national campus cyberinfrastructure and science driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839014","Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI","OAC","NSF Public Access Initiative","10/01/2018","08/17/2018","Nicholas Nystrom","PA","Carnegie-Mellon University","Standard Grant","Beth Plale","09/30/2019","$50,000.00","Paola Buitrago, Huajin Wang, Keith Webster","nystrom@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7414","7556","$0.00","The volume and heterogeneity of scientific data goes beyond a researcher?s ability to find relevant data, make different formats interoperable, and deal with difficult ontological and language issues. Progress requires bringing together experts in data curation with experts in Artificial Intelligence (AI) to begin a dialog and collaboration on AI tools to span the gap between data and its reuse.  The PIs will organize the conference ""Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI""; the objective is to bring together diverse stakeholders including research librarians and data managers, the AI community, and users and consumers of scientific data to dialog around critical places in the data lifecycle where AI could be groundbreaking.  <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823378","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Duncan Brown","NY","Syracuse University","Standard Grant","William Miller","04/30/2020","$75,000.00","","dabrown@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOÕs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833146","EAGER: Smart Community Big Data Co-op","OAC","BD Spokes -Big Data Regional I","05/01/2018","04/18/2018","Nick Maynard","DC","US Ignite, Inc.","Standard Grant","Alejandro Suarez","04/30/2020","$295,944.00","Gregory Dobler","nick.maynard@us-ignite.org","1150 18th Street NW, Suite 750","Washington","DC","200363880","2023659219","CSE","024Y","7916, 8083","$0.00","While many communities have deployed key components needed to deliver smart community services, many cities currently lack the necessary data platforms or staff expertise to handle the complexities of gathering, analyzing, or sharing smart community data. This challenge will become significantly more difficult as sensors, vehicles, and other sources provide more real-time data. Without such platforms, along with modeling and visualization tools, the promise of smart city technologies will remain unattainable, especially for smaller or low-income communities. Big data analytic tools have become a critical missing link to smart community solutions and there is a growing demand from a variety of computer science disciplines, along with industry researchers and municipal governments, to share best practices and best-of-breed tools. Creating an approach that allows for the development of secure, open, flexible, and scalable smart community big data solutions will have lasting societal impact. <br/><br/>Through the Smart Community Big Data Co-operative (the Co-op), US Ignite will leverage its 100 city, university, industry, and nonprofit partners to design, build and use the important capability of big data sharing to enhance local smart community efforts and generate enhanced community services. Communities are looking to quickly identify current gaps in their real-time data, applications, and modeling tools to design a cohesive smart community strategy with local partners. This current effort will extend this formula to launch new forms of smart community research, applications, and services that benefit end users through open data and data analytics. US Ignite will ensure alignment among all parties interests in supporting big data and smart community applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811176","Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with Multi-Scale Fluid-Kinetic Simulation Suite","OAC","Leadership-Class Computing","04/01/2018","03/27/2018","Nikolai Pogorelov","AL","University of Alabama in Huntsville","Standard Grant","Edward Walker","03/31/2020","$10,501.00","Jacob Heerikhuisen","np0002@uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","CSE","7781","9150","$0.00","The objective of this proposal is to use the possibilities provided by the Blue Waters supercomputer to model fundamental and challenging space physics problems.  The heliosphere is the sphere like region of space dominated by the Sun, which extends far beyond the orbit of Pluto.  The Solar Wind (SW) consists of ionized atoms from the Sun.  The project will model the SW flows in the inner and outer heliosphere, and compare the results with observational data.  It is anticipated that the project will provide a leap forward in the simulation of complex charged and neutral gas systems. Furthermore, the proposed approach to computational resource management for complex codes utilizing multiple algorithm technologies is expected to be a major advance to current approaches. The development of resource management technologies will be essential for all future modeling efforts that incorporate a wide diversity of scales and physical processes.<br/><br/>The analysis of flows of partially ionized plasma that are characterized by multiple or highly localized scales and multiple processes, will have a transformative impact for heliophysics. The project will address a variety of physical phenomena occurring throughout the solar system, such as the charge exchange processes between neutral and charged particles, the birth of pick-up ions (PUIs), the origin of energetic neutral atoms (ENAs), turbulence, the interplay of the heliopause instability and magnetic reconnection at the SW and local interstellar medium (LISM) interface, plasma wave generation in the LISM, the effect of the heliosphere on the TeV cosmic ray anisotropy, and more.  The project will also fit simulation results with observational data to constrain the properties of the LISM and refine time-dependent SW models. The project will incorporate the direct measurements from the New Parker Solar Probe and Solar Orbiter missions, as well as the in situ measurements of the SW from the Sun to Earth and further to the heliospheric boundary, from the New Horizons, Voyager, IBEX, and air shower observations. This project will extract the fundamental physics of plasma-neutral flows accompanied by the interaction with energetic particles. The goal of the modeling activities is to understand and interpret observations in a way previously unthinkable because of the limitations in both physical models and computing power. Components of the physical model and corresponding code routines will be made available in a publicly accessible simulation suite.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842088","2018 NSF Campus Cyberinfrastructure and Cybersecurity Innovation for  Cyberinfrastructure PI Workshop","OAC","Campus Cyberinfrastructure","09/01/2018","08/22/2018","Gwendolyn Huntoon","PA","Keystone Initiative for Network Based Education and Research","Standard Grant","Kevin Thompson","08/31/2019","$90,000.00","Jennifer Leasure","huntoon@kinber.org","5775 Allentown Blvd","Harrisburg","PA","171124051","7179637490","CSE","8080","","$0.00","The 2018 Campus Cyberinfrastructure and Cybersecurity Innovation for Cyberinfrastructure PI Workshop builds upon the success of the previous Campus Cyberinfrastructure Workshops providing an opportunity for recipients of all active NSF Campus Cyberinfrastructure (CC*) and Cybersecurity Innovation for Cyberinfrastructure (CICI) awards to meet in person, exchange project findings, interact with national cyberinfrastructure experts and collaborate across project areas and project regions. By again co-locating with the Quilt Fall Member Meeting, the broader scope encourages relationships between campus cyberinfrastructure, science driven applications, cybersecurity, and regional and national cyberinfrastructure resources. Based on last year's experience, we have updated the meeting schedule to provide additional opportunities for interaction between PIs themselves as well as between the three groups participating in the meetings. New to the 2018 PI Workshop is an expanded pre-workshop set of breakout sessions half-day breakout sessions on topics relevant to academic research networking challenges facing campuses and CC* and CICI PI teams. <br/><br/>The intellectual merit of this project is the exchange of project findings, interaction between national cyberinfrastructure experts, collaboration across project areas and project regions that will lead to new ideas, relationships and collaborations associated with an in person workshop. The half-day cyberinfrastructure engineering session will provide timely insights into growing practice of cyberinfrastructure facilitation in the campus environment that can be shared with the co-located meeting attendees as well as the community in general. The broader impact associated with the workshop includes the dissemination of workshop information, including the workshop presentations. By co-locating with the Quilt meeting the workshop continues to bring together stakeholders with a vested interest in leveraging campus cyberinfrastructure investments to provide the opportunity to develop stronger ties between campus cyberinfrastructure, science driven applications and regional and national cyberinfrastructure resources, leading to a broader set of collaborations and lasting impact on national campus cyberinfrastructure and science driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810774","Realistic Simulations of the Intergalactic Medium: The Search for Missing Physics - Part 2","OAC","Leadership-Class Computing","05/01/2018","03/22/2018","Michael Norman","CA","University of California-San Diego","Standard Grant","Edward Walker","10/31/2019","$7,750.00","","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","7781","","$0.00","The intergalactic medium (IGM) is the hot, X-ray emitting gas that permeates the space between galaxies.  Currently, standard numerical simulation cannot match several key observational features of the IGM.  The mismatch between the simulations and observation data is much too large to be caused by observational errors, and is seen in multiple comparisons using different statistics, and by different groups. This project proposes to resolve this open question in astrophysics by running very high resolution simulations on the Blue Waters supercomputer to make more accurate and robust estimates of the IGM.  Furthermore, the simulations and codes will be publicly available to allow others to make their own estimates from existing or new observations.<br/><br/>The project will simulate the hydrogen Lyman alpha forest (LYAF) using the newly developed extreme scale branch of the Enzo called Enzo-P. Enzo-P implements the rich suite of physics models from the widely used Enzo community code on top of a new, highly scalable AMR infrastructure called Cello. Both Enzo-P and Cello have been developed by the PIs group at UCSD. The combination of Enzo-P and Cello will permit for the first time the use of AMR throughout a large cosmological volume, removing the existing tradeoff between large volume statistics and high local resolution that characterized previous Enzo simulations. The project will apply this new capability to carry out simulations of the LAF with sufficiently high resolution to capture the gaseous halos of galaxies which contribute significantly to intergalactic absorption, while at the same time surveying a statistically significant volume of the universe.  Moreover, the project will construct synthetic LYAF spectra and analyze their statistical properties to see whether the inclusion of high column density absorbers in the model improves agreement observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834405","Second Annual Joint PI Meeting for the NSF BigData Research Program and the NSF Big Data Regional Innovation Hubs and Spokes Programs","OAC","BD Spokes -Big Data Regional I, Big Data Science &Engineering","07/01/2018","06/25/2018","Samarth Swarup","VA","Virginia Polytechnic Institute and State University","Standard Grant","Beth Plale","06/30/2019","$130,201.00","Vladimir Braverman","swarup@virginia.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","024Y, 8083","7556, 8083","$0.00","The second annual joint Principal Investigators meeting for the Big Data program and the Big Data Hubs and Spokes program provides updates to progress on intellectual objectives and information sharing.  The meeting will promote interaction between the participants in these programs, facilitate collaborations, expose students to the latest research, and situate the programs within the larger big data ecosystem.<br/><br/>The meeting will be a three-day meeting that includes one day dedicated to each program and a third day shared between the two. The agenda includes breakouts, panel discussions, lightning talks, and poster sessions to encourage intellectual engagement across the programs, thus setting conditions for new projects and multi-disciplinary activities. The meeting will produce a report that highlights outcomes for broader dissemination and sharing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945347","EAGER: Bridging the Last Mile; Towards an Assistive Cyberinfrastructure for Accelerating Computationally Driven Science","OAC","CYBERINFRASTRUCTURE","01/01/2020","05/28/2021","Rajiv Ramnath","OH","Ohio State University","Standard Grant","Seung-Jong Park","06/30/2022","$326,547.00","Jian Chen, Bryan Carstens","ramnath@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7231","019Z, 026Z, 7916, 8004, 9179","$0.00","Even though computational scientists have high-speed computers and powerful software tools at their disposal, their ability to make scientific discovery is held back because much of their time is inefficiently spent in data exploration, manipulation and visualization, preliminary analyses, and hit-and-miss attempts to find the right settings for their software tools to be used effectively, rather than in running their computations at scale. For example, researchers doing gene sequencing (a process in which the individual base nucleotides in an organism's DNA are identified) have to figure what sequencing method to use, which software package - out of several - to use for that sequencing method, what settings to use for the software package, and so on. Researchers would greatly benefit by learning from prior use of the software tools at their disposal, that is, from past experiences and mistakes, both their own, and those of other researchers in the field. However, only a small percentage of them are able to do so. This exploratory project will investigate whether effective, intelligent guidance can be extracted from past experience on specific applications using artificial intelligence techniques and then provided in a tailored manner to the individual researcher.  This will be done within the software tools being used by the researcher, and provided by the underlying cyberinfrastructure itself, so that anyone who uses these tools can benefit. This project will explore the feasibility of AI-based approaches for providing such assistance, and prototype, pilot and validate a range of capabilities, and widely disseminate results.<br/><br/>Current research and development in cyberinfrastructure(CI) for science  focuses on scaling the CI: developing algorithms that scale better,  automating pipelines, building scalable computational and data systems,  and optimizing system resource allocation. This project, on the other  hand, focuses on scaling the individual researcher, i.e. making her more  effective, through the application of a portfolio of techniques, from  observational studies of scientist-CI interactions, to end-to-end  instrumentation for recording and tracking these interactions across the  CI, building machine-learned models from these interactions, and embedding  and experimenting with these models within human-machine teaming  paradigms. These techniques will be given the best chance to succeed by  applying them in a methodological and technical framework that focuses on  specific applications. The proposed methods will vary participant  expertise, test hypotheses of performance, and use transparent ?guide me?  methodologies to establish dimensions of individual differences in  designing guidance. The exemplar domain science is genomics, where the  goal is to sequence and assemble the complete DNA of selected species, and  where the work in this project could be particularly transformative. The project team is  integrative, interdisciplinary and convergent; the investigators have  expertise in genomics, software engineering, systems, data science,  project management, and human-machine systems, and are working with a key  CI provider in the Ohio Supercomputer Center, and collectively advising a  small team of graduate students. The project is aligned with  the NSF Big Ideas of Harnessing the Data Revolution, Growing Convergence  Research and the Future of Work, and the Office of Advanced  Infrastructure criteria for software  cyberinfrastructure, since it is domain and computer sciences-driven,  innovative, collaborative and convergent, strategically managed, and  building on significant prior investments by  NSF - a clear path to  sustainability. This work could make computational scientists from many  science domains transformatively more productive, leading to accelerated  discovery. Additional broader impacts are through educational case-studies  for computational science, contributions to instrumentation standards, and  observational and empirical study methods for CI. A side contribution of  this work will be in assessing the usability of CI tools . This will  directly enable tool designers to build more usable tools. Broadening  participation has been emphasized: one of the principal investigators is a  woman. All three PIs have a history of recruiting and working with women  students. At least one of the funded graduate students will be an incoming  female student. The project team will additionally be closely  collaborating with two women students and a woman post-doctoral researcher  in the genomics laboratory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840197","CICI: SSC: Horizon: Secure Large-Scale Scientific Cloud Computing","OAC","Cybersecurity Innovation","09/01/2018","05/25/2021","Anton Burtsev","CA","University of California-Irvine","Standard Grant","Robert Beverly","08/31/2022","$1,031,925.00","Gene Tsudik","aburtsev@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","8027","8027, 9251","$0.00","Over the last decade, public and private clouds emerged as de facto platforms for computationally intensive scientific tasks. Today, huge volumes of many types of scientific data are routinely uploaded to the cloud. A large fraction of this data is privacy and/or security sensitive. Unfortunately, despite numerous advances in network and enterprise security, modern clouds remain inherently insecure. Recent experience shows that well-funded, targeted attacks manage to breach network perimeters of both public and private clouds. <br/><br/>Horizon is a novel cloud architecture aimed at providing data and computation security within a scientific cloud. Horizon builds upon three premises: (1) strong isolation on end-hosts, (2) fine-grained isolation in the cloud network, and (3) cloud-wide information flow control. To protect the end-hosts, Horizon develops a new layered hypervisor, and disaggregated virtualization stack with key features of: language safety, software fault isolation, and integrated software verification. To provide secure cloud network environment, Horizon relies on a new network architecture and implements a distributed network firewall, where all network communication and exchange of rights are mediated and controlled by the rules of the object capability system. To protect the cloud data, Horizon develops a set of abstractions and mechanisms to enforce cloud-wide information flow control. In Horizon all data is labeled. The hypervisor mediates all communication of each virtual machine and enforces propagation of labels and security checks for each cloud computation.<br/><br/>Horizon aims to provide a practical foundation for developing secure cloud infrastructure suitable for large-scale research workflows that require both speed and security. Horizon will be developed using entirely open-source components, and will be openly available to a broad community of scientists in academia and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2127459","EAGER: Democratizing the use of Advanced Computational Resources","OAC","CYBERINFRASTRUCTURE","06/01/2021","05/24/2021","Alan Blatecky","NC","Research Triangle Institute","Standard Grant","Alejandro Suarez","05/31/2022","$207,699.00","Joel Cutcher-Gershenfeld","arblatecky@gmail.com","3040 Cornwallis Road","Research Triangle Park","NC","277092194","9195416000","CSE","7231","7916","$0.00","Advanced cyberinfrastructure is becoming increasingly central to research across virtually all fields and disciplines. Yet, researchers in many fields and disciplines do not know what is available or how to engage with the cyberinfrastructure. Further, longstanding digital divides across universities and colleges limit access to cyberinfrastructure and the needed expertise to fully utilize advanced cyberinfrastructure capabilities. The environment also requires continuous learning and capacity building on the part of cyberinfrastructure professionals and institutions. This threatens to further exacerbate the ""digital divide"" our nation is experiencing. Unless we can find ways to more effectively provide access, training, education, and assistance to the much larger group of ""non-expert users"" from small colleges and underserved institutions, the digital divide will persist and will continue to expand. This EAGER project will address these gaps through a series of multiple focus group interviews in four general areas and will generate a needs assessment that can inform national investments in the research cyberinfrastructure.  <br/><br/>The goal of the first set of focus groups is to better understand the present state and future potential of advanced computing from students and researchers who are already motivated to use compute facilities but are not doing so currently. The goal of the second set of focus groups is to identify new applications that use compute resources including non-traditional approaches and research domains that currently do not use advanced cyberinfrastructure capabilities. The goal of the third set of focus groups is to identify mechanisms for engaging remote or underserved potential users of advanced computing resources and to better understand the barriers to their access and use of these resources. The goal of the fourth set of focus groups is to explore the opportunities and challenges under-served institutions and smaller campuses face in using advanced cyberinfrastructure resources. This will include a focus on identifying barriers to institutional support, including challenges facing campus leaders and administrators given additional pressures caused by the current pandemic.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2129723","Travel: WINS Travel Funds in Support of SCinet at SC21","OAC","Campus Cyberinfrastructure","10/01/2021","05/25/2021","Marla Meehl","CO","University Corporation For Atmospheric Res","Standard Grant","Kevin Thompson","03/31/2022","$37,500.00","","marla@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","8080","7556, 9102","$0.00","This travel grant supports five volunteers to attend the 2021 Supercomputing<br/>Conference (SC21) scheduled November 15 - 21, 2021 in St. Louis, Missouri, as part of<br/>the Womening in IT Networking at SC (WINS) program who will participate in building<br/>and operating SCinet, the advanced computer networks which supports the annual<br/>conference. The WINS program, started in 2015, enables talented early to mid-career<br/>women from diverse backgrounds and regions of the U.S. research and education IT<br/>community to volunteer to participate in SCinet and experience the hands-on<br/>development and construction of the conference network.<br/><br/>The intellectual merit associated with the project is to continue the expansion and<br/>broadening of knowledge and training for women network engineers and information<br/>technology professionals who desire to build expertise and continue their education and<br/>careers in network and computer systems. The broader impact associated with the<br/>program is through the diverse representation of organizations and applicant<br/>backgrounds representing Minority Serving Institutions, Established Program to<br/>Stimulate Competitive Research (EPSCoR) states, and historically underrepresented<br/>minority groups. Through the participation in SC and SCinet, the travel funds provide<br/>training, professional development, and career opportunities for women in a workforce<br/>that has been historically male and non-minority-dominated.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115082","CICI: SIVD: Discover and defend cyber vulnerabilities of deep learning medical diagnosis models to adversarial attacks","OAC","Cybersecurity Innovation","08/01/2021","05/11/2021","Shandong Wu","PA","University of Pittsburgh","Standard Grant","Robert Beverly","07/31/2024","$499,338.00","","wus3@upmc.edu","300 Murdoch Building","Pittsburgh","PA","152133203","4126247400","CSE","8027","7923, 8027","$0.00","This project aims to discover cyber vulnerabilities of deep learning-enabled medical imaging diagnosis tools against adversarial attacks and to develop defensive approaches in pursuit of safe artificial intelligence for healthcare. Artificial intelligence technologies, especially deep learning, have achieved remarkable success in the medical domain. Newly advanced adversarial attacks pose a new threat to cybersecurity of medical artificial intelligence diagnosis tools, but little is known about the characteristics and behaviors of this threat. While artificial intelligence tools are increasingly being incorporated in medical imaging informatics infrastructures, it is imminent to gain cybersecurity insights on medical context-motivated adversarial attacks for designing solutions to defend this threat. Medical adversarial attacks may lead to serious consequences including patient harm, liability of healthcare providers, and other ethical issues or crimes. It is imperative to study this emerging cybersecurity issue to mitigate the potential consequences and to ensure the safety of health care. This study contributes to providing safety evaluation and protective measures to medical imaging-based artificial intelligence diagnosis devices and clinical informatics infrastructures, and it sets the stage for researchers and regulatory agencies to investigate artificial intelligence-induced cybersecurity science and engineering issues in the medical domain. This study advances scientific discovery, clinical deployment, and practical applications of safe artificial intelligence medical systems, ultimately benefiting patient care, the general public, and society at large. <br/><br/>The technical goal of this study is to investigate mechanisms of generative adversarial network-generated medical imaging adversarial attacks, analyze behaviors of an artificial intelligence diagnosis system under such attacks, and develop various defensive strategies and methods. Generative adversarial network models are customized to generate medical context-motivated adversarial samples by ?inserting? or ?removing? malignant lesions in a varying resolution of digital mammogram images while maintaining the manipulated images to be visually imperceptible to true images. Four representative defensive methods, including the strategy of combining computational algorithms and human expert knowledge, are examined for defending against adversarial attacks. This project contributes algorithms, educational materials, and critical insights to bolster further research activities along the line of medical artificial intelligence cybersecurity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2114989","CICI: UCSS: SciAuth: Deploying Interoperable and Usable Authorization Tokens to Enable Scientific Collaborations","OAC","Cybersecurity Innovation","07/01/2021","05/11/2021","James Basney","IL","University of Illinois at Urbana-Champaign","Standard Grant","Robert Beverly","06/30/2024","$499,959.00","Brian Bockelman, Derek Weitzel","jbasney@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8027","7923, 8027","$0.00","The SciAuth project facilitates a cybersecurity transformation for NSF cyberinfrastructure, which is the scientific computing infrastructure across the nation that enables scientific productivity. The transformation at the heart of the project is a migration from cybersecurity technologies from 20 years ago to the cybersecurity standards of the modern Web. SciAuth provides needed leadership and coordination for this critical transformation through community engagement, coordinated adoption of community standards, integration with software cyberinfrastructure, security analysis and threat modeling, training, and workforce development. The project helps the community realize the benefits of an interoperable, modern cybersecurity ecosystem when transitioning between technologies, while maintaining the reliable and secure cyberinfrastructure upon which the scientific community depends. This transition to modern cybersecurity mechanisms is critical for enabling productive scientific collaborations across a diverse and distributed scientific cyberinfrastructure ecosystem. SciAuth builds on prior work by the NSF SciTokens project, in partnership with domain science projects and cyberinfrastructure providers, to realize this cybersecurity breakthrough across NSF cyberinfrastructure. SciAuth also supports the development of a diverse, globally competitive STEM workforce through a fellows program that pairs students across the country with mentors from the project to collaborate on student-led projects on the topic of cyberinfrastructure security.<br/><br/>The migration from X.509 user certificates to JSON Web Tokens is in progress across NSF cyberinfrastructure. This migration has facilitated a re-thinking of authentication and authorization among cyberinfrastructure providers: enabling federated authentication as a core capability, improving support for attribute, role, and capability-based authorization, and reducing reliance on prior identity-based authorization methods that created security and usability problems. Achieving the benefits of a fundamentally new security credential ecosystem in NSF cyberinfrastructure, while avoiding the temptation to simply re-implement old X.509 methods, requires leadership and coordination. SciAuth provides the needed leadership and coordination to make these breakthrough technologies usable by scientists across disciplines, project sizes, and software ecosystems by enabling coordinated deployments across cyberinfrastructures in active use today.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827127","CC* Networking Infrastructure: Bulldog Connectivity and Research","OAC","Campus Cyberinfrastructure","08/01/2018","09/06/2018","Damian Clarke","SC","South Carolina State University","Standard Grant","Kevin Thompson","07/31/2022","$443,726.00","Nikunja Swain, Donald Walter","damian.clarke@aamu.edu","300 College Street NE","Orangeburg","SC","291170001","8035367000","CSE","8080","","$0.00","The Bulldog Connectivity and Research Network  (BCR net) project at South Carolina State University (SCSU) creates a network infrastructure to accommodate increasing research activities in STEM and non-traditional areas such as the library, visual arts, museum and the planetarium. The campus enterprise network, plagued with bandwidth bottlenecks, supports only administrative, and academic needs, which competes for simultaneous use of limited resources. Disparate silos of unconnected computing resources have supported research computing in research labs with little scalability to the evolution of research needs. The BCR net provides a frictionless redundant design dedicated to research computing that delivers high-speed connectivity. Its design focuses on isolating high-throughput research functions through data paths on operationally efficient high-speed connections to research partners.<br/><br/>BCR net, separate from the campus enterprise network, gives SCSU researchers access to on-demand high bandwidth science DMZ connections to sharable storage of research data connected to high throughput data transfer nodes with secure digital connections. Transfers of large datasets and instructions will be enabled under projects such as the Robotically Controlled Telescope Consortium, the NSF Partnership in Observational and Computational Astronomy (POCA), the NSF SCI-STEPS INCLUDES Project. Other areas affected include; the Physics and Chemistry computational labs for astrophysics and cancer research, the digital media lab, the historical collection and archiving Orangeburg massacre collection, applied radiation sciences lab, the Nuclear Engineering program reactor simulation lab, and the computer science security lab.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004693","Elements: Cyber-infrastructure for Interactive Computation and Display of Materials Datasets","OAC","DMR SHORT TERM SUPPORT, Software Institutes","05/01/2020","05/01/2020","JoAnn Kuchera-Morin","CA","University of California-Santa Barbara","Standard Grant","Seung-Jong Park","04/30/2023","$450,000.00","Anton Van der Ven","jkm@create.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","1712, 8004","054Z, 077Z, 7923, 8004, 8396, 8399, 9216","$0.00","This project will accelerate the progress of scientific research by creating a software infrastructure named TINC (The Toolkit for Interactive Computation). TINC will allow scientists to interactively work with very complex information generated by high performance computing (HPC) clusters. It will provide software tools to assist scientists by facilitating virtual experimentation through interactive visualization, speeding up time to discovery. By tying together the scientists' workflow of computation, scientific data analysis in scripting languages, and visualization, TINC will enable new ways of sharing and disseminating results by allowing researchers to share not only their results, but also their interactive workflow as part of their publications. This research will begin its focus on an important and essential need in the materials science community, speeding up time to discovery of new materials through rapid prototyping using computation. The basic science to be generated through the application of the TINC infrastructure is the study of the electrochemical properties of electrode materials for Sodium ion batteries that will help overcome materials challenges that are preventing the commercialization of this promising technology for large-scale grid storage applications. This important proof of concept will facilitate delving deep into the science while focusing on the generalization of the tool to other disciplines as well. The ultimate goal of TINC is to create a new paradigm for high performance computing, facilitating ease of use by tying together interactive visualization with computation. This paradigm shift may facilitate a connection not only to a wider scientific community but also to an informed general public as well through TINC's focus on reproducibility and provenance tracing. <br/><br/>TINC is a computational toolkit that expedites data discovery by improving the interaction workflow in complex data analysis. This improvement is achieved by tightly integrating interactivity, computation and visualization with complex scientific data. By managing the connection between data parameters and on the fly computation, TINC simultaneously tackles the issues of reproducibility and interactive control in the exploration of data with large parameter spaces. With the integration of scripting languages and data notebooks, scientists can study their data with the ease of interactive computation and display. Through a robust caching mechanism it will enable new ways of sharing and disseminating results by allowing researchers to share not only their results, but also their interactive workflow as part of their publications. TINC will tightly integrate interactivity, computation and visualization in the research loop, allowing scientists to more quickly and more deeply understand, compare and validate their data. Thus, TINC will facilitate the merging of complex scientific computational models with high performance interactive visualization and will enable real-time exploration of empirical and theoretical models and large experimental datasets. Provided as a set of python and C++ libraries, TINC will handle parameter space mapping to data, interactive triggering of computation on this parameter space and caching to enable scalability, performance, full reproducibility and data provenance tracking. TINC will be applied to a statistical mechanics study of ionic transport mechanisms and ionic insertion processes in layered intercalation compounds that are candidate electrode materials for Sodium ion batteries. This is of critical importance to the area of materials simulation that focuses on the study of transport mechanisms in alloy systems, where visualizing specific mechanisms experimentally is difficult. TINC will allow computational researchers to propose and verify transport mechanisms in a way not previously possible.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018811","CC* Regional:  Promoting Research and Education at Small Colleges in Alabama through Network Architecture Enhancements","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/15/2020","10/20/2020","Samuel D'Angelo","GA","Georgia Tech Research Corporation","Continuing Grant","Kevin Thompson","06/30/2022","$730,545.00","Damian Clarke, David Bourrie","cas.dangelo@oit.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7231, 8080","","$0.00","Advancements in data-intensive scientific instrumentation have greatly surpassed the capability of some campus networking infrastructures to effectively connect data-producing facilities to powerful computing and storage systems. Georgia Tech (GT) working in partnership with Southern Light Rail (SLR) and its high-speed research network, Southern Crossroads (SoX), has established this project to increase connectivity to smaller and HBCU institutions in Alabama. As a result of this project, both Alabama Agricultural and Mechanical University (AAMU) and the University of South Alabama (USA) are able to transition their connectivity from a low-bandwidth ISP to a true high-speed R&E network to help increase their research efforts. Since their IT networking staff and budget are smaller and do not possess the expertise or funding in procuring and managing multiple internet providers, this award is allowing GT to install pre-configured hardware appliances for connectivity, performance management, and large data transfers at SoX.<br/> <br/>With the enhancement of the network at AAMU, they are able to enhance their research on the following: UAVs to manage agricultural data collection and analysis, astrophysics visualization, and virtual mentoring research. Similarly at the University of South Alabama, this project is leading to an increase in the data transfer between their university and industry partners eliminating the need to exchange physical hard drives. This is expanding their research in the field of multi-spectral imaging for medical and other life science applications, as well as analysis of sensor data from airplanes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827139","CC* Networking Infrastructure: The Roadrunner High-Performance Science, Engineering, and Business DMZ","OAC","Campus Cyberinfrastructure","07/01/2018","12/14/2020","Brent League","TX","University of Texas at San Antonio","Standard Grant","Kevin Thompson","12/31/2021","$500,000.00","Harry Millwater, Bernard Arulanandam, Brent League","BRENT.LEAGUE@UTSA.EDU","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8080","","$0.00","World-class research in cyber security, bioinformatics, cloud computing, machine learning, artificial intelligence, real-time computing and other related areas requires efficient and often near-immediate access to large data sets in order to reduce the time from theory to discovery. In addition, discoveries are often interdisciplinary and multi-institutional. As a result, a critical enabling feature for impactful, collaborative research is a dedicated high-speed network that is omnipresent across a campus. The University of Texas at San Antonio (UTSA), a Hispanic Serving Institution, is implementing a dedicated research network (DMZ) to facilitate data-intensive computation and research collaboration endeavors. This infrastructure fills a gap that currently exists in the ""last mile"" bottleneck from a research lab to the DMZ. <br/><br/>In particular, the project calls for installation of 10 Gb/s switches across campus that, in concert with the dedicated research network, provide 5-10X faster data transfer rates. These improvements foster access by UTSA faculty and students to the campus' high-performance computing facility, high-speed data storage, and visualization laboratory as well as the Texas Advanced Computing Center (TACC) in Austin, Texas. The new network also enhances experiential learning activities such as UTSA's annual CyberPatriot competition, undergraduate research projects in cloud and high-performance computing, cloud computing ELab training for undergraduates, as well as certification programs such as the Master's level certification in cloud computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2128738","Detail between Naval Postgraduate School and National Science Foundation for Dr. Robert Beverly.(Year 2)","OAC","","03/01/2021","04/06/2021","Robert Beverly","CA","Naval Postgraduate School","Contract Interagency Agreement","Carl Anderson","10/01/2021","$208,641.00","","rbeverly@nps.edu","1 University Circle","Monterey","CA","939435000","8316562271","CSE","0119","","$0.00",""
"2103778","Collaborative Research: Elements: Shared Data-Delivery Infrastructure to Enable Discovery with Next Generation Dark Matter and Computational Astrophysics Experiments","OAC","PHYSICS AT THE INFO FRONTIER, Software Institutes","08/01/2021","05/21/2021","Matthew Turk","IL","University of Illinois at Urbana-Champaign","Standard Grant","Robert Beverly","07/31/2024","$264,014.00","","mjturk@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7553, 8004","077Z, 7569, 7923, 8004","$0.00","Modern laboratories provide unprecedented sensitivity to the many different galactic-messengers that stream through our planet by the minute: cosmic rays, light from distant galaxies, elusive neutrinos, and possibly dark matter.  Combining this information with models and data from simulations provides insight into how our universe began and continues to evolve -- the scales at which objects first collapsed, the development of stars and galaxies, and the dynamics within our own galaxy.<br/><br/>However, this data is often inaccessible: scientists within an experiment or community struggle with the complex, custom-built programs they use to access the data.  And switching to a standard format is usually not an option: these data formats are designed for requirements that often do not include cross-experiment synthesis or linking.<br/><br/>Junior scientists - let alone the public - can struggle to generate new insights from the data because the data is difficult to access, understand and analyze.  The cross-cutting inquiry that could arise from clever reuse and combination of data from different experiments and simulations is rarely conducted.<br/><br/>This project makes data accessible both within and across collaborations, providing the infrastructure to search for signals in detectors across the globe.  Extending existing efforts to improve data access makes this project possible: yt is software that provides uniform access to simulation data; Kaitai is a data-description language that enables easy access to any data format; Rucio and other tools provide a standard interface that allows data downloads; and ServiceX can identify, subset and process data with little effort from the end user.<br/><br/>Scientists have built experiments that offer an incredible wealth of information about our world.  This project works to make that information accessible to everyone.<br/><br/>Technical Description<br/><br/>The Personal Data-Delivery infrastructure (PONDD) addresses the data challenges of existing dark matter and astrophysics experiments while requiring no changes to existing data formats.  This non-invasive, no-changes-necessary support for any file format provides opportunities to expand beyond our two identified use cases, dark matter searches and astrophysics simulations, into many other data-driven science domains that rely on custom file formats.<br/><br/>This work delivers an infrastructure that seamlessly delivers data in a well-supported format (such as Parquet) from multiple sources.  To successfully deliver cross-experiment data to end users, we bring together ongoing projects from High Energy Physics and the broader NSF community; while this project will involve development of software products (yt and Kaitai) it will also include synthesis of existing investments in cyberinfrastructure and efforts to improve their long-term sustainability.<br/><br/><br/>This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103707","Collaborative Research: Elements: Bifrost - A CPU/GPU Pipeline Framework for High Throughput Data Acquisition and Analysis","OAC","Software Institutes","07/01/2021","05/21/2021","Gregory Taylor","NM","University of New Mexico","Standard Grant","Robert Beverly","06/30/2024","$399,441.00","","gbtaylor@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","1798, 8004","077Z, 1207, 7923, 8004","$0.00","Modern computers, including cell phones and tablets, have sophisticated Graphics Processing Units (GPUs) that render the beautiful graphic displays in games.  We are developing software that takes advantage of these same GPUs for capturing and processing data from astronomical telescopes.  This allows us to benefit from all the years of effort spent developing these powerful computational tools. This software, known as Bifrost, is currently in use at the Long Wavelength Array (LWA), a radio telescope for exploration of a broad scientific portfolio ranging from the study of Cosmic Dawn when the first stars and galaxies lit up the Universe, to understanding the properties of the Earth's ionosphere.  We are actively developing Bifrost to make it both more powerful and easier to use for other telescopes.  Eventually we aim for Bifrost to be available as a more general purpose framework that can be applied to research projects beyond astronomy. <br/><br/>About 5 years ago we adopted a commodity equipment design for the second LWA station (LWA-SV) which makes use of computing servers with GPUs to handle the data capture, beamforming, and correlation at the station level.  Previously these functions were taken on by dedicated hardware referred to as the Digital Processor. However, this custom-hardware design was expensive to build and maintain, lacks flexibility, and cannot be easily replicated for future LWA stations.  In contrast the commodity approach is easier to maintain, much more flexible and expandable, and can be readily adapted to new LWA stations.  We are engaged in a concentrated effort to improve the underpinnings of Bifrost.  This involves increasing the data rates that Bifrost is capable of handling, improving the application programming interface, and providing tools to make it easier for users to develop and test new pipelines.  Through this award we are working with collaborators to incorporate Bifrost in telescopes and instruments currently under development.  The  availability of Bifrost will increase the scientific return of not only radio astronomy but also other areas where high throughput data processing is needed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018823","CC* Regional: Advancing Maryland Research and Education Network for Under-Resourced Institutions Through a Science DMZ and 10Gbps Upgrade","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","HAMID BARGHI","MD","University System of Maryland","Continuing Grant","Kevin Thompson","06/30/2022","$799,767.00","Eric Davidson","ray.barghi@usmd.edu","3300","ADELPHI","MD","207831600","3014452753","CSE","7231, 8080","","$0.00","MDREN, the Maryland Research and Education Network, an organization under the University System of Maryland, provides advanced network services to 40+ education, research, and public service institutions throughout the State of Maryland and connections to regional and national resources.<br/><br/>High-speed networking infrastructure is vital to researchers and STEM educators in our community. Smaller/under resourced/rural institutions like Frostburg State University (FSU), Salisbury University (SU), the four research labs that are part of University of Maryland Center for Environmental Sciences (UMCES),  the Maryland Psychiatric Research Center (MPRC) as part of University of Maryland School of Medicine (UMSOM), and HBCUs like University of Maryland Eastern Shore (UMES) and Morgan State University have serious limitations in accessing and transferring huge datasets amongst themselves and collaborators at NOAA, NASA, NIH, USGS, and others. <br/><br/>This proposal significantly increases network bandwidth for 6 institutions including research centers and labs by 10x and adds a Science DMZ for data transfers from other sites to enable the full potential of these institutions. These upgrades allow researchers at each institution to reliably connect to supercomputers and big data repositories, increase computational capabilities, and increase inter-institution collaboration.<br/> <br/>MDREN will a special path from the science DMZ machine to each lab that will facilitate high speed transfer. This would be significantly faster than doing transfers through campus firewalls.<br/><br/>This proposal helps rural or under-resourced institutions and HBCUs better collaborate with peer institutions in the greater Maryland region, collaborators around the world, and key science organizations such as NASA, NIH, and NOAA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925623","CC* Networking Infrastructure: Gate City Research Network - A Multi-Institution Science DMZ","OAC","Campus Cyberinfrastructure","07/01/2019","06/07/2019","Jeff Whitworth","NC","University of North Carolina Greensboro","Standard Grant","Kevin Thompson","06/30/2022","$499,912.00","Daniel Todd","jnwhitwo@uncg.edu","1111 Spring Garden Street","Greensboro","NC","274125013","3363345878","CSE","8080","","$0.00","The Gate City Research Network (GCRN) is a collaboration between the University of North Carolina at Greensboro (UNCG) and North Carolina A&T State University (NC A&T) to create a multi-institutional science DMZ supporting research activities through a dedicated, low-latency, high-speed research and education network connection. The GCRN enhances researcher access to high performance computing (HPC) resources supporting the competitive and innovative research environment state-wide and regionally by connecting the Southeastern Nanotechnology Infrastructure Corridor through the Joint School of Nanoscience and Nanoengineering (JSNN).<br/><br/>The GCRN, connected to the North Carolina Research and Education Network and Internet2, provides a dedicated, separate research network to overcome the latency challenges imposed on the enterprise networks that support the administrative and growing entertainment service traffic. The addition of a high-speed data transfer node facilitates fast transfers of large data to federated HPC environments at partner institutions and provides the foundation to enhance the end-to-end data flow processes from research instrumentation to analysis, simulation and modeling computational resources, significantly increasing the fundamental research capacity in disciplines such as chemistry, nano-engineering, nano-, computer-, and data science. The GCRN is committed to producing open access design, sustainability and governance documentation, use and performance data, and testing and operations protocols in support of developing a  21st-century data capable workforce and serving as a model for a scalable and efficient  multi-institutional science DMZ.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003820","Elements: Morpho-Cyberinfrastructure for scientists and engineers studying shape change","OAC","DMR SHORT TERM SUPPORT, Software Institutes","06/01/2020","04/15/2020","Timothy Atherton","MA","Tufts University","Standard Grant","Seung-Jong Park","05/31/2023","$600,000.00","James Adler","timothy.atherton@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","1712, 8004","077Z, 7573, 7923, 8004, 9216","$0.00","The goal of this project is to create an accessible open-source software package, Morpho, able to solve a wide variety of shape optimization, evolution and shapeshifting problems. These occur in numerous systems that cut across multiple NSF programs involving soft matter, an umbrella term for readily deformable materials, and includes soft robots, plastics, complex fluids, textiles, particulate media, glasses and biological materials as well as other applications in mathematics and computer science involving computational geometry. Shape change is an important feature of these systems, or a goal of the envisioned applications, but predicting their behavior is very challenging. There is presently a lack of appropriate simulation tools readily available to practitioners working in these domains inhibiting quantitative, mechanistic understanding of their behavior and optimization for applications. With Morpho, domain scientists gain a powerful new simulation tool that enables them to tackle larger and more complex shape evolution problems than presently possible. The project also creates a user community by providing extensive training opportunities including an immersive annual workshop, high quality documentation and a virtual community.<br/><br/>Shape optimization and evolution problems are numerically extremely challenging because the final shape is not known ahead of time: The numerical representation must be continuously monitored to ensure the solution obtained is correct and of high quality. The central innovation of Morpho is to regularize ill-posed shape problems by introducing auxiliary functionals that capture some notion of mesh quality. The aim of this work is to extend the range and complexity of problems Morpho can solve in two ways. The first is to allow the user to incorporate arbitrary types of manifolds, field quantities defined on the manifolds, discretizations, functionals and constraints pertinent to their problem. The second is to leverage multilevel algorithms and GPU computing to accelerate the simulations and enable the software to predict the dynamical response of the system given an initial configuration. The project also engages domain scientists in creating and using Morpho through a user-centered development process and a community driven science and education program, incorporating documentation, a repository of example code and tutorials, a virtual community and an annual training workshop. The resulting software and educational materials enable other researchers to simulate shape evolution in several emerging fields involving soft matter and other areas including active materials, soft robots, programmable materials and extreme mechanics. <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004014","Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution","OAC","Software Institutes","05/15/2020","04/28/2021","Jason Leigh","HI","University of Hawaii","Standard Grant","Seung-Jong Park","04/30/2025","$2,265,993.00","Mahdi Belcaid","leighj@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","8004","075Z, 077Z, 079Z, 7925, 8004, 9150, 9251","$0.00","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world.<br/> <br/>SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003931","Collaborative Research: Frameworks: Beyond the BLAS: A Framework for Accelerating Computational and Data Science","OAC","Software Institutes","05/01/2020","04/09/2020","Devin Matthews","TX","Southern Methodist University","Standard Grant","Seung-Jong Park","04/30/2023","$369,280.00","","damatthews@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147684708","CSE","8004","075Z, 077Z, 079Z, 7925, 8004","$0.00","Traditional scientific and machine learning high-performance computing software is often cast in terms of a set of fundamental operations, including the linear algebra functionality that underlies many applications.  For this reason, research into and development of open-source linear algebra software libraries has been a science infrastructure priority for decades.  An emerging trend that has disrupted this field is the recognition that scientific discovery can be made faster and/or more cost efficient by lowering the precision of computations, utilizing non-standard data types, and developing custom computational kernels.  The project will leverage insights into how to structure the required software so that the combinatorial explosion in software complexity remains manageable.  The outcome of the project will be a modern linear algebra software framework and application-focused libraries that will support future generations of computational applications in academia, at the national labs, and in industry.  In addition, the project will impact the training of the next generation of high-performance computing professions and help remove barriers into the field for members of traditionally underrepresented groups.<br/><br/>The proposed work will build on previous NSF-sponsored research in order to address the implementation of expanded precision (EP), mixed precision (MP), and mixed domain (MD) algorithms simultaneously in a single software solution.  Insights gained from a recent demonstration of MP/MD matrix multiplication will be extended by adding low precision types like float16 and bfloat16 and extended precision types like double-double.  The target Basic Linear Algebra Subprograms (BLAS) functionality will be expanded to all level-1, level-2, and level-3 operations which in turn will support new research on how best to exploit MP/MD for LAPACK functionality.  The new BLAS-like Library Instantiation Software (BLIS) framework will also be updated to provide the flexibility required to integrate extended dense linear algebra (DLA) operations. This flexible DLA framework will then be used to implement key functionality in computational and data science:  tensor contraction and factorization operations important to quantum chemistry (QC) and high-performance primitives for machine learning.  As a demonstration, these capabilities will be used to build state-of-the-art QC codes to perform coupled cluster polarization propagator and tensor-factorized coupled cluster calculations with full EP/MP/MD functionality, and the machine learning kernels will be integrated into computer vision and image recognition workflows.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103889","Elements: Machine Learning Quark Hadronization","OAC","PHYSICS AT THE INFO FRONTIER, Software Institutes","09/01/2021","04/26/2021","Jure Zupan","OH","University of Cincinnati Main Campus","Standard Grant","Robert Beverly","08/31/2024","$599,989.00","Philip Ilten","zupanje@ucmail.uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","7553, 8004","075Z, 077Z, 7569, 7923, 8004","$0.00","Particle physics explores the fundamental building blocks of nature, and their interactions. This is accomplished by colliding particles like protons together at high energies, producing many particles which are then measured by experimental detectors. To interpret the data, detailed simulations of particle collisions are needed. Monte Carlo event generators provide the simulations and are at the core of particle physics discoveries, such as the discovery of the Higgs boson at the Large Hadron Collider in 2012, forming a keystone of the cyberinfrastructure for particle physics. This project addresses a bottle-neck in Monte Carlo event generators ? how quarks are bound together inside protons and other composite particles through the process of hadronization. Hadronization models are improved by this project, including the speed of simulating the process of hadronization, fulfilling a critical step for understanding the scientific impact of upcoming large-scale particle, neutrino, and nuclear physics projects. <br/><br/>Event generators simulate particle collisions in three steps: a high energy collision at the small distances, evolution of the event into a large distance (low energy) configuration, and finally a hadronization into observable particles. The hadronization step is particularly challenging, because it cannot be directly calculated from first principles. Since hundreds of particles must be produced for each particle collision, algorithmic efficiency is necessary and requires innovative approaches. This project is applying Machine Learning (ML) techniques to hadronization in order to: connect to existing models of hadronization, understand the underlying description of data by current event generators augmented with ML, and provide a fast and accurate ML based simulation of hadronization. As part of the project, a platform is being built where user submitted modules, such as this ML hadronization module, can be incorporated in the most widely used event generator in the particle physics community, Pythia.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031980","Frontera Travel Grant: Development of Accurate, Transferable and Extensible Deep Neural Network Potentials for Molecules and Reactions","OAC","Leadership-Class Computing","07/01/2020","05/12/2020","Olexandr Isayev","PA","Carnegie-Mellon University","Standard Grant","Edward Walker","06/30/2022","$9,408.00","","olexandr@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103771","Collaborative Research: Elements: Bifrost - A CPU/GPU Pipeline Framework for High Throughput Data Acquisition and Analysis","OAC","Software Institutes","07/01/2021","05/21/2021","Christopher League","NY","Long Island University","Standard Grant","Robert Beverly","06/30/2024","$119,987.00","","christopher.league@liu.edu","700 NORTHERN BLVD","BROOKVILLE","NY","115481327","7184881413","CSE","1798, 8004","077Z, 1207, 7923, 8004, 9150","$0.00","Modern computers, including cell phones and tablets, have sophisticated Graphics Processing Units (GPUs) that render the beautiful graphic displays in games.  We are developing software that takes advantage of these same GPUs for capturing and processing data from astronomical telescopes.  This allows us to benefit from all the years of effort spent developing these powerful computational tools. This software, known as Bifrost, is currently in use at the Long Wavelength Array (LWA), a radio telescope for exploration of a broad scientific portfolio ranging from the study of Cosmic Dawn when the first stars and galaxies lit up the Universe, to understanding the properties of the Earth's ionosphere.  We are actively developing Bifrost to make it both more powerful and easier to use for other telescopes.  Eventually we aim for Bifrost to be available as a more general purpose framework that can be applied to research projects beyond astronomy. <br/><br/>About 5 years ago we adopted a commodity equipment design for the second LWA station (LWA-SV) which makes use of computing servers with GPUs to handle the data capture, beamforming, and correlation at the station level.  Previously these functions were taken on by dedicated hardware referred to as the Digital Processor. However, this custom-hardware design was expensive to build and maintain, lacks flexibility, and cannot be easily replicated for future LWA stations.  In contrast the commodity approach is easier to maintain, much more flexible and expandable, and can be readily adapted to new LWA stations.  We are engaged in a concentrated effort to improve the underpinnings of Bifrost.  This involves increasing the data rates that Bifrost is capable of handling, improving the application programming interface, and providing tools to make it easier for users to develop and test new pipelines.  Through this award we are working with collaborators to incorporate Bifrost in telescopes and instruments currently under development.  The  availability of Bifrost will increase the scientific return of not only radio astronomy but also other areas where high throughput data processing is needed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2102761","CDS&E: Multi-scale Coherent Structure Extraction and Tracking For Modern CFD Data Analysis","OAC","CDS&E","09/01/2021","05/21/2021","Guoning Chen","TX","University of Houston","Standard Grant","Seung-Jong Park","08/31/2024","$527,899.00","Di Yang, Rodolfo Ostilla Monico","gchen16@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8084","026Z, 8084","$0.00","Coherent structures are persistent and recognizable patterns that can be found in fluid flows. In turbulent flows, coherent structures are closely related to a diverse range of physical phenomena, and understanding their behavior is crucial for characterizing, predicting and controlling these flows. However, reliable identification and characterization of coherent structures is challenging due to their diversity and complex inter-relations across different space- and time-scales. This project brings together experts from both the data visualization and fluid mechanics communities to investigate novel solutions to multi-scale coherent structure extraction, separation, tracking, and visualization. It aims at significantly advancing the ability to analyze large datasets of turbulent flows stemming from computational fluid dynamic (CFD) simulations in a wide range of engineering and scientific applications. This project provides opportunities for both undergraduate and graduate students with different and diverse backgrounds to participate in the proposed research. The research outcomes can be integrated into the development of a number of undergraduate and graduate courses taught at the University of Houston. The outreach activities enabled by the proposed research help motivate more students to pursue a career in STEM related fields. <br/> <br/>To achieve an efficient and reliable analysis for large-scale turbulent flow data, this project aims to investigate a new multi-scale coherent structure representation that encodes relevant flow physics, statistics, and uncertainty information, and to develop a robust computation and exploration framework based on this new representation to support data-driven research. To enable this multi-scale analysis, this project applies a number of spatial and temporal domain decomposition strategies to the computational fluid dynamic (CFD) data. Multi-field analysis and high-dimensional data projection techniques are adapted to incorporate different physical attributes to the representation. A novel graph representation is leveraged to encode this multifaceted information in a concise and dimension-independent form to enable multi-scale feature extraction and tracking. A matrix representation of this graph is employed to accelerate its processing by utilizing the recent advances in large-scale matrix calculation. A new visual analytic paradigm is devised based on the proposed graph representation to aid the exploration and comprehension of different turbulence structures individually or collectively. The developed techniques implemented as a number of software libraries can be integrated into existing software, e.g., Paraview, for domain scientists to use in their daily research. The developed techniques can also be used as pre-processing toolboxes to quantify and extract coherent structures, which can then be visualized by existing software that are not suitable for direct library integration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1934712","Collaborative Research: Near Term Forecasts of Global Plant Distribution, Community Structure, and Ecosystem Function","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","09/01/2019","10/15/2020","Cory Merow","CT","University of Connecticut","Continuing Grant","Peter McCartney","08/31/2022","$943,275.00","Emmanouil Anagnostou, Efthymios Nikolopoulos","cory.merow@gmail.com","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","099Y, 7231","062Z, 7231","$0.00","This project is the first to explore how plant species distributions across the entire globe may respond to global change. The project brings together ecologists, environmental engineers, data scientists, and conservation stakeholders to determine optimal ways to integrate these data sources to make near term forecasts for all plants globally by addressing changes in (1) species' abundance and geographic distribution, (2) community structure, and (3) ecosystem function. This three-pronged approach is designed to span a range of approaches to understand the spectrum of possible futures consistent with current knowledge while integrating knowledge across scales of biological organization. These forecasts will be used along with input from conservation stakeholders to assess how differing conservation decisions can minimize the impacts of global change responses. An ultimate goal of the project is to automate a pipeline to ingest new incoming data, update forecasts, and serve these to end-users to enable a near-real time forecasting workflow to provide best-available predictions at any given time to inform conservation decisions. <br/><br/>A key aspect of these forecasts is their reliance on novel environmental information that better characterize the conditions that influence plant performance, including soil moisture and extreme weather events based on NASA satellite observations. These species-level predictions will be linked to community demography models that integrate a variety of relatively untapped data sources for understanding global change, including plant trait data, community plot data across the globe, highly detailed plot data from National Ecological Observatory Network (NEON) and Long Term Ecological Research (LTER) sites, and global biomass data from NASA's Global Ecosystem Dynamics Investigation (GEDI) mission. By integrating this wide variety of data sources, the mechanistic understanding needed to make robust near term forecasts can be made, to understand ecosystem properties like Net Primary productivity, Carbon stock, and resilience. Based on workshops with conservation stakeholders, researchers will determine how best to use this unique suite of forecasts to best inform different conservation questions in different regions of the world. The project will also result in an open, cleaned and curated database on global plant distributions. This will aid others in exploring data and predictions by delivering and visualizing complex future scenarios in an easy to use portal. All results of the project can be found at the website for the Biodiversity Informatics and Forecasting Institute or BIFI, at https://enquistlab.github.io/BIFI .<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2027529","Collaborative:RAPID: Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations","OAC","COVID-19 Research","05/15/2020","06/02/2020","Matthew Scotch","AZ","Arizona State University","Standard Grant","Seung-Jong Park","04/30/2022","$49,664.00","Matthew Scotch","Matthew.Scotch@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","158Y","077Z, 096Z, 7914, 8004","$0.00","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.<br/><br/>We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of  COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931397","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","Special Initiatives, Software Institutes","01/01/2020","09/04/2019","Claude Goldsmith","RI","Brown University","Standard Grant","Seung-Jong Park","12/31/2022","$354,191.00","","franklin_goldsmith@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 9150","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115130","CICI: UCSS: Helix++: Securing Open Science Platforms","OAC","Cybersecurity Innovation","08/01/2021","05/12/2021","Jack Davidson","VA","University of Virginia Main Campus","Standard Grant","Robert Beverly","07/31/2024","$498,021.00","Anh Nguyen-Tuong, Jason Hiser","jwd@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8027","7923, 8027","$0.00","The Helix++ project?s goal is to use recently developed, promising cybersecurity research results to secure open-science platforms. Securing these open-science platforms is vital because compromise of research infrastructure can have severe consequences, including the delay of critical research, corruption of research results, theft of intellectual property, and exposure of personally identifiable information. Beyond providing researchers customized, secure packages of widely used open software, the Helix++ project will provide insights and directions for future research and strategies for protecting critical cyber infrastructure. Using two existing operational open-science platforms at the University of Virginia, the project will investigate the interaction of technical and policy issues which are fundamental to infrastructure protection.<br/><br/>The open-source Helix++ project improves the security posture of open science platforms by applying cutting-edge cybersecurity techniques to diversify and harden software automatically. A distinguishing feature of Helix++ is that it does not require source code or build artifacts. It operates directly on software in binary form?even stripped executables and libraries. This feature is key as rebuilding applications from source is a time-consuming and often frustrating process. Helix++ enables the rapid generation and deployment of secure containers and virtual machines, wherein various applications and libraries are transformed to incorporate the Helix++ protections. Diversification breaks the software monoculture and makes attacks harder to execute as information needed for a successful attack will have changed unpredictably. Diversification also forces attackers to customize an attack for each target instead of attackers crafting an exploit that works reliably on all similarly configured targets. Hardening directly targets key attack classes. The combination of diversity and hardening provides defense-in-depth, as well as a moving target defense. Helix++ is evaluated on two open science platforms to demonstrate its efficacy and usability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003921","Collaborative Research: Frameworks: Beyond the BLAS: A framework for accelerating computational and data science","OAC","Software Institutes","05/01/2020","04/09/2020","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","04/30/2023","$812,680.00","Margaret Myers, Field Van Zee, Devangi Parikh","rvdg@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","075Z, 077Z, 079Z, 7925, 8004","$0.00","Traditional scientific and machine learning high-performance computing software is often cast in terms of a set of fundamental operations, including the linear algebra functionality that underlies many applications.  For this reason, research into and development of open-source linear algebra software libraries has been a science infrastructure priority for decades.  An emerging trend that has disrupted this field is the recognition that scientific discovery can be made faster and/or more cost efficient by lowering the precision of computations, utilizing non-standard data types, and developing custom computational kernels.  The project will leverage insights into how to structure the required software so that the combinatorial explosion in software complexity remains manageable.  The outcome of the project will be a modern linear algebra software framework and application-focused libraries that will support future generations of computational applications in academia, at the national labs, and in industry.  In addition, the project will impact the training of the next generation of high-performance computing professions and help remove barriers into the field for members of traditionally underrepresented groups.<br/><br/>The proposed work will build on previous NSF-sponsored research in order to address the implementation of expanded precision (EP), mixed precision (MP), and mixed domain (MD) algorithms simultaneously in a single software solution.  Insights gained from a recent demonstration of MP/MD matrix multiplication will be extended by adding low precision types like float16 and bfloat16 and extended precision types like double-double.  The target Basic Linear Algebra Subprograms (BLAS) functionality will be expanded to all level-1, level-2, and level-3 operations which in turn will support new research on how best to exploit MP/MD for LAPACK functionality.  The new BLAS-like Library Instantiation Software (BLIS) framework will also be updated to provide the flexibility required to integrate extended dense linear algebra (DLA) operations. This flexible DLA framework will then be used to implement key functionality in computational and data science:  tensor contraction and factorization operations important to quantum chemistry (QC) and high-performance primitives for machine learning.  As a demonstration, these capabilities will be used to build state-of-the-art QC codes to perform coupled cluster polarization propagator and tensor-factorized coupled cluster calculations with full EP/MP/MD functionality, and the machine learning kernels will be integrated into computer vision and image recognition workflows.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2003387","Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution","OAC","Software Institutes","05/15/2020","04/21/2020","Christopher North","VA","Virginia Polytechnic Institute and State University","Standard Grant","Seung-Jong Park","04/30/2025","$500,000.00","","north@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","075Z, 077Z, 079Z, 7925, 8004","$0.00","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world.<br/> <br/>SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004932","Collaborative Research: Frameworks: funcX: A Function Execution Service for Portability and Performance","OAC","Software Institutes","05/01/2020","04/17/2020","Daniel Katz","IL","University of Illinois at Urbana-Champaign","Standard Grant","Seung-Jong Park","04/30/2024","$481,234.00","","dskatz@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","077Z, 7925, 8004","$0.00","The funcX project is developing, deploying, and operating a new distributed computing cyberinfrastructure platform to enable researchers to build applications from programming functions that execute on different computing resources, from laptops to supercomputers. This cloud-hosted service democratizes access to advanced computing by providing intuitive interfaces for both registering remote computers as function executors and executing functions on these computers reliably, securely, and with high performance. Researchers can thus decompose monolithic applications into collections of reusable lightweight functions that can be run wherever makes the most sense, for example where data reside or where excess capacity is available. By simplifying access to specialized and high performance cyberinfrastructure and decreasing the time to discovery, the project serves the national interest, as stated in NSF's mission, by promoting the progress of science. A total of 33 diverse science, cyberinfrastructure, and software institute partners working with cutting-edge science applications and research cyberinfrastructure will directly benefit from the funcX platform.<br/><br/>This project develops funcX, a scalable and high-performance federated platform for managing the remote execution of (often short-duration) functions across diverse cyberinfrastructure systems, from edge accelerators to clusters, supercomputers, and clouds. funcX allows developers to decompose applications into collections of functions that can each be executed in the best location, in terms of cost, execution time, data movement costs, and/or energy consumption. It thus integrates the extreme convenience of the function as a service (FaaS) model, developed in industry for specific industry applications, with support for the specialized needs of scientific research. funcX addresses important barriers to these new uses of research cyberinfrastructure systems, by enabling the intuitive, flexible, and scalable execution of functions without regard to physical location, scheduler architecture, virtualization technology, administrative domain, or data location. Flexible open-source funcX agent software makes it easy to expose arbitrary computing systems as funcX computing platforms, thereby transforming existing cyberinfrastructure systems into high-performance function serving environments (endpoints). The cloud-hosted funcX service provides a REST interface for registering functions, discovering available endpoints, and managing the execution of functions on endpoints, all via a universal trust fabric and standard web authentication and authorization mechanisms. It dynamically creates and deploys containers that incorporate function dependencies and provide a secure and isolated environment for safe function execution. The project engages a diverse set of 11 science partners, 18 research computing and cyberinfrastructure projects, and 4 NSF Software Institutes, each supporting many NSF-funded researchers, to provide use cases for funcX, shape its design, and evaluate its implementation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931524","Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering","OAC","Software Institutes","01/01/2020","08/14/2019","Matthew Knepley","NY","SUNY at Buffalo","Standard Grant","Seung-Jong Park","12/31/2024","$504,431.00","","knepley@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.<br/><br/>This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004993","Elements: Advanced Lossless and Lossy Compression Algorithms for netCDF Datasets in Earth and Engineering Sciences (CANDEE)","OAC","PHYSICAL OCEANOGRAPHY, Climate & Large-Scale Dynamics, Software Institutes","07/01/2020","04/09/2020","Charles Zender","CA","University of California-Irvine","Standard Grant","Seung-Jong Park","06/30/2023","$599,916.00","","zender@uci.edu","160 Aldrich Hall","Irvine","CA","926977600","9498247295","CSE","1610, 5740, 8004","077Z, 4444, 7923, 8004","$0.00","Data compression is used to store and transmit digital data such as music, television, and satellite measurements more efficiently by reducing storage space and download times. The compression software broker that this project provides will facilitate the adoption of modern compression techniques in many branches of science. Compressors come in two flavors: lossless, those that perfectly preserve the original information; and lossy, those that irretrievably discard parts of the ""signal"" to further improve compression. Modern lossless and lossy compression improvements in efficiency, speed, and fidelity, are striking and will benefit critical research areas by permitting researchers to simulate, store, and analyze phenomena such as stellar evolution, chemical reactions, and hurricane formation at finer detail than before, with no extra storage costs. Since digital storage consumes power, better compression also reduces power consumption and associated greenhouse gas emissions. This project will develop the software infrastructure necessary for scientific researchers to seamlessly shift their applications to produce and use data stored with state-of-the-art lossless techniques, and by new lossy techniques that are more accurate than any others.<br/><br/>The two most widely-used self-describing dataset storage formats, HDF5 and netCDF4, support by default only one patent unencumbered lossless compression format, the venerable DEFLATE algorithm standardized in the 1990s. Our project will develop a dynamic and extensible software library of modern COmpressors and DECompressors (codecs) for scientific data called the Community Codec Repository (CCR). We will populate the CCR with cutting-edge open-source compression technology, including the LZ4, Facebook's Zstandard, and Google's Snappy codecs, and will implement default netCDF support for the CCR. Sequential lossy-then-lossless compression improves both the size and speed of compression/decompression yet is currently tedious to perform. We will implement a user-friendly method to ""chain"" codecs into sequential operations in memory (no intermediate files required) in our widely used netCDF Operators software package. We will also produce a new precision-preserving lossy codec, Granular Bit Grooming, that has unsurpassed compression ratio and statistical accuracy. Technical success will be evaluated by the size and speed improvements of compressing a prototypical geoscience/engineering ""big data"" project, the Coupled Model Intercomparison Project version 6 (CMIP6).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2031576","LARGE-SCALE ALL-ATOM SIMULATIONS OF NEUTRAL-SOLUTE TRANSPORTERS IN CELL-LIKE ENVIRONMENTS","OAC","Leadership-Class Computing","06/01/2020","05/01/2020","Liao Chen","TX","University of Texas at San Antonio","Standard Grant","Edward Walker","05/31/2023","$3,000.00","","liao.chen@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7781","","$0.00","For nearly four decades, the National Science Foundation (NSF) has played a leadership role in provisioning advanced cyberinfrastructure capabilities for the Nation's Science and Engineering (S&E) researchers. An important component in this investment is the leadership-class computing program that provides computational and data analytics capabilities at the largest scale to inspire transformative S&E discoveries that would not be possible otherwise. NSF's current leadership-class computing investment supports Frontera, the largest High-Performance Computing (HPC) system on a US academic campus. The Frontera system is deployed and operated by the Texas Advanced Computing Center (TACC) at the University of Texas (UT) at Austin. This travel grant will support the participation of researchers who have been awarded a computer time allocation on the Frontera system at a future Principal Investigator (PI) meeting organized by TACC. The award will also support travel for technical coordination between researchers and the Frontera project to ensure optimal and effective utilization of the Frontera system.<br/><br/>The goal of the PI meeting is to allow Frontera research users, as well as their students, to share scientific results, exchange practical lessons-learned, and present their overall experience from using the Frontera system. In addition to research presentations, the meeting will have ample time, as well as facilitated sessions, to promote increased interaction between Frontera research users and project staff. The outcome of the PI meeting will not only enable the Frontera project to better understand and serve the scientific research community, but also build a community to better represent the unique needs of S&E research that require access to NSF leadership computing facilities. To facilitate deeper coordination beyond the PI meeting, this award will also provide travel support to enable technical coordination between the research user teams and the Frontera project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018631","MRI: Acquisition of an Adaptive Computing Infrastructure to Support Compute- and Data-Intensive Multidisciplinary Research","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","08/01/2020","07/17/2020","Elise Miller-Hooks","VA","George Mason University","Standard Grant","Alejandro Suarez","07/31/2023","$750,000.00","Shobita Satyapal, Maria Emelianenko, Yue Cheng, Jayshree Sarma","miller@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","1189, 7231","1189","$0.00","A high-performance computing (HPC) system, named Hopper will be acquired by George Mason University (GMU). This HPC system will support high-impact research and education across all the multidisciplinary research institutes, and numerous departments and academic programs at George Mason University (GMU). Among other areas, Hopper will enable research and education projects focused on advancing transportation and infrastructure systems, geography and geoinformation sciences, astrophysics, social media analytics to support disaster informatics, computational fluid dynamics, materials science, natural hazards research, data mining, computer vision, automated vehicles, bioinformatics, neuroscience, and economics. Hopper will also support GMU?s large-scale high-performance computing users who will develop, prototype, and benchmark their applications locally before starting production runs on large nationally available systems. The instrument will support a very diverse community of researchers, educators and graduate and undergraduate researchers at GMU. It will also enable research projects by high school students engaged in summer research projects, community college students whose dreams are to attain their four-year degree, adult learners seeking their first college degree or to upskill to realize their professional aspirations. In addition to advancing cutting-edge research projects, Hopper will be instrumental in preparing the next generation of data scientists, mathematicians, engineers and scientists to meet the needs of the Nation?s thriving innovation economy.<br/><br/>Hopper will be managed centrally and shared with the entire Mason research community. The proposed system is based on an OpenStack framework and will be reconfigurable to support a diverse range of user needs at Mason. Hopper will consist of 4 OpenStack management nodes, 35 standard compute nodes, 2 high memory nodes with 3TB memory, and 6 nodes with graphics processing units (GPUs), all connected through InfiniBand (IB) switches to support low-latency high bandwidth communication between nodes and storage. The interconnect network will use HDR IB switches in a fat tree configuration and use port splitters to provide EDR IB connections to the nodes. There will be a redundant 25Gbe networking infrastructure with 100Gbe spine switches for node deployment and virtual machine (VM) access. The system will have 384 TB of Ceph based storage to back VM deployments, and 300 TB of high-speed low-latency BeeGFS storage for HPC use. Hopper will support both traditional as well as non-traditional (the long tail of science) high-performance computing users. This new system will replace GMU?s current HPC cluster, which is nearing the end of its useful life, is unable to keep up with growth in the number of users and was not designed to meet the needs of a diversity of usage modes now being requested by our faculty and student communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2115167","CICI:SIVD:Context-Aware Vulnerability Detection in Configurable Scientific Computing Environments","OAC","Cybersecurity Innovation","07/01/2021","05/11/2021","Mu Zhang","UT","University of Utah","Standard Grant","Robert Beverly","06/30/2024","$499,834.00","Sneha Kasera, Hari Sundar","muzhang@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8027","7923, 8027","$0.00","Computational infrastructures have increasingly become the enabling factor for scientific discovery, in critical application domains including seismic imaging, air quality monitoring, epidemiology, drug discovery and nuclear engineering. The security of these infrastructures is thus of crucial importance, as the vulnerabilities in their unique software stacks can cause significant damage to economy, environment, public health, and national security. This project aims to safeguard scientific computing infrastructures via automatically identifying hidden software vulnerabilities in a timely manner. Particularly, the goal of this project is to address the challenging problem of configuration-related security bugs in highly customizable high-performance computing environments. Detecting such vulnerabilities is a hard problem. The stateof- the-art general vulnerability analyzers are unable to capture the specific runtime contexts of multiple interdependent software elements in specialized scientific computing environments. To bridge this gap, this project connects advanced bug-finding techniques to dedicated high-performance computing settings. In addition, it also seeks to leverage the unique characteristics of scientific computing environments to facilitate vulnerability discovery. Hence, this research provides a comprehensive understanding of the software security problems in real-world scientific computing systems, and builds robust solutions to secure these systems.<br/><br/>Specifically, this project develops novel deployment-specific vulnerability detection techniques, that can (a) discover seemingly well-formed, yet inconsistent configuration values within scientific computing contexts, (b) detect cross-component vulnerabilities caused by the settings of interconnected computing software, and (c) take full advantage of the de facto workflow of high-performance computing systems to reduce the complexity of finding bugs. This research consists of three tasks: (1) it investigates the deployment contexts in real-world high-performance computing systems and develops both offline and online tools to automatically collect contextual information; (2) it applies extracted contexts to detecting misconfiguration and configuration-triggered code vulnerabilities at both deployment time and incrementally at runtime; (3) it tests the novel technique in real-world testbeds and scientific computing environments to evaluate its accuracy, efficiency and effectiveness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811597","Data-driven, biologically constrained biophysical computational model of the hippocampal network at full scale","OAC","Leadership-Class Computing","05/01/2018","04/09/2019","Ivan Soltesz","CA","Stanford University","Standard Grant","Edward Walker","04/30/2022","$4,230.00","","isoltesz@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7781","","$0.00","This project aims to fundamentally improve our understanding of the brain.  In particular, this project will construct a detailed, biologically realistic, computational model of the brain hippocampus.  The hippocampus is a major component of the brain that plays an important role in memory and other major human functions.  This detailed computational model of the hippocampus will allow researchers to understand the origins of specific behavioral level features of the brain, as well as treatment effectiveness for conditions such as epilepsy.  In addition, the project aims to investigate brain changes under different radiation exposure regimes.  Furthermore, the software infrastructure developed in the project, as well as simulation results, will be publicly released to the wider neuroscience community.<br/><br/>The overarching goal of the research projects is the construction of realistic and biophysically detailed computational models of the three major neuronal circuit layers in the hippocampus: the dentate gyrus (DG), CA3, and CA1. These three regions of the hippocampus are interconnected, and therefore the computational aspects of this research require building a detailed data-driven, full-scale computational model of the entire hippocampal formation and its inputs from the septum and the entorhinal cortex.  Information processing in the brain is organized and facilitated by the complex interactions of intrinsic biophysical properties of distinct neuronal types, neuronal morphology, and network connection topology. These properties give rise to specific types of network oscillations and other dynamic processes that govern neural information encoding and exchange. The hypotheses in this proposal are designed to create a detailed picture at unprecedented scale of how the intrinsic properties of hippocampal principal neurons and interneurons define the network activity under normal conditions, and how pathological changes in those properties under epileptic conditions disrupt hippocampal function.  The project has made public releases of the CA1 model code and simulation management tool (SimTracker), and have also published the CA1 simulation datasets that accompany the publication describing the main results of the CA1 work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1912166","EAGER: A Framework For Economical Cyber Security Inspection and Assurance","OAC","Cybersecurity Innovation","03/15/2019","02/21/2019","Theodore Allen","OH","Ohio State University","Standard Grant","Robert Beverly","02/28/2022","$300,000.00","Rajiv Ramnath, Laura Albert","allen.515@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8027","7916","$0.00","This project seeks to develop and apply smart inspection methods to keep the costs of cyber security low, while ensuring the security of scientific results. Many other industries use statistical sampling and sequential inspection methods for quality assurance. Yet, at present much of cyber security relies on attempted inspection of either 100% of items of a given type (such as top priority domain name server logs) or 0% (such as many types of possible hardware inspections). Working across two major universities, Ohio State University and the University of Wisconsin, the project develops new inspection methods, tailored to cyber security objectives. Specific objectives relate to hardware, cyber software vulnerabilities, log inspection, and emergency management and related scientific research. The project explores the usefulness of these new methods to senior quality professionals to support widespread adoption. By working smarter and not harder, scientists using this approach can feel more confident that their results are trustworthy, and, at the same time, the cost curve can be bent so that research can remain cost-competitive.<br/><br/>Specifically, the project models security metrics probabilistically and develops patterns of testing to estimate these metrics. The research addresses a variety of domains and cyber security challenges: (1) supercomputing log and hardware inspections, (2) ordinary department vulnerability sampling and estimation and improved logging and sampling (focusing on scientific infrastructure), (3) highly resourced department stratified sampling hardware, improved vulnerability decision-making, and strategic log generation and sampling, and (4) the UW Emergency Management system center. Metrics from attack-defend and standard quality assurance will be explored and serve as validation outputs for the techniques. This project seeks to meet the vital security needs by formulating an assurance framework that adapts and extends existing methods in quality control and assurance to the cyber domain. The assurance framework offers many advantages: it can address sequential decision-making, it can seek solutions that are robust to uncertain data, and it balances cost with threat reduction. The inspection methods to be studied include Multiple Complete Inspection & Inspector Inspection, Attempted 100% Inspection with Optimal Supplementation, Stratified Sampling, Sequential Sampling, and Optimal Multi-Fidelity Inspection. In each case, operating characteristic curves and other estimates of system integrity will be developed and applied. A key and unique element of the proposed approach is the combination of multiple inspection methods, including new methods, to provide a comprehensive framework for cost-effectively enhancing integrity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018936","CC* Compute: Compute Cluster for Computational Sciences at Louisiana State University Health Sciences Center ? New Orleans (LSUHSC-NO)","OAC","Campus Cyberinfrastructure","07/15/2020","07/28/2020","Christopher Taylor","LA","Louisiana State University Health Sciences Center","Standard Grant","Kevin Thompson","06/30/2022","$399,458.00","Carmen Canavier, Kenneth Boe, Andrew Chapple, Chindo Hicks","ctay15@lsuhsc.edu","433 Bolivar St.","New Orleans","LA","701122223","5045684804","CSE","8080","9150","$0.00","This project at Louisiana State University Health Sciences Center New Orleans (LSUHSC-NO) constructs a scalable 26-node high-performance computing cluster (HPC) named Tigerfish. Tigerfish provides LSUHSC-NO researchers with uninterrupted, local access to HPC resources to enable and accelerate their biomedical research. The project provides training to new and existing HPC users to ease their transition to utilizing both Tigerfish and other nationally available HPC resources to meet increasing computational needs of their research. Tigerfish supports research and education within multiple disciplines at LSUHSC-NO including Bioinformatics, Biostatistics, Computational Biology, Genetics, Computational Genomics, Microbiology, Neurology, and Proteomics. <br/><br/>This project has broader impacts for the community including training the next generation of researchers as a part of the Tiger Scholars program which provides high-school students with the opportunity to be exposed to the capabilities of HPC in research. Tigerfish is available as a computing resource supporting a high school, undergraduate, and postbaccalaureate student summer research program that provides summer research experiences to students coming to study at LSUHSC-NO from various parts of the country. Tigerfish provides an essential platform for our outreach to regional historically black colleges and universities which lack their own HPC resources for intensive computing. The recently launched Bioinformatics and Genomics program is able to access Tigerfish as an essential resource for their highly intensive computational needs and serves as a resource for the newly established MS Biomedical Sciences-Bioinformatics Track program. Finally, Tigerfish resources are shared with Open Science Grid for LSUHSC-NO to play its part in making HPC more accessible nationally.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828657","MRI: Development of a Co-Located Multi-User Immersive Display Instrument","OAC","Major Research Instrumentation, CYBERINFRASTRUCTURE","09/01/2018","08/23/2018","Dirk Reiners","AR","University of Arkansas Little Rock","Standard Grant","Stefan Robila","10/31/2020","$620,002.00","Carolina Cruz-Neira","dirk.reiners@ucf.edu","2801 South University","Little Rock","AR","722041000","5015698474","CSE","1189, 7231","1189, 9150","$0.00","The goal of this project is the development of a new kind of display system that allows multiple people to experience a virtual world together. The world will be perceived in a spatially consistent manner, combining a shared real space in which people can work, communicate and collaborate, while each maintaining their own personal 3D view. Such virtual reality environment will enable researchers to discuss and solve problems collaboratively without compromising their individual spatial understanding of the problem. This instrument will be used as a test and development platform to better understand effective means to collaboratively use Virtual Reality from a fundamental as well as application-oriented point of view. It will provide new ways to collaborate for applications in areas as diverse as material development, chemistry, molecular biology, archaeology, network analytics and others. The project will engage researchers and students in development of a new visualization instrument.<br/><br/>The system will combine existing 3D stereo display components with a new optical design to provide multiple users with their individual 3D view. It takes advantage of high-resolution displays and a custom microlens design that will be developed as part of the project. This includes developing an appropriate simulation system to define and specify the optimal lens sheet design as well as accounting for limitations in manufacturability of microlenses. The goal is to provide a group of 4 or more users with individually separate 3D views of the shared space inside a room-size environment that surrounds them with 3D screens. In addition to the optical design the project will develop the software infrastructure to drive the multi-user display. This includes calculating the correct images so that each user sees the correct views for their current position and orientation as well as fundamental interaction methodologies and methods to enable users from different disciplines to quickly and easily use the system productively to attack their problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004575","Elements: Cyberinfrastructure for streamlining coupled, simplified climate modeling within the Community Earth System Model","OAC","Climate & Large-Scale Dynamics, Software Institutes","05/01/2020","04/08/2020","Scott Bachman","CO","University Corporation For Atmospheric Res","Standard Grant","Seung-Jong Park","04/30/2023","$599,969.00","Gokhan Danabasoglu, Mariana Vertenstein, Isla Simpson","bachman@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","5740, 8004","077Z, 4444, 7923, 8004","$0.00","Global climate models have increased dramatically in their complexity over the last few decades, and now represent a massive array of coupled atmospheric, oceanic, cryospheric, biospheric and chemical processes. However, this complexity incurs substantial computational expense, and creates a challenge for researchers to perform targeted experiments and investigate specific dynamical processes. Idealizing the climate system through the creation of simplified models is a way of performing these types of highly focused studies while avoiding complexity and expense. However, the climate modeling community presently lacks the software infrastructure to easily configure these models using minimal physics packages, simplified domain geometries, or more streamlined model versions, within a single framework that bridges the gap between these simplified models and the fully coupled system. This project aims to fill this gap by developing a software toolchain to enable fast and seamless setup of coupled simplified models within NCAR?s Community Earth System Model (CESM). This Simpler Models toolchain will be officially supported as part of the CESM modeling framework, and will serve the needs of the research and academic communities across all climate science disciplines.<br/><br/>The overarching goal for the Simpler Models toolchain is to streamline the user workflow and enable research-ready coupled modeling at minimal expense in terms of human or computing time. The first component of the toolchain, the Simpler Models Query Tool, will advise users on the compatibility and configuration of different earth system model components based on their XML metadata. This tool will essentially let the user hone their modeling strategy without having to consult each component?s documentation or resorting to a trial and error approach. The toolchain will then allow for the configuration of model resolution and idealized domain geometries in such a way as to ensure compatibility across components. The workflow enabled by the toolchain will be structured so that compatibility of the coupled system is guaranteed first, followed by customizations at progressively higher levels of granularity within individual model components. The toolchain will be accompanied by user documentation that will be hosted on the CESM Simpler Models website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835573","Elements: Software: The Integrated Geoscience Observatory (InGeO)","OAC","Software Institutes, EarthCube, Space Weather Research","01/01/2019","09/11/2018","Asti Bhatt","CA","SRI International","Standard Grant","Seung-Jong Park","12/31/2021","$599,750.00","Russell Cosgrove","asti.bhatt@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","8004, 8074, 8089","026Z, 062Z, 077Z, 4444, 7923, 8004","$0.00","This award will support the development of the Integrated Geoscience Observatory (InGeO). InGeO is an online platform that integrates data and associated software tools contributed by researchers into a unified toolset to enable studying the convergent, systems science. Complex processes in the Sun-Earth system affect the habitability of earth. As technological progress continues, our society also becomes increasingly vulnerable to the disruptions introduced by these complex processes. Study of the Sun-Earth system is traditionally broken-up into separate geoscience disciplines, typically focusing on a specific region of the system. However, often the interaction between several regions end up significantly affecting terrestrial systems. For example, a chain of processes occurring between the Sun, magnetosphere, and ionosphere often result in loss of GPS signal that severely affects all the communication/navigation and infrastructure that depends on this technology. To broach the larger question of the interaction of the subsystems studied by the separate communities, it is necessary to overcome the barriers of communication posed by different observational instruments, software tools for interpreting data, and modeling methods.<br/><br/>To promote systems science, InGeO creates an integrated package of software tools specifically designed to help researchers find and integrate diverse observational data and distributes this toolkit to the community in the most flexible way possible. Building on the pilot project supported by the NSF EarthCube program, the specific features of the toolkit include: (a) linking diverse data sets from multiple observational data repositories; (b) using these data within a shareable computing environment, complete with a full selection of standard numerical and data-processing tools; and  (c) enabling an ever growing assortment of targeted, user-contributed tools, such as assimilative models or interpolation methods. The toolkit can be accessed and used either through a web-based computing environment or by downloading Docker containers for local installation, with a nearly seamless transition between the two. The toolkit is open-source and ensures credit attribution to data and software contributors, and the InGeO team works with members of scientific community (especially students and early career researchers) to improve the toolkit and build a sustainable, more connected user-base.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931347","Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA)","OAC","Software Institutes","11/01/2019","07/19/2019","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Seung-Jong Park","10/31/2024","$1,000,000.00","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers.  Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research.  The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.<br/><br/>Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1909900","OAC Core: SHF: Small: Enabling Rapid Design and Deployment of Deep Learning Models on Hardware Accelerators","OAC","OAC-Advanced Cyberinfrast Core","06/01/2019","05/20/2019","Tushar Krishna","GA","Georgia Tech Research Corporation","Standard Grant","Seung-Jong Park","05/31/2022","$500,000.00","","tushar@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","090Y","026Z, 9179","$0.00","Machine Learning (ML) has rapidly emerged as one of the foundational technologies of this century. It is pervasive in our lives today, from allowing us to unlock smartphones, to powering recommendation engines for almost any human activities (dinning, movies, services etc). The applications of ML are expected to become even more transformative in the future, especially in healthcare, autonomous transport, robotics, agriculture, education, and space exploration. ML models are computationally expensive, need large amounts of memory to store the trained model, and have strict runtime requirements. They cannot be run efficiently on general-purpose processors, which has led to an explosive growth in custom hardware accelerators for ML. However, getting good performance and energy-efficiency from these accelerators is itself challenging, as it relies on three components: the ML model itself, the hardware parameters, and the scheduling of computations in the ML model onto the limited compute and memory resources on the accelerator. The proposed research will develop an open-source software cyberinfrastructure called MAESTRO that can be used to analytically determine the performance and energy-efficiency of ML models over target hardware platforms, prior to actually building the hardware and deploying the model. MAESTRO will be extremely useful for students, researchers, and industry practitioners alike to learn about, design, and deploy custom ML solutions. The project will also engage undergraduate and high-school students to teach them about ML through outreach activities involving hackathons and hardware building.<br/><br/><br/>Mapping ML computations over finite compute elements within an accelerator, and understanding the corresponding data that needs to move across the memory hierarchy is a non-trivial problem; the space of all possible ways of slicing and dicing the model (known as ""dataflow"") is exponentially complex, and the benefits of any mapping vary across ML models and target accelerator. To address this, the PI will first develop a set of data-centric directives to directly describe the mapping of the ML model over the accelerator, which will enable precise calculations of data reuse opportunities across space and time to reduce overall data movement. Next, the PI will develop the MAESTRO analytical cost model framework to estimate reuse, end-to-end performance, and energy over the target hardware. Finally, a set of tools will be developed around MAESTRO to automatically search for and determine the optimal hardware/mapping/model given constraints of runtime, power, energy, or area.  The proposed framework will enable iterative innovation and co-design across the ML model, mapping and target hardware, and will therefore be highly valuable for ML model developers, compiler writers and computer architects. MAESTRO will be released and maintained on an open-source license, and the PI will run periodic tutorials to build an active user-base in the research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2101697","Detail between Naval Postgraduate School and National Science Foundation for Dr. Robert Beverly.(Year 1-continuation)","OAC","","10/09/2020","11/18/2020","Robert Beverly","CA","Naval Postgraduate School","Contract Interagency Agreement","Carl Anderson","04/09/2021","$109,145.00","","rbeverly@nps.edu","1 University Circle","Monterey","CA","939435000","8316562271","CSE","0119","","$0.00",""
"1835893","Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet","OAC","Data Cyberinfrastructure, Software Institutes","01/01/2019","09/06/2018","Luke Harmon","ID","Regents of the University of Idaho","Standard Grant","Seung-Jong Park","12/31/2022","$122,506.00","","lukeh@uidaho.edu","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","7726, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system.  <br/><br/><br/>MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1739423","SI2-SSE: A parallel computing framework for large-scale real-space and real-time TDDFT excited-states calculations","OAC","DMR SHORT TERM SUPPORT, Software Institutes","02/15/2018","02/02/2018","Eric Polizzi","MA","University of Massachusetts Amherst","Standard Grant","Robert Beverly","01/31/2022","$485,854.00","","polizzi@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1712, 8004","026Z, 7237, 7569, 8004, 8005","$0.00","The ability to control electronic materials and understand their properties has been a driving force for technological breakthroughs. The technology for electronic devices has been on a rapidly rising trajectory since the 1960s with the ability to fabricate ever smaller silicon transistors (`Moore's Law'), with today's device sizes in the nanometer range. With the rise of nanotechnology, atom-by-atom quantum simulations of emerging materials are becoming increasingly important to reliably supplement the current experimental investigations. Modeling and simulations of atomic systems are essential to assist the everyday work of numerous engineers and scientists and can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology) spanning the technological fields of computing, sensing and energy. This project will accelerate the development of quantum technologies and their impacts in the global economy. A new software will be produced to help capture many fundamental quantum effects which are increasingly important in nanotechnology for exploring and prototyping new revolutionary electronic materials and devices.<br/><br/>This project aims at developing and offering a new open source software, NESSIE, that can address the modern challenges encountered in material and device nano-engineering applications.  NESSIE will use the most cost-effective method to perform excited states calculations, the time-dependent density functional theory (TDDFT), in conjunction with a novel combination of numerical algorithms and physical and mathematical modeling techniques.  NESSIE will be capable of performing excited-state TDDFT calculations using full-potential (all-electron) in real-space (using finite element) and real-time. A new hierarchical parallelization strategy will allow NESSIE to tackle unprecedented atomistic finite size systems at this level of theory. The outcome of this project will open new perspectives for addressing the numerical challenges in real-time TDDFT excited-states calculations to operate the full range of electronic spectroscopy, and study the nanoscopic many-body effects in arbitrary complex molecules and very large-scale finite-size nanostructures. It is expected that the NESSIE software and associated numerical components will become a new valuable new tool for the scientific community, that could be applied to investigate the fundamental electronic properties of numerous nanostructured materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"2008772","OAC Core: Small: Architecture and Network-aware Partitioning Algorithms for Scalable PDE Solvers","OAC","OAC-Advanced Cyberinfrast Core","10/01/2020","05/13/2020","Hari Sundar","UT","University of Utah","Standard Grant","Seung-Jong Park","09/30/2023","$499,317.00","","hari@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","090Y","7923, 9150","$0.00","Solving large-scale partial differential equations (PDE) is common in science and engineering, from studying gravitational waves to designing aerodynamic cars. Given the scale of these problems, solving such PDEs requires supercomputing resources. The latest supercomputing architectures are different from the previous generation of leadership class architectures and and are characterized by high levels of diversity within and across machines. Such diversity and heterogeneity makes it extremely difficult to effectively distribute work, i.e., partition the data or tasks, across disparate computing resources. Since the primary objective of building leadership class machines is to further scientific discovery and national prosperity, it is essential that applications, old and new, are able to scale and utilize these machines to their full potential. This project develops novel data and task partitioning algorithms that factor in the architectural characteristics of modern supercomputers to enable efficient and scalable utilization of current and future computing architectures.<br/><br/>Existing data and task partitioning schemes do not explicitly consider the underlying architectural topology while partitioning or is done indirectly during the design of the algorithm and codes. Ignoring topology leads to loss of scalability and performance, while shifting the burden to algorithm/code design increases the development costs and complexity, and decreases portability.  While data and task partitioning for parallelization, along with mapping the partitions to processes, have been studied for a long time, they have not been considered as a combined problem. To a large extent this was due to the simple structures and symmetry that existed in cluster computing architectures. Inefficiencies that did not significantly impact performance and scalability ten years ago are starting to inhibit scalability and thereby scientific discovery. This project is a first step in ensuring that scientific discovery is not hampered as a result of the difficulty in porting codes to new architectures. This project develops new graph and space-filling-curve-based partitioning algorithms that are aware of the architectural topology and are able to automatically generate data/task partitions and mappings to address problems with current schemes. The algorithms and software developed as part of this proposal will have wide-ranging impact, by improving the performance and scalability of legacy applications and reducing the development cost and improving the portability of new applications, to run efficiently on systems ranging from simple shared memory architectures to the largest heterogeneous clusters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931592","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","Software Institutes","01/01/2020","09/04/2019","Kyle Niemeyer","OR","Oregon State University","Standard Grant","Seung-Jong Park","12/31/2022","$317,419.00","","kyle.niemeyer@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931539","Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains","OAC","Software Institutes","01/01/2020","09/04/2019","Bryan Weber","CT","University of Connecticut","Standard Grant","Seung-Jong Park","12/31/2022","$132,245.00","","bryan.weber@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.<br/><br/><br/>This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931372","Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering","OAC","Special Initiatives, Software Institutes","01/01/2020","08/14/2019","Neelesh Patankar","IL","Northwestern University","Standard Grant","Seung-Jong Park","12/31/2024","$619,718.00","","n-patankar@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","1642, 8004","026Z, 028E, 077Z, 7925, 8004, 8084, 9263","$0.00","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.<br/><br/>This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829415","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Wei Cheng","VA","Virginia Commonwealth University","Standard Grant","chun-hsi huang","10/31/2018","$100,000.00","","uwcheng@uw.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1946194","Towards the Future Direction of the NSF Program on the Cyberinfrastructure for Sustained Scientific Innovation (CSSI)","OAC","Software Institutes","08/15/2019","08/22/2019","Ritu Ritu","TX","University of Texas at Austin","Standard Grant","Robert Beverly","07/31/2020","$50,000.00","Xiaosong Li","ritu.arora@utsa.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 7556, 8004","$0.00","The NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program has been critical in not only supporting the development and deployment of innovative software and data products, but also in several workforce development and community-building initiatives. The projects funded through the CSSI program have promoted the nation's competitiveness and leadership in the fields of data, High-Performance Computing (HPC), networking, cybersecurity, software, and workforce development. A workshop on the ""Future Direction of the NSF CSSI Program"" can be instrumental in gathering the community-feedback on the current scope and objectives of the CSSI program and guiding its future direction such that the nation's leadership in the aforementioned fields is maintained. <br/><br/>The goal of the workshop on the ""Future Direction of the NSF CSSI Program"" is to bring together researchers and practitioners from the industry, academia, and government laboratories to assess the impact of the CSSI program, share best practices in supporting sustainable software and data products in present and future, and identify gaps (if any) in the scope of the CSSI program and needs of the community. A report resulting from this workshop will help inform NSF of future directions for the CSSI program. The workshop will strengthen multi-disciplinary collaborations on developing the cyberinfrastructure (software and data) for supporting research and innovation in different fields of science and technology. This project is critical for enabling the organization of the aforementioned workshop and supporting the participation of a diverse range of researchers and practitioners in the development of the future CSSI solicitations. Some participants may need financial support for traveling to the workshop. This project will help in supporting the travel of such participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835656","Framework: Sofware: Collaborative Research: CyberWater -An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","Data Cyberinfrastructure","01/01/2019","09/07/2018","Yang Zhang","NC","North Carolina State University","Standard Grant","Stefan Robila","03/31/2020","$95,000.00","","ya.zhang@northeastern.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2007775","OAC Core: SHF: SMALL: ICURE -- In-situ Analytics with Compressed or Summary Representations for Extreme-Scale Architectures","OAC","OAC-Advanced Cyberinfrast Core","07/01/2020","05/20/2020","Gagan Agrawal","OH","Ohio State University","Standard Grant","Robert Beverly","07/31/2020","$500,000.00","","gagrawal@augusta.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","090Y","7923","$0.00","Systems for High Performance Computing (HPC) have been providing rapidly increasing computing power. However, this growth has also led to systems where the memory and data movement bandwidth is relatively lower.  This makes analyzing the data from scientific simulations very challenging.  A paradigm called in-situ analytics has emerged in response.  This project is further improving this paradigm, by using what can be referred to as homomorphic compressions.  The idea of homomorphic compression is to compress the data in a way that queries can be directly executed on the compressed data (without need for decompression).  This project is developing such compression methods, developing techniques to perform such compression efficiently on Graphic Processing Units (GPUs), techniques for query processing using such compressed representations, and finally, an overall system that will simplify development of in-situ analytics implementations.   Overall, this project will be making analysis of data from simulations more effective on the upcoming systems for HPC.  This project will seek to broaden participation in computing through direct participation in the project development teams by undergraduate and graduate students from under-represented groups.  <br/><br/>Systems for High Performance Computing (HPC) have been providing rapidly increasing computing power. However, this growth has also led to systems where the memory and data movement bandwidth is relatively lower.  This makes analyzing the data from scientific simulations very challenging.  A paradigm called in-situ analytics has emerged in response.  This project is further improving this paradigm, by using what can be referred to as homomorphic compressions.  The idea of homomorphic compression is to compress the data in a way that queries can be directly executed on the compressed data (without need for decompression).  The resulting framework, ICURE, can facilitate in situ analytics on accelerators themselves, reduce overall memory requirements for the analytics, reduce total data movements costs, and even reduce the time cost of performing the analytics.  Achieving the goals of ICURE involves many open challenges. The first is the choice of summarization structure and its constructions. This project experiments with two different summary or concise representations:  bitmap indices and an integrated value index. The second issue is analyses methods using summary and compressed representations, where the focus is on the use of these representations for a variety of analyses tasks: computing aggregations, correlations, value-based joins, time-step selection, and interesting subregions analysis.   The third issue is automating placement and quality. Driven by the consideration of providing the lowest   interference between the simulation and analytics, this project automates decisions on placement of specific analytics operations and data within the node of HPC system. Similarly, automatic selection of sampling level driven by desired accuracy and overheads of the analyses is performed. This project will seek to broaden participation in computing through direct participation in the project development teams by undergraduate and graduate students from under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907821","OAC Core: Small: Devising Data-driven Methodologies by Employing Large-scale Empirical Data to Fingerprint, Attribute, Remediate and Analyze Internet-scale IoT Maliciousness","OAC","OAC-Advanced Cyberinfrast Core","07/01/2019","07/01/2019","Elias Bou-Harb","FL","Florida Atlantic University","Standard Grant","Robert Beverly","10/31/2019","$496,898.00","Nasir Ghani, Jorge Crichigno","elias.bouharb@utsa.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","090Y","026Z, 9179","$0.00","At least 20 billion devices will be connected to the Internet by 2023. Many of these devices transmit critical and sensitive system and personal data in real-time. Collectively known as ""the Internet of Things"" (IoT), this market represents a $267 billion per year industry. As valuable as this market is, security spending on the sector barely breaks 1%. Indeed, while IoT vendors continue to push more IoT devices to market, the security of these devices has often fallen in priority, making them easier to exploit. This drastically threatens the privacy of the consumers and the safety of mission-critical systems. While a number of research endeavors are currently taking place to address the IoT security problem, several challenges hinder their success. These include the lack of IoT monitoring capabilities once such devices are deployed, the shortage of remediation techniques when they are compromised, and the inadequacy of methodologies to permit the comprehension of the underlying IoT malicious infrastructures. To this end, this project will serve NSF's mission to promote the progress of science by developing data science methodologies to identify and remediate infected IoT devices in near real-time. The project will also promote cyber security research and training for minorities and K-12 students. Moreover, the project will contribute to operational cyber security by developing a large-scale cyberinfrastructure for IoT-relevant data and threat sharing, enabling hands-on cyber-science at large. <br/> <br/>The project will scrutinize close to 100 GB/hr of real-time unsolicited Internet-scale traffic to devise and develop efficient deep learning classifiers to fingerprint IoT devices, identifying their types and vendors, and disclosing their large-scale vulnerabilities and hosting environments. The project will design and develop fast greedy approximation algorithms for L1-norm Principal Component Analysis (PCA) data-dimensionality reduction, enabling the real-time execution of the Density Based Spatial Clustering of Application with Noise (DBSCAN) technique for detecting and attributing IoT orchestrated botnets. The project will also design scalable offensive security algorithms based on Internet-wide active measurements to offer macroscopic remediation strategies. The project will curate close to 3.5 million malware samples/day and around 1.3 million passive DNS records/day to build graph-theoretic models to uncover and characterize inter-related components which form the concept of IoT malicious cyberinfrastructure. Further, the project will analyze the evolution of such infrastructures to comprehend their modus operandi by devising efficiency graph similarity techniques in linear time, by designing and implementing algorithms rooted in graph kernels and min-hashing methods. The project will also (i) develop a unique cyberinfrastructure for IoT empirical data and cyber threat indexing and sharing, (ii) automate the devised algorithms and techniques by leveraging high speed, in-memory data processing technologies, (iii) generate IoT-specific detection signatures by exploring fuzzy hashing algorithms, and (iv) enable at-large access to the generated IoT artifacts through a secure API and a front-end mechanism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939887","Collaborative Research: Accelerating Synthetic Biology Discovery & Exploration through Knowledge Integration","OAC","ICB: Infrastructure Capacity f, HDR-Harnessing the Data Revolu","10/01/2019","10/07/2020","Brandon Sepulvado","IL","National Opinion Research Center","Continuing Grant","Peter McCartney","09/30/2021","$123,744.00","","sepulvado-brandon@norc.org","1155 E. 60th Street","Chicago","IL","606372745","7732566000","CSE","085Y, 099Y","1165","$0.00","The scientific challenge for this project is to accelerate discovery and exploration of the synthetic biology design space.  In particular, many parts used in synthetic biology come from or are initially tested in a simple bacteria, E. coli, but many potential applications in energy, agriculture, materials, and health require either different bacteria or higher level organisms (yeast for example). Currently, researchers use a trial-and-error approach because they cannot find reliable information about prior experiments with a given part of interest. This process simply cannot scale. Therefore, to achieve scale, a wide range of data must be harnessed to allow confidence to be determined about the likelihood of success. The quantity of data and the exponential increase in the publications generated by this field is creating a tipping point, but this data is not readily accessible to practitioners. To address this challenge, our multidisciplinary team of biological engineers, machine learning experts, data scientists, library scientists, and social scientists will build a knowledge system integrating disparate data and publication repositories in order to deliver effective and efficient access to collectively available information; doing so will enable expedited, knowledge-based synthetic biology design research.<br/><br/>This project will develop an open and integrated synthetic biology knowledge system (SBKS) that leverages existing data repositories and publications to create a single interface that transforms the way researchers access this information. Access to up-to-date information in multiple, heterogeneous sources will be provided via a federated approach. New methods based on machine learning will be developed to automatically generate ontology annotations in order to create connections between data in various repositories and information extracted from publications.  Provenance for each entity in SBKS will be tracked, and it will be utilized by new methods that are developed to assess bias and assign confidence scores to knowledge returned for each entity. An intuitive, natural-language-based interface and visualization functionality will be implemented for users to easily access and explore SBKS contents.  Additionally, as ethics is necessarily a part of synthetic biology research, data from text sources related to ethical concerns in synthetic biology will also be incorporated to inform researchers about ethical debates relevant to their search queries.  Finally, to test the SBKS API, a new genetic design tool, Kimera, will be developed that leverages the knowledge in SBKS to produce better designs.  The proposed SBKS will accelerate discovery and innovation by enabling researchers to learn from others' past experiences and to maximize the productivity of valuable experimental time on testing designs that have a higher likelihood of working when transformed to a new organism.  This research thus provides the potential for transformative research outcomes in the field of synthetic biology by leveraging data science to improve the field's epistemic culture. For more information please see https://synbioks.github.io.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811236","Collaborative Research: Petascale Simulations of Binary Neutron Star Mergers","OAC","Leadership-Class Computing","04/01/2018","03/30/2020","David Radice","NJ","Princeton University","Standard Grant","Edward Walker","03/31/2020","$0.00","","david.radice@psu.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","The era of multimessenger astronomy has been inaugurated with the extraordinary detection of the collision of two neutron stars (NSs) by the Laser Interferometer Gravitational-Wave Observatory (LIGO), and the subsequent observations by X-ray, optical, infrared, and radio facilities. These observations have started to revolutionize our understanding of many areas in physics, including the origins of the heavy elements, like silver, gold, and platinum. However, they also pose many pressing open questions.  This project will make use of the Blue Waters supercomputer to address these open questions.  State-of-the-art NS merger simulations will be performed on the Blue Waters supercomputer to examine all the different possible NS collision scenarios to understand the fundamental physical processes that generated the observation data. This will be followed by extremely high-resolution simulations considering the post-merger evolution.  Simulation results will be made publicly available, and dissemination via movies, public lectures and school visits are planned.<br/><br/>A systematic study of the evolution of NS binary systems compatible with the observed NS collision, GW170817, will be performed by means of merger simulations on Blue Waters, employing sophisticated microphysics and neutrino treatment.  These simulations will ascertain the viability of tidal torques and shocks as mechanisms for the ejection of matter during mergers, which in turn powers the observed optical and infrared transients and synthesizes heavy elements. The impact of the NS equation of state (EOS) will be evaluated by considering a set of 3 EOSs spanning the range of the current nuclear uncertainties. The range of possible outcomes of the merger as a function of the binary parameters and EOSs will be assessed. High-resolution general-relativistic magnetohydrodynamics simulations of the merger remnant will be performed, with sufficient resolution to determine the magnetorotational instability (MRI) and the angular-momentum redistribution in the remnant while employing a microphysical treatment of the NS matter. These simulation results will ascertain the role of magnetohydrodynamics processes in determining the lifetime of the remnant and the observational signatures of an early or delayed black-hole formation. The role played by magnetized winds in the powering of the optical and infrared emissions and their nucleosynthetic yields will be assessed.   In addition, this project will develop significant improvements to current open-source codes, and all improvements will be released to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1940340","Collaborative Research: Biology-guided neural networks for discovering phenotypic traits","OAC","HDR-Harnessing the Data Revolu, CYBERINFRASTRUCTURE","10/01/2019","09/17/2019","Paula Mabee","SD","University of South Dakota Main Campus","Continuing Grant","Peter McCartney","04/30/2020","$217,642.00","","mabee@battelleecology.org","414 E CLARK ST","vermillion","SD","570692307","6056775370","CSE","099Y, 7231","1165, 7231, 9150","$0.00","Unlike genetic data, the traits of organisms such as their visible features, are not available in databases for analysis.  The lack of machine-readable trait data has slowed progress on four grand challenge problems in biology: predicting the genes that generate traits, understanding the patterns of evolution, predicting the effects of ecological change, and species identification. This project will use advances in machine learning and machine-readable biological knowledge to create a new method to automatically identify traits from images of organisms.  Images of organisms are widely available, and this new method could be used to rapidly harvest traits that could be used to solve the grand challenges in biology.  Large image collections and corresponding digital data from fishes will be used in this study because of the extensive resources available for these organisms. The new machine learning model can be generalized to other disciplines that have similar machine-readable knowledge, and it will help in explaining the results of artificial intelligence, thus advancing the field of computer science.  The new method stands to benefit society in application to areas such as agriculture or medicine, where trait discovery from images is critical in disease diagnosis.  The project will support the education of students and postdocs in biology, computer science, and information science.  It will disseminate its findings through workshops, presentations, publications, and open access to data and code that it produces. <br/><br/>This project will leverage advances in state-of-the-art machine learning to develop a novel class of artificial neural networks that can exploit the machine readable and predictive knowledge about biology that is available in the form of phylogenies and anatomy ontologies.  These biology-guided neural networks are expected to automatically detect and predict traits from specimen images, with little training data. Image-based trait data derived from this work will enable progress in gene-phenotype mapping to novel traits and understanding patterns of evolution. The resulting machine learning model can be generalized to other disciplines that have formally structured knowledge, and will contribute to advances in computer science by going beyond black-box learning and making important advances toward Explainable Artificial Intelligence.  It may be extended to applied areas, such as agriculture or the biomedical domain. The research will be piloted using teleost fishes because of many high-quality data resources (digital images, evolutionary trees, anatomy ontology). Methods for automated metadata quality assessment and provenance tracking will be developed in the course of this project to ensure the results and processes are verifiable, replicable and reusable.  These will broadly impact the many domains that will adopt machine learning as a way to make discoveries from images. This convergent research will accelerate scientific discovery across the biological sciences and computer science by harnessing the data revolution in conjunction with biological knowledge.<br/><br/>This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity, and is jointly supported by the HDR and the Division of Biological Infrastructure within the NSF Directorate of Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839321","CICI: SSC: Real-Time Operating System and Network Security for Scientific Middleware","OAC","Cybersecurity Innovation","10/01/2018","08/17/2018","Gedare Bloom","DC","Howard University","Standard Grant","Robert Beverly","12/31/2019","$999,915.00","","gbloom@uccs.edu","2400 Sixth Street N W","Washington","DC","200599000","2028064759","CSE","8027","","$0.00","Remote monitoring and control of industrial control systems are protected using firewalls and user passwords. Cyberattacks that get past firewalls have unfettered access to command industrial control systems with potential to harm digital assets, environmental resources, and humans in proximity to the compromised system.  To prevent and mitigate such harms in scientific industrial control systems, this project enhances the security of open-source cyberinfrastructure used for high energy physics, astronomy, and space sciences. The results of this project enhance the security of scientific instruments used in particle accelerators, large-scale telescopes, satellites, and space probes. The benefits to science and the public include greater confidence in the fidelity of experimental data collected from these scientific instruments, and increased reliability of scientific cyberinfrastructure that reduces the costs associated with accidental misconfigurations or malicious cyberattacks.<br/><br/>The objective of this project is to enhance the security of the open-source Real-Time Executive for Multiprocessor Systems (RTEMS) real-time operating system and the Experimental Physics and Industrial Control System (EPICS) software and networks; RTEMS and EPICS are widely used cyberinfrastructure for controlling scientific instruments. The security enhancements span eight related project activities: (1) static analysis and security fuzzing as part of continuous integration; (2) cryptographic security for the open-source software development life cycle; (3) secure boot and update for remotely-managed scientific instruments; (4) open-source cryptographic libraries for secure communication; (5) real-time memory protection; (6) formal modeling and analysis of network protocols; (7) enhanced security event logging; and (8) network-based intrusion detection for scientific industrial control systems. The project outcomes provide a roadmap for enculturating cybersecurity best practices in open-source, open-science communities while advancing the state-of-the-art research in cyberinfrastructure software engineering and industrial control system security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1924272","Collaborative Research: CyberTraining: Conceptualization: Planning a Sustainable Ecosystem for Incorporating Parallel and Distributed Computing into Undergraduate Education","OAC","CyberTraining - Training-based","09/01/2019","09/20/2019","Sushil Prasad","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Almadena Chtchelkanova","12/31/2019","$423,921.00","","Sushil.prasad@utsa.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y","026Z","$0.00","In this era of pervasive multicore machines, GPUs, cloud services, big data, machine learning, and the Internet of Things, there is a critical need for an institute to create a sustainable, discipline-wide ecosystem for incorporating parallel and distributed computing (PDC) into undergraduate computing curricula. Such an institute would support the community of educators, students, and other stakeholders, with the goal of developing a workforce that is ready to meet the challenges of working with current and future computing fabrics. The investigators propose planning for such an institute (iPDC) that can help eliminate the longstanding barrier of the sequential computing paradigm such that, analogous to the establishment of the object oriented paradigm, the PDC paradigm is naturally integrated into Computer Science (CS) and Computer Engineering (CE) curricula across various institutions as recommended by the 2013 ACM/IEEE Computer Science Curricula and now by ABET.<br/><br/>Through the network of funded and unfunded collaborators, established contacts with instructors at institutions serving underrepresented groups, and outreach efforts, the project will robustly engage with stakeholder communities through four well-structured planning workshops, weekly teleconferences, and feedback and dissemination activities to formulate the key attributes of the institute.Broadening PDC education will further enable advances in science and engineering, which depend increasingly on PDC systems, by providing the next generation of practitioners and researchers with the necessary skills and knowledge to effectively exploit them. The curriculum standards, adoption, and dissemination activities will have synergistic international components. Overall, this project will facilitate a rich exchange of ideas within the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2017624","CyberTraining: Pilot: EBCS-TMPS: An Evidence Based Cybersecurity Training and Mentorship Program for Students","OAC","CyberTraining - Training-based, Secure &Trustworthy Cyberspace","07/01/2020","06/24/2020","David Maimon","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Joseph Whitmeyer","06/30/2022","$298,189.00","Robert Harrison, Richard Baskerville, Yubao Wu","dmaimon@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y, 8060","","$0.00","The Evidence Based Cybersecurity Training and Mentorship Program for Students (EBCS-TMPS) leverages an existing cyberinfrastructure to prepare students to become effective cybersecurity and law enforcement researchers.  Through this program, students are exposed to theoretical knowledge, practical skills, and hands-on experience that are most sought by governmental, industrial and law enforcement organizations.  This is accomplished using a standalone cyberinfrastructure created and maintained by researchers and practitioners in the Evidence Based Cybersecurity Research group in Georgia State University that enables the production of evidence-based cybersecurity research and supports the training of cybercrime researchers.  Among ultimate benefits are improved cybersecurity and a more secure and crime-free online environment.<br/><br/>The EBCS-TMPS engages academics and practicing cybersecurity professionals as co-creators of research outcomes.  During a two-months-long summer workshop, existing and strong academic-industry-law-enforcement partnerships are leveraged to train 30 undergraduate and graduate students in the deployment of better scientific efforts aimed at advancing CI-enabled and reliable research. The training program implements a wide range of activities to deliver key methodological and technical skills that are necessary for developing a cost effective cybersecurity posture for organizations (for industry track students), as well as for enforcing policies and laws aimed at disrupting online criminal activities in an effective manner (for both industry and law enforcement tracks). The interdisciplinary team of educators (composed of Criminology, Computer Science and Computer Information System scholars) supports students? development of knowledge in cybersecurity areas such as gathering and preserving evidence, implementing effective security policies, and technical aspects of security. The industry and law enforcement mentors support both the development of an industry-relevant curriculum for the EBCS-TMPS, and guided internships with their teams.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2028064","Detail between Naval Postgraduate School and National Science Foundation for Dr. Robert Beverly.(Year 1)","OAC","","04/01/2020","06/05/2020","Robert Beverly","CA","Naval Postgraduate School","Contract Interagency Agreement","Carl Anderson","10/01/2020","$153,192.00","","rbeverly@nps.edu","1 University Circle","Monterey","CA","939435000","8316562271","CSE","0119","","$0.00",""
"1811352","Collaborative Research: Petascale Simulations of Binary Neutron Star Mergers","OAC","XD-Extreme Digital, Leadership-Class Computing","04/01/2018","07/02/2019","Philipp Moesta","CA","University of California-Berkeley","Standard Grant","Edward Walker","09/30/2019","$6,585.00","","pmoesta@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7476, 7781","026Z","$0.00","The era of multimessenger astronomy has been inaugurated with the extraordinary detection of the collision of two neutron stars (NSs) by the Laser Interferometer Gravitational-Wave Observatory (LIGO), and the subsequent observations by X-ray, optical, infrared, and radio facilities. These observations have started to revolutionize our understanding of many areas in physics, including the origins of the heavy elements, like silver, gold, and platinum. However, they also pose many pressing open questions.  This project will make use of the Blue Waters supercomputer to address these open questions.  State-of-the-art NS merger simulations will be performed on the Blue Waters supercomputer to examine all the different possible NS collision scenarios to understand the fundamental physical processes that generated the observation data. This will be followed by extremely high-resolution simulations considering the post-merger evolution.  Simulation results will be made publicly available, and dissemination via movies, public lectures and school visits are planned.<br/><br/>A systematic study of the evolution of NS binary systems compatible with the observed NS collision, GW170817, will be performed by means of merger simulations on Blue Waters, employing sophisticated microphysics and neutrino treatment.  These simulations will ascertain the viability of tidal torques and shocks as mechanisms for the ejection of matter during mergers, which in turn powers the observed optical and infrared transients and synthesizes heavy elements. The impact of the NS equation of state (EOS) will be evaluated by considering a set of 3 EOSs spanning the range of the current nuclear uncertainties. The range of possible outcomes of the merger as a function of the binary parameters and EOSs will be assessed. High-resolution general-relativistic magnetohydrodynamics simulations of the merger remnant will be performed, with sufficient resolution to determine the magnetorotational instability (MRI) and the angular-momentum redistribution in the remnant while employing a microphysical treatment of the NS matter. These simulation results will ascertain the role of magnetohydrodynamics processes in determining the lifetime of the remnant and the observational signatures of an early or delayed black-hole formation. The role played by magnetized winds in the powering of the optical and infrared emissions and their nucleosynthetic yields will be assessed.   In addition, this project will develop significant improvements to current open-source codes, and all improvements will be released to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823372","Collaborative: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Michela Taufer","DE","University of Delaware","Standard Grant","Rajiv Ramnath","08/31/2018","$75,000.00","","taufer@utk.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084, 9150","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOÕs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
