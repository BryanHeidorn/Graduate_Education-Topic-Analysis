scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
dev.off()
wordcloud(
words,
probabilities,
scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
}
library(wordcloud)
# visualize topics as word cloud
for (TopicNum in 1:NumTopics){
set.seed(42)
topicToViz <- TopicNum # change for your own topic of interest
#topicToViz <- 1 # change for your own topic of interest
#topicToViz <- grep('telescope', topicNames)[1] # Or select a topic by a term contained in its name
# select to 100 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top100terms <-
sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:100]
words <- names(top100terms)
# extract the probabilites of each of the 100 terms
probabilities <-
sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:100]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
#topicFileName <- gsub(".csv", paste("Topic", toString(TopicNum), "WordCloud.png", sep = ""), filename)
png(paste(topicDir,"/WordCloud.png", sep=""), width = 1280, height = 800)
while (!is.null(dev.list()))  dev.off()
wordcloud(
words,
probabilities,
scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
dev.off()
wordcloud(
words,
probabilities,
scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
}
library(wordcloud)
# visualize topics as word cloud
#for (TopicNum in 1:NumTopics){
for (TopicNum in 1:18){
set.seed(42)
topicToViz <- TopicNum # change for your own topic of interest
#topicToViz <- 1 # change for your own topic of interest
#topicToViz <- grep('telescope', topicNames)[1] # Or select a topic by a term contained in its name
# select to 100 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top100terms <-
sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:100]
words <- names(top100terms)
# extract the probabilites of each of the 100 terms
probabilities <-
sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:100]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
#topicFileName <- gsub(".csv", paste("Topic", toString(TopicNum), "WordCloud.png", sep = ""), filename)
png(paste(topicDir,"/WordCloud.png", sep=""), width = 1280, height = 800)
while (!is.null(dev.list()))  dev.off()
wordcloud(
words,
probabilities,
scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
dev.off()
wordcloud(
words,
probabilities,
scale = c(4, .2),
random.order = FALSE,
color = mycolors
)
}
library(ggplot2)
png(paste(topicDir,"/TermsLDAHistogram.png", sep=""), width = 1280, height = 1600)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 3, scales = "free")
#topicFileName <- gsub(".csv","TermsLDAHistogram.png", filename)
#ggsave(topicFileName, height = 14, device = "png")
#png(paste(topicDir,"/TermsLDAHistogram.png", sep=""), width = 1280, height = 1600)
while (!is.null(dev.list()))  dev.off()
# Calculate the top terms in each topic
top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 3, scales = "free")
#Top 10 terms in each model
terms(desc_lda, 10)
##Examine which topics are associated with which documents
lda_gamma <- tidy(desc_lda, matrix = "gamma")
lda_gamma
library(data.table)
# Change the index to be a column. nsf_funding_save is in funding order.
setDT(nsf_funding_save, keep.rownames = TRUE)[]
#Change the column name form the "rn" defaculty to "FundingRank"
colnames(nsf_funding_save)[1] <- "FundingRank"
# Change class of nsf_funding_save$id to charcater(from integer)
nsf_funding_save$id <- as.character(nsf_funding_save$id)
lda_gamma2 <- dplyr::left_join(lda_gamma, nsf_funding_save,
select(FundingRank), by = c("document" = "id"))
# convert FundingRank to a number
lda_gamma2$FundingRank = as.numeric(lda_gamma2$FundingRank)#[lda_gamma2]
# convert Topic to a character
# Sort the topics by the amont of monney they braught in. Sum of document Gamma = 1
# The funding associated with each topic is the
# sum of all (gamma*funding amount for each grant)
lda_gamma2 <- lda_gamma2 %>% mutate(worth = gamma * amount)
# calculate the sum of the worth for each topic
topic_worth <- aggregate(lda_gamma2$worth, by=list(topic = lda_gamma2$topic), FUN = "sum")
#topic_worth <- aggregate(lda_gamma2$worth, by=list(topic = as.character(lda_gamma2$topic)), FUN = "sum")
colnames(topic_worth)[2] <- "WorthSum"
# sort by most worthy of topic
topic_worth <- topic_worth %>% arrange(desc(WorthSum))
# save topic_worth as CVS
setDT(topic_worth, keep.rownames = "TopicWorthRank")[]
lda_gamma3 <- left_join(lda_gamma2, topic_worth,
select(x, TopicWorthRank), by = "topic")
lda_gamma3$TopicWorthRank = as.numeric(lda_gamma3$TopicWorthRank)
p <- ggplot(lda_gamma3, aes(x=FundingRank, y=TopicWorthRank)) +
geom_point(aes(size=gamma), shape = 17) +   # draw points
scale_size_continuous(limits=c(.2,1)) +
xlim(c(0, length(nsf_funding_save$id))) +
ylim(c(1, NumTopics)) +
labs(subtitle="Topic by Document",
y="Topic #",
x="Document #",
title="Gamma Level sorted by Funding Level",
caption="Topic Analysis")
p
ggsave(paste(topicDir,"/DocTopic.png", sep=""), height = 10, device = "png")
#png(paste(topicDir,"/TermsLDAHistogram.png", sep=""), width = 1280, height = 1600)
while (!is.null(dev.list()))  dev.off()
p
#Join funding amount to data
nsf_funding_join <- nsf_funding_save
nsf_funding_join$document <- as.character(nsf_funding_join$id)
funding_gamma <- merge(lda_gamma, nsf_funding_join, by="document", all.x = TRUE)
funding_gamma <- subset(funding_gamma, select=-c(id, abstract))
funding_gamma
#Need to join lda_gamma with funding amount vector, that would be informative
doc_top <- funding_gamma %>%
group_by(document) %>%
#  top_n(1, gamma) %>%
top_n(5, gamma) %>%
ungroup() %>%
#  arrange(gamma)
arrange(document, desc(gamma))
write.csv(doc_top, paste(topicDir,"/topTopics5Documents.csv", sep=""))
doc_top
#Consensus document for each topic
document_topics <- doc_top %>%
count(document, topic, amount) %>%
group_by(topic) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = document, topic, amount)
document_topics
#See which documents misidentified, this is probably not useful for us, since documents can easily be in multiple topics
doc_top %>%
inner_join(document_topics, by = "topic") %>%
count(document, consensus)
library(flexdashboard)
library(shiny)
library(dplyr)
library(stm)
library(quanteda)
library(tidytext)
library(ggplot2)
library(scales)
p <- funding_gamma %>%
mutate(document = factor(document, levels = rev(unique(document)))) %>%
group_by(document) %>%
top_n(1) %>%
ungroup %>%
ggplot(aes(reorder(document, amount), y=gamma, label = NA, fill = as.factor(topic))) +
geom_col() +
geom_text(aes(document, 0.01), hjust = 0,
color = "white", size = .5) +
scale_fill_manual(values = c("#F48024", "#0077CC", "#5FBA7D",
"#8C60A7", "#89C5DA", "#DA5724", "#74D944", "#CE50CA", "#3F4921", "#C0717C", "#CBD588", "#5F7FC7",
"#673770", "#D3D93E", "#38333E", "#508578", "#D7C1B1", "#689030", "#AD6F3B", "#CD9BCD",
"#D14285", "#6DDE88", "#652926", "#7FDCC0", "#C84248", "#8569D5", "#5E738F", "#D1A33D",
"#8A7C64", "#599861", "#A48024", "#B077CC", "#CFBA7D", "#DFBA7D")) +
scale_y_continuous(expand = c(0,0),
labels = percent_format()) +
coord_flip() +
theme_minimal() +
theme(axis.text.y=element_blank()) +
labs(x = NULL, y = expression(gamma), fill = "Topic")
p
ggsave(paste(topicDir,"/TopicGammaMap.png", sep=""), height = 10, device = "png")
p
# This chink does a multidimentional scaling intertopic distance map.
# Required packages
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(devtools)
devtools::install_github("cpsievert/LDAvisData")
devtools::install_github("cpsievert/LDAvis")
#' Transform Model Output for Use with the LDAvis Package
#' https://gist.github.com/trinker/477d7ae65ff6ca73cace
#' Convert a \pkg{topicmodels} output into the JSON form required by the \pkg{LDAvis} package.
#'
#' @param model A \code{\link[]{topicmodel}} object.
#' @param \ldots Currently ignored.
#' @seealso \code{\link[LDAvis]{createJSON}}
#' @export
#' @examples
#' \dontrun{
#' data("AssociatedPress", package = "topicmodels")
#' model <- LDA(AssociatedPress[1:20,], control = list(alpha = 0.1), k = 3)
#' LDAvis::serVis(topicmodels2LDAvis(model))
#' }
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
lda_json <- topicmodels2LDAvis(desc_lda)
serVis(lda_json, reorder.topic = TRUE)
library(RJSONIO)
x <- fromJSON(lda_json)
### this will map the tpic numbers from the rest of the analysis to the multidimentional scaling topic numbers.
print("Original - MDS#")
for( i in 1:NumTopics) {
print(paste(i, which(x$topic.order == i)))
}
# Generate bigrams
# Bigram setup
library(tidyr)
#nsf_funding_bigram %>%
#  count(bigram, sort = TRUE)
# remove bigrams with stopwords
bigrams_separated <- nsf_funding_bigram %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)#replace all of the original words in the nsf_funding with lemmas
# This also needs to be done for the bigrams where the substatution happens for each of the two columns in the bigram. ???
lemma_bigram_new <- lemma_unique
#replace word 1 with lemma
lemma_bigram_new <- left_join(bigrams_filtered,lemma_bigram_new, by = c("word1" = "word")) %>%
mutate(word1 = ifelse(is.na(lemma), word_clean, lemma)) # %>%
# drop last three columns id, word1 and word2
lemma_bigram_new <- subset(lemma_bigram_new, select = c(id, word1, word2))
lemma_bigram_new <- left_join(lemma_bigram_new, lemma_unique, by = c("word2" = "word")) %>%
mutate(word2 = ifelse(is.na(lemma), word_clean, lemma)) # %>%
lemma_bigram_new <- subset(lemma_bigram_new, select = c(id, word1, word2))
#nsf_funding <- subset()
# new bigram counts:
bigram_counts <- lemma_bigram_new %>%
count(word1, word2, sort = TRUE)
bigram_counts
bigrams_united <- lemma_bigram_new %>%
unite(bigram, word1, word2, sep = " ")
bigrams_united %>%
count(bigram, sort = TRUE)
#
library(igraph)
bigram_graph <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#topicFileName <- gsub(".csv","bigraph.png", filename)
#png(topicFileName, width = 12, height = 8, units = 'in', res = 300)
png(paste(topicDir,"/bigraph.png", sep=""), width = 12, height = 8, units = 'in', res = 300)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
dev.off()
library(igraph)
bigram_graph <- bigram_counts %>%
filter(n > 5) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#topicFileName <- gsub(".csv","bigraph.png", filename)
#png(topicFileName, width = 12, height = 8, units = 'in', res = 300)
png(paste(topicDir,"/bigraph.png", sep=""), width = 12, height = 8, units = 'in', res = 300)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
dev.off()
library(igraph)
bigram_graph <- bigram_counts %>%
filter(n > 10) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#topicFileName <- gsub(".csv","bigraph.png", filename)
#png(topicFileName, width = 12, height = 8, units = 'in', res = 300)
png(paste(topicDir,"/bigraph.png", sep=""), width = 12, height = 8, units = 'in', res = 300)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
dev.off()
library(igraph)
bigram_graph <- bigram_counts %>%
filter(n > 6) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
#topicFileName <- gsub(".csv","bigraph.png", filename)
#png(topicFileName, width = 12, height = 8, units = 'in', res = 300)
png(paste(topicDir,"/bigraph.png", sep=""), width = 12, height = 8, units = 'in', res = 300)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
dev.off()
bigram_tf_idf <- bigrams_united %>%
count(id, bigram) %>%
bind_tf_idf(bigram, id, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf
# Graph the bigrams. THIS WILL TAKE SOME TIME
# categories are compressed
p <- bigram_tf_idf %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(id) %>%
top_n(5) %>%
ungroup %>%
ggplot(aes(bigram, tf_idf, fill = id)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~id, ncol = 2, scales = "free") +
coord_flip()
#topicFileName <- gsub(".csv","BigramTFIDFHistogram.png", filename)
#ggsave(topicFileName, height = 14, device = "png")
p
ggsave(paste(topicDir,"/BigramTFIDFHistogram.png", sep=""), height = 30, device = "png")
#png(paste(topicDir,"/TermsLDAHistogram.png", sep=""), width = 1280, height = 1600)
while (!is.null(dev.list()))  dev.off()
#Top documents per topic
library(ggplot2)
library(ggrepel)
number_of_documents = 5 #number of top docs to view
title <- paste("LDA Top Documents for", NumTopics, "Topics")
top_documents <- lda_gamma %>%
group_by(topic) %>%
arrange(topic, desc(gamma)) %>%
slice(seq_len(number_of_documents)) %>%
arrange(topic, gamma) %>%
mutate(row = row_number()) %>%
ungroup() #%>%
#re-label topics
#mutate(topic = paste("Topic", topic, sep = " "))
#topicFileName <- gsub(".csv","top_documents.csv", filename)
write.csv(top_documents, paste(topicDir,"/top_documents.csv", sep=""))
top_bigrams_for_docs <- bigram_tf_idf %>%
group_by(id) %>%
arrange(id, desc(tf_idf)) %>%
slice(seq_len(number_of_documents)) %>%
arrange(id, tf_idf) %>%
ungroup()  %>%
select(id, bigram)
# make the 5 bigrams into 5 columns
top_bigrams_for_docs_wide <-
dcast(setDT(top_bigrams_for_docs), id~rowid(id, prefix="bigram"), value.var="bigram")
# add the bigram columns to top_docs
### error - repeats of bigrams for too many documents....
# maybe not an error... documents in the smae topic are getting many of the same bigrams.
top_documents <- left_join(top_documents, top_bigrams_for_docs_wide,
select(bigram1, bigram2, bigram3, bigram4, bigram5),
by = c("document" = "id"))
#Join funding amount and title to top_docs_titles
nsf_funding_titles <- data_frame(document = MyData$AwardNumber,
title = MyData$Title, amount = MyData$AwardedAmountToDate)
# Convert the document #/grant number to a charctaer sting
nsf_funding_titles$document <- as.character(nsf_funding_titles$document)
top_documents <- full_join(top_documents, nsf_funding_titles, by = "document")
#top_docs_titles <- merge(top_documents, nsf_funding_titles, by="document", all.x = TRUE)
#top_docs_titles <- subset(top_docs_titles, select=-c(row))
#top_documents <- full_join(top_documents, top_docs_titles, by = "document")
# add topic worth and rank
top_documents <- full_join(top_documents, topic_worth, by = "topic")
#topicFileName <- gsub(".csv","top_documents.csv", filename)
write.csv(top_documents, paste(topicDir,"/top_documents.csv", sep=""))
#top_docs_titles
#Top documents per topic
#FIGURE IS BAD AND NEEDS WORK, NOT REALLY A USEFUL VISUALIZATION ANYWAY
title <- paste("LDA Top Documents for", NumTopics, "Topics")
#define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00")
#customize ggplot2's default theme settings
#this tutorial doesn't actually pass any parameters, but you may use it again in future tutorials so it's nice to have the options
theme_lyrics <- function(aticks = element_blank(),
pgminor = element_blank(),
lt = element_blank(),
lp = "none")
{
theme(plot.title = element_text(hjust = 0.5), #center the title
axis.ticks = aticks, #set axis ticks to on or off
panel.grid.minor = pgminor, #turn on or off the minor grid lines
legend.title = lt, #turn on or off the legend title
legend.position = lp) #turn on or off the legend
}
#customize the text tables for consistency using HTML formatting
my_kable_styling <- function(dat, caption) {
kable(dat, "html", escape = FALSE, caption = caption) %>%
kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
full_width = FALSE)
}
word_chart <- function(data, input, title) {
data %>%
#set y = 1 to just plot one variable and use word as the label
ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +
#you want the words, not the points
geom_point(color = "transparent") +
#make sure the labels don't overlap
geom_label_repel(nudge_x = .2,
direction = "y",
box.padding = 0.1,
segment.color = "transparent",
size = 3) +
facet_grid(~topic) +
theme_lyrics() +
theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
#axis.title.x = element_text(size = 9),
panel.grid = element_blank(), panel.background = element_blank(),
panel.border = element_rect("lightgray", fill = NA),
strip.text.x = element_text(size = 9)) +
labs(x = NULL, y = NULL, title = title) +
#xlab(NULL) + ylab(NULL) +
#ggtitle(title) +
coord_flip()
}
facet_wrap(~ topic, ncol = 10, scales = "free")
word_chart(top_documents, top_documents$document, title)
View(lda_gamma)
View(lda_gamma3)
#Probability distribution in each topic
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
geom_histogram(show.legend = FALSE) +
facet_wrap(~ topic, ncol = 4) +
scale_y_log10() +
labs(title = "Distribution of probability for each topic",
y = "Number of documents", x = expression(gamma))
View(top_bigrams_for_docs)
View(top_bigrams_for_docs_wide)
View(top_bigrams_for_docs)
View(top_documents)
View(top_bigrams_for_docs_wide)
View(top_bigrams_for_docs)
View(top_bigrams_for_docs_wide)
View(top_bigrams_for_docs)
View(top_documents)
View(top_bigrams_for_docs_wide)
