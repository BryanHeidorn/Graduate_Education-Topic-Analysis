"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1755880","CRII: SHF: Bespoke Data Representation Synthesis via Contextual Data Refinement","CCF","Software & Hardware Foundation","08/15/2018","01/05/2018","Benjamin Delaware","IN","Purdue University","Standard Grant","Anindya Banerjee","07/31/2021","$163,212.00","","bendy@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","7798","7943, 7944, 8228","$0.00","Nearly every modern programming language provides some mechanism for hiding the implementation details of reusable components behind some abstract interface. This interface acts as a contract, enforced by the language, that benefits both the developers and clients of such components: protecting the developers? design decisions from clients and enabling clients to safely swap in different implementations of the same interface. Recent advances in program synthesis have shown how custom implementations can be automatically built from high-level specifications of a client?s requirements, exploiting this contract to ensure that synthesized components satisfy the desired requirements. Existing approaches to language-enforced abstraction approaches can be too restrictive in this setting, however, as they require the synthesized implementation to work for any client. This disallows any implementations whose correctness are dependent on a particular client?s usage. The goal of this project is to relax this condition, enabling the synthesis of custom implementations that are tailored to a particular client while still providing the same strong abstraction guarantees that programmers expect from their programming languages. The intellectual merits are the development of a refined notion of modularity in programming languages, advancing the state of the art in the synthesis of correct, performant code. The project's broader significance and importance are the development of an approach that allows programmers to program against high-level abstractions without paying a performance penalty.<br/><br/>The project advances the state of the art in both the theoretical foundations of data abstraction and the development of verified software. The vehicle for the work's theoretical contributions is a formalization of a core calculus for data refinement. This calculus is used to reformulate the well-established notion of data refinement for abstract data types (ADTs) to incorporate information about a specific client's usage of an interface. A key component is the development of the metatheory proofs establishing that the standard property of representation independence under data refinement is preserved. This approach is used to improve the existing Fiat deductive synthesis framework, enabling clients to derive verified ADT implementations that are tailored to their particular usage. The augmented system is evaluated via the synthesis of custom implementations of the popular Haskell bytestring library for two open-source Haskell programs, using an existing derivation of a performant bytestring implementation in Fiat as a starting point.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750640","CAREER: A Stable Foundation for Trustworthy Data Analysis","CCF","Algorithmic Foundations","02/01/2018","02/25/2021","Jonathan Ullman","MA","Northeastern University","Continuing Grant","A. Funda Ergun","01/31/2023","$415,411.00","","jullman@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","1045, 7926, 9251","$0.00","Every day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner.  This project is about two such questions: (1) How can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?  Despite decades of research into methods for preventing false discovery, it remains a vexing problem for the scientific community.  (2) How can researchers use valuable but sensitive data to learn about a population without compromising the privacy of individuals in that data?  This task has proven to be quite delicate, and there have been several high profile attacks on supposedly anonymous datasets, causing a lack of confidence in the most commonly used approaches.  <br/><br/>Although they may seem unrelated, surprisingly, both of these questions can be addressed using stable algorithms---algorithms that are insensitive to small changes in their inputs.  In the past decade, differential privacy emerged as a strong form of algorithmic stability that guarantees a high degree of individual privacy, yet admits highly accurate data analysis.  More recently, differential privacy has been shown to prevent false discovery in interactive data analysis---the common scenario where the same dataset is analyzed repeatedly, which has been implicated in a ""statistical crisis in science.""<br/><br/>This project will take a unified approach to advancing the state-of-the-art in privacy and false discovery via algorithmic stability.  The main outcomes of this project will be building the theoretical foundations of interactive data analysis, developing new computationally efficient stable algorithms for central problems in these areas, understanding the limits of privacy and interactive data analysis both in theory and in practice, and broadening the reach of algorithmic stability to address other challenges in trustworthy data analysis."
"1750127","CAREER: New Mathematical Programming Techniques in Approximation and Online Algorithms","CCF","Special Projects - CCF, Algorithmic Foundations","02/01/2018","09/13/2021","Viswanath Nagarajan","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","A. Funda Ergun","01/31/2023","$508,002.00","","viswa@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","2878, 7796","1045, 7926, 9251","$0.00","Numerous applications in engineering, science and business are modeled and solved as combinatorial optimization problems. In today?s digital age, data collection and storage is inexpensive. The bottleneck lies in society's ability to solve increasingly larger problem instances, where the challenge typically stems from computational or informational limitations. This intractability can often be overcome by searching for approximately optimal instead of exactly optimal solutions. This project will design new mathematical-programming-based techniques that are broadly applicable in approximating combinatorial optimization problems. The project will strengthen connections of theoretical computer science to various fields of mathematics such as discrepancy theory, geometry, graph theory, optimization and probability. The PI will also collaborate with industry colleagues to disseminate the theoretical findings on some of the studied problems and assess their practical impact. The educational aspect of this project includes training undergraduate and graduate students, developing new course material and organizing a workshop for high-school teachers. <br/> <br/>Despite the wide range of possible combinatorial optimization problems, a common approach underlying numerous results in approximation and online algorithms is mathematical programming and rounding. This project will develop new techniques in this area by investigating (1) algorithms based on convex-programming hierarchies, (2) rounding algorithms based on recent advances in algorithmic discrepancy and (3) an online primal-dual framework for convex objectives. This project involves designing general algorithmic techniques (and identifying problem classes to which they apply) as well as improving the state-of-art on central problems such as k-Median, directed Steiner tree, the Beck-Fiala conjecture, unsplittable flow and online multicommodity routing. This project will also expand the applicability of the resulting techniques to areas such as combinatorics and operations research."
"1807311","Symposium on Discrete Algorithms Science (SODA) 2018 Travel Grant","CCF","Algorithmic Foundations","01/15/2018","01/11/2018","Clifford Stein","NY","Columbia University","Standard Grant","Tracy Kimbrel","12/31/2018","$15,000.00","","cliff@ieor.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7556, 7926","$0.00","This award will help support student and postdoctoral attendance at the Annual ACM/SIAM Symposium on Discrete Algorithms Science (SODA) 2018, which will be held January 7-10 2018 in New Orleans, Louisiana. SODA is co-sponsored by the Association for Computing Machinery (ACM) and the Society for Industrial and Applied Mathematics (SIAM) . SODA is the premier annual research conference in the field of discrete algorithms and one of the three premier conferences in theoretical computer science. SODA has been meeting annually since 1990 and in a typical year has over 300 attendees. It is co-located with two smaller workshops, ALENEX (Meeting on Algorithm Engineering and Experimentation) and ANALCO (Meeting on Analysis of Algorithms). SODA is attended by researchers from all over the world. The field of algorithms is a vibrant one, with high participation rates from young researchers, and many papers with student authors. For these student authors and student attendees, the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides. <br/><br/>The award will provide partial support to approximately twenty students, partly defraying the cost of travel and lodging. Efforts will be made to support students from under-represented groups."
"1817635","CAREER:Information Theoretic Methods for Private Information Retrieval and Search in Distributed Storage Systems","CCF","Comm & Information Foundations, Secure &Trustworthy Cyberspace","09/01/2017","04/05/2021","Salim El Rouayheb","NJ","Rutgers University New Brunswick","Continuing Grant","Phillip Regalia","02/28/2023","$639,961.00","","salim.elrouayheb@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797, 8060","1045, 7434, 7935, 9251","$0.00","The recent data revolution is driving many aspects of modern societal and economic progress. Most of this massive data is now stored in the cloud  to enable easy access for a myriad of users who wish to share information including, for example, photos, videos, publications, opinions, and scientific data. Unfortunately, this has come at the expense of the user's privacy whose online activity can be used to profile him/her, making large parts of the population an easy target for discrimination and possible persecution.  This research aims at addressing the privacy challenge of data in the cloud by focusing on the problem of Private Information Retrieval (PIR) and Search in distributed storage systems (DSSs). PIR schemes enable users to query data without revealing information about the queries and hence their personal preferences, tendencies, health, or other traits.<br/><br/>Classical information theoretic PIR schemes require data to be replicated, which is not a scalable solution given the exponential growth of data. This research aims at creating a unified framework for studying coding schemes that, in addition to providing data reliability, cater to the need of private queries. The focus of the proposed research is on (i) explicit constructions of codes and PIR schemes that address practical and important aspects of distributed storage, such as storage cost, network communication cost, disk reads, latency and computations; (ii) explicit constructions of codes and schemes for private keyword search; (iii) characterization of the fundamental limits and tradeoffs between reliability, privacy and the different system overheads; (iv) testing software implementations of the schemes on real genomic and social science data. The project also incorporates several educational and outreach efforts, including the development of new publicly accessible online content on information theory, security, and privacy in distributed storage systems as well as pre-college outreach through the Global Leaders Program at the PI's institution."
"1757636","REU Site: Research Experience for Undergraduates in High Efficiency Computing","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","07/19/2019","Euzeli dos Santos","IN","Indiana University","Standard Grant","Almadena Chtchelkanova","12/31/2022","$368,076.00","John Lee","eudossan@iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1139","9250","$0.00","Information Technology (IT) systems range from small embedded systems such as smartphones to large computer facilities such as data centers. Whether small or large, IT systems proportionately use lots of energy. They also waste a lot, as in shorter battery life or higher data-center operating costs, so improving their efficiency fulfills a critical national need. The objective of this project is to propose solutions to increase energy efficiency in IT  systems by: 1) developing a well-trained student workforce, and 2) proposing new techniques and approaches capable of reducing energy consumption and losses. Students will be introduced to the research area of applying power optimization techniques to IT systems. The target populations are underrepresented students and those from institutions that do not offer PhDs. In addition to academic mentoring by Indiana University Purdue University Indianapolis (IUPUI) faculty, this project includes mentors from industry.<br/><br/>Traditionally, IT curricula have been developed with disciplines that focus on parameters, such as performance and runtime optimization. Little or no attention has been devoted to achieving energy efficiency and consumption thresholds, especially from the software and algorithmic perspectives. This project will provide original experiences and singular opportunities for undergraduate students to work on innovative strategies to reduce energy consumption and losses in IT systems. In addition to the potential change in the career paths of the selected students, this project suggests how the electrical and computer engineering curricula can be impacted to incorporate energy efficiency in both hardware and software curricula.<br/><br/>This project will demonstrate novel methodologies in reducing energy consumption and losses within IT systems, which can be accomplished with a multidisciplinary approach. Topics will range from hardware to software solutions with efficiency as the main figure of merit. Traditionally, Multiple-Voltage Multiple-Frequency (MVMF) has been used to reduce energy consumption in computing systems. This project proposes MVMF2, which not only dynamically changes the computer clock frequency, but also dynamically changes power supply switching frequency for further loss reduction. While MVMF2 is considered a hardware optimization managed by software, the second research topic - a higher efficiency power supply based on a new power converter design - is purely a hardware solution. The third and fourth research topics are software solutions to reduce energy consumption in high memory demand applications and processing big matrix multiplication, respectively. The fifth research topic deals with energy consumption reduction techniques in wireless Internet of Things devices by modeling and optimizing power saving modes, such as reduced power operation, cyclical sleep, among others."
"1751040","CAREER: Fundamental Algorithms for Data-Limited Problems","CCF","Algorithmic Foundations","08/01/2018","09/11/2021","Eric Price","TX","University of Texas at Austin","Continuing Grant","A. Funda Ergun","07/31/2023","$499,575.00","","ecprice@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","1045, 7926","$0.00","From medical imaging to astronomy, scientific hypothesis testing to data analysis, computers are used in a wide variety of areas where computation is cheaper than data collection.  Such situations call for algorithms that are not only fast, but also data efficient. This project considers sub-linear algorithms for fundamental computational problems of interest in both theory and practice. It focuses on two basic questions: how many samples, or noisy observations from a signal, does it take to accurately reconstruct the signal, and how many samples from an object does it take to estimate a property of the object?<br/><br/>The PI will investigate ways to leverage knowledge of signal structure into improved signal reconstruction.  An example signal structure is the property of having a sparse Fourier transform; this has been well studied in the discrete setting, but is still poorly understood in the more realistic continuous setting. Another signal structure is that given by generative models built with deep convolutional neural networks; these have produced remarkably accurate models of images in recent years.  This project will use such models to estimate images more accurately from fewer measurements.  The PI will also investigate problems in distribution testing and graph sampling, with a goal of translating techniques from the distribution testing literature into the statistical hypothesis testing framework.  The PI will incorporate research into teaching, and mentor students at levels ranging from high school to graduate school.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750428","CAREER: Inferring Graph Structure via Spectral Representations of Network Processes","CCF","Special Projects - CCF, Comm & Information Foundations","04/01/2018","08/04/2020","Gonzalo Mateos Buckstein","NY","University of Rochester","Continuing Grant","Phillip Regalia","03/31/2023","$407,944.00","","gmateosb@ece.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","2878, 7797","1045, 7935","$0.00","Coping with the challenges found at the intersection of Network Science and Big Data necessitates fundamental breakthroughs in modeling, identification, and controllability of distributed network processes -- often conceptualized as signals defined on graphs. There is an evident mismatch between the scientific understanding of signals defined over regular domains (time or space) and graph-valued signals. Knowledge about time series was developed over the course of decades and boosted by real needs in areas such as communications, speech, or control. On the contrary, the prevalence of network-related signal processing problems and the access to quality network data are recent events. In this context, research in this project aims to push the frontiers of knowledge in network-analytic information processing, and thus make progress towards understanding the inherent complexities of strongly coupled systems such as the brain. Students will also be trained to tackle the problems at the intersection of Big Data and Network Science, thereby contributing to workforce development as well.<br/><br/>Under the assumption that the signals are related to the topology of the graph where they are supported, the goal of graph signal processing is to develop algorithms that fruitfully leverage this relational structure, and can make inferences about these relationships when they are only partially observed. Most graph signal processing efforts to date assume that the underlying network is known, and then analyze how the graph's algebraic and spectral characteristics impact the properties of the graph signals of interest. However, such assumption is often untenable in practice and arguably most graph construction schemes are largely informal, distinctly lacking an element of validation.  The intellectual merit of this research project is to investigate how to use information available from graph signals to learn the underlying graph topology, through innovative approaches that operate in the graph spectral domain. The idea is to consider the graph Fourier transform of the snapshot signals associated with an arbitrary graph and, among all the feasible networks, search for one that endows the resulting transforms with target spectral properties and the sought graph with appealing physical characteristics. Aligned with current trends in data-driven scientific inquiry into complex networked systems, the aim is to shift from: (i) descriptive accounts to inferential graph signal processing techniques that can explain as well as predict network behavior; and from (ii) ad hoc graph constructions to rigorous formulations rooted in well-defined models and principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755705","CRII: CIF: New Structure-Exploiting and Memory-Efficient Methods for Large-Scale Optimization and Data Analysis","CCF","Comm & Information Foundations","07/01/2018","01/29/2018","Paul Grigas","CA","University of California-Berkeley","Standard Grant","Phillip Regalia","06/30/2021","$175,000.00","","pgrigas@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7797","7935, 8228","$0.00","Large-scale optimization methods have been paramount to the successes of recent applications of machine learning and data analysis in a wide variety of domains. At the same time, certain structural properties of statistical models, such as sparsity or low-rank structure, have proven to be crucial for obtaining meaningful and accurate results in high dimensions. In addition to being highly scalable to large datasets, some optimization algorithms have the desirable property that they directly promote the aforementioned valuable structural properties of models. This project involves developing, analyzing, and implementing novel optimization algorithms that have such beneficial structure-exploiting and also memory-efficiency properties. This project directly involves the mentoring of graduate students, as well as integration of research results into an undergraduate level machine learning course and a graduate level course in optimization and statistical learning.<br/><br/>The foundation for this project is the Frank-Wolfe Method, a particular structure-exploiting first-order gradient optimization algorithm, and the related methodology of in-face directions. In-face directions automatically promote well-structured near-optimal solutions and have encouraging memory-efficiency properties. This research will investigate conditions whereby methods with in-face directions, as applied to convex relaxations of matrix completion and more general atomic norm regularization problems, are guaranteed to have a low memory footprint. Furthermore, this project will extend the reach of methods that incorporate in-face directions to new problem classes, including non-smooth objective functions, non-convex objective functions, and stochastic gradient estimates. The proposed optimization framework and in-face methodology applies very generally, and has potential for broader impact in several areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, and medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755955","CRII: AF: Practical Auction Design Using the Deferred-Acceptance Framework","CCF","Algorithmic Foundations","02/01/2018","04/06/2018","Vasilis Gkatzelis","PA","Drexel University","Standard Grant","Tracy Kimbrel","01/31/2020","$182,930.00","","gkatz@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158956342","CSE","7796","7796, 7932, 8228, 9251","$0.00","As the world grows increasingly interconnected, a more effective utilization of its scarce resources becomes possible through an unprecedented number of auctions. On a daily basis, human and software agents compete for an extremely diverse set of resources, ranging from the advertising space on web sites and the goods sold on online auction sites, to the electrical power supply that supports large cities and the landing slots of the world's busiest airports. Were it not for the auctions that regulate these resource allocation processes, massive amounts of social utility would be wasted; hence, it is imperative that these auctions are designed to the highest standard. Some of the optimization problems involved in designing such resource allocation mechanisms can be highly demanding from a computational standpoint. In reality, the design process is even more challenging, as the agents competing for these resources may be strategic and have their own interests at heart. <br/><br/>This project approaches resource allocation problems from the perspective of the designer who chooses the rules of the auction aiming to maximize her own objectives, such as efficiency, revenue, or fairness, despite the strategic behavior of the participants. The long literature on game theory and mechanism design has produced several celebrated auctions that maximize these objectives while at the same time providing attractive theoretical guarantees regarding the incentives of the participating agents, but many of these auctions are rarely used in practice. Two important drawbacks that render these auctions impractical are that: i) it may be non-trivial for the participating bidders to verify these auctions' incentive guarantees, and ii) these auctions often require that the bidders reveal their private preferences to the auctioneer and trust the implementation of the auction.<br/><br/>The long-term goal of this project is to develop a deeper understanding of the performance guarantees that an auction designer can achieve using practical auctions that avoid these drawbacks. Recent work by economists has proposed refined incentive properties that even non-experts can verify, as well as a framework for designing auctions that satisfy these incentive properties. This project aims to evaluate the performance of these auctions, and to extend the auction design framework to capture a much wider family of problem instances. As a result, it can lead to the design of novel and practical auctions that maximize the desired objectives, while contributing the algorithmic perspective of computer science to a line of research initiated by economists. <br/><br/>The proposed research combines techniques from both approximation algorithms and mechanism design. Specifically, the auction design framework that this project aims to analyze and generalize crucially depends on the design of backward greedy algorithms for deciding how the available resources should be allocated. Unlike forward greedy algorithms, the approximation guarantees achievable by backward greedy algorithms are not well understood even within the computer science literature, so this project will also contribute toward a deeper understanding of this interesting class of algorithms."
"1755825","CRII: SHF: Synthesis of Near-Tree Clock Networks with No Short Circuit Current that Can be Reconfigured into a Tree Topology","CCF","Software & Hardware Foundation","07/01/2018","01/30/2018","Rickard Ewetz","FL","The University of Central Florida Board of Trustees","Standard Grant","Almadena Chtchelkanova","06/30/2022","$174,453.00","","rickard.ewetz@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7945, 8228","$0.00","Many very large scale integration circuits deployed in the Internet of Things and in various high performance applications are required to support the use of high performance and low performance modes in order to minimize power consumption. Such circuits are synchronized by a clock signal that is distributed using a clock network. The clock network is required to reliably deliver the clock signal even while the circuit is under the influence of manufacturing and environmental variations. This research project will result in a clock network synthesis tool that is capable of reliably synchronizing components on circuits that operate in multiple modes. Moreover, a course on computer-aided design and research opportunities will be provided to students at the University of Central Florida. <br/><br/>The robustness and power consumption of a clock network is mainly dependent on the topology of the network. Existing clock networks have a topology in the form of a tree, near-tree, or non-tree. In this project, a synthesis tool that is capable of constructing clock networks with a mode reconfigurable topology will be developed. In high performance modes, the required robustness to variations is provided by reconfiguring the clock network into a near-tree topology. In low-performance modes, the power consumption is reduced by reconfiguring the clock network into a tree topology. Moreover, the proposed clock network structure has no short circuit current, regardless of whether the topology is reconfigured to be in the form of a tree or near-tree. No short circuit current is introduced in the proposed structure because there is only a single gate driving each net of interconnects. In particular, the project involves developing an entire-clock-network-at-the-same-time methodology. Techniques of reconfiguring the topology and improving the robustness of delivering both the rising and falling edge of the clock signal will be explored.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756013","CRII: SHF: Optimizing Deep Learning Training through Modeling and Scheduling Support","CCF","Software & Hardware Foundation","06/01/2018","01/23/2018","Feng Yan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Almadena Chtchelkanova","05/31/2021","$174,990.00","","fyan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","7798","7942, 8228, 9150","$0.00","Deep learning models trained on large amounts of data using lots of computing resources have recently achieved state-of-the-art training performance on important yet challenging artificial intelligence tasks. The success of deep learning has attracted significant research interest from hardware and software communities to improve training speed and efficiency. Despite the great efforts and rapid progress made, one important bridge to connect software and hardware support with deep learning domain knowledge is still missing: efficient configuration exploration and runtime scheduling. Both the quality of deep learning models and the training time are very sensitive to many adjustable parameters that are set before and during the training process, including the hyperparameter configurations (such as learning rate, momentum, number and size of hidden layers) and system configurations (such as thread parallelism, model parallelism, and data parallelism). Efficient exploration of hyperparameter configurations and judicious selection of system configurations is of great importance to find high-quality models with affordable time and cost. This is however a challenging problem due to a huge search space, expensive training runtime, sparsity of good configurations, and scarcity of time and resources.<br/><br/>The objective of this research work is to systematically study the unique properties of deep learning systems and workloads, and establish new modeling and scheduling methodologies for improving deep learning training. The PI aims to improve the efficiency of discovering high performing models through a dynamic scheduling methodology driven by a novel hyperparameter configuration classification approach. The PI aims at developing an accuracy- and efficiency-aware hybrid scheduling methodology that makes judicious scheduling decisions based on a global view of both the time dimension (accuracy potential) and spatial dimension (efficiency potential) information. This research work integrates techniques in workload characterization, performance modeling, resource management, and scheduling to dramatically speedup the training process while significantly reducing the cost in time and resources. More broadly, this project will gain foundational knowledge about the interaction between software-hardware support and deep learning domain knowledge. This knowledge can help design next generation deep learning systems and frameworks, making deep learning training handy for researchers and practitioners with limited system and machine learning domain expertise. This research will help enhance curriculum and provide research topics for both undergraduate and graduate students, especially students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750436","CAREER: Equilibrium Computation and Other Total Search Problems","CCF","Algorithmic Foundations","03/01/2018","09/08/2021","Ruta Mehta","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Tracy Kimbrel","02/28/2023","$499,906.00","","rutameht@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","1045, 7926, 7932, 9102","$0.00","Nash and market equilibria are two of the most fundamental solution concepts in computational game theory and economics, respectively. These concepts are applicable to many long-standing open questions in diverse fields such as cryptography, topology, and verification. Even though we have a fair understanding of these by now, many fundamental questions still remain unresolved in areas such as efficient approximation algorithms, beyond-worst-case analysis, and the relations between the sub-classes and the problems therein. This project aims to explore these questions by bringing together tools from equilibrium computation, sum-of-squares analysis, robust analysis, and other areas. The project is expected to provide efficient and robust algorithms for a large class of such problems, as well as to develop tools to obtain connections among problems from disparate fields and thereby bring insights into their complexity. The former will have a positive practical impact due to numerous applications in areas such as social network analysis and resource allocation. The project will involve and train graduate and undergraduate students at various levels of the project, integrate the findings with teaching, and make lecture notes and other material freely available online. The project will also reach students from underrepresented groups through mentoring workshops and the opportunities provided by initiatives at the University of Illinois at Urbana-Champaign.<br/><br/>The three main research goals of this project are: (i) understand the recent exponential time hypothesis for the class PPAD through the complexity of constant-approximate Nash equilibria, (ii) understand the relative complexity of problems in the class CLS coming from topology, verification, cryptography, etc., and (iii) develop beyond-worst-case analysis to explain the existence of simple and empirically fast algorithms for computing equilibria. These problems will involve developing novel tools for approximation and for beyond-worst-case analysis. Furthermore, new reduction techniques will need to be developed to relate the open problems from diverse fields and/or to prove hardness of approximation. The project will approach these by building on recent work on equilibrium computation and complexity, using tools from the sum-of-squares method, recent smoothed analysis techniques, and advances in proving lower bounds. Tools developed in the process will contribute to the burgeoning literature in these domains and open up avenues for further exploration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750443","CAREER: Communication, Information, and Interactive Compression","CCF","Special Projects - CCF, Algorithmic Foundations","02/01/2018","09/08/2021","Gillat Kol","NJ","Princeton University","Continuing Grant","Peter Brass","01/31/2023","$500,000.00","","gillat.kol@gmail.com","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","2878, 7796","1045, 7926, 7927","$0.00","Information theory had a profound impact on both the practical and theoretical communities, finding application in almost every branch of science and beyond. Information theory is especially relevant today with the rise of the internet and data-intensive applications. Lately, many of these applications are interactive, involving several communicating parties, and the trend for more interaction is growing.<br/><br/>Interactive information theory is a new field of study at the interface between information theory and computational complexity theory. The key ingredient that sets it apart from classical information theory is that it studies problems where parties are interacting and information flows in more than one direction. The goal of this project is to deepen our understanding of interactive information theory. Specifically, it considers the interactive compression problem, which is the interactive analogue of the data compression problem. Informally, the celebrated data compression theorems imply that every message can be compressed to its information content, measured using the Entropy function. The interactive compression problem asks whether the transcript of every communication protocol between several parties can be compressed to (roughly) its ""information content.""<br/><br/>An affirmative answer to this problem would yield a powerful method for proving nearly tight communication complexity lower bounds, and may shed light on other central problems in theoretical computer science that are closely related to it, such as the direct sum and parallel repetition problems for different models, and on the log-rank conjecture. Good compression protocols also suggest a new paradigm in protocol design, where one optimizes over the information revealed by the protocol and then applies a compression scheme to obtain a protocol with low information. The study of interactive compression can also be of interest to the privacy and information theory communities, which have considered similar notions.The PI will mentor students, give tutorials and create a new course on interactive information for Computer Science and Electrical Engineering majors.<br/>"
"1822191","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","Software & Hardware Foundation","10/01/2017","10/15/2020","Anthony Skjellum","TN","University of Tennessee Chattanooga","Continuing Grant","Almadena Chtchelkanova","05/31/2022","$659,277.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1750140","CAREER: New Directions in Graph Algorithms","CCF","Algorithmic Foundations","02/01/2018","09/11/2021","Debmalya Panigrahi","NC","Duke University","Continuing Grant","A. Funda Ergun","01/31/2023","$515,998.00","","debmalya.panigrahi@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","1045, 7926, 9251","$0.00","Networks such as the Internet, social networks, transportation maps, and communication backbones have an ubiquitous presence in modern life. Graph algorithms play a crucial role in these networks by providing a range of basic services such as navigation, traffic management, and robustness against physical failures. Moreover, graphs are useful in modeling interactions in a variety of systems that arise in physical, biological, and social sciences. This project identifies a set of common themes in the algorithmic challenges that arise in modern networks -- uncertainty of data, complex failure patterns, and gigantic scale -- and seeks generic solutions that address these core issues. The project is expected to provide new insights into classical graph optimization problems, while also creating new models, problem formulations, and research directions that embrace these broad challenges. This project will also train graduate and undergraduate researchers in theoretical computer science, with an emphasis on gender diversity and participation of underrepresented groups. <br/><br/>For over fifty years, graph algorithms have played a central role in the advancement of computer science, both in theory and practice. Modern networks have evolved in scale, structure, and functionality, inspiring new models, problems, and algorithms. This project focuses on three key research thrusts for modern graph algorithms:  (a) network design under unreliable or imprecise future predictions, by developing generic optimization techniques for uncertain and dynamic inputs; (b) the analysis of correlation effects in network failures by expanding the scope of classical metrics like minimum cuts to incorporate correlated failures of multiple network components; and (c) the design of highly efficient algorithms for large networks, focusing on the tradeoff between approximation and efficiency for fundamental graph optimization problems. The project will integrate tools from diverse areas such as combinatorial optimization, probability theory, mathematical programming, and continuous optimization to model and address these algorithmic questions, and the project is expected to shed new light on related questions in these domains as well."
"1750333","CAREER: New Algorithms for Submodular Optimization","CCF","Algorithmic Foundations","02/01/2018","09/11/2021","Alina Ene","MA","Trustees of Boston University","Continuing Grant","A. Funda Ergun","01/31/2023","$507,423.00","","aene@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","1045, 7926","$0.00","Submodular optimization provides general solutions to a wide range of applications from monitoring water distribution networks to summarizing large corpora of documents and speech recognition. Most of the existing submodular optimization algorithms are not suitable for modern datasets, since they are designed for worst-case instances and they suffer from prohibitive running times and poor empirical performance. This project aims to develop scalable algorithmic approaches with improved empirical performance for submodular optimization and to transfer theoretical insights to applications. The proposed research brings together insights from computer science, mathematics and optimization, and strengthens connections among these fields. The project will involve training the next wave of students and equipping them with technical tools to work in all these fields.<br/><br/>The project focuses on three inter-related research directions in submodular function optimization: (a) Design faster algorithms for minimizing submodular functions with a decomposable or sum structure. The approach is to build on a rich set of tools from both discrete and continuous optimization. (b) Design algorithms for constrained submodular maximization problems with improved approximation guarantees and faster running times. The focus is on settling the approximability of constrained submodular maximization problems with a non-monotone objective and on designing faster algorithms for central families of constraints. (c) Design algorithms and frameworks for allocation or labeling problems with submodular costs. The main goal is to obtain more expressive algorithmic frameworks and efficient algorithms."
"1750555","CAREER: Stable Foundations for Reliable Machine Learning","CCF","Algorithmic Foundations","02/01/2018","09/14/2021","Moritz Hardt","CA","University of California-Berkeley","Continuing Grant","A. Funda Ergun","01/31/2023","$499,434.00","","hardt@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7796","1045, 7926","$0.00","Across all sciences, researchers hope to use algorithms and machine learning to derive reliable insights from data, but often research findings turn out to be false or hard to replicate. Indeed, assessing the validity of insights suggested by data is presently a difficult and error-prone task. Even in industry, where machine learning has fueled dramatic advances, more principled ways of benchmarking and improving the performance of a machine learning system would make a major difference. What often work best in practice are poorly understood heuristics, leading to much guesswork with varying results. This inscrutable behavior of machine learning also has repercussions on society at large as more and more people struggle with the implications of algorithmic decisions in their daily lives. Fairness, interpretability, and transparency have become major talking points as algorithms increasingly aid or replace human judgment.<br/><br/>The PI aims to build guiding theory alongside scalable algorithms that make the practice of machine learning more reliable, transparent, and aligned with societal values. Focusing on algorithmic stability as a unifying technical framework, this proposal targets several foundational challenges including the design of a robust methodology to address the reliability crisis in data science, a working theory for why and when large artificial neural networks train and generalize well, and a universal framework to reason about generalization in unsupervised learning as is presently lacking. A particular emphasis is on application domains of societal impact. The PI has long been invested in topics such as privacy, fairness, accountability and transparency in machine learning not only through academic publications, but also through workshops, mentorship, teaching, and interdisciplinary engagements."
"1750716","CAREER: Faster and Smaller Sketches for Bigger Data","CCF","Algorithmic Foundations","02/01/2018","09/11/2021","Huy Nguyen","MA","Northeastern University","Continuing Grant","A. Funda Ergun","01/31/2023","$499,882.00","","hu.nguyen@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","1045, 7926","$0.00","The advent of new sensing and tracking technologies and expansive use of social networks detailing every walk of life have generated enormous new datasets. The difficulty in dealing with new datasets arises from not only the sheer volume but also the speed required for the analysis and the complex and heterogeneous nature of the data. Underlying these challenges is the need for suitable representations of the data that facilitate efficient computation and are sufficiently compact for storage and communication. This project aims to address fundamental gaps in our understanding of these representations (so-called sketches) and develops both new data representations and new algorithms for massive datasets in a holistic fashion. The project builds on techniques from a wide variety of areas including mathematical analysis, information theory, coding theory, combinatorics, and optimization, and enriches the deep connections among them. Undergraduate and graduate students will be trained and equipped with technical tools to work in these areas. The PI and the students involved in the project will also distill new findings into general audience surveys and give talks at workshops in different technical areas for broadest possible dissemination of information.<br/><br/>This project aims to study sketching algorithms by focusing on three main thrusts:(a) Study time complexity of sketches in streaming algorithms in both upper and lower bounds.(b) Develop new forms of sketches for distributed environments. The project focuses on sketching for submodular functions, a popular model for machine learning, computer vision, economics, etc. Problems in these applications are modeled as submodular maximization subject to various types of constraints. (c) Study space complexity of linear sketches in sparse recovery with respect to different recovery guarantees."
"1755619","CRII: AF: Strongly Polynomial Algorithms for Market Equilibria with Applications to Network Flows and Nash Social Welfare","CCF","Algorithmic Foundations","02/01/2018","01/26/2018","Jugal Garg","IL","University of Illinois at Urbana-Champaign","Standard Grant","A. Funda Ergun","01/31/2020","$175,000.00","","jugal@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7796, 7932, 8228","$0.00","Market equilibrium is a central solution concept in economics with applications in a variety of domains. It has found several surprising applications even in non-market settings which do not involve any exchange of money but only require the remarkable fairness and efficiency properties of equilibria.  These applications include scheduling, mechanism design, fair division, and others. Designing fast algorithms for computing equilibria in markets is vital for their many applications.<br/><br/>For a number of classical market settings with divisible goods, computing an equilibrium reduces to maximizing the geometric mean of agents' utilities, called the Nash social welfare. Nash social welfare is also considered an important fairness notion in allocating scarce resources in a non-market setting. Recently there has been a surge of interest in finding such an allocation with indivisible goods. However, the problem is NP-hard even when there are only two agents with additive valuations, and hence designing fast and near-optimal approximation algorithms is a crucial problem in this domain.<br/><br/>The PI aims to obtain fast algorithms for computing equilibria in fundamental market models and their extensions, and for the Nash welfare problem. Many of these reduce to network flow problems, which are central optimization problems that arise in numerous applications such as transportation and communication. Despite the extensive work on equilibrium computation and network flows, designing a strongly polynomial time algorithm for many of these market models has been long open. The first main thrust of this project is to make progress towards settling these open questions. New algorithmic techniques and tools, as well as structural insights will need to be developed to solve them. Recent work on the Nash welfare problem crucially uses the market model extensions and their connections with the flow problems. The second main thrust of this project is to obtain improved approximation algorithms for the Nash welfare problem.<br/><br/>The project will support and engage two PhD students, and an undergraduate student to implement the designed algorithms and test them for their practical efficiency. The new algorithmic techniques developed in this project will be incorporated into a graduate course on Games and Markets that the PI teaches. All the educational and implementation material will be made freely available online."
"1748988","CAREER: Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box","CCF","Software & Hardware Foundation","02/01/2018","09/17/2019","Jing Li","WI","University of Wisconsin-Madison","Continuing Grant","Sankar Basu","10/31/2020","$290,278.00","","janeli@seas.upenn.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","1045, 7945, 9102","$0.00","Large-scale graph analytics, the class of big data analytics that essentially explores the relationship among a vast collection of interconnected entities (e.g., ""friends"" in a social network), is becoming increasingly important due to its broad applicability, from machine learning to web search, precision medicine, and social sciences. However, the performance of graph processing systems is severely limited by the irregular data access patterns in graph computations. The existing solutions that have been developed for mainstream parallel computing are generally ineffective for massive, sparse real-world graphs due to the conventional computer architecture (i.e., von Neumann architecture) itself. In this project, new, fundamental methods will be explored in both theoretical and practical implementations to address this problem. It uniquely advances multiple fundamental cross-disciplinary areas in device, circuit, computer-aided design, and computer architecture and can be applied to address some of the most challenging ""big data"" problems ranging from fundamental research to everyday life. The research framework will be extended into an educational platform, providing a user-friendly framework for a laboratory-based curriculum and will serve the educational objectives for K-12 students, undergraduate and graduate students.<br/><br/>In this research, a new computing paradigm will be developed to fundamentally address the challenge in processing large-scale graphs and to achieve ultra-high computing efficiency, orders of magnitude higher in performance per watt than state-of-art mainstream computer. To this end, a holistic co-design and optimization of algorithm, software and hardware will be developed to leverage the great potential of emerging nonvolatile memory technology. A new computing model will be proposed and theoretically proven to be more efficient in runtime/area/energy than traditional von Neumann architecture in performing graph computation. Detailed micro-architectures and circuits will be designed and evaluated to best implement the proposed computing model for concept proof."
"1751064","CAREER: Functionality-Enhanced Devices for Extending Moore's Law","CCF","Software & Hardware Foundation","01/15/2018","05/06/2021","Pierre-Emmanuel Gaillardon","UT","University of Utah","Continuing Grant","Sankar Basu","12/31/2022","$492,292.00","","pierre-emmanuel.gaillardon@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","1045, 7945, 9251","$0.00","This CAREER project intends to develop high-energy-efficiency computing systems by making a more ""useful"" elementary device rather than only focusing on its performances. This enables addressing the critical scientific question of ""How can we keep pushing computing performance limits""?. For more than four decades, the semi-conductor ecosystem answered the demand for higher levels of performance by manufacturing devices with increasingly small dimensions. Nevertheless, there is still the largely unexplored route of increasing the basic switching primitive of the elementary transistors, i.e., enhancing their functionality rather than focusing only on reducing their size and/or improving their performances. This project is also relevant from an industrial perspective, as it proposes novel solutions to push device and systems performance without overextending investments to reach an ever-larger degree of integration. More importantly, this CAREER project (i) will involve graduate/undergraduate students tackling research on problems that are directly relevant for industry, ultimately boosting their employability, and (ii) will develop a scientific popularization YouTube channel as a mechanism to increase interest and broaden the participation of K-12 students by capitalizing on their online curiosity.<br/><br/>The proposed research aims to developing novel computing systems exploiting Three-Independent-Gate Field Effect Transistors (TIGFETs), a novel device technology capable of device-level functionalities typically not achievable by standard CMOS and leading to major benefits at gate and circuit levels. A TIGFET can, depending on its usage, achieve three totally unique modes of operations: (i) the dynamic reconfiguration of the device polarity; (ii) the dynamic control of the threshold voltage; and (iii) the dynamic control of the subthreshold slope beyond the thermal limit. In order to fully assess the potential of this technology, this CAREER project will (1) fabricate, characterize and model TIGFETs using advanced semi-conductor materials, (2) develop a complete design framework for TIGFETs, including a design kit, novel circuit primitives and dedicated design tools, and (3) evaluate TIGFET-enabled low-energy high-precision neural network computing kernels."
"1751356","CAREER: Information Theory of Dynamical Systems","CCF","Comm & Information Foundations","02/01/2018","02/18/2020","Victoria Kostina","CA","California Institute of Technology","Continuing Grant","Armand Makowski","01/31/2023","$452,662.00","","vkostina@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","1045, 7935, 9102, 9251","$0.00","Modern technology aims for a massively and diversely connected world populated by a seamless network of intelligent, dynamically distributed systems engaged in a shared interaction with the physical world and each other through unreliable sensors, actuators, and noisy communication channels. Classical information theory, while it has long served as an enabler of high speed, long-distance communication for point-to-point, delay-tolerant communication systems using coding over long blocks of symbols, lacks ready solutions to the new challenges of the era of massive, dynamic connectivity. Evolving networks are rather delay sensitive, so coding over long blocks of observed data will not be feasible. Information exchanges frequently seek to maximizing payoff, rather than simply recover the information sent, and are often event-triggered, prompting new mathematical models and tools to gain insights into jointly optimal sensing/coding/control strategies for these systems. The project will also offer undergraduate research opportunities in conjunction with Caltech's Summer Undergraduate Research Fellowships (SURF) program, alongside outreach activities to middle and high school students via Caltech's Center for Teaching, Learning and Outreach, in order to encourage future scientists and engineers.<br/><br/>The proposed research will advance the state of the art in understanding the fundamental trade-offs between communication and performance in dynamical systems. The project will draw upon the tools from both information theory and control theory to achieve its objectives, which are (1) to establish the fundamental information-theoretic trade-offs in delay-constrained causal source coding for dynamical systems under a distortion constraint; (2) to elucidate the information-theoretic trade-offs of control over noisy channels and propose new coding schemes with theoretical performance guarantees; and (3) to gain insight into jointly optimal sampling and coding strategies for tracking and control."
"1750047","CAREER: Advancing On-chip Network Architecture for GPUs","CCF","Special Projects - CCF, Software & Hardware Foundation","02/01/2018","08/02/2021","Lizhong Chen","OR","Oregon State University","Continuing Grant","Yuanyuan Yang","01/31/2023","$466,000.00","","chenliz@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","2878, 7798","1045, 7798, 7941, 9251","$0.00","Graphics Processing Units (GPUs) have been playing critical roles in numerous disciplines and sectors, as well as many emerging fields that might not otherwise be possible. Examples include autonomous driving, virtual reality, medical imaging, and parallel computing in HPC systems and data-centers. This success should be largely credited to the massively parallel computing capability of GPU architectures, which can integrate thousands of processing cores on a single chip. To continue meeting growing performance expectations and energy-efficiency, a key challenge over the next decade and beyond is how to support on-chip communications among the vast number of processing cores and provide fast and efficient data transfer to feed concurrent computations.<br/><br/>This project establishes an integrated research and education program to investigate cross-cutting approaches and techniques to advance and improve the effectiveness of on-chip networks (NoCs) in GPU systems, as well as to create academic course materials and outreach activities for the education and broad dissemination of the proposed subjects. The research component has three main thrusts: 1) increasing fundamental understanding of GPU NoCs by investigating, among many other open problems, the criticality, scalability and sensitivity of NoCs to GPU architecture; 2) designing and implementing cost-effective router-based and routerless GPU NoCs, and 3) co-optimizing NoC design with other GPU subsystems such as cache and warp scheduling. The education and outreach components include developing automated tools to increase the effectiveness of simulation-based course projects and engage students from diverse backgrounds, strengthening architecture course offerings, organizing interdisciplinary seminar courses, and leading various outreach activities on K-12 education, women and underrepresented groups."
"1749981","CAREER: Coded Caching for Wireless Content Delivery Networks: Challenges and Opportunities","CCF","Comm & Information Foundations","07/01/2018","07/01/2020","Soheil Mohajer","MN","University of Minnesota-Twin Cities","Continuing Grant","Armand Makowski","06/30/2023","$420,874.00","","soheil@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","1045, 7935, 9251","$0.00","The continuously increasing demand for broadband data has produced overwhelming network traffic and, despite steady improvements in wireless communication technology, data rates continue to fall short of the exponential growth in demand. As such, opportunistic transmission strategies based on network characteristics, demand profile, and content type, can help meet expectations. The major contributor to this traffic explosion is broadband video, characterized as being repeatedly requested by multiple users and having highly variant temporal traffic. Such properties provide an opportunity for caching the data at local storage units closer to users during the off-peak hours of the network, to reduce network traffic at peak hours. Recent developments in coded caching offer a promising solution for high data rates. This research will study challenges and benefits of employing caching in practical communication networks, and pursue a solid theoretical foundation for adopting caching as a universal resource in data delivery networks. <br/><br/>This project will target the fundamental theory and practical aspects of caching in data delivery networks by addressing several practical concerns. While caching gain degrades in real networks with time-varying channels, asynchronous and delay sensitive requests, and without a central coordinator, coding techniques will be used to improve the gain of caching. In addition, a rate-distortion theoretic framework will be developed to characterize the fundamental trade-off between cache size, delivery rate, and reconstruction quality, alongside efficient coding schemes to achieve this tradeoff. Following the success of multi-antenna communication systems, the interaction between caching gain and spatial diversity will be studied. A successful completion of this part will lead to an optimum resource (cache size, power, and rate) allocation as well as transmission scheduling in non-homogeneous Multi-Input-Multi-Output (MIMO) networks. Finally, software to simulate caching techniques for a wide range of networks and applications, and supporting both research and education, will be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751029","CAREER: System Support for Capable, Reliable Intermittently-Powered Computer Systems","CCF","CSR-Computer Systems Research, Software & Hardware Foundation","01/15/2018","08/06/2021","Brandon Lucia","PA","Carnegie-Mellon University","Continuing Grant","Yuanyuan Yang","12/31/2022","$599,151.00","","blucia@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7354, 7798","1045, 7354, 7941","$0.00","Energy-harvesting devices are an emerging class of tiny, embedded computing and sensing systems that operate by collecting small amounts of energy extracted from their environment, tapping into sources such as radio waves, solar energy, and vibration.  With system support to make batteryless devices operate reliably, these devices will be a key enabler of future applications including the Internet of things, disaster zone and conflict zone sensing and communication, wearable and implantable medical devices, and aerospace applications including chip-scale nano-satellites. Building reliable batteryless hardware and software computer systems is a challenge because, unlike in a typical system, energy is only intermittently available.  When a batteryless device's intermittent energy supply is interrupted, the system suddenly resets, erasing some of the computer's memory, retaining some parts of the computers memory, and requiring the system to collect more energy before it can continue operating.  The intermittent power interruptions that occur in a batteryless system lead to new kinds of software errors are extremely difficult for a system designer to understand and fix. <br/>The work in this project develops the tools and techniques required for non-experts to build batteryless computer systems that are highly reliable, efficient, and capable.  Making these systems reliable enables their use in societally important domains that demand dependable operation including in medical devices, defense applications, and emerging consumer space exploration systems.  This project will make batteryless systems accessible to even non-expert system builders, allowing entrepreneurs, researchers, and hobbyists to leverage these crucially important future computer sytems. Technology transfer of the ideas in the project will allow industry to incorporate batteryless computer systems into products, contributing to existing economic sectors such as the Internet of Things and Smart Cities devices.  Future industry adoption of the ideas in this proposal will contribute to the creation of new economic sectors around reliable, batteryless medical devices, disaster zone and war zone sensing and communication devices, and emerging small satellite-based devices.<br/>The project serves the national interest with a coherent, integrated research and education plan that will help to prepare the next generation of STEM innovators, technology workers, makers, and computational thinkers.  The project's products incorporate into graduate and undergraduate curricular development and research creating an immediate impact and equipping undergraduate and graduate students to innovate using batteryless computer systems.  The project's high school intern program component and high school outreach program enables early learners to understand and use batteryless computer systems and develop an early interest in STEM.  The work makes batteryless computing devices broadly accessible facilitating outreach outside of the STEM field, allowing even artists and designers to use products of this project in creative applications.  The purpose of outreach to high schoolers and creative users is to encourage involvement of under-represented minorities in computing and to build a more diverse and effective future STEM workforce.<br/><br/>"
"1750667","CAREER: Addressing Scalability Challenges in Designing Next-generation GPU-Based Heterogeneous Architectures","CCF","Special Projects - CCF, Software & Hardware Foundation","02/01/2018","08/02/2021","Adwait Jog","VA","College of William and Mary","Continuing Grant","Yuanyuan Yang","01/31/2023","$450,000.00","","ajog@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","2878, 7798","1045, 7798, 7941","$0.00","Graphics processing units (GPUs) are becoming default accelerators in many domains such as high-performance computing (HPC), deep learning, and virtual/augmented reality. Their close integration with high-performance multi-core CPU architectures is also allowing very efficient heterogeneous computing. Going forward, it is imperative that such GPU-based systems scale both in terms of performance and energy efficiency to meet the exascale (and beyond) computing demands of the future. However, sustained scaling of these systems is challenging primarily because a) fabricating a single large die provides very low yield, making it prohibitively expensive, b) memory hierarchy remains a critical performance and energy efficiency bottleneck, and c) programmability and application scalability is hindered by inefficiencies in the shared virtual memory and multi-application support.<br/><br/>This project seeks to address these scalability challenges by rethinking the design of future large-scale GPU-based systems. In particular, this research project revolves around three major components: a) design space exploration of cores (including their organization) and the entire memory hierarchy, b) development of data movement optimization techniques by identifying and then exploiting cache locality via novel synergistic caching and scheduling techniques, and c) improving resource utilization of large-scale system resources by enhancing shared virtual memory and multi-application execution support. All three research components will be evaluated on a newly-developed comprehensive evaluation infrastructure. The findings of this research will be incorporated into new and existing undergraduate and graduate courses. It is expected that the insights resulting from this research would have a long-term positive impact on GPU-based computing, thereby making our daily lives more productive."
"1750472","CAREER: A Comprehensive and Lightweight Framework for Transcriptome Analysis","CCF","Computational Biology","02/01/2018","03/10/2020","Robert Patro","NY","SUNY at Stony Brook","Continuing Grant","Mitra Basu","06/30/2020","$444,170.00","","rob.patro@gmail.com","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7931","1045, 7931","$0.00","Over the past decade, sequencing technologies have been developed that enable the profiling of gene expression across a wide variety of organisms and tissue types.  These technologies allow the investigation, on a transcriptome-wide scale, of how gene expression changes in different conditions, under various stimuli, and in different disease states. These technologies are transformative in progressing basic science (e.g., understanding cell biology) and applied science (e.g., approaches to drug development).  However, the deluge of data produced by these technologies brings with it a host of computational challenges, such as discovering if samples contain genes previously not annotated, accurately determining the sequence of these genes, and quantifying the abundance of all the genes expressed in a sample. Much effort has been dedicated to developing reliable computational methods for processing this data. Yet, even the best existing solutions are sometimes unsatisfactory in terms of their accuracy, and are becoming computationally burdensome given the rapid rate at which new data is being produced. <br/><br/>The goal of this project is to develop a new generation of accurate and lightweight methods for analyzing gene and transcript expression using sequencing data. These tools will apply new data structures and algorithmic ideas to the problems of mapping sequencing reads, discovering and assembling new transcripts, and accurately and robustly quantifying gene expression. Further, these methods will work in the context of both established technologies and the newly-emerging protocols that allow measuring cell-specific gene expression across thousands of individual cells.  The methods and software produced as a result of this project will help enable new discoveries by being more sensitive and accurate than existing approaches, will reduce costs by decreasing computational demands, and will speed up analyses by producing results more quickly than existing approaches. The outreach goals of this project include the creation of educational media including videos and a podcast series that will help convey key insights and benefits of new computational genomics methods to both practicing biologists as well as to the scientifically-interested public at large.<br/><br/>Lightweight quantification methods streamline many common transcriptomic analyses, like differential expression testing in well-annotated organisms and common tissue types. Yet, substantial challenges remain that prevent the use of lightweight methods in many analysis tasks, e.g., when novel transcripts should be considered, or when events such as intron retention may play an important role. This work will advance the accuracy and fundamental capabilities of lightweight transcriptome analysis methods. Specifically, a new graph-based data structure will be developed for indexing a collection of reference sequences. A lightweight alignment tool will be built around this index that will incorporate a statistical model that allows sharing of splicing contexts across large collections of samples to guide and inform difficult alignment problems. A multi-sample methodology for joint transcript discovery and quantification will also be developed, based on new approaches to modeling the joint likelihood of transcript sequences and their abundances. Efficient likelihood factorizations will allow this approach to remain computationally convenient. Finally, a suite of tools for processing and quantifying high-throughput, single-cell RNA-seq data will be developed.  These tools will adopt a novel approach for solving the cell barcoding, UMI deduplication, and gene expression estimation problems jointly, and in a unified statistical framework.  The underlying model will share statistical information between cells to improve clustering and quantification, and to analyze expression at the resolution supported by the data, i.e., as groups of distinguishable isoforms. All of these tools will be released as high-quality, open-source software."
"1755921","CRII: AF: Linear-Algebraic Pseudorandomness","CCF","Algorithmic Foundations","02/01/2018","01/11/2018","Michael Forbes","IL","University of Illinois at Urbana-Champaign","Standard Grant","A. Funda Ergun","01/31/2021","$175,000.00","","miforbes@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7796, 7927, 7932, 8228","$0.00","Error-resilient communication lies at the backbone of modern computer networks and storage systems.  Such schemes have the property of being pseudorandom in that they are explicitly constructed without randomness (which can be important, such as for efficient decoding), and yet (almost) share the error-resilience afforded by truly-random communication protocols.The construction and analysis of such schemes often uses linear algebra, and recently it has been shown that one can improve such schemes (in particular, list-decodable codes) by constructing certain pseudorandom linear-algebraic objects.  Such linear-algebraic pseudorandom notions have turned out to be interesting independent of their coding-theoretic applications, as they mirror many well-studied notions in statistical (min-entropy) pseudorandomness, have applications to improving the time- and randomness-efficiency of linear-algebraic algorithms, and have connections to the polynomial method in mathematics.This research will study the foundations of linear-algebraic pseudorandomness by attacking its main open problems, strengthening known connections, and discovering new connections.  This project also has a broader impact through course design, organization of workshops, and training of undergraduate and graduate students.<br/><br/>This project has two main focuses.The first is on explicit constructions of linear-algebraic pseudorandom objects. In particular, this project studies rank extractors (a small collection of dimension-reducing linear maps that will preserve the dimension of a linear space on average), subspace-evasive sets (a large set which has small intersections with any small-dimensional linear space), and dimension expanders (a small collection of linear maps which increase the dimension of any small-dimensional linear space).  While rank extractors and dimension expanders now have good constructions known, they lack optimality in several parameter regimes, and this project seeks to close these gaps, for example by tightening current analyses.In contrast, all known explicit constructions of subspace-evasive sets are far from optimal, and this project seeks new avenues of construction.Further, the project will explore the many relationships of these objects, such as with recent work on two-source (min-entropy) extractors, and known applications of expander graphs.<br/><br/>The second focus on this project is on the polynomial method, a method in mathematics which has yielded dramatic solutions to problems in combinatorics and number theory.It turns out that the linear-algebraic tools used in the analysis of the above pseudorandom objects has a natural phrasing in the language of the polynomial method.  This project will sharpen these linear-algebraic tools to obtain new results via the polynomial method."
"1755876","CRII: SHF: Enabling Neuroevolution in Hardware","CCF","Software & Hardware Foundation","01/15/2018","01/11/2018","Tushar Krishna","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","12/31/2019","$175,000.00","","tushar@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7945, 8228","$0.00","Over the past few years, machine learning algorithms, especially neural networks (NN) have seen a surge of popularity owing to their potential in solving a wide variety of complex problems across image classification and speech recognition. Unfortunately, in order to be effective, NNs need to have the appropriate topology (connections between neurons) for the task at hand and have the right weights on the connections. This is known as supervised learning and requires training the NN by running it through terabytes to petabytes of data. This form of machine learning is infeasible for the emerging domain of autonomous systems (robots/drones/cars) which will often operate in environments where the right topology for the task may be unknown or keep changing, and robust training data is not available. Autonomous systems need the ability to mirror human-like learning, where we are continuously learning, and often from experiences rather than being explicitly trained. This is known as reinforcement learning (RL). The goal of this project will be on enabling RL in energy-constrained autonomous devices. If successful, this research will enable mass proliferation of automated robots or drones to assist human society. The learnings will also be used to develop new courses on cross-layer support for machine learning. <br/><br/>The focus of the research will be on neuroevolution (NE), a class of RL algorithms that evolve NN topologies and weights using evolutionary algorithms.  The idea is to run multiple ""parent"" NNs in parallel, have the environment provide a reward (score) to the actions of all NNs, and create a population of new ""child"" NNs that preserve those nodes and connections from the parents that lead to actions producing the maximum reward. Running NE algorithms over multiple iterations has been shown to evolve complex behaviors in NNs. Unfortunately, NEs are computationally very expensive and have required large scale compute clusters running for hours before converging. A characterization of the computation and memory behavior of NE algorithms will be performed, and opportunities to massively parallelize these algorithms across genes (i.e., nodes and connections in the NN) will be explored. The evolutionary learning steps of crossover and mutation will be performed over specialized hardware engines, and a low-power architectural platform running NE algorithms at the edge will be demonstrated. Furthermore, the proposed research will serve as the foundation for further research in fast and energy-efficient RL algorithms to help realize general-purpose artificial intelligence."
"1750656","CAREER: Algorithm-Centric High Performance Graph Processing","CCF","Software & Hardware Foundation","02/01/2018","08/16/2021","Xuehai Qian","CA","University of Southern California","Continuing Grant","Almadena Chtchelkanova","01/31/2023","$450,000.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","1045, 7941","$0.00","With the advent of big data, large amounts of data are collected from numerous sources, such as social media, sensor feeds, and scientific experiments. Graph analytics has emerged as an important way to understand the relationships between heterogeneous types of data, allowing data analysts to draw valuable insights from patterns in the data for a wide range of applications, including machine learning tasks, natural language processing, anomaly detection, clustering, recommendation, social influence analysis, bioinformatics. Due to the broad applications, the research community tackled graph processing from multiple angles, including distributed, disk-based systems and in-memory graph processing. There are four key problems of today's graph processing research: 1) the gap between programming model and algorithm; 2) the lack of diversity in applications studied; 3) insufficient research on dynamic graphs and graph database; and 4) architectural supports focus only on classical problems. This proposal attempts to advance the graph processing systems by solving these major challenges.<br/><br/>This research proposes a novel approach ALCHEM, algorithm-centric high performance graph processing, which involves the collaborative designs of algorithms, programming model, systems, and architecture. This interdisciplinary research program takes the opportunity to explore or enhance the interactions between different layers, with the emphasis on algorithm efficiency. It contains four research thrusts: (1) Using graph abstraction as a bridge between programming model and algorithm to speed up the convergence; (2) Developing efficient execution model with specialization; (3) Building a graph database as a unified engine for relational and dynamic graph data; (4) Enhancing architecture with novel features to support new graph algorithms (e.g., random walk). The research will trigger close interactions between researchers in theory, system, and architecture. The project will engage women, minorities and undergraduates. Uniquely, it will not only train the students' system building skills, but also strengthen their algorithm understanding. The research outcomes will benefit the society by improving everyday life with better and faster recommendations, enhanced security, and better social relationships."
"1755922","CRII: SHF: Supporting Domain-Specific Inquiry with Rule-Based Modeling","CCF","Software & Hardware Foundation","03/01/2018","06/08/2021","Christopher Martens","NC","North Carolina State University","Standard Grant","Sol Greenspan","02/28/2022","$177,142.00","","crmarten@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","7798","7943, 7944, 8228, 9251","$0.00","Writing computer simulations of processes such as ecologies, economic models, social dynamics, government procedures, and biological regulation, can lead to greater understanding of these systems for novices and experts alive. Rule-based modeling allows novice programmers to specify and run these simulations without training in probability theory or traditional programming languages. The proposed work extends a rule-based modeling language to support self-directed learning of the language and domain-specific inquiry about the models. In the long term, this work will enable cross-communication between experts in different domains and will broaden participation in systems thinking skills applicable across STEM disciplines.<br/><br/>The underlying rule-based language, Ceptre, is based on a fragment of linear logic, enabling compositional reasoning about programs. The proposed work entails developing a structure editor for Ceptre designed to support the key principles of discoverability, supporting new users to learn the language in a self-directed manner, and correctness-by-construction, or the prevention of syntax errors through an editing interface that maps between only meaningful edit states. This part of the project will be rigorously validated with formal proofs of soundness and completeness of the editing interface with respect to the underlying language. The domain-specific inquiry component of the project will be accomplished by developing a language and algorithms to support queries and filters on simulation states, ""Why"" questions about program traces, statistical analysis on sets of traces, and other means of understanding the emergent behavior of simulations. These tools will be evaluated through user studies on populations of domain experts and programming novices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755890","CRII: SHF: Leveraging Synthesis for Dynamic Design Space Analysis","CCF","Software & Hardware Foundation","02/01/2018","01/29/2018","Hamid Bagheri","NE","University of Nebraska-Lincoln","Standard Grant","Sol Greenspan","01/31/2021","$174,973.00","","bagheri@unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","7798","7944, 8228, 9150","$0.00","Designing large, complex systems that demand certain functional and quality of service (QoS) objectives remains a significant engineering problem. Design space exploration and tradeoff analysis can play a pivotal role by revealing designs that human designers might miss, illuminating sensible and nonsensical tradeoffs, and helping decision-makers to balance tradeoffs that design decisions impose on diverse stakeholders. Despite its importance, systematic tradeoff design space analysis is perhaps one of the most elusive activities in the design of complex systems, as the cost, effort, and discipline needed to conduct tradespace analysis can be extremely high.  This research devises a suite of fully automated techniques to foster systematic analysis of design tradeoff spaces of software systems. By making tradespace analysis of complex software designs less expensive to conduct and more scalable, this project enables the vibrant software industry to improve the quality of its products.<br/><br/>The research project contains both analytical and empirical components. The analytical approaches involve developing models and algorithms that can discover design alternatives via automatically synthesizing spaces of design solutions from designers' high-level inputs, comparatively analyze such design spaces via automatically subjecting each design alternative to the execution of diverse evaluation operations, and facilitate support for design space evolution through leveraging a variety of optimization mechanisms. The empirical evaluation component contains conducting experiments in multiple steps: controlled experiments by means of test beds and experiments in the context of real-world systems. The project advances the state-of-the-art by making synthesis-driven software design more scalable and cost effective. The principles resulting from this research will help expand our understanding of the application domains and settings in which synthesis-based approach to design tradeoff space analysis can be practically used."
"1755762","CRII: SHF: Building Visibility into the Cognitive Processes of Software Engineers via Biosensors","CCF","Software & Hardware Foundation","02/01/2018","01/24/2018","Christopher Parnin","NC","North Carolina State University","Standard Grant","Sol Greenspan","01/31/2020","$159,662.00","","cjparnin@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","7798","7944, 8228, 9251","$0.00","The goal of this research project is to study cognitive effort associated with tasks that computer programmers perform. This is important because the limits of cognitive load present a barrier to improving a programmer's productivity and quality. Using newly available biometric technologies such as  fMRI, EMG and eye-tracking, it is possible to conduct studies of cognitive processes in programmers. Such studies make it possible to provide scientific answers to questions such as which programming language constructs are easier to use, what kinds of training produce better programmers, or how different programming environments and situational variables are more conducive to writing the best code. The main thrust of this project is to overcome the significant challenges that currently prevent researchers in the software engineering community from widely adopting these methods. This project performs some of the groundbreaking work that is needed to provide robust measurements of cognitive processes in programmers using the biometric technologies. The resulting tailored methodologies and measurement devices will support a community of researchers working in this space. <br/><br/>The project will develop techniques to use fMRI to measure difficulty in understanding certain programming constructs. This will be done by measuring cognitive effort, as measured by neural efficiency, and level of concentration via neural deactivation. Similarly, measurements of cognitive processes activated during other code comprehension tasks will be obtained.  The objective is to obtain a validated set of measures that predict the cognitive load associated with understanding code, which is necessary to create the scientific basis for the studies. The work will lead to combining different measurements, such as eye-tracking and EMG, to establish more sensitive measurement tools. The project will address technical concerns such as maintaining clean signal data from sensors and learning how to calibrate devices to maintain validity of results, as well as practical concerns such as how to perform measurements on human subjects without interfering with the measurements.  Ultimately, such studies should lead to a better understanding what constitutes best practices in software development and training of software engineers."
"1755874","CRII: SHF: A Memory-Centric Hardware Accelerator for Large Scale Data Clustering","CCF","Special Projects - CCF, Software & Hardware Foundation","02/01/2018","06/23/2020","Mahdi Nazm Bojnordi","UT","University of Utah","Standard Grant","Almadena Chtchelkanova","01/31/2022","$190,918.00","","bojnordi@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","2878, 7798","7941, 7942, 8228, 9251","$0.00","Clustering is a crucial tool for analyzing data in virtually every scientific and engineering discipline. The U.S. National Academy of Sciences (NAS) has recently announced ""the seven giants of statistical data analysis"" in which data clustering plays a central role. This report also emphasizes that more scalable solutions are required to enable time and space clustering for the future large scale data analyses. As a result, hardware and software innovations that can significantly improve energy-efficiency and performance of the data clustering techniques are necessary to make the future large scale data analysis practical. To this goal, the proposed research demonstrates a radically different vision of solving data clustering problems in the future, where large-scale clustering problems are mapped onto a memory-centric, non-Von Neumann computation substrate and solved in situ within the data arrays, with orders of magnitude greater performance and energy efficiency than contemporary computer systems.<br/><br/>The proposed project will leverage recent developments in resistive random access memory (RRAM) and algorithmic approaches for reformulating clustering problems within massively parallel frameworks, such as bit serial rank order filters, to build an extremely low power and fast memory substrate for future clustering applications. At the software level, novel algorithms will be developed to map different types of heterogeneous data clustering problems (including numerical and non-numerical data points) from scientific and engineering domains onto the proposed memristive accelerator. Programming models, software modules, and application libraries for hardware-software co-design, dynamic resource management, and memory allocation will be developed to give the user control of the data clustering process at runtime. At the hardware level, we will investigate techniques for optimizing power and performance of the memory modules constructed from novel resistive cells and interconnection networks. Architecture and software innovations will be disseminated to the broader research community through published papers, as well as tutorials on the emerging in situ computing platforms and software-hardware interfaces in non-Von Neumann computer systems. The educational component of this project will involve integrating the cell structure, the interconnection networks, the hierarchical software interface, and the control policies into the syllabus of an advanced computer architecture course."
"1757017","REU Site: EXERCISE - Explore Emerging Computing in Science and Engineering","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","07/22/2019","Enyue Lu","MD","Salisbury University","Standard Grant","Almadena Chtchelkanova","01/31/2022","$369,995.00","","ealu@salisbury.edu","1101 Camden Avenue","Salisbury","MD","218016837","4105436066","CSE","1139","9250","$0.00","The project is a renewal of the Research Experiences for Undergraduates (REU) EXERCISE (Explore Emerging Computing in Science and Engineering) site at Salisbury University (SU) for the next three years. EXERCISE is an interdisciplinary project that explores emerging paradigms in parallel computing with data and compute intensive applications in science and engineering. The goal of the project is to offer student participants, particularly from primarily undergraduate institutions (PUIs), a valuable research experience in parallel computing. The project will promote ""parallel thinking"", an important computational thinking skill guiding current generation students into the twenty-first century computing era. The site will prioritize recruiting under-represented students and females, and attract students from local historically black college and universities (HBCUs), PUIs, and community colleges on Maryland's Eastern Shore into computational science and engineering majors and the general Science, Technology, Engineering, and Mathematics (STEM) fields. The Principal Investigator, together with faculty mentors, will supervise a 10-week REU program that gives a diverse cohort of students a taste of computational thinking in the domain of parallel computing and also an understanding of the graduate school experience. The host institution SU will collaborate with the University of Maryland Eastern Shore, an HBCU, and the University of Maryland College Park for multi-disciplinary faculty expertise and diverse summer activities including field trips, social activities, high school outreach, and graduate school application information sessions.<br/><br/>Processing complex information and large data in conventional von Neumann computer architectures is becoming increasingly difficult. Computers are undertaking a fundamental turn toward concurrency architectures such as hyperthreading, multi-core, and many-core architectures. Emerging parallel and distributed computing paradigms adapted to these concurrent architectures have begun to demonstrate the power of solving problems with large datasets and high computational complexity in a wide range of applications. However, there are fundamental difficulties in program semantics related to process interleaving: a parallel program can yield inconsistent answers, or even crash, due to unpredictable interactions between simultaneous tasks. Secondly, communication, memory access, and I/O overhead may result in run-time delays. Finally, it is difficult to ensure that programs consume resources in a manner that simultaneously achieves efficiency and meets performance goals. The REU Site will focus on four aspects of parallel computing, namely: algorithms, software, architecture and applications to address these parallel computing challenges. Students will work with faculty mentors in completing cutting-edge research projects to tackle data and compute intensive applications that emphasize the above four aspects. By the end of program, students will acquire valuable skills, gain a broader and deeper understanding of research, and develop greater confidence in their abilities. In particular, they will be exposed to emerging paradigms in parallel computing such as Map-Reduce and Graphical Processing Unit computing, and will have opportunities to explore concurrent software and multiprocessor architectures, and design efficient parallel algorithms, and to tackle data and compute intensive problems in computer and social networks, image and natural language processing, pattern recognition and machine learning."
"1757828","REU Site: Software Safety and Reliability: Research and Application","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","07/30/2019","Weichen Wong","TX","University of Texas at Dallas","Standard Grant","Almadena Chtchelkanova","01/31/2022","$369,994.00","Gopal Gupta","ewong@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1139","9250","$0.00","As software safety and reliability are vital for safe and reliable operation of many systems and in reduction of catastrophic accidents, this Research Experiences for Undergraduates (REU) site at the University of Texas at Dallas provides cutting-edge research opportunities on these important topics to students, especially those from<br/>underrepresented groups or universities with limited resources. After gaining skills in critical thinking, oral and written communication, problem solving, and research methods, students will be encouraged and well prepared for graduate education and research careers in science and technology. Their interactions with mentors and other participants will provide avenues to share ideas and results. Research connections will be established with students' home institutions to continue collaboration among multiple institutions in different geographical regions. REU website will publicize research articles, videos and slides of lectures and project presentations, and announcements for effective community outreach.<br/><br/>The subject of this research is software safety and reliability, with a focus on evaluating the strengths and weaknesses of existing methodologies and current practices for safety assurance and reliability prediction.  Issues to be studied include how the introduction of software safety and reliability requirements may affect the software lifecycle, and studying how software processes, methods and tool support should be adjusted. Further exploration will emphasize how safe software systems can be unreliable; how reliable software systems can be unsafe; and how to make software systems both safe and reliable. Our findings will be compiled to provide a comprehensive picture of how software safety and reliability can be achieved effectively and efficiently. An important aspect of this site is a close collaboration with industry. REU students will take field trips to industry sponsors, allowing them to speak directly with practitioners to better understand how software safety and reliability is applied to real-life applications. Workshops on technical writing and oral presentation will be held to improve students' proficiency in preparing and delivering technical reports. Seminars on ethics and professional responsibility will also be given to ensure REU students behave properly and respectfully."
"1750920","CAREER: Advances in Graph Learning and Inference","CCF","Comm & Information Foundations","02/01/2018","01/29/2018","Chinmay Hegde","IA","Iowa State University","Continuing Grant","Phillip Regalia","12/31/2019","$160,413.00","","chinmay.h@nyu.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","1045, 7935","$0.00","Graph-based data processing algorithms impact a variety of application domains ranging from transportation networks, artificial intelligence systems, cellphone networks, social networks, and the Web. Nevertheless, the emergent big-data era poses key conceptual challenges: several existing graph-based methods used in practice exhibit unreasonably high running time; several other methods operate in the absence of correctness guarantees. These challenges severely imperil the safety and reliability of higher-level decision-making systems of which they are a part. This research introduces an innovative new computational framework for graph learning and inference that addresses these challenges. Specific applications studied in this project include: better approaches for monitoring roadway congestion and identify traffic incidents in a timely manner; root-cause analysis of complex events in social networks; and design of better personalized learning systems, lowering educational costs and increasing quality nationwide. Activities include integrated programs to increase participation of women and under-represented minorities in the computational sciences. <br/><br/>From a technical standpoint, the investigator pursues three research themes: (i) designing scalable non-convex algorithms for learning the edges (and weights) of an unknown graph given a sequence of independent static and/or time-varying local measurements; (ii) designing new approximation algorithms for utilizing the structure of a given graph to enable scalable post-hoc decision making in complex systems; (iii) developing provable algorithms for training special families of artificial neural networks, and filling gaps between rigorous theory and practice of neural network learning. Progress in each of the above themes will be extensively evaluated using real-world data from engineering applications including social network data, highway monitoring data, and fluid-flow simulation data. Collaborations with domain experts in each of these application areas will ensure that the new theory, tools, and software emerging from this project will lead to meaningful societal benefits."
